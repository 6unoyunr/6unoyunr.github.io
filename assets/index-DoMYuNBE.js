var SE=Object.defineProperty;var mb=e=>{throw TypeError(e)};var NE=(e,t,n)=>t in e?SE(e,t,{enumerable:!0,configurable:!0,writable:!0,value:n}):e[t]=n;var yn=(e,t,n)=>NE(e,typeof t!="symbol"?t+"":t,n),Cm=(e,t,n)=>t.has(e)||mb("Cannot "+n);var Q=(e,t,n)=>(Cm(e,t,"read from private field"),n?n.call(e):t.get(e)),Qe=(e,t,n)=>t.has(e)?mb("Cannot add the same private member more than once"):t instanceof WeakSet?t.add(e):t.set(e,n),Oe=(e,t,n,r)=>(Cm(e,t,"write to private field"),r?r.call(e,n):t.set(e,n),n),Qt=(e,t,n)=>(Cm(e,t,"access private method"),n);var ou=(e,t,n,r)=>({set _(i){Oe(e,t,i,n)},get _(){return Q(e,t,r)}});function CE(e,t){for(var n=0;n<t.length;n++){const r=t[n];if(typeof r!="string"&&!Array.isArray(r)){for(const i in r)if(i!=="default"&&!(i in e)){const a=Object.getOwnPropertyDescriptor(r,i);a&&Object.defineProperty(e,i,a.get?a:{enumerable:!0,get:()=>r[i]})}}}return Object.freeze(Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}))}(function(){const t=document.createElement("link").relList;if(t&&t.supports&&t.supports("modulepreload"))return;for(const i of document.querySelectorAll('link[rel="modulepreload"]'))r(i);new MutationObserver(i=>{for(const a of i)if(a.type==="childList")for(const s of a.addedNodes)s.tagName==="LINK"&&s.rel==="modulepreload"&&r(s)}).observe(document,{childList:!0,subtree:!0});function n(i){const a={};return i.integrity&&(a.integrity=i.integrity),i.referrerPolicy&&(a.referrerPolicy=i.referrerPolicy),i.crossOrigin==="use-credentials"?a.credentials="include":i.crossOrigin==="anonymous"?a.credentials="omit":a.credentials="same-origin",a}function r(i){if(i.ep)return;i.ep=!0;const a=n(i);fetch(i.href,a)}})();var kd=typeof globalThis<"u"?globalThis:typeof window<"u"?window:typeof global<"u"?global:typeof self<"u"?self:{};function x0(e){return e&&e.__esModule&&Object.prototype.hasOwnProperty.call(e,"default")?e.default:e}var j$={exports:{}},$0={},V$={exports:{}},Fe={};/**
 * @license React
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var Rc=Symbol.for("react.element"),AE=Symbol.for("react.portal"),LE=Symbol.for("react.fragment"),PE=Symbol.for("react.strict_mode"),IE=Symbol.for("react.profiler"),UE=Symbol.for("react.provider"),DE=Symbol.for("react.context"),ME=Symbol.for("react.forward_ref"),RE=Symbol.for("react.suspense"),OE=Symbol.for("react.memo"),zE=Symbol.for("react.lazy"),hb=Symbol.iterator;function BE(e){return e===null||typeof e!="object"?null:(e=hb&&e[hb]||e["@@iterator"],typeof e=="function"?e:null)}var H$={isMounted:function(){return!1},enqueueForceUpdate:function(){},enqueueReplaceState:function(){},enqueueSetState:function(){}},q$=Object.assign,G$={};function Ro(e,t,n){this.props=e,this.context=t,this.refs=G$,this.updater=n||H$}Ro.prototype.isReactComponent={};Ro.prototype.setState=function(e,t){if(typeof e!="object"&&typeof e!="function"&&e!=null)throw Error("setState(...): takes an object of state variables to update or a function which returns an object of state variables.");this.updater.enqueueSetState(this,e,t,"setState")};Ro.prototype.forceUpdate=function(e){this.updater.enqueueForceUpdate(this,e,"forceUpdate")};function W$(){}W$.prototype=Ro.prototype;function fg(e,t,n){this.props=e,this.context=t,this.refs=G$,this.updater=n||H$}var gg=fg.prototype=new W$;gg.constructor=fg;q$(gg,Ro.prototype);gg.isPureReactComponent=!0;var pb=Array.isArray,K$=Object.prototype.hasOwnProperty,bg={current:null},Y$={key:!0,ref:!0,__self:!0,__source:!0};function X$(e,t,n){var r,i={},a=null,s=null;if(t!=null)for(r in t.ref!==void 0&&(s=t.ref),t.key!==void 0&&(a=""+t.key),t)K$.call(t,r)&&!Y$.hasOwnProperty(r)&&(i[r]=t[r]);var o=arguments.length-2;if(o===1)i.children=n;else if(1<o){for(var l=Array(o),c=0;c<o;c++)l[c]=arguments[c+2];i.children=l}if(e&&e.defaultProps)for(r in o=e.defaultProps,o)i[r]===void 0&&(i[r]=o[r]);return{$$typeof:Rc,type:e,key:a,ref:s,props:i,_owner:bg.current}}function FE(e,t){return{$$typeof:Rc,type:e.type,key:t,ref:e.ref,props:e.props,_owner:e._owner}}function vg(e){return typeof e=="object"&&e!==null&&e.$$typeof===Rc}function jE(e){var t={"=":"=0",":":"=2"};return"$"+e.replace(/[=:]/g,function(n){return t[n]})}var fb=/\/+/g;function Am(e,t){return typeof e=="object"&&e!==null&&e.key!=null?jE(""+e.key):t.toString(36)}function Zu(e,t,n,r,i){var a=typeof e;(a==="undefined"||a==="boolean")&&(e=null);var s=!1;if(e===null)s=!0;else switch(a){case"string":case"number":s=!0;break;case"object":switch(e.$$typeof){case Rc:case AE:s=!0}}if(s)return s=e,i=i(s),e=r===""?"."+Am(s,0):r,pb(i)?(n="",e!=null&&(n=e.replace(fb,"$&/")+"/"),Zu(i,t,n,"",function(c){return c})):i!=null&&(vg(i)&&(i=FE(i,n+(!i.key||s&&s.key===i.key?"":(""+i.key).replace(fb,"$&/")+"/")+e)),t.push(i)),1;if(s=0,r=r===""?".":r+":",pb(e))for(var o=0;o<e.length;o++){a=e[o];var l=r+Am(a,o);s+=Zu(a,t,n,l,i)}else if(l=BE(e),typeof l=="function")for(e=l.call(e),o=0;!(a=e.next()).done;)a=a.value,l=r+Am(a,o++),s+=Zu(a,t,n,l,i);else if(a==="object")throw t=String(e),Error("Objects are not valid as a React child (found: "+(t==="[object Object]"?"object with keys {"+Object.keys(e).join(", ")+"}":t)+"). If you meant to render a collection of children, use an array instead.");return s}function lu(e,t,n){if(e==null)return e;var r=[],i=0;return Zu(e,r,"","",function(a){return t.call(n,a,i++)}),r}function VE(e){if(e._status===-1){var t=e._result;t=t(),t.then(function(n){(e._status===0||e._status===-1)&&(e._status=1,e._result=n)},function(n){(e._status===0||e._status===-1)&&(e._status=2,e._result=n)}),e._status===-1&&(e._status=0,e._result=t)}if(e._status===1)return e._result.default;throw e._result}var gn={current:null},Ju={transition:null},HE={ReactCurrentDispatcher:gn,ReactCurrentBatchConfig:Ju,ReactCurrentOwner:bg};function Q$(){throw Error("act(...) is not supported in production builds of React.")}Fe.Children={map:lu,forEach:function(e,t,n){lu(e,function(){t.apply(this,arguments)},n)},count:function(e){var t=0;return lu(e,function(){t++}),t},toArray:function(e){return lu(e,function(t){return t})||[]},only:function(e){if(!vg(e))throw Error("React.Children.only expected to receive a single React element child.");return e}};Fe.Component=Ro;Fe.Fragment=LE;Fe.Profiler=IE;Fe.PureComponent=fg;Fe.StrictMode=PE;Fe.Suspense=RE;Fe.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=HE;Fe.act=Q$;Fe.cloneElement=function(e,t,n){if(e==null)throw Error("React.cloneElement(...): The argument must be a React element, but you passed "+e+".");var r=q$({},e.props),i=e.key,a=e.ref,s=e._owner;if(t!=null){if(t.ref!==void 0&&(a=t.ref,s=bg.current),t.key!==void 0&&(i=""+t.key),e.type&&e.type.defaultProps)var o=e.type.defaultProps;for(l in t)K$.call(t,l)&&!Y$.hasOwnProperty(l)&&(r[l]=t[l]===void 0&&o!==void 0?o[l]:t[l])}var l=arguments.length-2;if(l===1)r.children=n;else if(1<l){o=Array(l);for(var c=0;c<l;c++)o[c]=arguments[c+2];r.children=o}return{$$typeof:Rc,type:e.type,key:i,ref:a,props:r,_owner:s}};Fe.createContext=function(e){return e={$$typeof:DE,_currentValue:e,_currentValue2:e,_threadCount:0,Provider:null,Consumer:null,_defaultValue:null,_globalName:null},e.Provider={$$typeof:UE,_context:e},e.Consumer=e};Fe.createElement=X$;Fe.createFactory=function(e){var t=X$.bind(null,e);return t.type=e,t};Fe.createRef=function(){return{current:null}};Fe.forwardRef=function(e){return{$$typeof:ME,render:e}};Fe.isValidElement=vg;Fe.lazy=function(e){return{$$typeof:zE,_payload:{_status:-1,_result:e},_init:VE}};Fe.memo=function(e,t){return{$$typeof:OE,type:e,compare:t===void 0?null:t}};Fe.startTransition=function(e){var t=Ju.transition;Ju.transition={};try{e()}finally{Ju.transition=t}};Fe.unstable_act=Q$;Fe.useCallback=function(e,t){return gn.current.useCallback(e,t)};Fe.useContext=function(e){return gn.current.useContext(e)};Fe.useDebugValue=function(){};Fe.useDeferredValue=function(e){return gn.current.useDeferredValue(e)};Fe.useEffect=function(e,t){return gn.current.useEffect(e,t)};Fe.useId=function(){return gn.current.useId()};Fe.useImperativeHandle=function(e,t,n){return gn.current.useImperativeHandle(e,t,n)};Fe.useInsertionEffect=function(e,t){return gn.current.useInsertionEffect(e,t)};Fe.useLayoutEffect=function(e,t){return gn.current.useLayoutEffect(e,t)};Fe.useMemo=function(e,t){return gn.current.useMemo(e,t)};Fe.useReducer=function(e,t,n){return gn.current.useReducer(e,t,n)};Fe.useRef=function(e){return gn.current.useRef(e)};Fe.useState=function(e){return gn.current.useState(e)};Fe.useSyncExternalStore=function(e,t,n){return gn.current.useSyncExternalStore(e,t,n)};Fe.useTransition=function(){return gn.current.useTransition()};Fe.version="18.3.1";V$.exports=Fe;var T=V$.exports;const pe=x0(T),Z$=CE({__proto__:null,default:pe},[T]);/**
 * @license React
 * react-jsx-runtime.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var qE=T,GE=Symbol.for("react.element"),WE=Symbol.for("react.fragment"),KE=Object.prototype.hasOwnProperty,YE=qE.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED.ReactCurrentOwner,XE={key:!0,ref:!0,__self:!0,__source:!0};function J$(e,t,n){var r,i={},a=null,s=null;n!==void 0&&(a=""+n),t.key!==void 0&&(a=""+t.key),t.ref!==void 0&&(s=t.ref);for(r in t)KE.call(t,r)&&!XE.hasOwnProperty(r)&&(i[r]=t[r]);if(e&&e.defaultProps)for(r in t=e.defaultProps,t)i[r]===void 0&&(i[r]=t[r]);return{$$typeof:GE,type:e,key:a,ref:s,props:i,_owner:YE.current}}$0.Fragment=WE;$0.jsx=J$;$0.jsxs=J$;j$.exports=$0;var h=j$.exports,ey={exports:{}},Yn={},ty={exports:{}},ny={};/**
 * @license React
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */(function(e){function t(B,X){var P=B.length;B.push(X);e:for(;0<P;){var se=P-1>>>1,he=B[se];if(0<i(he,X))B[se]=X,B[P]=he,P=se;else break e}}function n(B){return B.length===0?null:B[0]}function r(B){if(B.length===0)return null;var X=B[0],P=B.pop();if(P!==X){B[0]=P;e:for(var se=0,he=B.length,D=he>>>1;se<D;){var Ee=2*(se+1)-1,je=B[Ee],xe=Ee+1,be=B[xe];if(0>i(je,P))xe<he&&0>i(be,je)?(B[se]=be,B[xe]=P,se=xe):(B[se]=je,B[Ee]=P,se=Ee);else if(xe<he&&0>i(be,P))B[se]=be,B[xe]=P,se=xe;else break e}}return X}function i(B,X){var P=B.sortIndex-X.sortIndex;return P!==0?P:B.id-X.id}if(typeof performance=="object"&&typeof performance.now=="function"){var a=performance;e.unstable_now=function(){return a.now()}}else{var s=Date,o=s.now();e.unstable_now=function(){return s.now()-o}}var l=[],c=[],u=1,d=null,m=3,p=!1,x=!1,g=!1,w=typeof setTimeout=="function"?setTimeout:null,v=typeof clearTimeout=="function"?clearTimeout:null,$=typeof setImmediate<"u"?setImmediate:null;typeof navigator<"u"&&navigator.scheduling!==void 0&&navigator.scheduling.isInputPending!==void 0&&navigator.scheduling.isInputPending.bind(navigator.scheduling);function _(B){for(var X=n(c);X!==null;){if(X.callback===null)r(c);else if(X.startTime<=B)r(c),X.sortIndex=X.expirationTime,t(l,X);else break;X=n(c)}}function C(B){if(g=!1,_(B),!x)if(n(l)!==null)x=!0,J(k);else{var X=n(c);X!==null&&ae(C,X.startTime-B)}}function k(B,X){x=!1,g&&(g=!1,v(U),U=-1),p=!0;var P=m;try{for(_(X),d=n(l);d!==null&&(!(d.expirationTime>X)||B&&!G());){var se=d.callback;if(typeof se=="function"){d.callback=null,m=d.priorityLevel;var he=se(d.expirationTime<=X);X=e.unstable_now(),typeof he=="function"?d.callback=he:d===n(l)&&r(l),_(X)}else r(l);d=n(l)}if(d!==null)var D=!0;else{var Ee=n(c);Ee!==null&&ae(C,Ee.startTime-X),D=!1}return D}finally{d=null,m=P,p=!1}}var S=!1,L=null,U=-1,F=5,q=-1;function G(){return!(e.unstable_now()-q<F)}function H(){if(L!==null){var B=e.unstable_now();q=B;var X=!0;try{X=L(!0,B)}finally{X?ne():(S=!1,L=null)}}else S=!1}var ne;if(typeof $=="function")ne=function(){$(H)};else if(typeof MessageChannel<"u"){var K=new MessageChannel,te=K.port2;K.port1.onmessage=H,ne=function(){te.postMessage(null)}}else ne=function(){w(H,0)};function J(B){L=B,S||(S=!0,ne())}function ae(B,X){U=w(function(){B(e.unstable_now())},X)}e.unstable_IdlePriority=5,e.unstable_ImmediatePriority=1,e.unstable_LowPriority=4,e.unstable_NormalPriority=3,e.unstable_Profiling=null,e.unstable_UserBlockingPriority=2,e.unstable_cancelCallback=function(B){B.callback=null},e.unstable_continueExecution=function(){x||p||(x=!0,J(k))},e.unstable_forceFrameRate=function(B){0>B||125<B?console.error("forceFrameRate takes a positive int between 0 and 125, forcing frame rates higher than 125 fps is not supported"):F=0<B?Math.floor(1e3/B):5},e.unstable_getCurrentPriorityLevel=function(){return m},e.unstable_getFirstCallbackNode=function(){return n(l)},e.unstable_next=function(B){switch(m){case 1:case 2:case 3:var X=3;break;default:X=m}var P=m;m=X;try{return B()}finally{m=P}},e.unstable_pauseExecution=function(){},e.unstable_requestPaint=function(){},e.unstable_runWithPriority=function(B,X){switch(B){case 1:case 2:case 3:case 4:case 5:break;default:B=3}var P=m;m=B;try{return X()}finally{m=P}},e.unstable_scheduleCallback=function(B,X,P){var se=e.unstable_now();switch(typeof P=="object"&&P!==null?(P=P.delay,P=typeof P=="number"&&0<P?se+P:se):P=se,B){case 1:var he=-1;break;case 2:he=250;break;case 5:he=1073741823;break;case 4:he=1e4;break;default:he=5e3}return he=P+he,B={id:u++,callback:X,priorityLevel:B,startTime:P,expirationTime:he,sortIndex:-1},P>se?(B.sortIndex=P,t(c,B),n(l)===null&&B===n(c)&&(g?(v(U),U=-1):g=!0,ae(C,P-se))):(B.sortIndex=he,t(l,B),x||p||(x=!0,J(k))),B},e.unstable_shouldYield=G,e.unstable_wrapCallback=function(B){var X=m;return function(){var P=m;m=X;try{return B.apply(this,arguments)}finally{m=P}}}})(ny);ty.exports=ny;var QE=ty.exports;/**
 * @license React
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var ZE=T,Kn=QE;function le(e){for(var t="https://reactjs.org/docs/error-decoder.html?invariant="+e,n=1;n<arguments.length;n++)t+="&args[]="+encodeURIComponent(arguments[n]);return"Minified React error #"+e+"; visit "+t+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}var ry=new Set,Ql={};function vs(e,t){_o(e,t),_o(e+"Capture",t)}function _o(e,t){for(Ql[e]=t,e=0;e<t.length;e++)ry.add(t[e])}var Ci=!(typeof window>"u"||typeof window.document>"u"||typeof window.document.createElement>"u"),op=Object.prototype.hasOwnProperty,JE=/^[:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD][:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD\-.0-9\u00B7\u0300-\u036F\u203F-\u2040]*$/,gb={},bb={};function ek(e){return op.call(bb,e)?!0:op.call(gb,e)?!1:JE.test(e)?bb[e]=!0:(gb[e]=!0,!1)}function tk(e,t,n,r){if(n!==null&&n.type===0)return!1;switch(typeof t){case"function":case"symbol":return!0;case"boolean":return r?!1:n!==null?!n.acceptsBooleans:(e=e.toLowerCase().slice(0,5),e!=="data-"&&e!=="aria-");default:return!1}}function nk(e,t,n,r){if(t===null||typeof t>"u"||tk(e,t,n,r))return!0;if(r)return!1;if(n!==null)switch(n.type){case 3:return!t;case 4:return t===!1;case 5:return isNaN(t);case 6:return isNaN(t)||1>t}return!1}function bn(e,t,n,r,i,a,s){this.acceptsBooleans=t===2||t===3||t===4,this.attributeName=r,this.attributeNamespace=i,this.mustUseProperty=n,this.propertyName=e,this.type=t,this.sanitizeURL=a,this.removeEmptyString=s}var Xt={};"children dangerouslySetInnerHTML defaultValue defaultChecked innerHTML suppressContentEditableWarning suppressHydrationWarning style".split(" ").forEach(function(e){Xt[e]=new bn(e,0,!1,e,null,!1,!1)});[["acceptCharset","accept-charset"],["className","class"],["htmlFor","for"],["httpEquiv","http-equiv"]].forEach(function(e){var t=e[0];Xt[t]=new bn(t,1,!1,e[1],null,!1,!1)});["contentEditable","draggable","spellCheck","value"].forEach(function(e){Xt[e]=new bn(e,2,!1,e.toLowerCase(),null,!1,!1)});["autoReverse","externalResourcesRequired","focusable","preserveAlpha"].forEach(function(e){Xt[e]=new bn(e,2,!1,e,null,!1,!1)});"allowFullScreen async autoFocus autoPlay controls default defer disabled disablePictureInPicture disableRemotePlayback formNoValidate hidden loop noModule noValidate open playsInline readOnly required reversed scoped seamless itemScope".split(" ").forEach(function(e){Xt[e]=new bn(e,3,!1,e.toLowerCase(),null,!1,!1)});["checked","multiple","muted","selected"].forEach(function(e){Xt[e]=new bn(e,3,!0,e,null,!1,!1)});["capture","download"].forEach(function(e){Xt[e]=new bn(e,4,!1,e,null,!1,!1)});["cols","rows","size","span"].forEach(function(e){Xt[e]=new bn(e,6,!1,e,null,!1,!1)});["rowSpan","start"].forEach(function(e){Xt[e]=new bn(e,5,!1,e.toLowerCase(),null,!1,!1)});var xg=/[\-:]([a-z])/g;function $g(e){return e[1].toUpperCase()}"accent-height alignment-baseline arabic-form baseline-shift cap-height clip-path clip-rule color-interpolation color-interpolation-filters color-profile color-rendering dominant-baseline enable-background fill-opacity fill-rule flood-color flood-opacity font-family font-size font-size-adjust font-stretch font-style font-variant font-weight glyph-name glyph-orientation-horizontal glyph-orientation-vertical horiz-adv-x horiz-origin-x image-rendering letter-spacing lighting-color marker-end marker-mid marker-start overline-position overline-thickness paint-order panose-1 pointer-events rendering-intent shape-rendering stop-color stop-opacity strikethrough-position strikethrough-thickness stroke-dasharray stroke-dashoffset stroke-linecap stroke-linejoin stroke-miterlimit stroke-opacity stroke-width text-anchor text-decoration text-rendering underline-position underline-thickness unicode-bidi unicode-range units-per-em v-alphabetic v-hanging v-ideographic v-mathematical vector-effect vert-adv-y vert-origin-x vert-origin-y word-spacing writing-mode xmlns:xlink x-height".split(" ").forEach(function(e){var t=e.replace(xg,$g);Xt[t]=new bn(t,1,!1,e,null,!1,!1)});"xlink:actuate xlink:arcrole xlink:role xlink:show xlink:title xlink:type".split(" ").forEach(function(e){var t=e.replace(xg,$g);Xt[t]=new bn(t,1,!1,e,"http://www.w3.org/1999/xlink",!1,!1)});["xml:base","xml:lang","xml:space"].forEach(function(e){var t=e.replace(xg,$g);Xt[t]=new bn(t,1,!1,e,"http://www.w3.org/XML/1998/namespace",!1,!1)});["tabIndex","crossOrigin"].forEach(function(e){Xt[e]=new bn(e,1,!1,e.toLowerCase(),null,!1,!1)});Xt.xlinkHref=new bn("xlinkHref",1,!1,"xlink:href","http://www.w3.org/1999/xlink",!0,!1);["src","href","action","formAction"].forEach(function(e){Xt[e]=new bn(e,1,!1,e.toLowerCase(),null,!0,!0)});function yg(e,t,n,r){var i=Xt.hasOwnProperty(t)?Xt[t]:null;(i!==null?i.type!==0:r||!(2<t.length)||t[0]!=="o"&&t[0]!=="O"||t[1]!=="n"&&t[1]!=="N")&&(nk(t,n,i,r)&&(n=null),r||i===null?ek(t)&&(n===null?e.removeAttribute(t):e.setAttribute(t,""+n)):i.mustUseProperty?e[i.propertyName]=n===null?i.type===3?!1:"":n:(t=i.attributeName,r=i.attributeNamespace,n===null?e.removeAttribute(t):(i=i.type,n=i===3||i===4&&n===!0?"":""+n,r?e.setAttributeNS(r,t,n):e.setAttribute(t,n))))}var Bi=ZE.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED,cu=Symbol.for("react.element"),Ds=Symbol.for("react.portal"),Ms=Symbol.for("react.fragment"),_g=Symbol.for("react.strict_mode"),lp=Symbol.for("react.profiler"),iy=Symbol.for("react.provider"),ay=Symbol.for("react.context"),wg=Symbol.for("react.forward_ref"),cp=Symbol.for("react.suspense"),up=Symbol.for("react.suspense_list"),Tg=Symbol.for("react.memo"),ea=Symbol.for("react.lazy"),sy=Symbol.for("react.offscreen"),vb=Symbol.iterator;function rl(e){return e===null||typeof e!="object"?null:(e=vb&&e[vb]||e["@@iterator"],typeof e=="function"?e:null)}var $t=Object.assign,Lm;function vl(e){if(Lm===void 0)try{throw Error()}catch(n){var t=n.stack.trim().match(/\n( *(at )?)/);Lm=t&&t[1]||""}return`
`+Lm+e}var Pm=!1;function Im(e,t){if(!e||Pm)return"";Pm=!0;var n=Error.prepareStackTrace;Error.prepareStackTrace=void 0;try{if(t)if(t=function(){throw Error()},Object.defineProperty(t.prototype,"props",{set:function(){throw Error()}}),typeof Reflect=="object"&&Reflect.construct){try{Reflect.construct(t,[])}catch(c){var r=c}Reflect.construct(e,[],t)}else{try{t.call()}catch(c){r=c}e.call(t.prototype)}else{try{throw Error()}catch(c){r=c}e()}}catch(c){if(c&&r&&typeof c.stack=="string"){for(var i=c.stack.split(`
`),a=r.stack.split(`
`),s=i.length-1,o=a.length-1;1<=s&&0<=o&&i[s]!==a[o];)o--;for(;1<=s&&0<=o;s--,o--)if(i[s]!==a[o]){if(s!==1||o!==1)do if(s--,o--,0>o||i[s]!==a[o]){var l=`
`+i[s].replace(" at new "," at ");return e.displayName&&l.includes("<anonymous>")&&(l=l.replace("<anonymous>",e.displayName)),l}while(1<=s&&0<=o);break}}}finally{Pm=!1,Error.prepareStackTrace=n}return(e=e?e.displayName||e.name:"")?vl(e):""}function rk(e){switch(e.tag){case 5:return vl(e.type);case 16:return vl("Lazy");case 13:return vl("Suspense");case 19:return vl("SuspenseList");case 0:case 2:case 15:return e=Im(e.type,!1),e;case 11:return e=Im(e.type.render,!1),e;case 1:return e=Im(e.type,!0),e;default:return""}}function dp(e){if(e==null)return null;if(typeof e=="function")return e.displayName||e.name||null;if(typeof e=="string")return e;switch(e){case Ms:return"Fragment";case Ds:return"Portal";case lp:return"Profiler";case _g:return"StrictMode";case cp:return"Suspense";case up:return"SuspenseList"}if(typeof e=="object")switch(e.$$typeof){case ay:return(e.displayName||"Context")+".Consumer";case iy:return(e._context.displayName||"Context")+".Provider";case wg:var t=e.render;return e=e.displayName,e||(e=t.displayName||t.name||"",e=e!==""?"ForwardRef("+e+")":"ForwardRef"),e;case Tg:return t=e.displayName||null,t!==null?t:dp(e.type)||"Memo";case ea:t=e._payload,e=e._init;try{return dp(e(t))}catch{}}return null}function ik(e){var t=e.type;switch(e.tag){case 24:return"Cache";case 9:return(t.displayName||"Context")+".Consumer";case 10:return(t._context.displayName||"Context")+".Provider";case 18:return"DehydratedFragment";case 11:return e=t.render,e=e.displayName||e.name||"",t.displayName||(e!==""?"ForwardRef("+e+")":"ForwardRef");case 7:return"Fragment";case 5:return t;case 4:return"Portal";case 3:return"Root";case 6:return"Text";case 16:return dp(t);case 8:return t===_g?"StrictMode":"Mode";case 22:return"Offscreen";case 12:return"Profiler";case 21:return"Scope";case 13:return"Suspense";case 19:return"SuspenseList";case 25:return"TracingMarker";case 1:case 0:case 17:case 2:case 14:case 15:if(typeof t=="function")return t.displayName||t.name||null;if(typeof t=="string")return t}return null}function wa(e){switch(typeof e){case"boolean":case"number":case"string":case"undefined":return e;case"object":return e;default:return""}}function oy(e){var t=e.type;return(e=e.nodeName)&&e.toLowerCase()==="input"&&(t==="checkbox"||t==="radio")}function ak(e){var t=oy(e)?"checked":"value",n=Object.getOwnPropertyDescriptor(e.constructor.prototype,t),r=""+e[t];if(!e.hasOwnProperty(t)&&typeof n<"u"&&typeof n.get=="function"&&typeof n.set=="function"){var i=n.get,a=n.set;return Object.defineProperty(e,t,{configurable:!0,get:function(){return i.call(this)},set:function(s){r=""+s,a.call(this,s)}}),Object.defineProperty(e,t,{enumerable:n.enumerable}),{getValue:function(){return r},setValue:function(s){r=""+s},stopTracking:function(){e._valueTracker=null,delete e[t]}}}}function uu(e){e._valueTracker||(e._valueTracker=ak(e))}function ly(e){if(!e)return!1;var t=e._valueTracker;if(!t)return!0;var n=t.getValue(),r="";return e&&(r=oy(e)?e.checked?"true":"false":e.value),e=r,e!==n?(t.setValue(e),!0):!1}function Sd(e){if(e=e||(typeof document<"u"?document:void 0),typeof e>"u")return null;try{return e.activeElement||e.body}catch{return e.body}}function mp(e,t){var n=t.checked;return $t({},t,{defaultChecked:void 0,defaultValue:void 0,value:void 0,checked:n??e._wrapperState.initialChecked})}function xb(e,t){var n=t.defaultValue==null?"":t.defaultValue,r=t.checked!=null?t.checked:t.defaultChecked;n=wa(t.value!=null?t.value:n),e._wrapperState={initialChecked:r,initialValue:n,controlled:t.type==="checkbox"||t.type==="radio"?t.checked!=null:t.value!=null}}function cy(e,t){t=t.checked,t!=null&&yg(e,"checked",t,!1)}function hp(e,t){cy(e,t);var n=wa(t.value),r=t.type;if(n!=null)r==="number"?(n===0&&e.value===""||e.value!=n)&&(e.value=""+n):e.value!==""+n&&(e.value=""+n);else if(r==="submit"||r==="reset"){e.removeAttribute("value");return}t.hasOwnProperty("value")?pp(e,t.type,n):t.hasOwnProperty("defaultValue")&&pp(e,t.type,wa(t.defaultValue)),t.checked==null&&t.defaultChecked!=null&&(e.defaultChecked=!!t.defaultChecked)}function $b(e,t,n){if(t.hasOwnProperty("value")||t.hasOwnProperty("defaultValue")){var r=t.type;if(!(r!=="submit"&&r!=="reset"||t.value!==void 0&&t.value!==null))return;t=""+e._wrapperState.initialValue,n||t===e.value||(e.value=t),e.defaultValue=t}n=e.name,n!==""&&(e.name=""),e.defaultChecked=!!e._wrapperState.initialChecked,n!==""&&(e.name=n)}function pp(e,t,n){(t!=="number"||Sd(e.ownerDocument)!==e)&&(n==null?e.defaultValue=""+e._wrapperState.initialValue:e.defaultValue!==""+n&&(e.defaultValue=""+n))}var xl=Array.isArray;function Js(e,t,n,r){if(e=e.options,t){t={};for(var i=0;i<n.length;i++)t["$"+n[i]]=!0;for(n=0;n<e.length;n++)i=t.hasOwnProperty("$"+e[n].value),e[n].selected!==i&&(e[n].selected=i),i&&r&&(e[n].defaultSelected=!0)}else{for(n=""+wa(n),t=null,i=0;i<e.length;i++){if(e[i].value===n){e[i].selected=!0,r&&(e[i].defaultSelected=!0);return}t!==null||e[i].disabled||(t=e[i])}t!==null&&(t.selected=!0)}}function fp(e,t){if(t.dangerouslySetInnerHTML!=null)throw Error(le(91));return $t({},t,{value:void 0,defaultValue:void 0,children:""+e._wrapperState.initialValue})}function yb(e,t){var n=t.value;if(n==null){if(n=t.children,t=t.defaultValue,n!=null){if(t!=null)throw Error(le(92));if(xl(n)){if(1<n.length)throw Error(le(93));n=n[0]}t=n}t==null&&(t=""),n=t}e._wrapperState={initialValue:wa(n)}}function uy(e,t){var n=wa(t.value),r=wa(t.defaultValue);n!=null&&(n=""+n,n!==e.value&&(e.value=n),t.defaultValue==null&&e.defaultValue!==n&&(e.defaultValue=n)),r!=null&&(e.defaultValue=""+r)}function _b(e){var t=e.textContent;t===e._wrapperState.initialValue&&t!==""&&t!==null&&(e.value=t)}function dy(e){switch(e){case"svg":return"http://www.w3.org/2000/svg";case"math":return"http://www.w3.org/1998/Math/MathML";default:return"http://www.w3.org/1999/xhtml"}}function gp(e,t){return e==null||e==="http://www.w3.org/1999/xhtml"?dy(t):e==="http://www.w3.org/2000/svg"&&t==="foreignObject"?"http://www.w3.org/1999/xhtml":e}var du,my=function(e){return typeof MSApp<"u"&&MSApp.execUnsafeLocalFunction?function(t,n,r,i){MSApp.execUnsafeLocalFunction(function(){return e(t,n,r,i)})}:e}(function(e,t){if(e.namespaceURI!=="http://www.w3.org/2000/svg"||"innerHTML"in e)e.innerHTML=t;else{for(du=du||document.createElement("div"),du.innerHTML="<svg>"+t.valueOf().toString()+"</svg>",t=du.firstChild;e.firstChild;)e.removeChild(e.firstChild);for(;t.firstChild;)e.appendChild(t.firstChild)}});function Zl(e,t){if(t){var n=e.firstChild;if(n&&n===e.lastChild&&n.nodeType===3){n.nodeValue=t;return}}e.textContent=t}var Sl={animationIterationCount:!0,aspectRatio:!0,borderImageOutset:!0,borderImageSlice:!0,borderImageWidth:!0,boxFlex:!0,boxFlexGroup:!0,boxOrdinalGroup:!0,columnCount:!0,columns:!0,flex:!0,flexGrow:!0,flexPositive:!0,flexShrink:!0,flexNegative:!0,flexOrder:!0,gridArea:!0,gridRow:!0,gridRowEnd:!0,gridRowSpan:!0,gridRowStart:!0,gridColumn:!0,gridColumnEnd:!0,gridColumnSpan:!0,gridColumnStart:!0,fontWeight:!0,lineClamp:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,tabSize:!0,widows:!0,zIndex:!0,zoom:!0,fillOpacity:!0,floodOpacity:!0,stopOpacity:!0,strokeDasharray:!0,strokeDashoffset:!0,strokeMiterlimit:!0,strokeOpacity:!0,strokeWidth:!0},sk=["Webkit","ms","Moz","O"];Object.keys(Sl).forEach(function(e){sk.forEach(function(t){t=t+e.charAt(0).toUpperCase()+e.substring(1),Sl[t]=Sl[e]})});function hy(e,t,n){return t==null||typeof t=="boolean"||t===""?"":n||typeof t!="number"||t===0||Sl.hasOwnProperty(e)&&Sl[e]?(""+t).trim():t+"px"}function py(e,t){e=e.style;for(var n in t)if(t.hasOwnProperty(n)){var r=n.indexOf("--")===0,i=hy(n,t[n],r);n==="float"&&(n="cssFloat"),r?e.setProperty(n,i):e[n]=i}}var ok=$t({menuitem:!0},{area:!0,base:!0,br:!0,col:!0,embed:!0,hr:!0,img:!0,input:!0,keygen:!0,link:!0,meta:!0,param:!0,source:!0,track:!0,wbr:!0});function bp(e,t){if(t){if(ok[e]&&(t.children!=null||t.dangerouslySetInnerHTML!=null))throw Error(le(137,e));if(t.dangerouslySetInnerHTML!=null){if(t.children!=null)throw Error(le(60));if(typeof t.dangerouslySetInnerHTML!="object"||!("__html"in t.dangerouslySetInnerHTML))throw Error(le(61))}if(t.style!=null&&typeof t.style!="object")throw Error(le(62))}}function vp(e,t){if(e.indexOf("-")===-1)return typeof t.is=="string";switch(e){case"annotation-xml":case"color-profile":case"font-face":case"font-face-src":case"font-face-uri":case"font-face-format":case"font-face-name":case"missing-glyph":return!1;default:return!0}}var xp=null;function Eg(e){return e=e.target||e.srcElement||window,e.correspondingUseElement&&(e=e.correspondingUseElement),e.nodeType===3?e.parentNode:e}var $p=null,eo=null,to=null;function wb(e){if(e=Bc(e)){if(typeof $p!="function")throw Error(le(280));var t=e.stateNode;t&&(t=E0(t),$p(e.stateNode,e.type,t))}}function fy(e){eo?to?to.push(e):to=[e]:eo=e}function gy(){if(eo){var e=eo,t=to;if(to=eo=null,wb(e),t)for(e=0;e<t.length;e++)wb(t[e])}}function by(e,t){return e(t)}function vy(){}var Um=!1;function xy(e,t,n){if(Um)return e(t,n);Um=!0;try{return by(e,t,n)}finally{Um=!1,(eo!==null||to!==null)&&(vy(),gy())}}function Jl(e,t){var n=e.stateNode;if(n===null)return null;var r=E0(n);if(r===null)return null;n=r[t];e:switch(t){case"onClick":case"onClickCapture":case"onDoubleClick":case"onDoubleClickCapture":case"onMouseDown":case"onMouseDownCapture":case"onMouseMove":case"onMouseMoveCapture":case"onMouseUp":case"onMouseUpCapture":case"onMouseEnter":(r=!r.disabled)||(e=e.type,r=!(e==="button"||e==="input"||e==="select"||e==="textarea")),e=!r;break e;default:e=!1}if(e)return null;if(n&&typeof n!="function")throw Error(le(231,t,typeof n));return n}var yp=!1;if(Ci)try{var il={};Object.defineProperty(il,"passive",{get:function(){yp=!0}}),window.addEventListener("test",il,il),window.removeEventListener("test",il,il)}catch{yp=!1}function lk(e,t,n,r,i,a,s,o,l){var c=Array.prototype.slice.call(arguments,3);try{t.apply(n,c)}catch(u){this.onError(u)}}var Nl=!1,Nd=null,Cd=!1,_p=null,ck={onError:function(e){Nl=!0,Nd=e}};function uk(e,t,n,r,i,a,s,o,l){Nl=!1,Nd=null,lk.apply(ck,arguments)}function dk(e,t,n,r,i,a,s,o,l){if(uk.apply(this,arguments),Nl){if(Nl){var c=Nd;Nl=!1,Nd=null}else throw Error(le(198));Cd||(Cd=!0,_p=c)}}function xs(e){var t=e,n=e;if(e.alternate)for(;t.return;)t=t.return;else{e=t;do t=e,t.flags&4098&&(n=t.return),e=t.return;while(e)}return t.tag===3?n:null}function $y(e){if(e.tag===13){var t=e.memoizedState;if(t===null&&(e=e.alternate,e!==null&&(t=e.memoizedState)),t!==null)return t.dehydrated}return null}function Tb(e){if(xs(e)!==e)throw Error(le(188))}function mk(e){var t=e.alternate;if(!t){if(t=xs(e),t===null)throw Error(le(188));return t!==e?null:e}for(var n=e,r=t;;){var i=n.return;if(i===null)break;var a=i.alternate;if(a===null){if(r=i.return,r!==null){n=r;continue}break}if(i.child===a.child){for(a=i.child;a;){if(a===n)return Tb(i),e;if(a===r)return Tb(i),t;a=a.sibling}throw Error(le(188))}if(n.return!==r.return)n=i,r=a;else{for(var s=!1,o=i.child;o;){if(o===n){s=!0,n=i,r=a;break}if(o===r){s=!0,r=i,n=a;break}o=o.sibling}if(!s){for(o=a.child;o;){if(o===n){s=!0,n=a,r=i;break}if(o===r){s=!0,r=a,n=i;break}o=o.sibling}if(!s)throw Error(le(189))}}if(n.alternate!==r)throw Error(le(190))}if(n.tag!==3)throw Error(le(188));return n.stateNode.current===n?e:t}function yy(e){return e=mk(e),e!==null?_y(e):null}function _y(e){if(e.tag===5||e.tag===6)return e;for(e=e.child;e!==null;){var t=_y(e);if(t!==null)return t;e=e.sibling}return null}var wy=Kn.unstable_scheduleCallback,Eb=Kn.unstable_cancelCallback,hk=Kn.unstable_shouldYield,pk=Kn.unstable_requestPaint,Nt=Kn.unstable_now,fk=Kn.unstable_getCurrentPriorityLevel,kg=Kn.unstable_ImmediatePriority,Ty=Kn.unstable_UserBlockingPriority,Ad=Kn.unstable_NormalPriority,gk=Kn.unstable_LowPriority,Ey=Kn.unstable_IdlePriority,y0=null,ni=null;function bk(e){if(ni&&typeof ni.onCommitFiberRoot=="function")try{ni.onCommitFiberRoot(y0,e,void 0,(e.current.flags&128)===128)}catch{}}var Nr=Math.clz32?Math.clz32:$k,vk=Math.log,xk=Math.LN2;function $k(e){return e>>>=0,e===0?32:31-(vk(e)/xk|0)|0}var mu=64,hu=4194304;function $l(e){switch(e&-e){case 1:return 1;case 2:return 2;case 4:return 4;case 8:return 8;case 16:return 16;case 32:return 32;case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:return e&4194240;case 4194304:case 8388608:case 16777216:case 33554432:case 67108864:return e&130023424;case 134217728:return 134217728;case 268435456:return 268435456;case 536870912:return 536870912;case 1073741824:return 1073741824;default:return e}}function Ld(e,t){var n=e.pendingLanes;if(n===0)return 0;var r=0,i=e.suspendedLanes,a=e.pingedLanes,s=n&268435455;if(s!==0){var o=s&~i;o!==0?r=$l(o):(a&=s,a!==0&&(r=$l(a)))}else s=n&~i,s!==0?r=$l(s):a!==0&&(r=$l(a));if(r===0)return 0;if(t!==0&&t!==r&&!(t&i)&&(i=r&-r,a=t&-t,i>=a||i===16&&(a&4194240)!==0))return t;if(r&4&&(r|=n&16),t=e.entangledLanes,t!==0)for(e=e.entanglements,t&=r;0<t;)n=31-Nr(t),i=1<<n,r|=e[n],t&=~i;return r}function yk(e,t){switch(e){case 1:case 2:case 4:return t+250;case 8:case 16:case 32:case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:return t+5e3;case 4194304:case 8388608:case 16777216:case 33554432:case 67108864:return-1;case 134217728:case 268435456:case 536870912:case 1073741824:return-1;default:return-1}}function _k(e,t){for(var n=e.suspendedLanes,r=e.pingedLanes,i=e.expirationTimes,a=e.pendingLanes;0<a;){var s=31-Nr(a),o=1<<s,l=i[s];l===-1?(!(o&n)||o&r)&&(i[s]=yk(o,t)):l<=t&&(e.expiredLanes|=o),a&=~o}}function wp(e){return e=e.pendingLanes&-1073741825,e!==0?e:e&1073741824?1073741824:0}function ky(){var e=mu;return mu<<=1,!(mu&4194240)&&(mu=64),e}function Dm(e){for(var t=[],n=0;31>n;n++)t.push(e);return t}function Oc(e,t,n){e.pendingLanes|=t,t!==536870912&&(e.suspendedLanes=0,e.pingedLanes=0),e=e.eventTimes,t=31-Nr(t),e[t]=n}function wk(e,t){var n=e.pendingLanes&~t;e.pendingLanes=t,e.suspendedLanes=0,e.pingedLanes=0,e.expiredLanes&=t,e.mutableReadLanes&=t,e.entangledLanes&=t,t=e.entanglements;var r=e.eventTimes;for(e=e.expirationTimes;0<n;){var i=31-Nr(n),a=1<<i;t[i]=0,r[i]=-1,e[i]=-1,n&=~a}}function Sg(e,t){var n=e.entangledLanes|=t;for(e=e.entanglements;n;){var r=31-Nr(n),i=1<<r;i&t|e[r]&t&&(e[r]|=t),n&=~i}}var Je=0;function Sy(e){return e&=-e,1<e?4<e?e&268435455?16:536870912:4:1}var Ny,Ng,Cy,Ay,Ly,Tp=!1,pu=[],fa=null,ga=null,ba=null,ec=new Map,tc=new Map,na=[],Tk="mousedown mouseup touchcancel touchend touchstart auxclick dblclick pointercancel pointerdown pointerup dragend dragstart drop compositionend compositionstart keydown keypress keyup input textInput copy cut paste click change contextmenu reset submit".split(" ");function kb(e,t){switch(e){case"focusin":case"focusout":fa=null;break;case"dragenter":case"dragleave":ga=null;break;case"mouseover":case"mouseout":ba=null;break;case"pointerover":case"pointerout":ec.delete(t.pointerId);break;case"gotpointercapture":case"lostpointercapture":tc.delete(t.pointerId)}}function al(e,t,n,r,i,a){return e===null||e.nativeEvent!==a?(e={blockedOn:t,domEventName:n,eventSystemFlags:r,nativeEvent:a,targetContainers:[i]},t!==null&&(t=Bc(t),t!==null&&Ng(t)),e):(e.eventSystemFlags|=r,t=e.targetContainers,i!==null&&t.indexOf(i)===-1&&t.push(i),e)}function Ek(e,t,n,r,i){switch(t){case"focusin":return fa=al(fa,e,t,n,r,i),!0;case"dragenter":return ga=al(ga,e,t,n,r,i),!0;case"mouseover":return ba=al(ba,e,t,n,r,i),!0;case"pointerover":var a=i.pointerId;return ec.set(a,al(ec.get(a)||null,e,t,n,r,i)),!0;case"gotpointercapture":return a=i.pointerId,tc.set(a,al(tc.get(a)||null,e,t,n,r,i)),!0}return!1}function Py(e){var t=Xa(e.target);if(t!==null){var n=xs(t);if(n!==null){if(t=n.tag,t===13){if(t=$y(n),t!==null){e.blockedOn=t,Ly(e.priority,function(){Cy(n)});return}}else if(t===3&&n.stateNode.current.memoizedState.isDehydrated){e.blockedOn=n.tag===3?n.stateNode.containerInfo:null;return}}}e.blockedOn=null}function ed(e){if(e.blockedOn!==null)return!1;for(var t=e.targetContainers;0<t.length;){var n=Ep(e.domEventName,e.eventSystemFlags,t[0],e.nativeEvent);if(n===null){n=e.nativeEvent;var r=new n.constructor(n.type,n);xp=r,n.target.dispatchEvent(r),xp=null}else return t=Bc(n),t!==null&&Ng(t),e.blockedOn=n,!1;t.shift()}return!0}function Sb(e,t,n){ed(e)&&n.delete(t)}function kk(){Tp=!1,fa!==null&&ed(fa)&&(fa=null),ga!==null&&ed(ga)&&(ga=null),ba!==null&&ed(ba)&&(ba=null),ec.forEach(Sb),tc.forEach(Sb)}function sl(e,t){e.blockedOn===t&&(e.blockedOn=null,Tp||(Tp=!0,Kn.unstable_scheduleCallback(Kn.unstable_NormalPriority,kk)))}function nc(e){function t(i){return sl(i,e)}if(0<pu.length){sl(pu[0],e);for(var n=1;n<pu.length;n++){var r=pu[n];r.blockedOn===e&&(r.blockedOn=null)}}for(fa!==null&&sl(fa,e),ga!==null&&sl(ga,e),ba!==null&&sl(ba,e),ec.forEach(t),tc.forEach(t),n=0;n<na.length;n++)r=na[n],r.blockedOn===e&&(r.blockedOn=null);for(;0<na.length&&(n=na[0],n.blockedOn===null);)Py(n),n.blockedOn===null&&na.shift()}var no=Bi.ReactCurrentBatchConfig,Pd=!0;function Sk(e,t,n,r){var i=Je,a=no.transition;no.transition=null;try{Je=1,Cg(e,t,n,r)}finally{Je=i,no.transition=a}}function Nk(e,t,n,r){var i=Je,a=no.transition;no.transition=null;try{Je=4,Cg(e,t,n,r)}finally{Je=i,no.transition=a}}function Cg(e,t,n,r){if(Pd){var i=Ep(e,t,n,r);if(i===null)qm(e,t,r,Id,n),kb(e,r);else if(Ek(i,e,t,n,r))r.stopPropagation();else if(kb(e,r),t&4&&-1<Tk.indexOf(e)){for(;i!==null;){var a=Bc(i);if(a!==null&&Ny(a),a=Ep(e,t,n,r),a===null&&qm(e,t,r,Id,n),a===i)break;i=a}i!==null&&r.stopPropagation()}else qm(e,t,r,null,n)}}var Id=null;function Ep(e,t,n,r){if(Id=null,e=Eg(r),e=Xa(e),e!==null)if(t=xs(e),t===null)e=null;else if(n=t.tag,n===13){if(e=$y(t),e!==null)return e;e=null}else if(n===3){if(t.stateNode.current.memoizedState.isDehydrated)return t.tag===3?t.stateNode.containerInfo:null;e=null}else t!==e&&(e=null);return Id=e,null}function Iy(e){switch(e){case"cancel":case"click":case"close":case"contextmenu":case"copy":case"cut":case"auxclick":case"dblclick":case"dragend":case"dragstart":case"drop":case"focusin":case"focusout":case"input":case"invalid":case"keydown":case"keypress":case"keyup":case"mousedown":case"mouseup":case"paste":case"pause":case"play":case"pointercancel":case"pointerdown":case"pointerup":case"ratechange":case"reset":case"resize":case"seeked":case"submit":case"touchcancel":case"touchend":case"touchstart":case"volumechange":case"change":case"selectionchange":case"textInput":case"compositionstart":case"compositionend":case"compositionupdate":case"beforeblur":case"afterblur":case"beforeinput":case"blur":case"fullscreenchange":case"focus":case"hashchange":case"popstate":case"select":case"selectstart":return 1;case"drag":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"mousemove":case"mouseout":case"mouseover":case"pointermove":case"pointerout":case"pointerover":case"scroll":case"toggle":case"touchmove":case"wheel":case"mouseenter":case"mouseleave":case"pointerenter":case"pointerleave":return 4;case"message":switch(fk()){case kg:return 1;case Ty:return 4;case Ad:case gk:return 16;case Ey:return 536870912;default:return 16}default:return 16}}var ca=null,Ag=null,td=null;function Uy(){if(td)return td;var e,t=Ag,n=t.length,r,i="value"in ca?ca.value:ca.textContent,a=i.length;for(e=0;e<n&&t[e]===i[e];e++);var s=n-e;for(r=1;r<=s&&t[n-r]===i[a-r];r++);return td=i.slice(e,1<r?1-r:void 0)}function nd(e){var t=e.keyCode;return"charCode"in e?(e=e.charCode,e===0&&t===13&&(e=13)):e=t,e===10&&(e=13),32<=e||e===13?e:0}function fu(){return!0}function Nb(){return!1}function Xn(e){function t(n,r,i,a,s){this._reactName=n,this._targetInst=i,this.type=r,this.nativeEvent=a,this.target=s,this.currentTarget=null;for(var o in e)e.hasOwnProperty(o)&&(n=e[o],this[o]=n?n(a):a[o]);return this.isDefaultPrevented=(a.defaultPrevented!=null?a.defaultPrevented:a.returnValue===!1)?fu:Nb,this.isPropagationStopped=Nb,this}return $t(t.prototype,{preventDefault:function(){this.defaultPrevented=!0;var n=this.nativeEvent;n&&(n.preventDefault?n.preventDefault():typeof n.returnValue!="unknown"&&(n.returnValue=!1),this.isDefaultPrevented=fu)},stopPropagation:function(){var n=this.nativeEvent;n&&(n.stopPropagation?n.stopPropagation():typeof n.cancelBubble!="unknown"&&(n.cancelBubble=!0),this.isPropagationStopped=fu)},persist:function(){},isPersistent:fu}),t}var Oo={eventPhase:0,bubbles:0,cancelable:0,timeStamp:function(e){return e.timeStamp||Date.now()},defaultPrevented:0,isTrusted:0},Lg=Xn(Oo),zc=$t({},Oo,{view:0,detail:0}),Ck=Xn(zc),Mm,Rm,ol,_0=$t({},zc,{screenX:0,screenY:0,clientX:0,clientY:0,pageX:0,pageY:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,getModifierState:Pg,button:0,buttons:0,relatedTarget:function(e){return e.relatedTarget===void 0?e.fromElement===e.srcElement?e.toElement:e.fromElement:e.relatedTarget},movementX:function(e){return"movementX"in e?e.movementX:(e!==ol&&(ol&&e.type==="mousemove"?(Mm=e.screenX-ol.screenX,Rm=e.screenY-ol.screenY):Rm=Mm=0,ol=e),Mm)},movementY:function(e){return"movementY"in e?e.movementY:Rm}}),Cb=Xn(_0),Ak=$t({},_0,{dataTransfer:0}),Lk=Xn(Ak),Pk=$t({},zc,{relatedTarget:0}),Om=Xn(Pk),Ik=$t({},Oo,{animationName:0,elapsedTime:0,pseudoElement:0}),Uk=Xn(Ik),Dk=$t({},Oo,{clipboardData:function(e){return"clipboardData"in e?e.clipboardData:window.clipboardData}}),Mk=Xn(Dk),Rk=$t({},Oo,{data:0}),Ab=Xn(Rk),Ok={Esc:"Escape",Spacebar:" ",Left:"ArrowLeft",Up:"ArrowUp",Right:"ArrowRight",Down:"ArrowDown",Del:"Delete",Win:"OS",Menu:"ContextMenu",Apps:"ContextMenu",Scroll:"ScrollLock",MozPrintableKey:"Unidentified"},zk={8:"Backspace",9:"Tab",12:"Clear",13:"Enter",16:"Shift",17:"Control",18:"Alt",19:"Pause",20:"CapsLock",27:"Escape",32:" ",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"ArrowLeft",38:"ArrowUp",39:"ArrowRight",40:"ArrowDown",45:"Insert",46:"Delete",112:"F1",113:"F2",114:"F3",115:"F4",116:"F5",117:"F6",118:"F7",119:"F8",120:"F9",121:"F10",122:"F11",123:"F12",144:"NumLock",145:"ScrollLock",224:"Meta"},Bk={Alt:"altKey",Control:"ctrlKey",Meta:"metaKey",Shift:"shiftKey"};function Fk(e){var t=this.nativeEvent;return t.getModifierState?t.getModifierState(e):(e=Bk[e])?!!t[e]:!1}function Pg(){return Fk}var jk=$t({},zc,{key:function(e){if(e.key){var t=Ok[e.key]||e.key;if(t!=="Unidentified")return t}return e.type==="keypress"?(e=nd(e),e===13?"Enter":String.fromCharCode(e)):e.type==="keydown"||e.type==="keyup"?zk[e.keyCode]||"Unidentified":""},code:0,location:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,repeat:0,locale:0,getModifierState:Pg,charCode:function(e){return e.type==="keypress"?nd(e):0},keyCode:function(e){return e.type==="keydown"||e.type==="keyup"?e.keyCode:0},which:function(e){return e.type==="keypress"?nd(e):e.type==="keydown"||e.type==="keyup"?e.keyCode:0}}),Vk=Xn(jk),Hk=$t({},_0,{pointerId:0,width:0,height:0,pressure:0,tangentialPressure:0,tiltX:0,tiltY:0,twist:0,pointerType:0,isPrimary:0}),Lb=Xn(Hk),qk=$t({},zc,{touches:0,targetTouches:0,changedTouches:0,altKey:0,metaKey:0,ctrlKey:0,shiftKey:0,getModifierState:Pg}),Gk=Xn(qk),Wk=$t({},Oo,{propertyName:0,elapsedTime:0,pseudoElement:0}),Kk=Xn(Wk),Yk=$t({},_0,{deltaX:function(e){return"deltaX"in e?e.deltaX:"wheelDeltaX"in e?-e.wheelDeltaX:0},deltaY:function(e){return"deltaY"in e?e.deltaY:"wheelDeltaY"in e?-e.wheelDeltaY:"wheelDelta"in e?-e.wheelDelta:0},deltaZ:0,deltaMode:0}),Xk=Xn(Yk),Qk=[9,13,27,32],Ig=Ci&&"CompositionEvent"in window,Cl=null;Ci&&"documentMode"in document&&(Cl=document.documentMode);var Zk=Ci&&"TextEvent"in window&&!Cl,Dy=Ci&&(!Ig||Cl&&8<Cl&&11>=Cl),Pb=" ",Ib=!1;function My(e,t){switch(e){case"keyup":return Qk.indexOf(t.keyCode)!==-1;case"keydown":return t.keyCode!==229;case"keypress":case"mousedown":case"focusout":return!0;default:return!1}}function Ry(e){return e=e.detail,typeof e=="object"&&"data"in e?e.data:null}var Rs=!1;function Jk(e,t){switch(e){case"compositionend":return Ry(t);case"keypress":return t.which!==32?null:(Ib=!0,Pb);case"textInput":return e=t.data,e===Pb&&Ib?null:e;default:return null}}function eS(e,t){if(Rs)return e==="compositionend"||!Ig&&My(e,t)?(e=Uy(),td=Ag=ca=null,Rs=!1,e):null;switch(e){case"paste":return null;case"keypress":if(!(t.ctrlKey||t.altKey||t.metaKey)||t.ctrlKey&&t.altKey){if(t.char&&1<t.char.length)return t.char;if(t.which)return String.fromCharCode(t.which)}return null;case"compositionend":return Dy&&t.locale!=="ko"?null:t.data;default:return null}}var tS={color:!0,date:!0,datetime:!0,"datetime-local":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0};function Ub(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t==="input"?!!tS[e.type]:t==="textarea"}function Oy(e,t,n,r){fy(r),t=Ud(t,"onChange"),0<t.length&&(n=new Lg("onChange","change",null,n,r),e.push({event:n,listeners:t}))}var Al=null,rc=null;function nS(e){Yy(e,0)}function w0(e){var t=Bs(e);if(ly(t))return e}function rS(e,t){if(e==="change")return t}var zy=!1;if(Ci){var zm;if(Ci){var Bm="oninput"in document;if(!Bm){var Db=document.createElement("div");Db.setAttribute("oninput","return;"),Bm=typeof Db.oninput=="function"}zm=Bm}else zm=!1;zy=zm&&(!document.documentMode||9<document.documentMode)}function Mb(){Al&&(Al.detachEvent("onpropertychange",By),rc=Al=null)}function By(e){if(e.propertyName==="value"&&w0(rc)){var t=[];Oy(t,rc,e,Eg(e)),xy(nS,t)}}function iS(e,t,n){e==="focusin"?(Mb(),Al=t,rc=n,Al.attachEvent("onpropertychange",By)):e==="focusout"&&Mb()}function aS(e){if(e==="selectionchange"||e==="keyup"||e==="keydown")return w0(rc)}function sS(e,t){if(e==="click")return w0(t)}function oS(e,t){if(e==="input"||e==="change")return w0(t)}function lS(e,t){return e===t&&(e!==0||1/e===1/t)||e!==e&&t!==t}var Ar=typeof Object.is=="function"?Object.is:lS;function ic(e,t){if(Ar(e,t))return!0;if(typeof e!="object"||e===null||typeof t!="object"||t===null)return!1;var n=Object.keys(e),r=Object.keys(t);if(n.length!==r.length)return!1;for(r=0;r<n.length;r++){var i=n[r];if(!op.call(t,i)||!Ar(e[i],t[i]))return!1}return!0}function Rb(e){for(;e&&e.firstChild;)e=e.firstChild;return e}function Ob(e,t){var n=Rb(e);e=0;for(var r;n;){if(n.nodeType===3){if(r=e+n.textContent.length,e<=t&&r>=t)return{node:n,offset:t-e};e=r}e:{for(;n;){if(n.nextSibling){n=n.nextSibling;break e}n=n.parentNode}n=void 0}n=Rb(n)}}function Fy(e,t){return e&&t?e===t?!0:e&&e.nodeType===3?!1:t&&t.nodeType===3?Fy(e,t.parentNode):"contains"in e?e.contains(t):e.compareDocumentPosition?!!(e.compareDocumentPosition(t)&16):!1:!1}function jy(){for(var e=window,t=Sd();t instanceof e.HTMLIFrameElement;){try{var n=typeof t.contentWindow.location.href=="string"}catch{n=!1}if(n)e=t.contentWindow;else break;t=Sd(e.document)}return t}function Ug(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t&&(t==="input"&&(e.type==="text"||e.type==="search"||e.type==="tel"||e.type==="url"||e.type==="password")||t==="textarea"||e.contentEditable==="true")}function cS(e){var t=jy(),n=e.focusedElem,r=e.selectionRange;if(t!==n&&n&&n.ownerDocument&&Fy(n.ownerDocument.documentElement,n)){if(r!==null&&Ug(n)){if(t=r.start,e=r.end,e===void 0&&(e=t),"selectionStart"in n)n.selectionStart=t,n.selectionEnd=Math.min(e,n.value.length);else if(e=(t=n.ownerDocument||document)&&t.defaultView||window,e.getSelection){e=e.getSelection();var i=n.textContent.length,a=Math.min(r.start,i);r=r.end===void 0?a:Math.min(r.end,i),!e.extend&&a>r&&(i=r,r=a,a=i),i=Ob(n,a);var s=Ob(n,r);i&&s&&(e.rangeCount!==1||e.anchorNode!==i.node||e.anchorOffset!==i.offset||e.focusNode!==s.node||e.focusOffset!==s.offset)&&(t=t.createRange(),t.setStart(i.node,i.offset),e.removeAllRanges(),a>r?(e.addRange(t),e.extend(s.node,s.offset)):(t.setEnd(s.node,s.offset),e.addRange(t)))}}for(t=[],e=n;e=e.parentNode;)e.nodeType===1&&t.push({element:e,left:e.scrollLeft,top:e.scrollTop});for(typeof n.focus=="function"&&n.focus(),n=0;n<t.length;n++)e=t[n],e.element.scrollLeft=e.left,e.element.scrollTop=e.top}}var uS=Ci&&"documentMode"in document&&11>=document.documentMode,Os=null,kp=null,Ll=null,Sp=!1;function zb(e,t,n){var r=n.window===n?n.document:n.nodeType===9?n:n.ownerDocument;Sp||Os==null||Os!==Sd(r)||(r=Os,"selectionStart"in r&&Ug(r)?r={start:r.selectionStart,end:r.selectionEnd}:(r=(r.ownerDocument&&r.ownerDocument.defaultView||window).getSelection(),r={anchorNode:r.anchorNode,anchorOffset:r.anchorOffset,focusNode:r.focusNode,focusOffset:r.focusOffset}),Ll&&ic(Ll,r)||(Ll=r,r=Ud(kp,"onSelect"),0<r.length&&(t=new Lg("onSelect","select",null,t,n),e.push({event:t,listeners:r}),t.target=Os)))}function gu(e,t){var n={};return n[e.toLowerCase()]=t.toLowerCase(),n["Webkit"+e]="webkit"+t,n["Moz"+e]="moz"+t,n}var zs={animationend:gu("Animation","AnimationEnd"),animationiteration:gu("Animation","AnimationIteration"),animationstart:gu("Animation","AnimationStart"),transitionend:gu("Transition","TransitionEnd")},Fm={},Vy={};Ci&&(Vy=document.createElement("div").style,"AnimationEvent"in window||(delete zs.animationend.animation,delete zs.animationiteration.animation,delete zs.animationstart.animation),"TransitionEvent"in window||delete zs.transitionend.transition);function T0(e){if(Fm[e])return Fm[e];if(!zs[e])return e;var t=zs[e],n;for(n in t)if(t.hasOwnProperty(n)&&n in Vy)return Fm[e]=t[n];return e}var Hy=T0("animationend"),qy=T0("animationiteration"),Gy=T0("animationstart"),Wy=T0("transitionend"),Ky=new Map,Bb="abort auxClick cancel canPlay canPlayThrough click close contextMenu copy cut drag dragEnd dragEnter dragExit dragLeave dragOver dragStart drop durationChange emptied encrypted ended error gotPointerCapture input invalid keyDown keyPress keyUp load loadedData loadedMetadata loadStart lostPointerCapture mouseDown mouseMove mouseOut mouseOver mouseUp paste pause play playing pointerCancel pointerDown pointerMove pointerOut pointerOver pointerUp progress rateChange reset resize seeked seeking stalled submit suspend timeUpdate touchCancel touchEnd touchStart volumeChange scroll toggle touchMove waiting wheel".split(" ");function Ua(e,t){Ky.set(e,t),vs(t,[e])}for(var jm=0;jm<Bb.length;jm++){var Vm=Bb[jm],dS=Vm.toLowerCase(),mS=Vm[0].toUpperCase()+Vm.slice(1);Ua(dS,"on"+mS)}Ua(Hy,"onAnimationEnd");Ua(qy,"onAnimationIteration");Ua(Gy,"onAnimationStart");Ua("dblclick","onDoubleClick");Ua("focusin","onFocus");Ua("focusout","onBlur");Ua(Wy,"onTransitionEnd");_o("onMouseEnter",["mouseout","mouseover"]);_o("onMouseLeave",["mouseout","mouseover"]);_o("onPointerEnter",["pointerout","pointerover"]);_o("onPointerLeave",["pointerout","pointerover"]);vs("onChange","change click focusin focusout input keydown keyup selectionchange".split(" "));vs("onSelect","focusout contextmenu dragend focusin keydown keyup mousedown mouseup selectionchange".split(" "));vs("onBeforeInput",["compositionend","keypress","textInput","paste"]);vs("onCompositionEnd","compositionend focusout keydown keypress keyup mousedown".split(" "));vs("onCompositionStart","compositionstart focusout keydown keypress keyup mousedown".split(" "));vs("onCompositionUpdate","compositionupdate focusout keydown keypress keyup mousedown".split(" "));var yl="abort canplay canplaythrough durationchange emptied encrypted ended error loadeddata loadedmetadata loadstart pause play playing progress ratechange resize seeked seeking stalled suspend timeupdate volumechange waiting".split(" "),hS=new Set("cancel close invalid load scroll toggle".split(" ").concat(yl));function Fb(e,t,n){var r=e.type||"unknown-event";e.currentTarget=n,dk(r,t,void 0,e),e.currentTarget=null}function Yy(e,t){t=(t&4)!==0;for(var n=0;n<e.length;n++){var r=e[n],i=r.event;r=r.listeners;e:{var a=void 0;if(t)for(var s=r.length-1;0<=s;s--){var o=r[s],l=o.instance,c=o.currentTarget;if(o=o.listener,l!==a&&i.isPropagationStopped())break e;Fb(i,o,c),a=l}else for(s=0;s<r.length;s++){if(o=r[s],l=o.instance,c=o.currentTarget,o=o.listener,l!==a&&i.isPropagationStopped())break e;Fb(i,o,c),a=l}}}if(Cd)throw e=_p,Cd=!1,_p=null,e}function ct(e,t){var n=t[Pp];n===void 0&&(n=t[Pp]=new Set);var r=e+"__bubble";n.has(r)||(Xy(t,e,2,!1),n.add(r))}function Hm(e,t,n){var r=0;t&&(r|=4),Xy(n,e,r,t)}var bu="_reactListening"+Math.random().toString(36).slice(2);function ac(e){if(!e[bu]){e[bu]=!0,ry.forEach(function(n){n!=="selectionchange"&&(hS.has(n)||Hm(n,!1,e),Hm(n,!0,e))});var t=e.nodeType===9?e:e.ownerDocument;t===null||t[bu]||(t[bu]=!0,Hm("selectionchange",!1,t))}}function Xy(e,t,n,r){switch(Iy(t)){case 1:var i=Sk;break;case 4:i=Nk;break;default:i=Cg}n=i.bind(null,t,n,e),i=void 0,!yp||t!=="touchstart"&&t!=="touchmove"&&t!=="wheel"||(i=!0),r?i!==void 0?e.addEventListener(t,n,{capture:!0,passive:i}):e.addEventListener(t,n,!0):i!==void 0?e.addEventListener(t,n,{passive:i}):e.addEventListener(t,n,!1)}function qm(e,t,n,r,i){var a=r;if(!(t&1)&&!(t&2)&&r!==null)e:for(;;){if(r===null)return;var s=r.tag;if(s===3||s===4){var o=r.stateNode.containerInfo;if(o===i||o.nodeType===8&&o.parentNode===i)break;if(s===4)for(s=r.return;s!==null;){var l=s.tag;if((l===3||l===4)&&(l=s.stateNode.containerInfo,l===i||l.nodeType===8&&l.parentNode===i))return;s=s.return}for(;o!==null;){if(s=Xa(o),s===null)return;if(l=s.tag,l===5||l===6){r=a=s;continue e}o=o.parentNode}}r=r.return}xy(function(){var c=a,u=Eg(n),d=[];e:{var m=Ky.get(e);if(m!==void 0){var p=Lg,x=e;switch(e){case"keypress":if(nd(n)===0)break e;case"keydown":case"keyup":p=Vk;break;case"focusin":x="focus",p=Om;break;case"focusout":x="blur",p=Om;break;case"beforeblur":case"afterblur":p=Om;break;case"click":if(n.button===2)break e;case"auxclick":case"dblclick":case"mousedown":case"mousemove":case"mouseup":case"mouseout":case"mouseover":case"contextmenu":p=Cb;break;case"drag":case"dragend":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"dragstart":case"drop":p=Lk;break;case"touchcancel":case"touchend":case"touchmove":case"touchstart":p=Gk;break;case Hy:case qy:case Gy:p=Uk;break;case Wy:p=Kk;break;case"scroll":p=Ck;break;case"wheel":p=Xk;break;case"copy":case"cut":case"paste":p=Mk;break;case"gotpointercapture":case"lostpointercapture":case"pointercancel":case"pointerdown":case"pointermove":case"pointerout":case"pointerover":case"pointerup":p=Lb}var g=(t&4)!==0,w=!g&&e==="scroll",v=g?m!==null?m+"Capture":null:m;g=[];for(var $=c,_;$!==null;){_=$;var C=_.stateNode;if(_.tag===5&&C!==null&&(_=C,v!==null&&(C=Jl($,v),C!=null&&g.push(sc($,C,_)))),w)break;$=$.return}0<g.length&&(m=new p(m,x,null,n,u),d.push({event:m,listeners:g}))}}if(!(t&7)){e:{if(m=e==="mouseover"||e==="pointerover",p=e==="mouseout"||e==="pointerout",m&&n!==xp&&(x=n.relatedTarget||n.fromElement)&&(Xa(x)||x[Ai]))break e;if((p||m)&&(m=u.window===u?u:(m=u.ownerDocument)?m.defaultView||m.parentWindow:window,p?(x=n.relatedTarget||n.toElement,p=c,x=x?Xa(x):null,x!==null&&(w=xs(x),x!==w||x.tag!==5&&x.tag!==6)&&(x=null)):(p=null,x=c),p!==x)){if(g=Cb,C="onMouseLeave",v="onMouseEnter",$="mouse",(e==="pointerout"||e==="pointerover")&&(g=Lb,C="onPointerLeave",v="onPointerEnter",$="pointer"),w=p==null?m:Bs(p),_=x==null?m:Bs(x),m=new g(C,$+"leave",p,n,u),m.target=w,m.relatedTarget=_,C=null,Xa(u)===c&&(g=new g(v,$+"enter",x,n,u),g.target=_,g.relatedTarget=w,C=g),w=C,p&&x)t:{for(g=p,v=x,$=0,_=g;_;_=Ns(_))$++;for(_=0,C=v;C;C=Ns(C))_++;for(;0<$-_;)g=Ns(g),$--;for(;0<_-$;)v=Ns(v),_--;for(;$--;){if(g===v||v!==null&&g===v.alternate)break t;g=Ns(g),v=Ns(v)}g=null}else g=null;p!==null&&jb(d,m,p,g,!1),x!==null&&w!==null&&jb(d,w,x,g,!0)}}e:{if(m=c?Bs(c):window,p=m.nodeName&&m.nodeName.toLowerCase(),p==="select"||p==="input"&&m.type==="file")var k=rS;else if(Ub(m))if(zy)k=oS;else{k=aS;var S=iS}else(p=m.nodeName)&&p.toLowerCase()==="input"&&(m.type==="checkbox"||m.type==="radio")&&(k=sS);if(k&&(k=k(e,c))){Oy(d,k,n,u);break e}S&&S(e,m,c),e==="focusout"&&(S=m._wrapperState)&&S.controlled&&m.type==="number"&&pp(m,"number",m.value)}switch(S=c?Bs(c):window,e){case"focusin":(Ub(S)||S.contentEditable==="true")&&(Os=S,kp=c,Ll=null);break;case"focusout":Ll=kp=Os=null;break;case"mousedown":Sp=!0;break;case"contextmenu":case"mouseup":case"dragend":Sp=!1,zb(d,n,u);break;case"selectionchange":if(uS)break;case"keydown":case"keyup":zb(d,n,u)}var L;if(Ig)e:{switch(e){case"compositionstart":var U="onCompositionStart";break e;case"compositionend":U="onCompositionEnd";break e;case"compositionupdate":U="onCompositionUpdate";break e}U=void 0}else Rs?My(e,n)&&(U="onCompositionEnd"):e==="keydown"&&n.keyCode===229&&(U="onCompositionStart");U&&(Dy&&n.locale!=="ko"&&(Rs||U!=="onCompositionStart"?U==="onCompositionEnd"&&Rs&&(L=Uy()):(ca=u,Ag="value"in ca?ca.value:ca.textContent,Rs=!0)),S=Ud(c,U),0<S.length&&(U=new Ab(U,e,null,n,u),d.push({event:U,listeners:S}),L?U.data=L:(L=Ry(n),L!==null&&(U.data=L)))),(L=Zk?Jk(e,n):eS(e,n))&&(c=Ud(c,"onBeforeInput"),0<c.length&&(u=new Ab("onBeforeInput","beforeinput",null,n,u),d.push({event:u,listeners:c}),u.data=L))}Yy(d,t)})}function sc(e,t,n){return{instance:e,listener:t,currentTarget:n}}function Ud(e,t){for(var n=t+"Capture",r=[];e!==null;){var i=e,a=i.stateNode;i.tag===5&&a!==null&&(i=a,a=Jl(e,n),a!=null&&r.unshift(sc(e,a,i)),a=Jl(e,t),a!=null&&r.push(sc(e,a,i))),e=e.return}return r}function Ns(e){if(e===null)return null;do e=e.return;while(e&&e.tag!==5);return e||null}function jb(e,t,n,r,i){for(var a=t._reactName,s=[];n!==null&&n!==r;){var o=n,l=o.alternate,c=o.stateNode;if(l!==null&&l===r)break;o.tag===5&&c!==null&&(o=c,i?(l=Jl(n,a),l!=null&&s.unshift(sc(n,l,o))):i||(l=Jl(n,a),l!=null&&s.push(sc(n,l,o)))),n=n.return}s.length!==0&&e.push({event:t,listeners:s})}var pS=/\r\n?/g,fS=/\u0000|\uFFFD/g;function Vb(e){return(typeof e=="string"?e:""+e).replace(pS,`
`).replace(fS,"")}function vu(e,t,n){if(t=Vb(t),Vb(e)!==t&&n)throw Error(le(425))}function Dd(){}var Np=null,Cp=null;function Ap(e,t){return e==="textarea"||e==="noscript"||typeof t.children=="string"||typeof t.children=="number"||typeof t.dangerouslySetInnerHTML=="object"&&t.dangerouslySetInnerHTML!==null&&t.dangerouslySetInnerHTML.__html!=null}var Lp=typeof setTimeout=="function"?setTimeout:void 0,gS=typeof clearTimeout=="function"?clearTimeout:void 0,Hb=typeof Promise=="function"?Promise:void 0,bS=typeof queueMicrotask=="function"?queueMicrotask:typeof Hb<"u"?function(e){return Hb.resolve(null).then(e).catch(vS)}:Lp;function vS(e){setTimeout(function(){throw e})}function Gm(e,t){var n=t,r=0;do{var i=n.nextSibling;if(e.removeChild(n),i&&i.nodeType===8)if(n=i.data,n==="/$"){if(r===0){e.removeChild(i),nc(t);return}r--}else n!=="$"&&n!=="$?"&&n!=="$!"||r++;n=i}while(n);nc(t)}function va(e){for(;e!=null;e=e.nextSibling){var t=e.nodeType;if(t===1||t===3)break;if(t===8){if(t=e.data,t==="$"||t==="$!"||t==="$?")break;if(t==="/$")return null}}return e}function qb(e){e=e.previousSibling;for(var t=0;e;){if(e.nodeType===8){var n=e.data;if(n==="$"||n==="$!"||n==="$?"){if(t===0)return e;t--}else n==="/$"&&t++}e=e.previousSibling}return null}var zo=Math.random().toString(36).slice(2),Xr="__reactFiber$"+zo,oc="__reactProps$"+zo,Ai="__reactContainer$"+zo,Pp="__reactEvents$"+zo,xS="__reactListeners$"+zo,$S="__reactHandles$"+zo;function Xa(e){var t=e[Xr];if(t)return t;for(var n=e.parentNode;n;){if(t=n[Ai]||n[Xr]){if(n=t.alternate,t.child!==null||n!==null&&n.child!==null)for(e=qb(e);e!==null;){if(n=e[Xr])return n;e=qb(e)}return t}e=n,n=e.parentNode}return null}function Bc(e){return e=e[Xr]||e[Ai],!e||e.tag!==5&&e.tag!==6&&e.tag!==13&&e.tag!==3?null:e}function Bs(e){if(e.tag===5||e.tag===6)return e.stateNode;throw Error(le(33))}function E0(e){return e[oc]||null}var Ip=[],Fs=-1;function Da(e){return{current:e}}function ut(e){0>Fs||(e.current=Ip[Fs],Ip[Fs]=null,Fs--)}function it(e,t){Fs++,Ip[Fs]=e.current,e.current=t}var Ta={},an=Da(Ta),Nn=Da(!1),ds=Ta;function wo(e,t){var n=e.type.contextTypes;if(!n)return Ta;var r=e.stateNode;if(r&&r.__reactInternalMemoizedUnmaskedChildContext===t)return r.__reactInternalMemoizedMaskedChildContext;var i={},a;for(a in n)i[a]=t[a];return r&&(e=e.stateNode,e.__reactInternalMemoizedUnmaskedChildContext=t,e.__reactInternalMemoizedMaskedChildContext=i),i}function Cn(e){return e=e.childContextTypes,e!=null}function Md(){ut(Nn),ut(an)}function Gb(e,t,n){if(an.current!==Ta)throw Error(le(168));it(an,t),it(Nn,n)}function Qy(e,t,n){var r=e.stateNode;if(t=t.childContextTypes,typeof r.getChildContext!="function")return n;r=r.getChildContext();for(var i in r)if(!(i in t))throw Error(le(108,ik(e)||"Unknown",i));return $t({},n,r)}function Rd(e){return e=(e=e.stateNode)&&e.__reactInternalMemoizedMergedChildContext||Ta,ds=an.current,it(an,e),it(Nn,Nn.current),!0}function Wb(e,t,n){var r=e.stateNode;if(!r)throw Error(le(169));n?(e=Qy(e,t,ds),r.__reactInternalMemoizedMergedChildContext=e,ut(Nn),ut(an),it(an,e)):ut(Nn),it(Nn,n)}var $i=null,k0=!1,Wm=!1;function Zy(e){$i===null?$i=[e]:$i.push(e)}function yS(e){k0=!0,Zy(e)}function Ma(){if(!Wm&&$i!==null){Wm=!0;var e=0,t=Je;try{var n=$i;for(Je=1;e<n.length;e++){var r=n[e];do r=r(!0);while(r!==null)}$i=null,k0=!1}catch(i){throw $i!==null&&($i=$i.slice(e+1)),wy(kg,Ma),i}finally{Je=t,Wm=!1}}return null}var js=[],Vs=0,Od=null,zd=0,nr=[],rr=0,ms=null,wi=1,Ti="";function Ga(e,t){js[Vs++]=zd,js[Vs++]=Od,Od=e,zd=t}function Jy(e,t,n){nr[rr++]=wi,nr[rr++]=Ti,nr[rr++]=ms,ms=e;var r=wi;e=Ti;var i=32-Nr(r)-1;r&=~(1<<i),n+=1;var a=32-Nr(t)+i;if(30<a){var s=i-i%5;a=(r&(1<<s)-1).toString(32),r>>=s,i-=s,wi=1<<32-Nr(t)+i|n<<i|r,Ti=a+e}else wi=1<<a|n<<i|r,Ti=e}function Dg(e){e.return!==null&&(Ga(e,1),Jy(e,1,0))}function Mg(e){for(;e===Od;)Od=js[--Vs],js[Vs]=null,zd=js[--Vs],js[Vs]=null;for(;e===ms;)ms=nr[--rr],nr[rr]=null,Ti=nr[--rr],nr[rr]=null,wi=nr[--rr],nr[rr]=null}var Gn=null,Hn=null,mt=!1,Sr=null;function e8(e,t){var n=or(5,null,null,0);n.elementType="DELETED",n.stateNode=t,n.return=e,t=e.deletions,t===null?(e.deletions=[n],e.flags|=16):t.push(n)}function Kb(e,t){switch(e.tag){case 5:var n=e.type;return t=t.nodeType!==1||n.toLowerCase()!==t.nodeName.toLowerCase()?null:t,t!==null?(e.stateNode=t,Gn=e,Hn=va(t.firstChild),!0):!1;case 6:return t=e.pendingProps===""||t.nodeType!==3?null:t,t!==null?(e.stateNode=t,Gn=e,Hn=null,!0):!1;case 13:return t=t.nodeType!==8?null:t,t!==null?(n=ms!==null?{id:wi,overflow:Ti}:null,e.memoizedState={dehydrated:t,treeContext:n,retryLane:1073741824},n=or(18,null,null,0),n.stateNode=t,n.return=e,e.child=n,Gn=e,Hn=null,!0):!1;default:return!1}}function Up(e){return(e.mode&1)!==0&&(e.flags&128)===0}function Dp(e){if(mt){var t=Hn;if(t){var n=t;if(!Kb(e,t)){if(Up(e))throw Error(le(418));t=va(n.nextSibling);var r=Gn;t&&Kb(e,t)?e8(r,n):(e.flags=e.flags&-4097|2,mt=!1,Gn=e)}}else{if(Up(e))throw Error(le(418));e.flags=e.flags&-4097|2,mt=!1,Gn=e}}}function Yb(e){for(e=e.return;e!==null&&e.tag!==5&&e.tag!==3&&e.tag!==13;)e=e.return;Gn=e}function xu(e){if(e!==Gn)return!1;if(!mt)return Yb(e),mt=!0,!1;var t;if((t=e.tag!==3)&&!(t=e.tag!==5)&&(t=e.type,t=t!=="head"&&t!=="body"&&!Ap(e.type,e.memoizedProps)),t&&(t=Hn)){if(Up(e))throw t8(),Error(le(418));for(;t;)e8(e,t),t=va(t.nextSibling)}if(Yb(e),e.tag===13){if(e=e.memoizedState,e=e!==null?e.dehydrated:null,!e)throw Error(le(317));e:{for(e=e.nextSibling,t=0;e;){if(e.nodeType===8){var n=e.data;if(n==="/$"){if(t===0){Hn=va(e.nextSibling);break e}t--}else n!=="$"&&n!=="$!"&&n!=="$?"||t++}e=e.nextSibling}Hn=null}}else Hn=Gn?va(e.stateNode.nextSibling):null;return!0}function t8(){for(var e=Hn;e;)e=va(e.nextSibling)}function To(){Hn=Gn=null,mt=!1}function Rg(e){Sr===null?Sr=[e]:Sr.push(e)}var _S=Bi.ReactCurrentBatchConfig;function ll(e,t,n){if(e=n.ref,e!==null&&typeof e!="function"&&typeof e!="object"){if(n._owner){if(n=n._owner,n){if(n.tag!==1)throw Error(le(309));var r=n.stateNode}if(!r)throw Error(le(147,e));var i=r,a=""+e;return t!==null&&t.ref!==null&&typeof t.ref=="function"&&t.ref._stringRef===a?t.ref:(t=function(s){var o=i.refs;s===null?delete o[a]:o[a]=s},t._stringRef=a,t)}if(typeof e!="string")throw Error(le(284));if(!n._owner)throw Error(le(290,e))}return e}function $u(e,t){throw e=Object.prototype.toString.call(t),Error(le(31,e==="[object Object]"?"object with keys {"+Object.keys(t).join(", ")+"}":e))}function Xb(e){var t=e._init;return t(e._payload)}function n8(e){function t(v,$){if(e){var _=v.deletions;_===null?(v.deletions=[$],v.flags|=16):_.push($)}}function n(v,$){if(!e)return null;for(;$!==null;)t(v,$),$=$.sibling;return null}function r(v,$){for(v=new Map;$!==null;)$.key!==null?v.set($.key,$):v.set($.index,$),$=$.sibling;return v}function i(v,$){return v=_a(v,$),v.index=0,v.sibling=null,v}function a(v,$,_){return v.index=_,e?(_=v.alternate,_!==null?(_=_.index,_<$?(v.flags|=2,$):_):(v.flags|=2,$)):(v.flags|=1048576,$)}function s(v){return e&&v.alternate===null&&(v.flags|=2),v}function o(v,$,_,C){return $===null||$.tag!==6?($=eh(_,v.mode,C),$.return=v,$):($=i($,_),$.return=v,$)}function l(v,$,_,C){var k=_.type;return k===Ms?u(v,$,_.props.children,C,_.key):$!==null&&($.elementType===k||typeof k=="object"&&k!==null&&k.$$typeof===ea&&Xb(k)===$.type)?(C=i($,_.props),C.ref=ll(v,$,_),C.return=v,C):(C=cd(_.type,_.key,_.props,null,v.mode,C),C.ref=ll(v,$,_),C.return=v,C)}function c(v,$,_,C){return $===null||$.tag!==4||$.stateNode.containerInfo!==_.containerInfo||$.stateNode.implementation!==_.implementation?($=th(_,v.mode,C),$.return=v,$):($=i($,_.children||[]),$.return=v,$)}function u(v,$,_,C,k){return $===null||$.tag!==7?($=ss(_,v.mode,C,k),$.return=v,$):($=i($,_),$.return=v,$)}function d(v,$,_){if(typeof $=="string"&&$!==""||typeof $=="number")return $=eh(""+$,v.mode,_),$.return=v,$;if(typeof $=="object"&&$!==null){switch($.$$typeof){case cu:return _=cd($.type,$.key,$.props,null,v.mode,_),_.ref=ll(v,null,$),_.return=v,_;case Ds:return $=th($,v.mode,_),$.return=v,$;case ea:var C=$._init;return d(v,C($._payload),_)}if(xl($)||rl($))return $=ss($,v.mode,_,null),$.return=v,$;$u(v,$)}return null}function m(v,$,_,C){var k=$!==null?$.key:null;if(typeof _=="string"&&_!==""||typeof _=="number")return k!==null?null:o(v,$,""+_,C);if(typeof _=="object"&&_!==null){switch(_.$$typeof){case cu:return _.key===k?l(v,$,_,C):null;case Ds:return _.key===k?c(v,$,_,C):null;case ea:return k=_._init,m(v,$,k(_._payload),C)}if(xl(_)||rl(_))return k!==null?null:u(v,$,_,C,null);$u(v,_)}return null}function p(v,$,_,C,k){if(typeof C=="string"&&C!==""||typeof C=="number")return v=v.get(_)||null,o($,v,""+C,k);if(typeof C=="object"&&C!==null){switch(C.$$typeof){case cu:return v=v.get(C.key===null?_:C.key)||null,l($,v,C,k);case Ds:return v=v.get(C.key===null?_:C.key)||null,c($,v,C,k);case ea:var S=C._init;return p(v,$,_,S(C._payload),k)}if(xl(C)||rl(C))return v=v.get(_)||null,u($,v,C,k,null);$u($,C)}return null}function x(v,$,_,C){for(var k=null,S=null,L=$,U=$=0,F=null;L!==null&&U<_.length;U++){L.index>U?(F=L,L=null):F=L.sibling;var q=m(v,L,_[U],C);if(q===null){L===null&&(L=F);break}e&&L&&q.alternate===null&&t(v,L),$=a(q,$,U),S===null?k=q:S.sibling=q,S=q,L=F}if(U===_.length)return n(v,L),mt&&Ga(v,U),k;if(L===null){for(;U<_.length;U++)L=d(v,_[U],C),L!==null&&($=a(L,$,U),S===null?k=L:S.sibling=L,S=L);return mt&&Ga(v,U),k}for(L=r(v,L);U<_.length;U++)F=p(L,v,U,_[U],C),F!==null&&(e&&F.alternate!==null&&L.delete(F.key===null?U:F.key),$=a(F,$,U),S===null?k=F:S.sibling=F,S=F);return e&&L.forEach(function(G){return t(v,G)}),mt&&Ga(v,U),k}function g(v,$,_,C){var k=rl(_);if(typeof k!="function")throw Error(le(150));if(_=k.call(_),_==null)throw Error(le(151));for(var S=k=null,L=$,U=$=0,F=null,q=_.next();L!==null&&!q.done;U++,q=_.next()){L.index>U?(F=L,L=null):F=L.sibling;var G=m(v,L,q.value,C);if(G===null){L===null&&(L=F);break}e&&L&&G.alternate===null&&t(v,L),$=a(G,$,U),S===null?k=G:S.sibling=G,S=G,L=F}if(q.done)return n(v,L),mt&&Ga(v,U),k;if(L===null){for(;!q.done;U++,q=_.next())q=d(v,q.value,C),q!==null&&($=a(q,$,U),S===null?k=q:S.sibling=q,S=q);return mt&&Ga(v,U),k}for(L=r(v,L);!q.done;U++,q=_.next())q=p(L,v,U,q.value,C),q!==null&&(e&&q.alternate!==null&&L.delete(q.key===null?U:q.key),$=a(q,$,U),S===null?k=q:S.sibling=q,S=q);return e&&L.forEach(function(H){return t(v,H)}),mt&&Ga(v,U),k}function w(v,$,_,C){if(typeof _=="object"&&_!==null&&_.type===Ms&&_.key===null&&(_=_.props.children),typeof _=="object"&&_!==null){switch(_.$$typeof){case cu:e:{for(var k=_.key,S=$;S!==null;){if(S.key===k){if(k=_.type,k===Ms){if(S.tag===7){n(v,S.sibling),$=i(S,_.props.children),$.return=v,v=$;break e}}else if(S.elementType===k||typeof k=="object"&&k!==null&&k.$$typeof===ea&&Xb(k)===S.type){n(v,S.sibling),$=i(S,_.props),$.ref=ll(v,S,_),$.return=v,v=$;break e}n(v,S);break}else t(v,S);S=S.sibling}_.type===Ms?($=ss(_.props.children,v.mode,C,_.key),$.return=v,v=$):(C=cd(_.type,_.key,_.props,null,v.mode,C),C.ref=ll(v,$,_),C.return=v,v=C)}return s(v);case Ds:e:{for(S=_.key;$!==null;){if($.key===S)if($.tag===4&&$.stateNode.containerInfo===_.containerInfo&&$.stateNode.implementation===_.implementation){n(v,$.sibling),$=i($,_.children||[]),$.return=v,v=$;break e}else{n(v,$);break}else t(v,$);$=$.sibling}$=th(_,v.mode,C),$.return=v,v=$}return s(v);case ea:return S=_._init,w(v,$,S(_._payload),C)}if(xl(_))return x(v,$,_,C);if(rl(_))return g(v,$,_,C);$u(v,_)}return typeof _=="string"&&_!==""||typeof _=="number"?(_=""+_,$!==null&&$.tag===6?(n(v,$.sibling),$=i($,_),$.return=v,v=$):(n(v,$),$=eh(_,v.mode,C),$.return=v,v=$),s(v)):n(v,$)}return w}var Eo=n8(!0),r8=n8(!1),Bd=Da(null),Fd=null,Hs=null,Og=null;function zg(){Og=Hs=Fd=null}function Bg(e){var t=Bd.current;ut(Bd),e._currentValue=t}function Mp(e,t,n){for(;e!==null;){var r=e.alternate;if((e.childLanes&t)!==t?(e.childLanes|=t,r!==null&&(r.childLanes|=t)):r!==null&&(r.childLanes&t)!==t&&(r.childLanes|=t),e===n)break;e=e.return}}function ro(e,t){Fd=e,Og=Hs=null,e=e.dependencies,e!==null&&e.firstContext!==null&&(e.lanes&t&&(kn=!0),e.firstContext=null)}function mr(e){var t=e._currentValue;if(Og!==e)if(e={context:e,memoizedValue:t,next:null},Hs===null){if(Fd===null)throw Error(le(308));Hs=e,Fd.dependencies={lanes:0,firstContext:e}}else Hs=Hs.next=e;return t}var Qa=null;function Fg(e){Qa===null?Qa=[e]:Qa.push(e)}function i8(e,t,n,r){var i=t.interleaved;return i===null?(n.next=n,Fg(t)):(n.next=i.next,i.next=n),t.interleaved=n,Li(e,r)}function Li(e,t){e.lanes|=t;var n=e.alternate;for(n!==null&&(n.lanes|=t),n=e,e=e.return;e!==null;)e.childLanes|=t,n=e.alternate,n!==null&&(n.childLanes|=t),n=e,e=e.return;return n.tag===3?n.stateNode:null}var ta=!1;function jg(e){e.updateQueue={baseState:e.memoizedState,firstBaseUpdate:null,lastBaseUpdate:null,shared:{pending:null,interleaved:null,lanes:0},effects:null}}function a8(e,t){e=e.updateQueue,t.updateQueue===e&&(t.updateQueue={baseState:e.baseState,firstBaseUpdate:e.firstBaseUpdate,lastBaseUpdate:e.lastBaseUpdate,shared:e.shared,effects:e.effects})}function ki(e,t){return{eventTime:e,lane:t,tag:0,payload:null,callback:null,next:null}}function xa(e,t,n){var r=e.updateQueue;if(r===null)return null;if(r=r.shared,qe&2){var i=r.pending;return i===null?t.next=t:(t.next=i.next,i.next=t),r.pending=t,Li(e,n)}return i=r.interleaved,i===null?(t.next=t,Fg(r)):(t.next=i.next,i.next=t),r.interleaved=t,Li(e,n)}function rd(e,t,n){if(t=t.updateQueue,t!==null&&(t=t.shared,(n&4194240)!==0)){var r=t.lanes;r&=e.pendingLanes,n|=r,t.lanes=n,Sg(e,n)}}function Qb(e,t){var n=e.updateQueue,r=e.alternate;if(r!==null&&(r=r.updateQueue,n===r)){var i=null,a=null;if(n=n.firstBaseUpdate,n!==null){do{var s={eventTime:n.eventTime,lane:n.lane,tag:n.tag,payload:n.payload,callback:n.callback,next:null};a===null?i=a=s:a=a.next=s,n=n.next}while(n!==null);a===null?i=a=t:a=a.next=t}else i=a=t;n={baseState:r.baseState,firstBaseUpdate:i,lastBaseUpdate:a,shared:r.shared,effects:r.effects},e.updateQueue=n;return}e=n.lastBaseUpdate,e===null?n.firstBaseUpdate=t:e.next=t,n.lastBaseUpdate=t}function jd(e,t,n,r){var i=e.updateQueue;ta=!1;var a=i.firstBaseUpdate,s=i.lastBaseUpdate,o=i.shared.pending;if(o!==null){i.shared.pending=null;var l=o,c=l.next;l.next=null,s===null?a=c:s.next=c,s=l;var u=e.alternate;u!==null&&(u=u.updateQueue,o=u.lastBaseUpdate,o!==s&&(o===null?u.firstBaseUpdate=c:o.next=c,u.lastBaseUpdate=l))}if(a!==null){var d=i.baseState;s=0,u=c=l=null,o=a;do{var m=o.lane,p=o.eventTime;if((r&m)===m){u!==null&&(u=u.next={eventTime:p,lane:0,tag:o.tag,payload:o.payload,callback:o.callback,next:null});e:{var x=e,g=o;switch(m=t,p=n,g.tag){case 1:if(x=g.payload,typeof x=="function"){d=x.call(p,d,m);break e}d=x;break e;case 3:x.flags=x.flags&-65537|128;case 0:if(x=g.payload,m=typeof x=="function"?x.call(p,d,m):x,m==null)break e;d=$t({},d,m);break e;case 2:ta=!0}}o.callback!==null&&o.lane!==0&&(e.flags|=64,m=i.effects,m===null?i.effects=[o]:m.push(o))}else p={eventTime:p,lane:m,tag:o.tag,payload:o.payload,callback:o.callback,next:null},u===null?(c=u=p,l=d):u=u.next=p,s|=m;if(o=o.next,o===null){if(o=i.shared.pending,o===null)break;m=o,o=m.next,m.next=null,i.lastBaseUpdate=m,i.shared.pending=null}}while(!0);if(u===null&&(l=d),i.baseState=l,i.firstBaseUpdate=c,i.lastBaseUpdate=u,t=i.shared.interleaved,t!==null){i=t;do s|=i.lane,i=i.next;while(i!==t)}else a===null&&(i.shared.lanes=0);ps|=s,e.lanes=s,e.memoizedState=d}}function Zb(e,t,n){if(e=t.effects,t.effects=null,e!==null)for(t=0;t<e.length;t++){var r=e[t],i=r.callback;if(i!==null){if(r.callback=null,r=n,typeof i!="function")throw Error(le(191,i));i.call(r)}}}var Fc={},ri=Da(Fc),lc=Da(Fc),cc=Da(Fc);function Za(e){if(e===Fc)throw Error(le(174));return e}function Vg(e,t){switch(it(cc,t),it(lc,e),it(ri,Fc),e=t.nodeType,e){case 9:case 11:t=(t=t.documentElement)?t.namespaceURI:gp(null,"");break;default:e=e===8?t.parentNode:t,t=e.namespaceURI||null,e=e.tagName,t=gp(t,e)}ut(ri),it(ri,t)}function ko(){ut(ri),ut(lc),ut(cc)}function s8(e){Za(cc.current);var t=Za(ri.current),n=gp(t,e.type);t!==n&&(it(lc,e),it(ri,n))}function Hg(e){lc.current===e&&(ut(ri),ut(lc))}var ft=Da(0);function Vd(e){for(var t=e;t!==null;){if(t.tag===13){var n=t.memoizedState;if(n!==null&&(n=n.dehydrated,n===null||n.data==="$?"||n.data==="$!"))return t}else if(t.tag===19&&t.memoizedProps.revealOrder!==void 0){if(t.flags&128)return t}else if(t.child!==null){t.child.return=t,t=t.child;continue}if(t===e)break;for(;t.sibling===null;){if(t.return===null||t.return===e)return null;t=t.return}t.sibling.return=t.return,t=t.sibling}return null}var Km=[];function qg(){for(var e=0;e<Km.length;e++)Km[e]._workInProgressVersionPrimary=null;Km.length=0}var id=Bi.ReactCurrentDispatcher,Ym=Bi.ReactCurrentBatchConfig,hs=0,vt=null,Rt=null,Vt=null,Hd=!1,Pl=!1,uc=0,wS=0;function Zt(){throw Error(le(321))}function Gg(e,t){if(t===null)return!1;for(var n=0;n<t.length&&n<e.length;n++)if(!Ar(e[n],t[n]))return!1;return!0}function Wg(e,t,n,r,i,a){if(hs=a,vt=t,t.memoizedState=null,t.updateQueue=null,t.lanes=0,id.current=e===null||e.memoizedState===null?SS:NS,e=n(r,i),Pl){a=0;do{if(Pl=!1,uc=0,25<=a)throw Error(le(301));a+=1,Vt=Rt=null,t.updateQueue=null,id.current=CS,e=n(r,i)}while(Pl)}if(id.current=qd,t=Rt!==null&&Rt.next!==null,hs=0,Vt=Rt=vt=null,Hd=!1,t)throw Error(le(300));return e}function Kg(){var e=uc!==0;return uc=0,e}function zr(){var e={memoizedState:null,baseState:null,baseQueue:null,queue:null,next:null};return Vt===null?vt.memoizedState=Vt=e:Vt=Vt.next=e,Vt}function hr(){if(Rt===null){var e=vt.alternate;e=e!==null?e.memoizedState:null}else e=Rt.next;var t=Vt===null?vt.memoizedState:Vt.next;if(t!==null)Vt=t,Rt=e;else{if(e===null)throw Error(le(310));Rt=e,e={memoizedState:Rt.memoizedState,baseState:Rt.baseState,baseQueue:Rt.baseQueue,queue:Rt.queue,next:null},Vt===null?vt.memoizedState=Vt=e:Vt=Vt.next=e}return Vt}function dc(e,t){return typeof t=="function"?t(e):t}function Xm(e){var t=hr(),n=t.queue;if(n===null)throw Error(le(311));n.lastRenderedReducer=e;var r=Rt,i=r.baseQueue,a=n.pending;if(a!==null){if(i!==null){var s=i.next;i.next=a.next,a.next=s}r.baseQueue=i=a,n.pending=null}if(i!==null){a=i.next,r=r.baseState;var o=s=null,l=null,c=a;do{var u=c.lane;if((hs&u)===u)l!==null&&(l=l.next={lane:0,action:c.action,hasEagerState:c.hasEagerState,eagerState:c.eagerState,next:null}),r=c.hasEagerState?c.eagerState:e(r,c.action);else{var d={lane:u,action:c.action,hasEagerState:c.hasEagerState,eagerState:c.eagerState,next:null};l===null?(o=l=d,s=r):l=l.next=d,vt.lanes|=u,ps|=u}c=c.next}while(c!==null&&c!==a);l===null?s=r:l.next=o,Ar(r,t.memoizedState)||(kn=!0),t.memoizedState=r,t.baseState=s,t.baseQueue=l,n.lastRenderedState=r}if(e=n.interleaved,e!==null){i=e;do a=i.lane,vt.lanes|=a,ps|=a,i=i.next;while(i!==e)}else i===null&&(n.lanes=0);return[t.memoizedState,n.dispatch]}function Qm(e){var t=hr(),n=t.queue;if(n===null)throw Error(le(311));n.lastRenderedReducer=e;var r=n.dispatch,i=n.pending,a=t.memoizedState;if(i!==null){n.pending=null;var s=i=i.next;do a=e(a,s.action),s=s.next;while(s!==i);Ar(a,t.memoizedState)||(kn=!0),t.memoizedState=a,t.baseQueue===null&&(t.baseState=a),n.lastRenderedState=a}return[a,r]}function o8(){}function l8(e,t){var n=vt,r=hr(),i=t(),a=!Ar(r.memoizedState,i);if(a&&(r.memoizedState=i,kn=!0),r=r.queue,Yg(d8.bind(null,n,r,e),[e]),r.getSnapshot!==t||a||Vt!==null&&Vt.memoizedState.tag&1){if(n.flags|=2048,mc(9,u8.bind(null,n,r,i,t),void 0,null),qt===null)throw Error(le(349));hs&30||c8(n,t,i)}return i}function c8(e,t,n){e.flags|=16384,e={getSnapshot:t,value:n},t=vt.updateQueue,t===null?(t={lastEffect:null,stores:null},vt.updateQueue=t,t.stores=[e]):(n=t.stores,n===null?t.stores=[e]:n.push(e))}function u8(e,t,n,r){t.value=n,t.getSnapshot=r,m8(t)&&h8(e)}function d8(e,t,n){return n(function(){m8(t)&&h8(e)})}function m8(e){var t=e.getSnapshot;e=e.value;try{var n=t();return!Ar(e,n)}catch{return!0}}function h8(e){var t=Li(e,1);t!==null&&Cr(t,e,1,-1)}function Jb(e){var t=zr();return typeof e=="function"&&(e=e()),t.memoizedState=t.baseState=e,e={pending:null,interleaved:null,lanes:0,dispatch:null,lastRenderedReducer:dc,lastRenderedState:e},t.queue=e,e=e.dispatch=kS.bind(null,vt,e),[t.memoizedState,e]}function mc(e,t,n,r){return e={tag:e,create:t,destroy:n,deps:r,next:null},t=vt.updateQueue,t===null?(t={lastEffect:null,stores:null},vt.updateQueue=t,t.lastEffect=e.next=e):(n=t.lastEffect,n===null?t.lastEffect=e.next=e:(r=n.next,n.next=e,e.next=r,t.lastEffect=e)),e}function p8(){return hr().memoizedState}function ad(e,t,n,r){var i=zr();vt.flags|=e,i.memoizedState=mc(1|t,n,void 0,r===void 0?null:r)}function S0(e,t,n,r){var i=hr();r=r===void 0?null:r;var a=void 0;if(Rt!==null){var s=Rt.memoizedState;if(a=s.destroy,r!==null&&Gg(r,s.deps)){i.memoizedState=mc(t,n,a,r);return}}vt.flags|=e,i.memoizedState=mc(1|t,n,a,r)}function ev(e,t){return ad(8390656,8,e,t)}function Yg(e,t){return S0(2048,8,e,t)}function f8(e,t){return S0(4,2,e,t)}function g8(e,t){return S0(4,4,e,t)}function b8(e,t){if(typeof t=="function")return e=e(),t(e),function(){t(null)};if(t!=null)return e=e(),t.current=e,function(){t.current=null}}function v8(e,t,n){return n=n!=null?n.concat([e]):null,S0(4,4,b8.bind(null,t,e),n)}function Xg(){}function x8(e,t){var n=hr();t=t===void 0?null:t;var r=n.memoizedState;return r!==null&&t!==null&&Gg(t,r[1])?r[0]:(n.memoizedState=[e,t],e)}function $8(e,t){var n=hr();t=t===void 0?null:t;var r=n.memoizedState;return r!==null&&t!==null&&Gg(t,r[1])?r[0]:(e=e(),n.memoizedState=[e,t],e)}function y8(e,t,n){return hs&21?(Ar(n,t)||(n=ky(),vt.lanes|=n,ps|=n,e.baseState=!0),t):(e.baseState&&(e.baseState=!1,kn=!0),e.memoizedState=n)}function TS(e,t){var n=Je;Je=n!==0&&4>n?n:4,e(!0);var r=Ym.transition;Ym.transition={};try{e(!1),t()}finally{Je=n,Ym.transition=r}}function _8(){return hr().memoizedState}function ES(e,t,n){var r=ya(e);if(n={lane:r,action:n,hasEagerState:!1,eagerState:null,next:null},w8(e))T8(t,n);else if(n=i8(e,t,n,r),n!==null){var i=fn();Cr(n,e,r,i),E8(n,t,r)}}function kS(e,t,n){var r=ya(e),i={lane:r,action:n,hasEagerState:!1,eagerState:null,next:null};if(w8(e))T8(t,i);else{var a=e.alternate;if(e.lanes===0&&(a===null||a.lanes===0)&&(a=t.lastRenderedReducer,a!==null))try{var s=t.lastRenderedState,o=a(s,n);if(i.hasEagerState=!0,i.eagerState=o,Ar(o,s)){var l=t.interleaved;l===null?(i.next=i,Fg(t)):(i.next=l.next,l.next=i),t.interleaved=i;return}}catch{}finally{}n=i8(e,t,i,r),n!==null&&(i=fn(),Cr(n,e,r,i),E8(n,t,r))}}function w8(e){var t=e.alternate;return e===vt||t!==null&&t===vt}function T8(e,t){Pl=Hd=!0;var n=e.pending;n===null?t.next=t:(t.next=n.next,n.next=t),e.pending=t}function E8(e,t,n){if(n&4194240){var r=t.lanes;r&=e.pendingLanes,n|=r,t.lanes=n,Sg(e,n)}}var qd={readContext:mr,useCallback:Zt,useContext:Zt,useEffect:Zt,useImperativeHandle:Zt,useInsertionEffect:Zt,useLayoutEffect:Zt,useMemo:Zt,useReducer:Zt,useRef:Zt,useState:Zt,useDebugValue:Zt,useDeferredValue:Zt,useTransition:Zt,useMutableSource:Zt,useSyncExternalStore:Zt,useId:Zt,unstable_isNewReconciler:!1},SS={readContext:mr,useCallback:function(e,t){return zr().memoizedState=[e,t===void 0?null:t],e},useContext:mr,useEffect:ev,useImperativeHandle:function(e,t,n){return n=n!=null?n.concat([e]):null,ad(4194308,4,b8.bind(null,t,e),n)},useLayoutEffect:function(e,t){return ad(4194308,4,e,t)},useInsertionEffect:function(e,t){return ad(4,2,e,t)},useMemo:function(e,t){var n=zr();return t=t===void 0?null:t,e=e(),n.memoizedState=[e,t],e},useReducer:function(e,t,n){var r=zr();return t=n!==void 0?n(t):t,r.memoizedState=r.baseState=t,e={pending:null,interleaved:null,lanes:0,dispatch:null,lastRenderedReducer:e,lastRenderedState:t},r.queue=e,e=e.dispatch=ES.bind(null,vt,e),[r.memoizedState,e]},useRef:function(e){var t=zr();return e={current:e},t.memoizedState=e},useState:Jb,useDebugValue:Xg,useDeferredValue:function(e){return zr().memoizedState=e},useTransition:function(){var e=Jb(!1),t=e[0];return e=TS.bind(null,e[1]),zr().memoizedState=e,[t,e]},useMutableSource:function(){},useSyncExternalStore:function(e,t,n){var r=vt,i=zr();if(mt){if(n===void 0)throw Error(le(407));n=n()}else{if(n=t(),qt===null)throw Error(le(349));hs&30||c8(r,t,n)}i.memoizedState=n;var a={value:n,getSnapshot:t};return i.queue=a,ev(d8.bind(null,r,a,e),[e]),r.flags|=2048,mc(9,u8.bind(null,r,a,n,t),void 0,null),n},useId:function(){var e=zr(),t=qt.identifierPrefix;if(mt){var n=Ti,r=wi;n=(r&~(1<<32-Nr(r)-1)).toString(32)+n,t=":"+t+"R"+n,n=uc++,0<n&&(t+="H"+n.toString(32)),t+=":"}else n=wS++,t=":"+t+"r"+n.toString(32)+":";return e.memoizedState=t},unstable_isNewReconciler:!1},NS={readContext:mr,useCallback:x8,useContext:mr,useEffect:Yg,useImperativeHandle:v8,useInsertionEffect:f8,useLayoutEffect:g8,useMemo:$8,useReducer:Xm,useRef:p8,useState:function(){return Xm(dc)},useDebugValue:Xg,useDeferredValue:function(e){var t=hr();return y8(t,Rt.memoizedState,e)},useTransition:function(){var e=Xm(dc)[0],t=hr().memoizedState;return[e,t]},useMutableSource:o8,useSyncExternalStore:l8,useId:_8,unstable_isNewReconciler:!1},CS={readContext:mr,useCallback:x8,useContext:mr,useEffect:Yg,useImperativeHandle:v8,useInsertionEffect:f8,useLayoutEffect:g8,useMemo:$8,useReducer:Qm,useRef:p8,useState:function(){return Qm(dc)},useDebugValue:Xg,useDeferredValue:function(e){var t=hr();return Rt===null?t.memoizedState=e:y8(t,Rt.memoizedState,e)},useTransition:function(){var e=Qm(dc)[0],t=hr().memoizedState;return[e,t]},useMutableSource:o8,useSyncExternalStore:l8,useId:_8,unstable_isNewReconciler:!1};function yr(e,t){if(e&&e.defaultProps){t=$t({},t),e=e.defaultProps;for(var n in e)t[n]===void 0&&(t[n]=e[n]);return t}return t}function Rp(e,t,n,r){t=e.memoizedState,n=n(r,t),n=n==null?t:$t({},t,n),e.memoizedState=n,e.lanes===0&&(e.updateQueue.baseState=n)}var N0={isMounted:function(e){return(e=e._reactInternals)?xs(e)===e:!1},enqueueSetState:function(e,t,n){e=e._reactInternals;var r=fn(),i=ya(e),a=ki(r,i);a.payload=t,n!=null&&(a.callback=n),t=xa(e,a,i),t!==null&&(Cr(t,e,i,r),rd(t,e,i))},enqueueReplaceState:function(e,t,n){e=e._reactInternals;var r=fn(),i=ya(e),a=ki(r,i);a.tag=1,a.payload=t,n!=null&&(a.callback=n),t=xa(e,a,i),t!==null&&(Cr(t,e,i,r),rd(t,e,i))},enqueueForceUpdate:function(e,t){e=e._reactInternals;var n=fn(),r=ya(e),i=ki(n,r);i.tag=2,t!=null&&(i.callback=t),t=xa(e,i,r),t!==null&&(Cr(t,e,r,n),rd(t,e,r))}};function tv(e,t,n,r,i,a,s){return e=e.stateNode,typeof e.shouldComponentUpdate=="function"?e.shouldComponentUpdate(r,a,s):t.prototype&&t.prototype.isPureReactComponent?!ic(n,r)||!ic(i,a):!0}function k8(e,t,n){var r=!1,i=Ta,a=t.contextType;return typeof a=="object"&&a!==null?a=mr(a):(i=Cn(t)?ds:an.current,r=t.contextTypes,a=(r=r!=null)?wo(e,i):Ta),t=new t(n,a),e.memoizedState=t.state!==null&&t.state!==void 0?t.state:null,t.updater=N0,e.stateNode=t,t._reactInternals=e,r&&(e=e.stateNode,e.__reactInternalMemoizedUnmaskedChildContext=i,e.__reactInternalMemoizedMaskedChildContext=a),t}function nv(e,t,n,r){e=t.state,typeof t.componentWillReceiveProps=="function"&&t.componentWillReceiveProps(n,r),typeof t.UNSAFE_componentWillReceiveProps=="function"&&t.UNSAFE_componentWillReceiveProps(n,r),t.state!==e&&N0.enqueueReplaceState(t,t.state,null)}function Op(e,t,n,r){var i=e.stateNode;i.props=n,i.state=e.memoizedState,i.refs={},jg(e);var a=t.contextType;typeof a=="object"&&a!==null?i.context=mr(a):(a=Cn(t)?ds:an.current,i.context=wo(e,a)),i.state=e.memoizedState,a=t.getDerivedStateFromProps,typeof a=="function"&&(Rp(e,t,a,n),i.state=e.memoizedState),typeof t.getDerivedStateFromProps=="function"||typeof i.getSnapshotBeforeUpdate=="function"||typeof i.UNSAFE_componentWillMount!="function"&&typeof i.componentWillMount!="function"||(t=i.state,typeof i.componentWillMount=="function"&&i.componentWillMount(),typeof i.UNSAFE_componentWillMount=="function"&&i.UNSAFE_componentWillMount(),t!==i.state&&N0.enqueueReplaceState(i,i.state,null),jd(e,n,i,r),i.state=e.memoizedState),typeof i.componentDidMount=="function"&&(e.flags|=4194308)}function So(e,t){try{var n="",r=t;do n+=rk(r),r=r.return;while(r);var i=n}catch(a){i=`
Error generating stack: `+a.message+`
`+a.stack}return{value:e,source:t,stack:i,digest:null}}function Zm(e,t,n){return{value:e,source:null,stack:n??null,digest:t??null}}function zp(e,t){try{console.error(t.value)}catch(n){setTimeout(function(){throw n})}}var AS=typeof WeakMap=="function"?WeakMap:Map;function S8(e,t,n){n=ki(-1,n),n.tag=3,n.payload={element:null};var r=t.value;return n.callback=function(){Wd||(Wd=!0,Yp=r),zp(e,t)},n}function N8(e,t,n){n=ki(-1,n),n.tag=3;var r=e.type.getDerivedStateFromError;if(typeof r=="function"){var i=t.value;n.payload=function(){return r(i)},n.callback=function(){zp(e,t)}}var a=e.stateNode;return a!==null&&typeof a.componentDidCatch=="function"&&(n.callback=function(){zp(e,t),typeof r!="function"&&($a===null?$a=new Set([this]):$a.add(this));var s=t.stack;this.componentDidCatch(t.value,{componentStack:s!==null?s:""})}),n}function rv(e,t,n){var r=e.pingCache;if(r===null){r=e.pingCache=new AS;var i=new Set;r.set(t,i)}else i=r.get(t),i===void 0&&(i=new Set,r.set(t,i));i.has(n)||(i.add(n),e=HS.bind(null,e,t,n),t.then(e,e))}function iv(e){do{var t;if((t=e.tag===13)&&(t=e.memoizedState,t=t!==null?t.dehydrated!==null:!0),t)return e;e=e.return}while(e!==null);return null}function av(e,t,n,r,i){return e.mode&1?(e.flags|=65536,e.lanes=i,e):(e===t?e.flags|=65536:(e.flags|=128,n.flags|=131072,n.flags&=-52805,n.tag===1&&(n.alternate===null?n.tag=17:(t=ki(-1,1),t.tag=2,xa(n,t,1))),n.lanes|=1),e)}var LS=Bi.ReactCurrentOwner,kn=!1;function dn(e,t,n,r){t.child=e===null?r8(t,null,n,r):Eo(t,e.child,n,r)}function sv(e,t,n,r,i){n=n.render;var a=t.ref;return ro(t,i),r=Wg(e,t,n,r,a,i),n=Kg(),e!==null&&!kn?(t.updateQueue=e.updateQueue,t.flags&=-2053,e.lanes&=~i,Pi(e,t,i)):(mt&&n&&Dg(t),t.flags|=1,dn(e,t,r,i),t.child)}function ov(e,t,n,r,i){if(e===null){var a=n.type;return typeof a=="function"&&!i1(a)&&a.defaultProps===void 0&&n.compare===null&&n.defaultProps===void 0?(t.tag=15,t.type=a,C8(e,t,a,r,i)):(e=cd(n.type,null,r,t,t.mode,i),e.ref=t.ref,e.return=t,t.child=e)}if(a=e.child,!(e.lanes&i)){var s=a.memoizedProps;if(n=n.compare,n=n!==null?n:ic,n(s,r)&&e.ref===t.ref)return Pi(e,t,i)}return t.flags|=1,e=_a(a,r),e.ref=t.ref,e.return=t,t.child=e}function C8(e,t,n,r,i){if(e!==null){var a=e.memoizedProps;if(ic(a,r)&&e.ref===t.ref)if(kn=!1,t.pendingProps=r=a,(e.lanes&i)!==0)e.flags&131072&&(kn=!0);else return t.lanes=e.lanes,Pi(e,t,i)}return Bp(e,t,n,r,i)}function A8(e,t,n){var r=t.pendingProps,i=r.children,a=e!==null?e.memoizedState:null;if(r.mode==="hidden")if(!(t.mode&1))t.memoizedState={baseLanes:0,cachePool:null,transitions:null},it(Gs,Bn),Bn|=n;else{if(!(n&1073741824))return e=a!==null?a.baseLanes|n:n,t.lanes=t.childLanes=1073741824,t.memoizedState={baseLanes:e,cachePool:null,transitions:null},t.updateQueue=null,it(Gs,Bn),Bn|=e,null;t.memoizedState={baseLanes:0,cachePool:null,transitions:null},r=a!==null?a.baseLanes:n,it(Gs,Bn),Bn|=r}else a!==null?(r=a.baseLanes|n,t.memoizedState=null):r=n,it(Gs,Bn),Bn|=r;return dn(e,t,i,n),t.child}function L8(e,t){var n=t.ref;(e===null&&n!==null||e!==null&&e.ref!==n)&&(t.flags|=512,t.flags|=2097152)}function Bp(e,t,n,r,i){var a=Cn(n)?ds:an.current;return a=wo(t,a),ro(t,i),n=Wg(e,t,n,r,a,i),r=Kg(),e!==null&&!kn?(t.updateQueue=e.updateQueue,t.flags&=-2053,e.lanes&=~i,Pi(e,t,i)):(mt&&r&&Dg(t),t.flags|=1,dn(e,t,n,i),t.child)}function lv(e,t,n,r,i){if(Cn(n)){var a=!0;Rd(t)}else a=!1;if(ro(t,i),t.stateNode===null)sd(e,t),k8(t,n,r),Op(t,n,r,i),r=!0;else if(e===null){var s=t.stateNode,o=t.memoizedProps;s.props=o;var l=s.context,c=n.contextType;typeof c=="object"&&c!==null?c=mr(c):(c=Cn(n)?ds:an.current,c=wo(t,c));var u=n.getDerivedStateFromProps,d=typeof u=="function"||typeof s.getSnapshotBeforeUpdate=="function";d||typeof s.UNSAFE_componentWillReceiveProps!="function"&&typeof s.componentWillReceiveProps!="function"||(o!==r||l!==c)&&nv(t,s,r,c),ta=!1;var m=t.memoizedState;s.state=m,jd(t,r,s,i),l=t.memoizedState,o!==r||m!==l||Nn.current||ta?(typeof u=="function"&&(Rp(t,n,u,r),l=t.memoizedState),(o=ta||tv(t,n,o,r,m,l,c))?(d||typeof s.UNSAFE_componentWillMount!="function"&&typeof s.componentWillMount!="function"||(typeof s.componentWillMount=="function"&&s.componentWillMount(),typeof s.UNSAFE_componentWillMount=="function"&&s.UNSAFE_componentWillMount()),typeof s.componentDidMount=="function"&&(t.flags|=4194308)):(typeof s.componentDidMount=="function"&&(t.flags|=4194308),t.memoizedProps=r,t.memoizedState=l),s.props=r,s.state=l,s.context=c,r=o):(typeof s.componentDidMount=="function"&&(t.flags|=4194308),r=!1)}else{s=t.stateNode,a8(e,t),o=t.memoizedProps,c=t.type===t.elementType?o:yr(t.type,o),s.props=c,d=t.pendingProps,m=s.context,l=n.contextType,typeof l=="object"&&l!==null?l=mr(l):(l=Cn(n)?ds:an.current,l=wo(t,l));var p=n.getDerivedStateFromProps;(u=typeof p=="function"||typeof s.getSnapshotBeforeUpdate=="function")||typeof s.UNSAFE_componentWillReceiveProps!="function"&&typeof s.componentWillReceiveProps!="function"||(o!==d||m!==l)&&nv(t,s,r,l),ta=!1,m=t.memoizedState,s.state=m,jd(t,r,s,i);var x=t.memoizedState;o!==d||m!==x||Nn.current||ta?(typeof p=="function"&&(Rp(t,n,p,r),x=t.memoizedState),(c=ta||tv(t,n,c,r,m,x,l)||!1)?(u||typeof s.UNSAFE_componentWillUpdate!="function"&&typeof s.componentWillUpdate!="function"||(typeof s.componentWillUpdate=="function"&&s.componentWillUpdate(r,x,l),typeof s.UNSAFE_componentWillUpdate=="function"&&s.UNSAFE_componentWillUpdate(r,x,l)),typeof s.componentDidUpdate=="function"&&(t.flags|=4),typeof s.getSnapshotBeforeUpdate=="function"&&(t.flags|=1024)):(typeof s.componentDidUpdate!="function"||o===e.memoizedProps&&m===e.memoizedState||(t.flags|=4),typeof s.getSnapshotBeforeUpdate!="function"||o===e.memoizedProps&&m===e.memoizedState||(t.flags|=1024),t.memoizedProps=r,t.memoizedState=x),s.props=r,s.state=x,s.context=l,r=c):(typeof s.componentDidUpdate!="function"||o===e.memoizedProps&&m===e.memoizedState||(t.flags|=4),typeof s.getSnapshotBeforeUpdate!="function"||o===e.memoizedProps&&m===e.memoizedState||(t.flags|=1024),r=!1)}return Fp(e,t,n,r,a,i)}function Fp(e,t,n,r,i,a){L8(e,t);var s=(t.flags&128)!==0;if(!r&&!s)return i&&Wb(t,n,!1),Pi(e,t,a);r=t.stateNode,LS.current=t;var o=s&&typeof n.getDerivedStateFromError!="function"?null:r.render();return t.flags|=1,e!==null&&s?(t.child=Eo(t,e.child,null,a),t.child=Eo(t,null,o,a)):dn(e,t,o,a),t.memoizedState=r.state,i&&Wb(t,n,!0),t.child}function P8(e){var t=e.stateNode;t.pendingContext?Gb(e,t.pendingContext,t.pendingContext!==t.context):t.context&&Gb(e,t.context,!1),Vg(e,t.containerInfo)}function cv(e,t,n,r,i){return To(),Rg(i),t.flags|=256,dn(e,t,n,r),t.child}var jp={dehydrated:null,treeContext:null,retryLane:0};function Vp(e){return{baseLanes:e,cachePool:null,transitions:null}}function I8(e,t,n){var r=t.pendingProps,i=ft.current,a=!1,s=(t.flags&128)!==0,o;if((o=s)||(o=e!==null&&e.memoizedState===null?!1:(i&2)!==0),o?(a=!0,t.flags&=-129):(e===null||e.memoizedState!==null)&&(i|=1),it(ft,i&1),e===null)return Dp(t),e=t.memoizedState,e!==null&&(e=e.dehydrated,e!==null)?(t.mode&1?e.data==="$!"?t.lanes=8:t.lanes=1073741824:t.lanes=1,null):(s=r.children,e=r.fallback,a?(r=t.mode,a=t.child,s={mode:"hidden",children:s},!(r&1)&&a!==null?(a.childLanes=0,a.pendingProps=s):a=L0(s,r,0,null),e=ss(e,r,n,null),a.return=t,e.return=t,a.sibling=e,t.child=a,t.child.memoizedState=Vp(n),t.memoizedState=jp,e):Qg(t,s));if(i=e.memoizedState,i!==null&&(o=i.dehydrated,o!==null))return PS(e,t,s,r,o,i,n);if(a){a=r.fallback,s=t.mode,i=e.child,o=i.sibling;var l={mode:"hidden",children:r.children};return!(s&1)&&t.child!==i?(r=t.child,r.childLanes=0,r.pendingProps=l,t.deletions=null):(r=_a(i,l),r.subtreeFlags=i.subtreeFlags&14680064),o!==null?a=_a(o,a):(a=ss(a,s,n,null),a.flags|=2),a.return=t,r.return=t,r.sibling=a,t.child=r,r=a,a=t.child,s=e.child.memoizedState,s=s===null?Vp(n):{baseLanes:s.baseLanes|n,cachePool:null,transitions:s.transitions},a.memoizedState=s,a.childLanes=e.childLanes&~n,t.memoizedState=jp,r}return a=e.child,e=a.sibling,r=_a(a,{mode:"visible",children:r.children}),!(t.mode&1)&&(r.lanes=n),r.return=t,r.sibling=null,e!==null&&(n=t.deletions,n===null?(t.deletions=[e],t.flags|=16):n.push(e)),t.child=r,t.memoizedState=null,r}function Qg(e,t){return t=L0({mode:"visible",children:t},e.mode,0,null),t.return=e,e.child=t}function yu(e,t,n,r){return r!==null&&Rg(r),Eo(t,e.child,null,n),e=Qg(t,t.pendingProps.children),e.flags|=2,t.memoizedState=null,e}function PS(e,t,n,r,i,a,s){if(n)return t.flags&256?(t.flags&=-257,r=Zm(Error(le(422))),yu(e,t,s,r)):t.memoizedState!==null?(t.child=e.child,t.flags|=128,null):(a=r.fallback,i=t.mode,r=L0({mode:"visible",children:r.children},i,0,null),a=ss(a,i,s,null),a.flags|=2,r.return=t,a.return=t,r.sibling=a,t.child=r,t.mode&1&&Eo(t,e.child,null,s),t.child.memoizedState=Vp(s),t.memoizedState=jp,a);if(!(t.mode&1))return yu(e,t,s,null);if(i.data==="$!"){if(r=i.nextSibling&&i.nextSibling.dataset,r)var o=r.dgst;return r=o,a=Error(le(419)),r=Zm(a,r,void 0),yu(e,t,s,r)}if(o=(s&e.childLanes)!==0,kn||o){if(r=qt,r!==null){switch(s&-s){case 4:i=2;break;case 16:i=8;break;case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:case 4194304:case 8388608:case 16777216:case 33554432:case 67108864:i=32;break;case 536870912:i=268435456;break;default:i=0}i=i&(r.suspendedLanes|s)?0:i,i!==0&&i!==a.retryLane&&(a.retryLane=i,Li(e,i),Cr(r,e,i,-1))}return r1(),r=Zm(Error(le(421))),yu(e,t,s,r)}return i.data==="$?"?(t.flags|=128,t.child=e.child,t=qS.bind(null,e),i._reactRetry=t,null):(e=a.treeContext,Hn=va(i.nextSibling),Gn=t,mt=!0,Sr=null,e!==null&&(nr[rr++]=wi,nr[rr++]=Ti,nr[rr++]=ms,wi=e.id,Ti=e.overflow,ms=t),t=Qg(t,r.children),t.flags|=4096,t)}function uv(e,t,n){e.lanes|=t;var r=e.alternate;r!==null&&(r.lanes|=t),Mp(e.return,t,n)}function Jm(e,t,n,r,i){var a=e.memoizedState;a===null?e.memoizedState={isBackwards:t,rendering:null,renderingStartTime:0,last:r,tail:n,tailMode:i}:(a.isBackwards=t,a.rendering=null,a.renderingStartTime=0,a.last=r,a.tail=n,a.tailMode=i)}function U8(e,t,n){var r=t.pendingProps,i=r.revealOrder,a=r.tail;if(dn(e,t,r.children,n),r=ft.current,r&2)r=r&1|2,t.flags|=128;else{if(e!==null&&e.flags&128)e:for(e=t.child;e!==null;){if(e.tag===13)e.memoizedState!==null&&uv(e,n,t);else if(e.tag===19)uv(e,n,t);else if(e.child!==null){e.child.return=e,e=e.child;continue}if(e===t)break e;for(;e.sibling===null;){if(e.return===null||e.return===t)break e;e=e.return}e.sibling.return=e.return,e=e.sibling}r&=1}if(it(ft,r),!(t.mode&1))t.memoizedState=null;else switch(i){case"forwards":for(n=t.child,i=null;n!==null;)e=n.alternate,e!==null&&Vd(e)===null&&(i=n),n=n.sibling;n=i,n===null?(i=t.child,t.child=null):(i=n.sibling,n.sibling=null),Jm(t,!1,i,n,a);break;case"backwards":for(n=null,i=t.child,t.child=null;i!==null;){if(e=i.alternate,e!==null&&Vd(e)===null){t.child=i;break}e=i.sibling,i.sibling=n,n=i,i=e}Jm(t,!0,n,null,a);break;case"together":Jm(t,!1,null,null,void 0);break;default:t.memoizedState=null}return t.child}function sd(e,t){!(t.mode&1)&&e!==null&&(e.alternate=null,t.alternate=null,t.flags|=2)}function Pi(e,t,n){if(e!==null&&(t.dependencies=e.dependencies),ps|=t.lanes,!(n&t.childLanes))return null;if(e!==null&&t.child!==e.child)throw Error(le(153));if(t.child!==null){for(e=t.child,n=_a(e,e.pendingProps),t.child=n,n.return=t;e.sibling!==null;)e=e.sibling,n=n.sibling=_a(e,e.pendingProps),n.return=t;n.sibling=null}return t.child}function IS(e,t,n){switch(t.tag){case 3:P8(t),To();break;case 5:s8(t);break;case 1:Cn(t.type)&&Rd(t);break;case 4:Vg(t,t.stateNode.containerInfo);break;case 10:var r=t.type._context,i=t.memoizedProps.value;it(Bd,r._currentValue),r._currentValue=i;break;case 13:if(r=t.memoizedState,r!==null)return r.dehydrated!==null?(it(ft,ft.current&1),t.flags|=128,null):n&t.child.childLanes?I8(e,t,n):(it(ft,ft.current&1),e=Pi(e,t,n),e!==null?e.sibling:null);it(ft,ft.current&1);break;case 19:if(r=(n&t.childLanes)!==0,e.flags&128){if(r)return U8(e,t,n);t.flags|=128}if(i=t.memoizedState,i!==null&&(i.rendering=null,i.tail=null,i.lastEffect=null),it(ft,ft.current),r)break;return null;case 22:case 23:return t.lanes=0,A8(e,t,n)}return Pi(e,t,n)}var D8,Hp,M8,R8;D8=function(e,t){for(var n=t.child;n!==null;){if(n.tag===5||n.tag===6)e.appendChild(n.stateNode);else if(n.tag!==4&&n.child!==null){n.child.return=n,n=n.child;continue}if(n===t)break;for(;n.sibling===null;){if(n.return===null||n.return===t)return;n=n.return}n.sibling.return=n.return,n=n.sibling}};Hp=function(){};M8=function(e,t,n,r){var i=e.memoizedProps;if(i!==r){e=t.stateNode,Za(ri.current);var a=null;switch(n){case"input":i=mp(e,i),r=mp(e,r),a=[];break;case"select":i=$t({},i,{value:void 0}),r=$t({},r,{value:void 0}),a=[];break;case"textarea":i=fp(e,i),r=fp(e,r),a=[];break;default:typeof i.onClick!="function"&&typeof r.onClick=="function"&&(e.onclick=Dd)}bp(n,r);var s;n=null;for(c in i)if(!r.hasOwnProperty(c)&&i.hasOwnProperty(c)&&i[c]!=null)if(c==="style"){var o=i[c];for(s in o)o.hasOwnProperty(s)&&(n||(n={}),n[s]="")}else c!=="dangerouslySetInnerHTML"&&c!=="children"&&c!=="suppressContentEditableWarning"&&c!=="suppressHydrationWarning"&&c!=="autoFocus"&&(Ql.hasOwnProperty(c)?a||(a=[]):(a=a||[]).push(c,null));for(c in r){var l=r[c];if(o=i!=null?i[c]:void 0,r.hasOwnProperty(c)&&l!==o&&(l!=null||o!=null))if(c==="style")if(o){for(s in o)!o.hasOwnProperty(s)||l&&l.hasOwnProperty(s)||(n||(n={}),n[s]="");for(s in l)l.hasOwnProperty(s)&&o[s]!==l[s]&&(n||(n={}),n[s]=l[s])}else n||(a||(a=[]),a.push(c,n)),n=l;else c==="dangerouslySetInnerHTML"?(l=l?l.__html:void 0,o=o?o.__html:void 0,l!=null&&o!==l&&(a=a||[]).push(c,l)):c==="children"?typeof l!="string"&&typeof l!="number"||(a=a||[]).push(c,""+l):c!=="suppressContentEditableWarning"&&c!=="suppressHydrationWarning"&&(Ql.hasOwnProperty(c)?(l!=null&&c==="onScroll"&&ct("scroll",e),a||o===l||(a=[])):(a=a||[]).push(c,l))}n&&(a=a||[]).push("style",n);var c=a;(t.updateQueue=c)&&(t.flags|=4)}};R8=function(e,t,n,r){n!==r&&(t.flags|=4)};function cl(e,t){if(!mt)switch(e.tailMode){case"hidden":t=e.tail;for(var n=null;t!==null;)t.alternate!==null&&(n=t),t=t.sibling;n===null?e.tail=null:n.sibling=null;break;case"collapsed":n=e.tail;for(var r=null;n!==null;)n.alternate!==null&&(r=n),n=n.sibling;r===null?t||e.tail===null?e.tail=null:e.tail.sibling=null:r.sibling=null}}function Jt(e){var t=e.alternate!==null&&e.alternate.child===e.child,n=0,r=0;if(t)for(var i=e.child;i!==null;)n|=i.lanes|i.childLanes,r|=i.subtreeFlags&14680064,r|=i.flags&14680064,i.return=e,i=i.sibling;else for(i=e.child;i!==null;)n|=i.lanes|i.childLanes,r|=i.subtreeFlags,r|=i.flags,i.return=e,i=i.sibling;return e.subtreeFlags|=r,e.childLanes=n,t}function US(e,t,n){var r=t.pendingProps;switch(Mg(t),t.tag){case 2:case 16:case 15:case 0:case 11:case 7:case 8:case 12:case 9:case 14:return Jt(t),null;case 1:return Cn(t.type)&&Md(),Jt(t),null;case 3:return r=t.stateNode,ko(),ut(Nn),ut(an),qg(),r.pendingContext&&(r.context=r.pendingContext,r.pendingContext=null),(e===null||e.child===null)&&(xu(t)?t.flags|=4:e===null||e.memoizedState.isDehydrated&&!(t.flags&256)||(t.flags|=1024,Sr!==null&&(Zp(Sr),Sr=null))),Hp(e,t),Jt(t),null;case 5:Hg(t);var i=Za(cc.current);if(n=t.type,e!==null&&t.stateNode!=null)M8(e,t,n,r,i),e.ref!==t.ref&&(t.flags|=512,t.flags|=2097152);else{if(!r){if(t.stateNode===null)throw Error(le(166));return Jt(t),null}if(e=Za(ri.current),xu(t)){r=t.stateNode,n=t.type;var a=t.memoizedProps;switch(r[Xr]=t,r[oc]=a,e=(t.mode&1)!==0,n){case"dialog":ct("cancel",r),ct("close",r);break;case"iframe":case"object":case"embed":ct("load",r);break;case"video":case"audio":for(i=0;i<yl.length;i++)ct(yl[i],r);break;case"source":ct("error",r);break;case"img":case"image":case"link":ct("error",r),ct("load",r);break;case"details":ct("toggle",r);break;case"input":xb(r,a),ct("invalid",r);break;case"select":r._wrapperState={wasMultiple:!!a.multiple},ct("invalid",r);break;case"textarea":yb(r,a),ct("invalid",r)}bp(n,a),i=null;for(var s in a)if(a.hasOwnProperty(s)){var o=a[s];s==="children"?typeof o=="string"?r.textContent!==o&&(a.suppressHydrationWarning!==!0&&vu(r.textContent,o,e),i=["children",o]):typeof o=="number"&&r.textContent!==""+o&&(a.suppressHydrationWarning!==!0&&vu(r.textContent,o,e),i=["children",""+o]):Ql.hasOwnProperty(s)&&o!=null&&s==="onScroll"&&ct("scroll",r)}switch(n){case"input":uu(r),$b(r,a,!0);break;case"textarea":uu(r),_b(r);break;case"select":case"option":break;default:typeof a.onClick=="function"&&(r.onclick=Dd)}r=i,t.updateQueue=r,r!==null&&(t.flags|=4)}else{s=i.nodeType===9?i:i.ownerDocument,e==="http://www.w3.org/1999/xhtml"&&(e=dy(n)),e==="http://www.w3.org/1999/xhtml"?n==="script"?(e=s.createElement("div"),e.innerHTML="<script><\/script>",e=e.removeChild(e.firstChild)):typeof r.is=="string"?e=s.createElement(n,{is:r.is}):(e=s.createElement(n),n==="select"&&(s=e,r.multiple?s.multiple=!0:r.size&&(s.size=r.size))):e=s.createElementNS(e,n),e[Xr]=t,e[oc]=r,D8(e,t,!1,!1),t.stateNode=e;e:{switch(s=vp(n,r),n){case"dialog":ct("cancel",e),ct("close",e),i=r;break;case"iframe":case"object":case"embed":ct("load",e),i=r;break;case"video":case"audio":for(i=0;i<yl.length;i++)ct(yl[i],e);i=r;break;case"source":ct("error",e),i=r;break;case"img":case"image":case"link":ct("error",e),ct("load",e),i=r;break;case"details":ct("toggle",e),i=r;break;case"input":xb(e,r),i=mp(e,r),ct("invalid",e);break;case"option":i=r;break;case"select":e._wrapperState={wasMultiple:!!r.multiple},i=$t({},r,{value:void 0}),ct("invalid",e);break;case"textarea":yb(e,r),i=fp(e,r),ct("invalid",e);break;default:i=r}bp(n,i),o=i;for(a in o)if(o.hasOwnProperty(a)){var l=o[a];a==="style"?py(e,l):a==="dangerouslySetInnerHTML"?(l=l?l.__html:void 0,l!=null&&my(e,l)):a==="children"?typeof l=="string"?(n!=="textarea"||l!=="")&&Zl(e,l):typeof l=="number"&&Zl(e,""+l):a!=="suppressContentEditableWarning"&&a!=="suppressHydrationWarning"&&a!=="autoFocus"&&(Ql.hasOwnProperty(a)?l!=null&&a==="onScroll"&&ct("scroll",e):l!=null&&yg(e,a,l,s))}switch(n){case"input":uu(e),$b(e,r,!1);break;case"textarea":uu(e),_b(e);break;case"option":r.value!=null&&e.setAttribute("value",""+wa(r.value));break;case"select":e.multiple=!!r.multiple,a=r.value,a!=null?Js(e,!!r.multiple,a,!1):r.defaultValue!=null&&Js(e,!!r.multiple,r.defaultValue,!0);break;default:typeof i.onClick=="function"&&(e.onclick=Dd)}switch(n){case"button":case"input":case"select":case"textarea":r=!!r.autoFocus;break e;case"img":r=!0;break e;default:r=!1}}r&&(t.flags|=4)}t.ref!==null&&(t.flags|=512,t.flags|=2097152)}return Jt(t),null;case 6:if(e&&t.stateNode!=null)R8(e,t,e.memoizedProps,r);else{if(typeof r!="string"&&t.stateNode===null)throw Error(le(166));if(n=Za(cc.current),Za(ri.current),xu(t)){if(r=t.stateNode,n=t.memoizedProps,r[Xr]=t,(a=r.nodeValue!==n)&&(e=Gn,e!==null))switch(e.tag){case 3:vu(r.nodeValue,n,(e.mode&1)!==0);break;case 5:e.memoizedProps.suppressHydrationWarning!==!0&&vu(r.nodeValue,n,(e.mode&1)!==0)}a&&(t.flags|=4)}else r=(n.nodeType===9?n:n.ownerDocument).createTextNode(r),r[Xr]=t,t.stateNode=r}return Jt(t),null;case 13:if(ut(ft),r=t.memoizedState,e===null||e.memoizedState!==null&&e.memoizedState.dehydrated!==null){if(mt&&Hn!==null&&t.mode&1&&!(t.flags&128))t8(),To(),t.flags|=98560,a=!1;else if(a=xu(t),r!==null&&r.dehydrated!==null){if(e===null){if(!a)throw Error(le(318));if(a=t.memoizedState,a=a!==null?a.dehydrated:null,!a)throw Error(le(317));a[Xr]=t}else To(),!(t.flags&128)&&(t.memoizedState=null),t.flags|=4;Jt(t),a=!1}else Sr!==null&&(Zp(Sr),Sr=null),a=!0;if(!a)return t.flags&65536?t:null}return t.flags&128?(t.lanes=n,t):(r=r!==null,r!==(e!==null&&e.memoizedState!==null)&&r&&(t.child.flags|=8192,t.mode&1&&(e===null||ft.current&1?Ot===0&&(Ot=3):r1())),t.updateQueue!==null&&(t.flags|=4),Jt(t),null);case 4:return ko(),Hp(e,t),e===null&&ac(t.stateNode.containerInfo),Jt(t),null;case 10:return Bg(t.type._context),Jt(t),null;case 17:return Cn(t.type)&&Md(),Jt(t),null;case 19:if(ut(ft),a=t.memoizedState,a===null)return Jt(t),null;if(r=(t.flags&128)!==0,s=a.rendering,s===null)if(r)cl(a,!1);else{if(Ot!==0||e!==null&&e.flags&128)for(e=t.child;e!==null;){if(s=Vd(e),s!==null){for(t.flags|=128,cl(a,!1),r=s.updateQueue,r!==null&&(t.updateQueue=r,t.flags|=4),t.subtreeFlags=0,r=n,n=t.child;n!==null;)a=n,e=r,a.flags&=14680066,s=a.alternate,s===null?(a.childLanes=0,a.lanes=e,a.child=null,a.subtreeFlags=0,a.memoizedProps=null,a.memoizedState=null,a.updateQueue=null,a.dependencies=null,a.stateNode=null):(a.childLanes=s.childLanes,a.lanes=s.lanes,a.child=s.child,a.subtreeFlags=0,a.deletions=null,a.memoizedProps=s.memoizedProps,a.memoizedState=s.memoizedState,a.updateQueue=s.updateQueue,a.type=s.type,e=s.dependencies,a.dependencies=e===null?null:{lanes:e.lanes,firstContext:e.firstContext}),n=n.sibling;return it(ft,ft.current&1|2),t.child}e=e.sibling}a.tail!==null&&Nt()>No&&(t.flags|=128,r=!0,cl(a,!1),t.lanes=4194304)}else{if(!r)if(e=Vd(s),e!==null){if(t.flags|=128,r=!0,n=e.updateQueue,n!==null&&(t.updateQueue=n,t.flags|=4),cl(a,!0),a.tail===null&&a.tailMode==="hidden"&&!s.alternate&&!mt)return Jt(t),null}else 2*Nt()-a.renderingStartTime>No&&n!==1073741824&&(t.flags|=128,r=!0,cl(a,!1),t.lanes=4194304);a.isBackwards?(s.sibling=t.child,t.child=s):(n=a.last,n!==null?n.sibling=s:t.child=s,a.last=s)}return a.tail!==null?(t=a.tail,a.rendering=t,a.tail=t.sibling,a.renderingStartTime=Nt(),t.sibling=null,n=ft.current,it(ft,r?n&1|2:n&1),t):(Jt(t),null);case 22:case 23:return n1(),r=t.memoizedState!==null,e!==null&&e.memoizedState!==null!==r&&(t.flags|=8192),r&&t.mode&1?Bn&1073741824&&(Jt(t),t.subtreeFlags&6&&(t.flags|=8192)):Jt(t),null;case 24:return null;case 25:return null}throw Error(le(156,t.tag))}function DS(e,t){switch(Mg(t),t.tag){case 1:return Cn(t.type)&&Md(),e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 3:return ko(),ut(Nn),ut(an),qg(),e=t.flags,e&65536&&!(e&128)?(t.flags=e&-65537|128,t):null;case 5:return Hg(t),null;case 13:if(ut(ft),e=t.memoizedState,e!==null&&e.dehydrated!==null){if(t.alternate===null)throw Error(le(340));To()}return e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 19:return ut(ft),null;case 4:return ko(),null;case 10:return Bg(t.type._context),null;case 22:case 23:return n1(),null;case 24:return null;default:return null}}var _u=!1,nn=!1,MS=typeof WeakSet=="function"?WeakSet:Set,ve=null;function qs(e,t){var n=e.ref;if(n!==null)if(typeof n=="function")try{n(null)}catch(r){kt(e,t,r)}else n.current=null}function qp(e,t,n){try{n()}catch(r){kt(e,t,r)}}var dv=!1;function RS(e,t){if(Np=Pd,e=jy(),Ug(e)){if("selectionStart"in e)var n={start:e.selectionStart,end:e.selectionEnd};else e:{n=(n=e.ownerDocument)&&n.defaultView||window;var r=n.getSelection&&n.getSelection();if(r&&r.rangeCount!==0){n=r.anchorNode;var i=r.anchorOffset,a=r.focusNode;r=r.focusOffset;try{n.nodeType,a.nodeType}catch{n=null;break e}var s=0,o=-1,l=-1,c=0,u=0,d=e,m=null;t:for(;;){for(var p;d!==n||i!==0&&d.nodeType!==3||(o=s+i),d!==a||r!==0&&d.nodeType!==3||(l=s+r),d.nodeType===3&&(s+=d.nodeValue.length),(p=d.firstChild)!==null;)m=d,d=p;for(;;){if(d===e)break t;if(m===n&&++c===i&&(o=s),m===a&&++u===r&&(l=s),(p=d.nextSibling)!==null)break;d=m,m=d.parentNode}d=p}n=o===-1||l===-1?null:{start:o,end:l}}else n=null}n=n||{start:0,end:0}}else n=null;for(Cp={focusedElem:e,selectionRange:n},Pd=!1,ve=t;ve!==null;)if(t=ve,e=t.child,(t.subtreeFlags&1028)!==0&&e!==null)e.return=t,ve=e;else for(;ve!==null;){t=ve;try{var x=t.alternate;if(t.flags&1024)switch(t.tag){case 0:case 11:case 15:break;case 1:if(x!==null){var g=x.memoizedProps,w=x.memoizedState,v=t.stateNode,$=v.getSnapshotBeforeUpdate(t.elementType===t.type?g:yr(t.type,g),w);v.__reactInternalSnapshotBeforeUpdate=$}break;case 3:var _=t.stateNode.containerInfo;_.nodeType===1?_.textContent="":_.nodeType===9&&_.documentElement&&_.removeChild(_.documentElement);break;case 5:case 6:case 4:case 17:break;default:throw Error(le(163))}}catch(C){kt(t,t.return,C)}if(e=t.sibling,e!==null){e.return=t.return,ve=e;break}ve=t.return}return x=dv,dv=!1,x}function Il(e,t,n){var r=t.updateQueue;if(r=r!==null?r.lastEffect:null,r!==null){var i=r=r.next;do{if((i.tag&e)===e){var a=i.destroy;i.destroy=void 0,a!==void 0&&qp(t,n,a)}i=i.next}while(i!==r)}}function C0(e,t){if(t=t.updateQueue,t=t!==null?t.lastEffect:null,t!==null){var n=t=t.next;do{if((n.tag&e)===e){var r=n.create;n.destroy=r()}n=n.next}while(n!==t)}}function Gp(e){var t=e.ref;if(t!==null){var n=e.stateNode;switch(e.tag){case 5:e=n;break;default:e=n}typeof t=="function"?t(e):t.current=e}}function O8(e){var t=e.alternate;t!==null&&(e.alternate=null,O8(t)),e.child=null,e.deletions=null,e.sibling=null,e.tag===5&&(t=e.stateNode,t!==null&&(delete t[Xr],delete t[oc],delete t[Pp],delete t[xS],delete t[$S])),e.stateNode=null,e.return=null,e.dependencies=null,e.memoizedProps=null,e.memoizedState=null,e.pendingProps=null,e.stateNode=null,e.updateQueue=null}function z8(e){return e.tag===5||e.tag===3||e.tag===4}function mv(e){e:for(;;){for(;e.sibling===null;){if(e.return===null||z8(e.return))return null;e=e.return}for(e.sibling.return=e.return,e=e.sibling;e.tag!==5&&e.tag!==6&&e.tag!==18;){if(e.flags&2||e.child===null||e.tag===4)continue e;e.child.return=e,e=e.child}if(!(e.flags&2))return e.stateNode}}function Wp(e,t,n){var r=e.tag;if(r===5||r===6)e=e.stateNode,t?n.nodeType===8?n.parentNode.insertBefore(e,t):n.insertBefore(e,t):(n.nodeType===8?(t=n.parentNode,t.insertBefore(e,n)):(t=n,t.appendChild(e)),n=n._reactRootContainer,n!=null||t.onclick!==null||(t.onclick=Dd));else if(r!==4&&(e=e.child,e!==null))for(Wp(e,t,n),e=e.sibling;e!==null;)Wp(e,t,n),e=e.sibling}function Kp(e,t,n){var r=e.tag;if(r===5||r===6)e=e.stateNode,t?n.insertBefore(e,t):n.appendChild(e);else if(r!==4&&(e=e.child,e!==null))for(Kp(e,t,n),e=e.sibling;e!==null;)Kp(e,t,n),e=e.sibling}var Gt=null,Er=!1;function Vi(e,t,n){for(n=n.child;n!==null;)B8(e,t,n),n=n.sibling}function B8(e,t,n){if(ni&&typeof ni.onCommitFiberUnmount=="function")try{ni.onCommitFiberUnmount(y0,n)}catch{}switch(n.tag){case 5:nn||qs(n,t);case 6:var r=Gt,i=Er;Gt=null,Vi(e,t,n),Gt=r,Er=i,Gt!==null&&(Er?(e=Gt,n=n.stateNode,e.nodeType===8?e.parentNode.removeChild(n):e.removeChild(n)):Gt.removeChild(n.stateNode));break;case 18:Gt!==null&&(Er?(e=Gt,n=n.stateNode,e.nodeType===8?Gm(e.parentNode,n):e.nodeType===1&&Gm(e,n),nc(e)):Gm(Gt,n.stateNode));break;case 4:r=Gt,i=Er,Gt=n.stateNode.containerInfo,Er=!0,Vi(e,t,n),Gt=r,Er=i;break;case 0:case 11:case 14:case 15:if(!nn&&(r=n.updateQueue,r!==null&&(r=r.lastEffect,r!==null))){i=r=r.next;do{var a=i,s=a.destroy;a=a.tag,s!==void 0&&(a&2||a&4)&&qp(n,t,s),i=i.next}while(i!==r)}Vi(e,t,n);break;case 1:if(!nn&&(qs(n,t),r=n.stateNode,typeof r.componentWillUnmount=="function"))try{r.props=n.memoizedProps,r.state=n.memoizedState,r.componentWillUnmount()}catch(o){kt(n,t,o)}Vi(e,t,n);break;case 21:Vi(e,t,n);break;case 22:n.mode&1?(nn=(r=nn)||n.memoizedState!==null,Vi(e,t,n),nn=r):Vi(e,t,n);break;default:Vi(e,t,n)}}function hv(e){var t=e.updateQueue;if(t!==null){e.updateQueue=null;var n=e.stateNode;n===null&&(n=e.stateNode=new MS),t.forEach(function(r){var i=GS.bind(null,e,r);n.has(r)||(n.add(r),r.then(i,i))})}}function br(e,t){var n=t.deletions;if(n!==null)for(var r=0;r<n.length;r++){var i=n[r];try{var a=e,s=t,o=s;e:for(;o!==null;){switch(o.tag){case 5:Gt=o.stateNode,Er=!1;break e;case 3:Gt=o.stateNode.containerInfo,Er=!0;break e;case 4:Gt=o.stateNode.containerInfo,Er=!0;break e}o=o.return}if(Gt===null)throw Error(le(160));B8(a,s,i),Gt=null,Er=!1;var l=i.alternate;l!==null&&(l.return=null),i.return=null}catch(c){kt(i,t,c)}}if(t.subtreeFlags&12854)for(t=t.child;t!==null;)F8(t,e),t=t.sibling}function F8(e,t){var n=e.alternate,r=e.flags;switch(e.tag){case 0:case 11:case 14:case 15:if(br(t,e),Or(e),r&4){try{Il(3,e,e.return),C0(3,e)}catch(g){kt(e,e.return,g)}try{Il(5,e,e.return)}catch(g){kt(e,e.return,g)}}break;case 1:br(t,e),Or(e),r&512&&n!==null&&qs(n,n.return);break;case 5:if(br(t,e),Or(e),r&512&&n!==null&&qs(n,n.return),e.flags&32){var i=e.stateNode;try{Zl(i,"")}catch(g){kt(e,e.return,g)}}if(r&4&&(i=e.stateNode,i!=null)){var a=e.memoizedProps,s=n!==null?n.memoizedProps:a,o=e.type,l=e.updateQueue;if(e.updateQueue=null,l!==null)try{o==="input"&&a.type==="radio"&&a.name!=null&&cy(i,a),vp(o,s);var c=vp(o,a);for(s=0;s<l.length;s+=2){var u=l[s],d=l[s+1];u==="style"?py(i,d):u==="dangerouslySetInnerHTML"?my(i,d):u==="children"?Zl(i,d):yg(i,u,d,c)}switch(o){case"input":hp(i,a);break;case"textarea":uy(i,a);break;case"select":var m=i._wrapperState.wasMultiple;i._wrapperState.wasMultiple=!!a.multiple;var p=a.value;p!=null?Js(i,!!a.multiple,p,!1):m!==!!a.multiple&&(a.defaultValue!=null?Js(i,!!a.multiple,a.defaultValue,!0):Js(i,!!a.multiple,a.multiple?[]:"",!1))}i[oc]=a}catch(g){kt(e,e.return,g)}}break;case 6:if(br(t,e),Or(e),r&4){if(e.stateNode===null)throw Error(le(162));i=e.stateNode,a=e.memoizedProps;try{i.nodeValue=a}catch(g){kt(e,e.return,g)}}break;case 3:if(br(t,e),Or(e),r&4&&n!==null&&n.memoizedState.isDehydrated)try{nc(t.containerInfo)}catch(g){kt(e,e.return,g)}break;case 4:br(t,e),Or(e);break;case 13:br(t,e),Or(e),i=e.child,i.flags&8192&&(a=i.memoizedState!==null,i.stateNode.isHidden=a,!a||i.alternate!==null&&i.alternate.memoizedState!==null||(e1=Nt())),r&4&&hv(e);break;case 22:if(u=n!==null&&n.memoizedState!==null,e.mode&1?(nn=(c=nn)||u,br(t,e),nn=c):br(t,e),Or(e),r&8192){if(c=e.memoizedState!==null,(e.stateNode.isHidden=c)&&!u&&e.mode&1)for(ve=e,u=e.child;u!==null;){for(d=ve=u;ve!==null;){switch(m=ve,p=m.child,m.tag){case 0:case 11:case 14:case 15:Il(4,m,m.return);break;case 1:qs(m,m.return);var x=m.stateNode;if(typeof x.componentWillUnmount=="function"){r=m,n=m.return;try{t=r,x.props=t.memoizedProps,x.state=t.memoizedState,x.componentWillUnmount()}catch(g){kt(r,n,g)}}break;case 5:qs(m,m.return);break;case 22:if(m.memoizedState!==null){fv(d);continue}}p!==null?(p.return=m,ve=p):fv(d)}u=u.sibling}e:for(u=null,d=e;;){if(d.tag===5){if(u===null){u=d;try{i=d.stateNode,c?(a=i.style,typeof a.setProperty=="function"?a.setProperty("display","none","important"):a.display="none"):(o=d.stateNode,l=d.memoizedProps.style,s=l!=null&&l.hasOwnProperty("display")?l.display:null,o.style.display=hy("display",s))}catch(g){kt(e,e.return,g)}}}else if(d.tag===6){if(u===null)try{d.stateNode.nodeValue=c?"":d.memoizedProps}catch(g){kt(e,e.return,g)}}else if((d.tag!==22&&d.tag!==23||d.memoizedState===null||d===e)&&d.child!==null){d.child.return=d,d=d.child;continue}if(d===e)break e;for(;d.sibling===null;){if(d.return===null||d.return===e)break e;u===d&&(u=null),d=d.return}u===d&&(u=null),d.sibling.return=d.return,d=d.sibling}}break;case 19:br(t,e),Or(e),r&4&&hv(e);break;case 21:break;default:br(t,e),Or(e)}}function Or(e){var t=e.flags;if(t&2){try{e:{for(var n=e.return;n!==null;){if(z8(n)){var r=n;break e}n=n.return}throw Error(le(160))}switch(r.tag){case 5:var i=r.stateNode;r.flags&32&&(Zl(i,""),r.flags&=-33);var a=mv(e);Kp(e,a,i);break;case 3:case 4:var s=r.stateNode.containerInfo,o=mv(e);Wp(e,o,s);break;default:throw Error(le(161))}}catch(l){kt(e,e.return,l)}e.flags&=-3}t&4096&&(e.flags&=-4097)}function OS(e,t,n){ve=e,j8(e)}function j8(e,t,n){for(var r=(e.mode&1)!==0;ve!==null;){var i=ve,a=i.child;if(i.tag===22&&r){var s=i.memoizedState!==null||_u;if(!s){var o=i.alternate,l=o!==null&&o.memoizedState!==null||nn;o=_u;var c=nn;if(_u=s,(nn=l)&&!c)for(ve=i;ve!==null;)s=ve,l=s.child,s.tag===22&&s.memoizedState!==null?gv(i):l!==null?(l.return=s,ve=l):gv(i);for(;a!==null;)ve=a,j8(a),a=a.sibling;ve=i,_u=o,nn=c}pv(e)}else i.subtreeFlags&8772&&a!==null?(a.return=i,ve=a):pv(e)}}function pv(e){for(;ve!==null;){var t=ve;if(t.flags&8772){var n=t.alternate;try{if(t.flags&8772)switch(t.tag){case 0:case 11:case 15:nn||C0(5,t);break;case 1:var r=t.stateNode;if(t.flags&4&&!nn)if(n===null)r.componentDidMount();else{var i=t.elementType===t.type?n.memoizedProps:yr(t.type,n.memoizedProps);r.componentDidUpdate(i,n.memoizedState,r.__reactInternalSnapshotBeforeUpdate)}var a=t.updateQueue;a!==null&&Zb(t,a,r);break;case 3:var s=t.updateQueue;if(s!==null){if(n=null,t.child!==null)switch(t.child.tag){case 5:n=t.child.stateNode;break;case 1:n=t.child.stateNode}Zb(t,s,n)}break;case 5:var o=t.stateNode;if(n===null&&t.flags&4){n=o;var l=t.memoizedProps;switch(t.type){case"button":case"input":case"select":case"textarea":l.autoFocus&&n.focus();break;case"img":l.src&&(n.src=l.src)}}break;case 6:break;case 4:break;case 12:break;case 13:if(t.memoizedState===null){var c=t.alternate;if(c!==null){var u=c.memoizedState;if(u!==null){var d=u.dehydrated;d!==null&&nc(d)}}}break;case 19:case 17:case 21:case 22:case 23:case 25:break;default:throw Error(le(163))}nn||t.flags&512&&Gp(t)}catch(m){kt(t,t.return,m)}}if(t===e){ve=null;break}if(n=t.sibling,n!==null){n.return=t.return,ve=n;break}ve=t.return}}function fv(e){for(;ve!==null;){var t=ve;if(t===e){ve=null;break}var n=t.sibling;if(n!==null){n.return=t.return,ve=n;break}ve=t.return}}function gv(e){for(;ve!==null;){var t=ve;try{switch(t.tag){case 0:case 11:case 15:var n=t.return;try{C0(4,t)}catch(l){kt(t,n,l)}break;case 1:var r=t.stateNode;if(typeof r.componentDidMount=="function"){var i=t.return;try{r.componentDidMount()}catch(l){kt(t,i,l)}}var a=t.return;try{Gp(t)}catch(l){kt(t,a,l)}break;case 5:var s=t.return;try{Gp(t)}catch(l){kt(t,s,l)}}}catch(l){kt(t,t.return,l)}if(t===e){ve=null;break}var o=t.sibling;if(o!==null){o.return=t.return,ve=o;break}ve=t.return}}var zS=Math.ceil,Gd=Bi.ReactCurrentDispatcher,Zg=Bi.ReactCurrentOwner,cr=Bi.ReactCurrentBatchConfig,qe=0,qt=null,It=null,Yt=0,Bn=0,Gs=Da(0),Ot=0,hc=null,ps=0,A0=0,Jg=0,Ul=null,En=null,e1=0,No=1/0,xi=null,Wd=!1,Yp=null,$a=null,wu=!1,ua=null,Kd=0,Dl=0,Xp=null,od=-1,ld=0;function fn(){return qe&6?Nt():od!==-1?od:od=Nt()}function ya(e){return e.mode&1?qe&2&&Yt!==0?Yt&-Yt:_S.transition!==null?(ld===0&&(ld=ky()),ld):(e=Je,e!==0||(e=window.event,e=e===void 0?16:Iy(e.type)),e):1}function Cr(e,t,n,r){if(50<Dl)throw Dl=0,Xp=null,Error(le(185));Oc(e,n,r),(!(qe&2)||e!==qt)&&(e===qt&&(!(qe&2)&&(A0|=n),Ot===4&&ra(e,Yt)),An(e,r),n===1&&qe===0&&!(t.mode&1)&&(No=Nt()+500,k0&&Ma()))}function An(e,t){var n=e.callbackNode;_k(e,t);var r=Ld(e,e===qt?Yt:0);if(r===0)n!==null&&Eb(n),e.callbackNode=null,e.callbackPriority=0;else if(t=r&-r,e.callbackPriority!==t){if(n!=null&&Eb(n),t===1)e.tag===0?yS(bv.bind(null,e)):Zy(bv.bind(null,e)),bS(function(){!(qe&6)&&Ma()}),n=null;else{switch(Sy(r)){case 1:n=kg;break;case 4:n=Ty;break;case 16:n=Ad;break;case 536870912:n=Ey;break;default:n=Ad}n=X8(n,V8.bind(null,e))}e.callbackPriority=t,e.callbackNode=n}}function V8(e,t){if(od=-1,ld=0,qe&6)throw Error(le(327));var n=e.callbackNode;if(io()&&e.callbackNode!==n)return null;var r=Ld(e,e===qt?Yt:0);if(r===0)return null;if(r&30||r&e.expiredLanes||t)t=Yd(e,r);else{t=r;var i=qe;qe|=2;var a=q8();(qt!==e||Yt!==t)&&(xi=null,No=Nt()+500,as(e,t));do try{jS();break}catch(o){H8(e,o)}while(!0);zg(),Gd.current=a,qe=i,It!==null?t=0:(qt=null,Yt=0,t=Ot)}if(t!==0){if(t===2&&(i=wp(e),i!==0&&(r=i,t=Qp(e,i))),t===1)throw n=hc,as(e,0),ra(e,r),An(e,Nt()),n;if(t===6)ra(e,r);else{if(i=e.current.alternate,!(r&30)&&!BS(i)&&(t=Yd(e,r),t===2&&(a=wp(e),a!==0&&(r=a,t=Qp(e,a))),t===1))throw n=hc,as(e,0),ra(e,r),An(e,Nt()),n;switch(e.finishedWork=i,e.finishedLanes=r,t){case 0:case 1:throw Error(le(345));case 2:Wa(e,En,xi);break;case 3:if(ra(e,r),(r&130023424)===r&&(t=e1+500-Nt(),10<t)){if(Ld(e,0)!==0)break;if(i=e.suspendedLanes,(i&r)!==r){fn(),e.pingedLanes|=e.suspendedLanes&i;break}e.timeoutHandle=Lp(Wa.bind(null,e,En,xi),t);break}Wa(e,En,xi);break;case 4:if(ra(e,r),(r&4194240)===r)break;for(t=e.eventTimes,i=-1;0<r;){var s=31-Nr(r);a=1<<s,s=t[s],s>i&&(i=s),r&=~a}if(r=i,r=Nt()-r,r=(120>r?120:480>r?480:1080>r?1080:1920>r?1920:3e3>r?3e3:4320>r?4320:1960*zS(r/1960))-r,10<r){e.timeoutHandle=Lp(Wa.bind(null,e,En,xi),r);break}Wa(e,En,xi);break;case 5:Wa(e,En,xi);break;default:throw Error(le(329))}}}return An(e,Nt()),e.callbackNode===n?V8.bind(null,e):null}function Qp(e,t){var n=Ul;return e.current.memoizedState.isDehydrated&&(as(e,t).flags|=256),e=Yd(e,t),e!==2&&(t=En,En=n,t!==null&&Zp(t)),e}function Zp(e){En===null?En=e:En.push.apply(En,e)}function BS(e){for(var t=e;;){if(t.flags&16384){var n=t.updateQueue;if(n!==null&&(n=n.stores,n!==null))for(var r=0;r<n.length;r++){var i=n[r],a=i.getSnapshot;i=i.value;try{if(!Ar(a(),i))return!1}catch{return!1}}}if(n=t.child,t.subtreeFlags&16384&&n!==null)n.return=t,t=n;else{if(t===e)break;for(;t.sibling===null;){if(t.return===null||t.return===e)return!0;t=t.return}t.sibling.return=t.return,t=t.sibling}}return!0}function ra(e,t){for(t&=~Jg,t&=~A0,e.suspendedLanes|=t,e.pingedLanes&=~t,e=e.expirationTimes;0<t;){var n=31-Nr(t),r=1<<n;e[n]=-1,t&=~r}}function bv(e){if(qe&6)throw Error(le(327));io();var t=Ld(e,0);if(!(t&1))return An(e,Nt()),null;var n=Yd(e,t);if(e.tag!==0&&n===2){var r=wp(e);r!==0&&(t=r,n=Qp(e,r))}if(n===1)throw n=hc,as(e,0),ra(e,t),An(e,Nt()),n;if(n===6)throw Error(le(345));return e.finishedWork=e.current.alternate,e.finishedLanes=t,Wa(e,En,xi),An(e,Nt()),null}function t1(e,t){var n=qe;qe|=1;try{return e(t)}finally{qe=n,qe===0&&(No=Nt()+500,k0&&Ma())}}function fs(e){ua!==null&&ua.tag===0&&!(qe&6)&&io();var t=qe;qe|=1;var n=cr.transition,r=Je;try{if(cr.transition=null,Je=1,e)return e()}finally{Je=r,cr.transition=n,qe=t,!(qe&6)&&Ma()}}function n1(){Bn=Gs.current,ut(Gs)}function as(e,t){e.finishedWork=null,e.finishedLanes=0;var n=e.timeoutHandle;if(n!==-1&&(e.timeoutHandle=-1,gS(n)),It!==null)for(n=It.return;n!==null;){var r=n;switch(Mg(r),r.tag){case 1:r=r.type.childContextTypes,r!=null&&Md();break;case 3:ko(),ut(Nn),ut(an),qg();break;case 5:Hg(r);break;case 4:ko();break;case 13:ut(ft);break;case 19:ut(ft);break;case 10:Bg(r.type._context);break;case 22:case 23:n1()}n=n.return}if(qt=e,It=e=_a(e.current,null),Yt=Bn=t,Ot=0,hc=null,Jg=A0=ps=0,En=Ul=null,Qa!==null){for(t=0;t<Qa.length;t++)if(n=Qa[t],r=n.interleaved,r!==null){n.interleaved=null;var i=r.next,a=n.pending;if(a!==null){var s=a.next;a.next=i,r.next=s}n.pending=r}Qa=null}return e}function H8(e,t){do{var n=It;try{if(zg(),id.current=qd,Hd){for(var r=vt.memoizedState;r!==null;){var i=r.queue;i!==null&&(i.pending=null),r=r.next}Hd=!1}if(hs=0,Vt=Rt=vt=null,Pl=!1,uc=0,Zg.current=null,n===null||n.return===null){Ot=1,hc=t,It=null;break}e:{var a=e,s=n.return,o=n,l=t;if(t=Yt,o.flags|=32768,l!==null&&typeof l=="object"&&typeof l.then=="function"){var c=l,u=o,d=u.tag;if(!(u.mode&1)&&(d===0||d===11||d===15)){var m=u.alternate;m?(u.updateQueue=m.updateQueue,u.memoizedState=m.memoizedState,u.lanes=m.lanes):(u.updateQueue=null,u.memoizedState=null)}var p=iv(s);if(p!==null){p.flags&=-257,av(p,s,o,a,t),p.mode&1&&rv(a,c,t),t=p,l=c;var x=t.updateQueue;if(x===null){var g=new Set;g.add(l),t.updateQueue=g}else x.add(l);break e}else{if(!(t&1)){rv(a,c,t),r1();break e}l=Error(le(426))}}else if(mt&&o.mode&1){var w=iv(s);if(w!==null){!(w.flags&65536)&&(w.flags|=256),av(w,s,o,a,t),Rg(So(l,o));break e}}a=l=So(l,o),Ot!==4&&(Ot=2),Ul===null?Ul=[a]:Ul.push(a),a=s;do{switch(a.tag){case 3:a.flags|=65536,t&=-t,a.lanes|=t;var v=S8(a,l,t);Qb(a,v);break e;case 1:o=l;var $=a.type,_=a.stateNode;if(!(a.flags&128)&&(typeof $.getDerivedStateFromError=="function"||_!==null&&typeof _.componentDidCatch=="function"&&($a===null||!$a.has(_)))){a.flags|=65536,t&=-t,a.lanes|=t;var C=N8(a,o,t);Qb(a,C);break e}}a=a.return}while(a!==null)}W8(n)}catch(k){t=k,It===n&&n!==null&&(It=n=n.return);continue}break}while(!0)}function q8(){var e=Gd.current;return Gd.current=qd,e===null?qd:e}function r1(){(Ot===0||Ot===3||Ot===2)&&(Ot=4),qt===null||!(ps&268435455)&&!(A0&268435455)||ra(qt,Yt)}function Yd(e,t){var n=qe;qe|=2;var r=q8();(qt!==e||Yt!==t)&&(xi=null,as(e,t));do try{FS();break}catch(i){H8(e,i)}while(!0);if(zg(),qe=n,Gd.current=r,It!==null)throw Error(le(261));return qt=null,Yt=0,Ot}function FS(){for(;It!==null;)G8(It)}function jS(){for(;It!==null&&!hk();)G8(It)}function G8(e){var t=Y8(e.alternate,e,Bn);e.memoizedProps=e.pendingProps,t===null?W8(e):It=t,Zg.current=null}function W8(e){var t=e;do{var n=t.alternate;if(e=t.return,t.flags&32768){if(n=DS(n,t),n!==null){n.flags&=32767,It=n;return}if(e!==null)e.flags|=32768,e.subtreeFlags=0,e.deletions=null;else{Ot=6,It=null;return}}else if(n=US(n,t,Bn),n!==null){It=n;return}if(t=t.sibling,t!==null){It=t;return}It=t=e}while(t!==null);Ot===0&&(Ot=5)}function Wa(e,t,n){var r=Je,i=cr.transition;try{cr.transition=null,Je=1,VS(e,t,n,r)}finally{cr.transition=i,Je=r}return null}function VS(e,t,n,r){do io();while(ua!==null);if(qe&6)throw Error(le(327));n=e.finishedWork;var i=e.finishedLanes;if(n===null)return null;if(e.finishedWork=null,e.finishedLanes=0,n===e.current)throw Error(le(177));e.callbackNode=null,e.callbackPriority=0;var a=n.lanes|n.childLanes;if(wk(e,a),e===qt&&(It=qt=null,Yt=0),!(n.subtreeFlags&2064)&&!(n.flags&2064)||wu||(wu=!0,X8(Ad,function(){return io(),null})),a=(n.flags&15990)!==0,n.subtreeFlags&15990||a){a=cr.transition,cr.transition=null;var s=Je;Je=1;var o=qe;qe|=4,Zg.current=null,RS(e,n),F8(n,e),cS(Cp),Pd=!!Np,Cp=Np=null,e.current=n,OS(n),pk(),qe=o,Je=s,cr.transition=a}else e.current=n;if(wu&&(wu=!1,ua=e,Kd=i),a=e.pendingLanes,a===0&&($a=null),bk(n.stateNode),An(e,Nt()),t!==null)for(r=e.onRecoverableError,n=0;n<t.length;n++)i=t[n],r(i.value,{componentStack:i.stack,digest:i.digest});if(Wd)throw Wd=!1,e=Yp,Yp=null,e;return Kd&1&&e.tag!==0&&io(),a=e.pendingLanes,a&1?e===Xp?Dl++:(Dl=0,Xp=e):Dl=0,Ma(),null}function io(){if(ua!==null){var e=Sy(Kd),t=cr.transition,n=Je;try{if(cr.transition=null,Je=16>e?16:e,ua===null)var r=!1;else{if(e=ua,ua=null,Kd=0,qe&6)throw Error(le(331));var i=qe;for(qe|=4,ve=e.current;ve!==null;){var a=ve,s=a.child;if(ve.flags&16){var o=a.deletions;if(o!==null){for(var l=0;l<o.length;l++){var c=o[l];for(ve=c;ve!==null;){var u=ve;switch(u.tag){case 0:case 11:case 15:Il(8,u,a)}var d=u.child;if(d!==null)d.return=u,ve=d;else for(;ve!==null;){u=ve;var m=u.sibling,p=u.return;if(O8(u),u===c){ve=null;break}if(m!==null){m.return=p,ve=m;break}ve=p}}}var x=a.alternate;if(x!==null){var g=x.child;if(g!==null){x.child=null;do{var w=g.sibling;g.sibling=null,g=w}while(g!==null)}}ve=a}}if(a.subtreeFlags&2064&&s!==null)s.return=a,ve=s;else e:for(;ve!==null;){if(a=ve,a.flags&2048)switch(a.tag){case 0:case 11:case 15:Il(9,a,a.return)}var v=a.sibling;if(v!==null){v.return=a.return,ve=v;break e}ve=a.return}}var $=e.current;for(ve=$;ve!==null;){s=ve;var _=s.child;if(s.subtreeFlags&2064&&_!==null)_.return=s,ve=_;else e:for(s=$;ve!==null;){if(o=ve,o.flags&2048)try{switch(o.tag){case 0:case 11:case 15:C0(9,o)}}catch(k){kt(o,o.return,k)}if(o===s){ve=null;break e}var C=o.sibling;if(C!==null){C.return=o.return,ve=C;break e}ve=o.return}}if(qe=i,Ma(),ni&&typeof ni.onPostCommitFiberRoot=="function")try{ni.onPostCommitFiberRoot(y0,e)}catch{}r=!0}return r}finally{Je=n,cr.transition=t}}return!1}function vv(e,t,n){t=So(n,t),t=S8(e,t,1),e=xa(e,t,1),t=fn(),e!==null&&(Oc(e,1,t),An(e,t))}function kt(e,t,n){if(e.tag===3)vv(e,e,n);else for(;t!==null;){if(t.tag===3){vv(t,e,n);break}else if(t.tag===1){var r=t.stateNode;if(typeof t.type.getDerivedStateFromError=="function"||typeof r.componentDidCatch=="function"&&($a===null||!$a.has(r))){e=So(n,e),e=N8(t,e,1),t=xa(t,e,1),e=fn(),t!==null&&(Oc(t,1,e),An(t,e));break}}t=t.return}}function HS(e,t,n){var r=e.pingCache;r!==null&&r.delete(t),t=fn(),e.pingedLanes|=e.suspendedLanes&n,qt===e&&(Yt&n)===n&&(Ot===4||Ot===3&&(Yt&130023424)===Yt&&500>Nt()-e1?as(e,0):Jg|=n),An(e,t)}function K8(e,t){t===0&&(e.mode&1?(t=hu,hu<<=1,!(hu&130023424)&&(hu=4194304)):t=1);var n=fn();e=Li(e,t),e!==null&&(Oc(e,t,n),An(e,n))}function qS(e){var t=e.memoizedState,n=0;t!==null&&(n=t.retryLane),K8(e,n)}function GS(e,t){var n=0;switch(e.tag){case 13:var r=e.stateNode,i=e.memoizedState;i!==null&&(n=i.retryLane);break;case 19:r=e.stateNode;break;default:throw Error(le(314))}r!==null&&r.delete(t),K8(e,n)}var Y8;Y8=function(e,t,n){if(e!==null)if(e.memoizedProps!==t.pendingProps||Nn.current)kn=!0;else{if(!(e.lanes&n)&&!(t.flags&128))return kn=!1,IS(e,t,n);kn=!!(e.flags&131072)}else kn=!1,mt&&t.flags&1048576&&Jy(t,zd,t.index);switch(t.lanes=0,t.tag){case 2:var r=t.type;sd(e,t),e=t.pendingProps;var i=wo(t,an.current);ro(t,n),i=Wg(null,t,r,e,i,n);var a=Kg();return t.flags|=1,typeof i=="object"&&i!==null&&typeof i.render=="function"&&i.$$typeof===void 0?(t.tag=1,t.memoizedState=null,t.updateQueue=null,Cn(r)?(a=!0,Rd(t)):a=!1,t.memoizedState=i.state!==null&&i.state!==void 0?i.state:null,jg(t),i.updater=N0,t.stateNode=i,i._reactInternals=t,Op(t,r,e,n),t=Fp(null,t,r,!0,a,n)):(t.tag=0,mt&&a&&Dg(t),dn(null,t,i,n),t=t.child),t;case 16:r=t.elementType;e:{switch(sd(e,t),e=t.pendingProps,i=r._init,r=i(r._payload),t.type=r,i=t.tag=KS(r),e=yr(r,e),i){case 0:t=Bp(null,t,r,e,n);break e;case 1:t=lv(null,t,r,e,n);break e;case 11:t=sv(null,t,r,e,n);break e;case 14:t=ov(null,t,r,yr(r.type,e),n);break e}throw Error(le(306,r,""))}return t;case 0:return r=t.type,i=t.pendingProps,i=t.elementType===r?i:yr(r,i),Bp(e,t,r,i,n);case 1:return r=t.type,i=t.pendingProps,i=t.elementType===r?i:yr(r,i),lv(e,t,r,i,n);case 3:e:{if(P8(t),e===null)throw Error(le(387));r=t.pendingProps,a=t.memoizedState,i=a.element,a8(e,t),jd(t,r,null,n);var s=t.memoizedState;if(r=s.element,a.isDehydrated)if(a={element:r,isDehydrated:!1,cache:s.cache,pendingSuspenseBoundaries:s.pendingSuspenseBoundaries,transitions:s.transitions},t.updateQueue.baseState=a,t.memoizedState=a,t.flags&256){i=So(Error(le(423)),t),t=cv(e,t,r,n,i);break e}else if(r!==i){i=So(Error(le(424)),t),t=cv(e,t,r,n,i);break e}else for(Hn=va(t.stateNode.containerInfo.firstChild),Gn=t,mt=!0,Sr=null,n=r8(t,null,r,n),t.child=n;n;)n.flags=n.flags&-3|4096,n=n.sibling;else{if(To(),r===i){t=Pi(e,t,n);break e}dn(e,t,r,n)}t=t.child}return t;case 5:return s8(t),e===null&&Dp(t),r=t.type,i=t.pendingProps,a=e!==null?e.memoizedProps:null,s=i.children,Ap(r,i)?s=null:a!==null&&Ap(r,a)&&(t.flags|=32),L8(e,t),dn(e,t,s,n),t.child;case 6:return e===null&&Dp(t),null;case 13:return I8(e,t,n);case 4:return Vg(t,t.stateNode.containerInfo),r=t.pendingProps,e===null?t.child=Eo(t,null,r,n):dn(e,t,r,n),t.child;case 11:return r=t.type,i=t.pendingProps,i=t.elementType===r?i:yr(r,i),sv(e,t,r,i,n);case 7:return dn(e,t,t.pendingProps,n),t.child;case 8:return dn(e,t,t.pendingProps.children,n),t.child;case 12:return dn(e,t,t.pendingProps.children,n),t.child;case 10:e:{if(r=t.type._context,i=t.pendingProps,a=t.memoizedProps,s=i.value,it(Bd,r._currentValue),r._currentValue=s,a!==null)if(Ar(a.value,s)){if(a.children===i.children&&!Nn.current){t=Pi(e,t,n);break e}}else for(a=t.child,a!==null&&(a.return=t);a!==null;){var o=a.dependencies;if(o!==null){s=a.child;for(var l=o.firstContext;l!==null;){if(l.context===r){if(a.tag===1){l=ki(-1,n&-n),l.tag=2;var c=a.updateQueue;if(c!==null){c=c.shared;var u=c.pending;u===null?l.next=l:(l.next=u.next,u.next=l),c.pending=l}}a.lanes|=n,l=a.alternate,l!==null&&(l.lanes|=n),Mp(a.return,n,t),o.lanes|=n;break}l=l.next}}else if(a.tag===10)s=a.type===t.type?null:a.child;else if(a.tag===18){if(s=a.return,s===null)throw Error(le(341));s.lanes|=n,o=s.alternate,o!==null&&(o.lanes|=n),Mp(s,n,t),s=a.sibling}else s=a.child;if(s!==null)s.return=a;else for(s=a;s!==null;){if(s===t){s=null;break}if(a=s.sibling,a!==null){a.return=s.return,s=a;break}s=s.return}a=s}dn(e,t,i.children,n),t=t.child}return t;case 9:return i=t.type,r=t.pendingProps.children,ro(t,n),i=mr(i),r=r(i),t.flags|=1,dn(e,t,r,n),t.child;case 14:return r=t.type,i=yr(r,t.pendingProps),i=yr(r.type,i),ov(e,t,r,i,n);case 15:return C8(e,t,t.type,t.pendingProps,n);case 17:return r=t.type,i=t.pendingProps,i=t.elementType===r?i:yr(r,i),sd(e,t),t.tag=1,Cn(r)?(e=!0,Rd(t)):e=!1,ro(t,n),k8(t,r,i),Op(t,r,i,n),Fp(null,t,r,!0,e,n);case 19:return U8(e,t,n);case 22:return A8(e,t,n)}throw Error(le(156,t.tag))};function X8(e,t){return wy(e,t)}function WS(e,t,n,r){this.tag=e,this.key=n,this.sibling=this.child=this.return=this.stateNode=this.type=this.elementType=null,this.index=0,this.ref=null,this.pendingProps=t,this.dependencies=this.memoizedState=this.updateQueue=this.memoizedProps=null,this.mode=r,this.subtreeFlags=this.flags=0,this.deletions=null,this.childLanes=this.lanes=0,this.alternate=null}function or(e,t,n,r){return new WS(e,t,n,r)}function i1(e){return e=e.prototype,!(!e||!e.isReactComponent)}function KS(e){if(typeof e=="function")return i1(e)?1:0;if(e!=null){if(e=e.$$typeof,e===wg)return 11;if(e===Tg)return 14}return 2}function _a(e,t){var n=e.alternate;return n===null?(n=or(e.tag,t,e.key,e.mode),n.elementType=e.elementType,n.type=e.type,n.stateNode=e.stateNode,n.alternate=e,e.alternate=n):(n.pendingProps=t,n.type=e.type,n.flags=0,n.subtreeFlags=0,n.deletions=null),n.flags=e.flags&14680064,n.childLanes=e.childLanes,n.lanes=e.lanes,n.child=e.child,n.memoizedProps=e.memoizedProps,n.memoizedState=e.memoizedState,n.updateQueue=e.updateQueue,t=e.dependencies,n.dependencies=t===null?null:{lanes:t.lanes,firstContext:t.firstContext},n.sibling=e.sibling,n.index=e.index,n.ref=e.ref,n}function cd(e,t,n,r,i,a){var s=2;if(r=e,typeof e=="function")i1(e)&&(s=1);else if(typeof e=="string")s=5;else e:switch(e){case Ms:return ss(n.children,i,a,t);case _g:s=8,i|=8;break;case lp:return e=or(12,n,t,i|2),e.elementType=lp,e.lanes=a,e;case cp:return e=or(13,n,t,i),e.elementType=cp,e.lanes=a,e;case up:return e=or(19,n,t,i),e.elementType=up,e.lanes=a,e;case sy:return L0(n,i,a,t);default:if(typeof e=="object"&&e!==null)switch(e.$$typeof){case iy:s=10;break e;case ay:s=9;break e;case wg:s=11;break e;case Tg:s=14;break e;case ea:s=16,r=null;break e}throw Error(le(130,e==null?e:typeof e,""))}return t=or(s,n,t,i),t.elementType=e,t.type=r,t.lanes=a,t}function ss(e,t,n,r){return e=or(7,e,r,t),e.lanes=n,e}function L0(e,t,n,r){return e=or(22,e,r,t),e.elementType=sy,e.lanes=n,e.stateNode={isHidden:!1},e}function eh(e,t,n){return e=or(6,e,null,t),e.lanes=n,e}function th(e,t,n){return t=or(4,e.children!==null?e.children:[],e.key,t),t.lanes=n,t.stateNode={containerInfo:e.containerInfo,pendingChildren:null,implementation:e.implementation},t}function YS(e,t,n,r,i){this.tag=t,this.containerInfo=e,this.finishedWork=this.pingCache=this.current=this.pendingChildren=null,this.timeoutHandle=-1,this.callbackNode=this.pendingContext=this.context=null,this.callbackPriority=0,this.eventTimes=Dm(0),this.expirationTimes=Dm(-1),this.entangledLanes=this.finishedLanes=this.mutableReadLanes=this.expiredLanes=this.pingedLanes=this.suspendedLanes=this.pendingLanes=0,this.entanglements=Dm(0),this.identifierPrefix=r,this.onRecoverableError=i,this.mutableSourceEagerHydrationData=null}function a1(e,t,n,r,i,a,s,o,l){return e=new YS(e,t,n,o,l),t===1?(t=1,a===!0&&(t|=8)):t=0,a=or(3,null,null,t),e.current=a,a.stateNode=e,a.memoizedState={element:r,isDehydrated:n,cache:null,transitions:null,pendingSuspenseBoundaries:null},jg(a),e}function XS(e,t,n){var r=3<arguments.length&&arguments[3]!==void 0?arguments[3]:null;return{$$typeof:Ds,key:r==null?null:""+r,children:e,containerInfo:t,implementation:n}}function Q8(e){if(!e)return Ta;e=e._reactInternals;e:{if(xs(e)!==e||e.tag!==1)throw Error(le(170));var t=e;do{switch(t.tag){case 3:t=t.stateNode.context;break e;case 1:if(Cn(t.type)){t=t.stateNode.__reactInternalMemoizedMergedChildContext;break e}}t=t.return}while(t!==null);throw Error(le(171))}if(e.tag===1){var n=e.type;if(Cn(n))return Qy(e,n,t)}return t}function Z8(e,t,n,r,i,a,s,o,l){return e=a1(n,r,!0,e,i,a,s,o,l),e.context=Q8(null),n=e.current,r=fn(),i=ya(n),a=ki(r,i),a.callback=t??null,xa(n,a,i),e.current.lanes=i,Oc(e,i,r),An(e,r),e}function P0(e,t,n,r){var i=t.current,a=fn(),s=ya(i);return n=Q8(n),t.context===null?t.context=n:t.pendingContext=n,t=ki(a,s),t.payload={element:e},r=r===void 0?null:r,r!==null&&(t.callback=r),e=xa(i,t,s),e!==null&&(Cr(e,i,s,a),rd(e,i,s)),s}function Xd(e){if(e=e.current,!e.child)return null;switch(e.child.tag){case 5:return e.child.stateNode;default:return e.child.stateNode}}function xv(e,t){if(e=e.memoizedState,e!==null&&e.dehydrated!==null){var n=e.retryLane;e.retryLane=n!==0&&n<t?n:t}}function s1(e,t){xv(e,t),(e=e.alternate)&&xv(e,t)}function QS(){return null}var J8=typeof reportError=="function"?reportError:function(e){console.error(e)};function o1(e){this._internalRoot=e}I0.prototype.render=o1.prototype.render=function(e){var t=this._internalRoot;if(t===null)throw Error(le(409));P0(e,t,null,null)};I0.prototype.unmount=o1.prototype.unmount=function(){var e=this._internalRoot;if(e!==null){this._internalRoot=null;var t=e.containerInfo;fs(function(){P0(null,e,null,null)}),t[Ai]=null}};function I0(e){this._internalRoot=e}I0.prototype.unstable_scheduleHydration=function(e){if(e){var t=Ay();e={blockedOn:null,target:e,priority:t};for(var n=0;n<na.length&&t!==0&&t<na[n].priority;n++);na.splice(n,0,e),n===0&&Py(e)}};function l1(e){return!(!e||e.nodeType!==1&&e.nodeType!==9&&e.nodeType!==11)}function U0(e){return!(!e||e.nodeType!==1&&e.nodeType!==9&&e.nodeType!==11&&(e.nodeType!==8||e.nodeValue!==" react-mount-point-unstable "))}function $v(){}function ZS(e,t,n,r,i){if(i){if(typeof r=="function"){var a=r;r=function(){var c=Xd(s);a.call(c)}}var s=Z8(t,r,e,0,null,!1,!1,"",$v);return e._reactRootContainer=s,e[Ai]=s.current,ac(e.nodeType===8?e.parentNode:e),fs(),s}for(;i=e.lastChild;)e.removeChild(i);if(typeof r=="function"){var o=r;r=function(){var c=Xd(l);o.call(c)}}var l=a1(e,0,!1,null,null,!1,!1,"",$v);return e._reactRootContainer=l,e[Ai]=l.current,ac(e.nodeType===8?e.parentNode:e),fs(function(){P0(t,l,n,r)}),l}function D0(e,t,n,r,i){var a=n._reactRootContainer;if(a){var s=a;if(typeof i=="function"){var o=i;i=function(){var l=Xd(s);o.call(l)}}P0(t,s,e,i)}else s=ZS(n,t,e,i,r);return Xd(s)}Ny=function(e){switch(e.tag){case 3:var t=e.stateNode;if(t.current.memoizedState.isDehydrated){var n=$l(t.pendingLanes);n!==0&&(Sg(t,n|1),An(t,Nt()),!(qe&6)&&(No=Nt()+500,Ma()))}break;case 13:fs(function(){var r=Li(e,1);if(r!==null){var i=fn();Cr(r,e,1,i)}}),s1(e,1)}};Ng=function(e){if(e.tag===13){var t=Li(e,134217728);if(t!==null){var n=fn();Cr(t,e,134217728,n)}s1(e,134217728)}};Cy=function(e){if(e.tag===13){var t=ya(e),n=Li(e,t);if(n!==null){var r=fn();Cr(n,e,t,r)}s1(e,t)}};Ay=function(){return Je};Ly=function(e,t){var n=Je;try{return Je=e,t()}finally{Je=n}};$p=function(e,t,n){switch(t){case"input":if(hp(e,n),t=n.name,n.type==="radio"&&t!=null){for(n=e;n.parentNode;)n=n.parentNode;for(n=n.querySelectorAll("input[name="+JSON.stringify(""+t)+'][type="radio"]'),t=0;t<n.length;t++){var r=n[t];if(r!==e&&r.form===e.form){var i=E0(r);if(!i)throw Error(le(90));ly(r),hp(r,i)}}}break;case"textarea":uy(e,n);break;case"select":t=n.value,t!=null&&Js(e,!!n.multiple,t,!1)}};by=t1;vy=fs;var JS={usingClientEntryPoint:!1,Events:[Bc,Bs,E0,fy,gy,t1]},ul={findFiberByHostInstance:Xa,bundleType:0,version:"18.3.1",rendererPackageName:"react-dom"},eN={bundleType:ul.bundleType,version:ul.version,rendererPackageName:ul.rendererPackageName,rendererConfig:ul.rendererConfig,overrideHookState:null,overrideHookStateDeletePath:null,overrideHookStateRenamePath:null,overrideProps:null,overridePropsDeletePath:null,overridePropsRenamePath:null,setErrorHandler:null,setSuspenseHandler:null,scheduleUpdate:null,currentDispatcherRef:Bi.ReactCurrentDispatcher,findHostInstanceByFiber:function(e){return e=yy(e),e===null?null:e.stateNode},findFiberByHostInstance:ul.findFiberByHostInstance||QS,findHostInstancesForRefresh:null,scheduleRefresh:null,scheduleRoot:null,setRefreshHandler:null,getCurrentFiber:null,reconcilerVersion:"18.3.1-next-f1338f8080-20240426"};if(typeof __REACT_DEVTOOLS_GLOBAL_HOOK__<"u"){var Tu=__REACT_DEVTOOLS_GLOBAL_HOOK__;if(!Tu.isDisabled&&Tu.supportsFiber)try{y0=Tu.inject(eN),ni=Tu}catch{}}Yn.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=JS;Yn.createPortal=function(e,t){var n=2<arguments.length&&arguments[2]!==void 0?arguments[2]:null;if(!l1(t))throw Error(le(200));return XS(e,t,null,n)};Yn.createRoot=function(e,t){if(!l1(e))throw Error(le(299));var n=!1,r="",i=J8;return t!=null&&(t.unstable_strictMode===!0&&(n=!0),t.identifierPrefix!==void 0&&(r=t.identifierPrefix),t.onRecoverableError!==void 0&&(i=t.onRecoverableError)),t=a1(e,1,!1,null,null,n,!1,r,i),e[Ai]=t.current,ac(e.nodeType===8?e.parentNode:e),new o1(t)};Yn.findDOMNode=function(e){if(e==null)return null;if(e.nodeType===1)return e;var t=e._reactInternals;if(t===void 0)throw typeof e.render=="function"?Error(le(188)):(e=Object.keys(e).join(","),Error(le(268,e)));return e=yy(t),e=e===null?null:e.stateNode,e};Yn.flushSync=function(e){return fs(e)};Yn.hydrate=function(e,t,n){if(!U0(t))throw Error(le(200));return D0(null,e,t,!0,n)};Yn.hydrateRoot=function(e,t,n){if(!l1(e))throw Error(le(405));var r=n!=null&&n.hydratedSources||null,i=!1,a="",s=J8;if(n!=null&&(n.unstable_strictMode===!0&&(i=!0),n.identifierPrefix!==void 0&&(a=n.identifierPrefix),n.onRecoverableError!==void 0&&(s=n.onRecoverableError)),t=Z8(t,null,e,1,n??null,i,!1,a,s),e[Ai]=t.current,ac(e),r)for(e=0;e<r.length;e++)n=r[e],i=n._getVersion,i=i(n._source),t.mutableSourceEagerHydrationData==null?t.mutableSourceEagerHydrationData=[n,i]:t.mutableSourceEagerHydrationData.push(n,i);return new I0(t)};Yn.render=function(e,t,n){if(!U0(t))throw Error(le(200));return D0(null,e,t,!1,n)};Yn.unmountComponentAtNode=function(e){if(!U0(e))throw Error(le(40));return e._reactRootContainer?(fs(function(){D0(null,null,e,!1,function(){e._reactRootContainer=null,e[Ai]=null})}),!0):!1};Yn.unstable_batchedUpdates=t1;Yn.unstable_renderSubtreeIntoContainer=function(e,t,n,r){if(!U0(n))throw Error(le(200));if(e==null||e._reactInternals===void 0)throw Error(le(38));return D0(e,t,n,!1,r)};Yn.version="18.3.1-next-f1338f8080-20240426";function e6(){if(!(typeof __REACT_DEVTOOLS_GLOBAL_HOOK__>"u"||typeof __REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE!="function"))try{__REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE(e6)}catch(e){console.error(e)}}e6(),ey.exports=Yn;var $s=ey.exports;const t6=x0($s);var n6,yv=$s;n6=yv.createRoot,yv.hydrateRoot;const tN=1,nN=1e6;let nh=0;function rN(){return nh=(nh+1)%Number.MAX_SAFE_INTEGER,nh.toString()}const rh=new Map,_v=e=>{if(rh.has(e))return;const t=setTimeout(()=>{rh.delete(e),Ml({type:"REMOVE_TOAST",toastId:e})},nN);rh.set(e,t)},iN=(e,t)=>{switch(t.type){case"ADD_TOAST":return{...e,toasts:[t.toast,...e.toasts].slice(0,tN)};case"UPDATE_TOAST":return{...e,toasts:e.toasts.map(n=>n.id===t.toast.id?{...n,...t.toast}:n)};case"DISMISS_TOAST":{const{toastId:n}=t;return n?_v(n):e.toasts.forEach(r=>{_v(r.id)}),{...e,toasts:e.toasts.map(r=>r.id===n||n===void 0?{...r,open:!1}:r)}}case"REMOVE_TOAST":return t.toastId===void 0?{...e,toasts:[]}:{...e,toasts:e.toasts.filter(n=>n.id!==t.toastId)}}},ud=[];let dd={toasts:[]};function Ml(e){dd=iN(dd,e),ud.forEach(t=>{t(dd)})}function aN({...e}){const t=rN(),n=i=>Ml({type:"UPDATE_TOAST",toast:{...i,id:t}}),r=()=>Ml({type:"DISMISS_TOAST",toastId:t});return Ml({type:"ADD_TOAST",toast:{...e,id:t,open:!0,onOpenChange:i=>{i||r()}}}),{id:t,dismiss:r,update:n}}function r6(){const[e,t]=T.useState(dd);return T.useEffect(()=>(ud.push(t),()=>{const n=ud.indexOf(t);n>-1&&ud.splice(n,1)}),[e]),{...e,toast:aN,dismiss:n=>Ml({type:"DISMISS_TOAST",toastId:n})}}function Ae(e,t,{checkForDefaultPrevented:n=!0}={}){return function(i){if(e==null||e(i),n===!1||!i.defaultPrevented)return t==null?void 0:t(i)}}function sN(e,t){typeof e=="function"?e(t):e!=null&&(e.current=t)}function i6(...e){return t=>e.forEach(n=>sN(n,t))}function yt(...e){return T.useCallback(i6(...e),e)}function oN(e,t=[]){let n=[];function r(a,s){const o=T.createContext(s),l=n.length;n=[...n,s];function c(d){const{scope:m,children:p,...x}=d,g=(m==null?void 0:m[e][l])||o,w=T.useMemo(()=>x,Object.values(x));return h.jsx(g.Provider,{value:w,children:p})}function u(d,m){const p=(m==null?void 0:m[e][l])||o,x=T.useContext(p);if(x)return x;if(s!==void 0)return s;throw new Error(`\`${d}\` must be used within \`${a}\``)}return c.displayName=a+"Provider",[c,u]}const i=()=>{const a=n.map(s=>T.createContext(s));return function(o){const l=(o==null?void 0:o[e])||a;return T.useMemo(()=>({[`__scope${e}`]:{...o,[e]:l}}),[o,l])}};return i.scopeName=e,[r,lN(i,...t)]}function lN(...e){const t=e[0];if(e.length===1)return t;const n=()=>{const r=e.map(i=>({useScope:i(),scopeName:i.scopeName}));return function(a){const s=r.reduce((o,{useScope:l,scopeName:c})=>{const d=l(a)[`__scope${c}`];return{...o,...d}},{});return T.useMemo(()=>({[`__scope${t.scopeName}`]:s}),[s])}};return n.scopeName=t.scopeName,n}var Co=T.forwardRef((e,t)=>{const{children:n,...r}=e,i=T.Children.toArray(n),a=i.find(cN);if(a){const s=a.props.children,o=i.map(l=>l===a?T.Children.count(s)>1?T.Children.only(null):T.isValidElement(s)?s.props.children:null:l);return h.jsx(Jp,{...r,ref:t,children:T.isValidElement(s)?T.cloneElement(s,void 0,o):null})}return h.jsx(Jp,{...r,ref:t,children:n})});Co.displayName="Slot";var Jp=T.forwardRef((e,t)=>{const{children:n,...r}=e;if(T.isValidElement(n)){const i=dN(n);return T.cloneElement(n,{...uN(r,n.props),ref:t?i6(t,i):i})}return T.Children.count(n)>1?T.Children.only(null):null});Jp.displayName="SlotClone";var a6=({children:e})=>h.jsx(h.Fragment,{children:e});function cN(e){return T.isValidElement(e)&&e.type===a6}function uN(e,t){const n={...t};for(const r in t){const i=e[r],a=t[r];/^on[A-Z]/.test(r)?i&&a?n[r]=(...o)=>{a(...o),i(...o)}:i&&(n[r]=i):r==="style"?n[r]={...i,...a}:r==="className"&&(n[r]=[i,a].filter(Boolean).join(" "))}return{...e,...n}}function dN(e){var r,i;let t=(r=Object.getOwnPropertyDescriptor(e.props,"ref"))==null?void 0:r.get,n=t&&"isReactWarning"in t&&t.isReactWarning;return n?e.ref:(t=(i=Object.getOwnPropertyDescriptor(e,"ref"))==null?void 0:i.get,n=t&&"isReactWarning"in t&&t.isReactWarning,n?e.props.ref:e.props.ref||e.ref)}function c1(e){const t=e+"CollectionProvider",[n,r]=oN(t),[i,a]=n(t,{collectionRef:{current:null},itemMap:new Map}),s=p=>{const{scope:x,children:g}=p,w=pe.useRef(null),v=pe.useRef(new Map).current;return h.jsx(i,{scope:x,itemMap:v,collectionRef:w,children:g})};s.displayName=t;const o=e+"CollectionSlot",l=pe.forwardRef((p,x)=>{const{scope:g,children:w}=p,v=a(o,g),$=yt(x,v.collectionRef);return h.jsx(Co,{ref:$,children:w})});l.displayName=o;const c=e+"CollectionItemSlot",u="data-radix-collection-item",d=pe.forwardRef((p,x)=>{const{scope:g,children:w,...v}=p,$=pe.useRef(null),_=yt(x,$),C=a(c,g);return pe.useEffect(()=>(C.itemMap.set($,{ref:$,...v}),()=>void C.itemMap.delete($))),h.jsx(Co,{[u]:"",ref:_,children:w})});d.displayName=c;function m(p){const x=a(e+"CollectionConsumer",p);return pe.useCallback(()=>{const w=x.collectionRef.current;if(!w)return[];const v=Array.from(w.querySelectorAll(`[${u}]`));return Array.from(x.itemMap.values()).sort((C,k)=>v.indexOf(C.ref.current)-v.indexOf(k.ref.current))},[x.collectionRef,x.itemMap])}return[{Provider:s,Slot:l,ItemSlot:d},m,r]}function M0(e,t=[]){let n=[];function r(a,s){const o=T.createContext(s),l=n.length;n=[...n,s];const c=d=>{var v;const{scope:m,children:p,...x}=d,g=((v=m==null?void 0:m[e])==null?void 0:v[l])||o,w=T.useMemo(()=>x,Object.values(x));return h.jsx(g.Provider,{value:w,children:p})};c.displayName=a+"Provider";function u(d,m){var g;const p=((g=m==null?void 0:m[e])==null?void 0:g[l])||o,x=T.useContext(p);if(x)return x;if(s!==void 0)return s;throw new Error(`\`${d}\` must be used within \`${a}\``)}return[c,u]}const i=()=>{const a=n.map(s=>T.createContext(s));return function(o){const l=(o==null?void 0:o[e])||a;return T.useMemo(()=>({[`__scope${e}`]:{...o,[e]:l}}),[o,l])}};return i.scopeName=e,[r,mN(i,...t)]}function mN(...e){const t=e[0];if(e.length===1)return t;const n=()=>{const r=e.map(i=>({useScope:i(),scopeName:i.scopeName}));return function(a){const s=r.reduce((o,{useScope:l,scopeName:c})=>{const d=l(a)[`__scope${c}`];return{...o,...d}},{});return T.useMemo(()=>({[`__scope${t.scopeName}`]:s}),[s])}};return n.scopeName=t.scopeName,n}var hN=["a","button","div","form","h2","h3","img","input","label","li","nav","ol","p","span","svg","ul"],We=hN.reduce((e,t)=>{const n=T.forwardRef((r,i)=>{const{asChild:a,...s}=r,o=a?Co:t;return typeof window<"u"&&(window[Symbol.for("radix-ui")]=!0),h.jsx(o,{...s,ref:i})});return n.displayName=`Primitive.${t}`,{...e,[t]:n}},{});function s6(e,t){e&&$s.flushSync(()=>e.dispatchEvent(t))}function Pn(e){const t=T.useRef(e);return T.useEffect(()=>{t.current=e}),T.useMemo(()=>(...n)=>{var r;return(r=t.current)==null?void 0:r.call(t,...n)},[])}function pN(e,t=globalThis==null?void 0:globalThis.document){const n=Pn(e);T.useEffect(()=>{const r=i=>{i.key==="Escape"&&n(i)};return t.addEventListener("keydown",r,{capture:!0}),()=>t.removeEventListener("keydown",r,{capture:!0})},[n,t])}var fN="DismissableLayer",ef="dismissableLayer.update",gN="dismissableLayer.pointerDownOutside",bN="dismissableLayer.focusOutside",wv,o6=T.createContext({layers:new Set,layersWithOutsidePointerEventsDisabled:new Set,branches:new Set}),R0=T.forwardRef((e,t)=>{const{disableOutsidePointerEvents:n=!1,onEscapeKeyDown:r,onPointerDownOutside:i,onFocusOutside:a,onInteractOutside:s,onDismiss:o,...l}=e,c=T.useContext(o6),[u,d]=T.useState(null),m=(u==null?void 0:u.ownerDocument)??(globalThis==null?void 0:globalThis.document),[,p]=T.useState({}),x=yt(t,L=>d(L)),g=Array.from(c.layers),[w]=[...c.layersWithOutsidePointerEventsDisabled].slice(-1),v=g.indexOf(w),$=u?g.indexOf(u):-1,_=c.layersWithOutsidePointerEventsDisabled.size>0,C=$>=v,k=xN(L=>{const U=L.target,F=[...c.branches].some(q=>q.contains(U));!C||F||(i==null||i(L),s==null||s(L),L.defaultPrevented||o==null||o())},m),S=$N(L=>{const U=L.target;[...c.branches].some(q=>q.contains(U))||(a==null||a(L),s==null||s(L),L.defaultPrevented||o==null||o())},m);return pN(L=>{$===c.layers.size-1&&(r==null||r(L),!L.defaultPrevented&&o&&(L.preventDefault(),o()))},m),T.useEffect(()=>{if(u)return n&&(c.layersWithOutsidePointerEventsDisabled.size===0&&(wv=m.body.style.pointerEvents,m.body.style.pointerEvents="none"),c.layersWithOutsidePointerEventsDisabled.add(u)),c.layers.add(u),Tv(),()=>{n&&c.layersWithOutsidePointerEventsDisabled.size===1&&(m.body.style.pointerEvents=wv)}},[u,m,n,c]),T.useEffect(()=>()=>{u&&(c.layers.delete(u),c.layersWithOutsidePointerEventsDisabled.delete(u),Tv())},[u,c]),T.useEffect(()=>{const L=()=>p({});return document.addEventListener(ef,L),()=>document.removeEventListener(ef,L)},[]),h.jsx(We.div,{...l,ref:x,style:{pointerEvents:_?C?"auto":"none":void 0,...e.style},onFocusCapture:Ae(e.onFocusCapture,S.onFocusCapture),onBlurCapture:Ae(e.onBlurCapture,S.onBlurCapture),onPointerDownCapture:Ae(e.onPointerDownCapture,k.onPointerDownCapture)})});R0.displayName=fN;var vN="DismissableLayerBranch",l6=T.forwardRef((e,t)=>{const n=T.useContext(o6),r=T.useRef(null),i=yt(t,r);return T.useEffect(()=>{const a=r.current;if(a)return n.branches.add(a),()=>{n.branches.delete(a)}},[n.branches]),h.jsx(We.div,{...e,ref:i})});l6.displayName=vN;function xN(e,t=globalThis==null?void 0:globalThis.document){const n=Pn(e),r=T.useRef(!1),i=T.useRef(()=>{});return T.useEffect(()=>{const a=o=>{if(o.target&&!r.current){let l=function(){c6(gN,n,c,{discrete:!0})};const c={originalEvent:o};o.pointerType==="touch"?(t.removeEventListener("click",i.current),i.current=l,t.addEventListener("click",i.current,{once:!0})):l()}else t.removeEventListener("click",i.current);r.current=!1},s=window.setTimeout(()=>{t.addEventListener("pointerdown",a)},0);return()=>{window.clearTimeout(s),t.removeEventListener("pointerdown",a),t.removeEventListener("click",i.current)}},[t,n]),{onPointerDownCapture:()=>r.current=!0}}function $N(e,t=globalThis==null?void 0:globalThis.document){const n=Pn(e),r=T.useRef(!1);return T.useEffect(()=>{const i=a=>{a.target&&!r.current&&c6(bN,n,{originalEvent:a},{discrete:!1})};return t.addEventListener("focusin",i),()=>t.removeEventListener("focusin",i)},[t,n]),{onFocusCapture:()=>r.current=!0,onBlurCapture:()=>r.current=!1}}function Tv(){const e=new CustomEvent(ef);document.dispatchEvent(e)}function c6(e,t,n,{discrete:r}){const i=n.originalEvent.target,a=new CustomEvent(e,{bubbles:!1,cancelable:!0,detail:n});t&&i.addEventListener(e,t,{once:!0}),r?s6(i,a):i.dispatchEvent(a)}var yN=R0,_N=l6,sn=globalThis!=null&&globalThis.document?T.useLayoutEffect:()=>{},wN="Portal",u1=T.forwardRef((e,t)=>{var o;const{container:n,...r}=e,[i,a]=T.useState(!1);sn(()=>a(!0),[]);const s=n||i&&((o=globalThis==null?void 0:globalThis.document)==null?void 0:o.body);return s?t6.createPortal(h.jsx(We.div,{...r,ref:t}),s):null});u1.displayName=wN;function TN(e,t){return T.useReducer((n,r)=>t[n][r]??n,e)}var O0=e=>{const{present:t,children:n}=e,r=EN(t),i=typeof n=="function"?n({present:r.isPresent}):T.Children.only(n),a=yt(r.ref,kN(i));return typeof n=="function"||r.isPresent?T.cloneElement(i,{ref:a}):null};O0.displayName="Presence";function EN(e){const[t,n]=T.useState(),r=T.useRef({}),i=T.useRef(e),a=T.useRef("none"),s=e?"mounted":"unmounted",[o,l]=TN(s,{mounted:{UNMOUNT:"unmounted",ANIMATION_OUT:"unmountSuspended"},unmountSuspended:{MOUNT:"mounted",ANIMATION_END:"unmounted"},unmounted:{MOUNT:"mounted"}});return T.useEffect(()=>{const c=Eu(r.current);a.current=o==="mounted"?c:"none"},[o]),sn(()=>{const c=r.current,u=i.current;if(u!==e){const m=a.current,p=Eu(c);e?l("MOUNT"):p==="none"||(c==null?void 0:c.display)==="none"?l("UNMOUNT"):l(u&&m!==p?"ANIMATION_OUT":"UNMOUNT"),i.current=e}},[e,l]),sn(()=>{if(t){let c;const u=t.ownerDocument.defaultView??window,d=p=>{const g=Eu(r.current).includes(p.animationName);if(p.target===t&&g&&(l("ANIMATION_END"),!i.current)){const w=t.style.animationFillMode;t.style.animationFillMode="forwards",c=u.setTimeout(()=>{t.style.animationFillMode==="forwards"&&(t.style.animationFillMode=w)})}},m=p=>{p.target===t&&(a.current=Eu(r.current))};return t.addEventListener("animationstart",m),t.addEventListener("animationcancel",d),t.addEventListener("animationend",d),()=>{u.clearTimeout(c),t.removeEventListener("animationstart",m),t.removeEventListener("animationcancel",d),t.removeEventListener("animationend",d)}}else l("ANIMATION_END")},[t,l]),{isPresent:["mounted","unmountSuspended"].includes(o),ref:T.useCallback(c=>{c&&(r.current=getComputedStyle(c)),n(c)},[])}}function Eu(e){return(e==null?void 0:e.animationName)||"none"}function kN(e){var r,i;let t=(r=Object.getOwnPropertyDescriptor(e.props,"ref"))==null?void 0:r.get,n=t&&"isReactWarning"in t&&t.isReactWarning;return n?e.ref:(t=(i=Object.getOwnPropertyDescriptor(e,"ref"))==null?void 0:i.get,n=t&&"isReactWarning"in t&&t.isReactWarning,n?e.props.ref:e.props.ref||e.ref)}function pc({prop:e,defaultProp:t,onChange:n=()=>{}}){const[r,i]=SN({defaultProp:t,onChange:n}),a=e!==void 0,s=a?e:r,o=Pn(n),l=T.useCallback(c=>{if(a){const d=typeof c=="function"?c(e):c;d!==e&&o(d)}else i(c)},[a,e,i,o]);return[s,l]}function SN({defaultProp:e,onChange:t}){const n=T.useState(e),[r]=n,i=T.useRef(r),a=Pn(t);return T.useEffect(()=>{i.current!==r&&(a(r),i.current=r)},[r,i,a]),n}var NN="VisuallyHidden",jc=T.forwardRef((e,t)=>h.jsx(We.span,{...e,ref:t,style:{position:"absolute",border:0,width:1,height:1,padding:0,margin:-1,overflow:"hidden",clip:"rect(0, 0, 0, 0)",whiteSpace:"nowrap",wordWrap:"normal",...e.style}}));jc.displayName=NN;var CN=jc,d1="ToastProvider",[m1,AN,LN]=c1("Toast"),[u6,uZ]=M0("Toast",[LN]),[PN,z0]=u6(d1),d6=e=>{const{__scopeToast:t,label:n="Notification",duration:r=5e3,swipeDirection:i="right",swipeThreshold:a=50,children:s}=e,[o,l]=T.useState(null),[c,u]=T.useState(0),d=T.useRef(!1),m=T.useRef(!1);return n.trim()||console.error(`Invalid prop \`label\` supplied to \`${d1}\`. Expected non-empty \`string\`.`),h.jsx(m1.Provider,{scope:t,children:h.jsx(PN,{scope:t,label:n,duration:r,swipeDirection:i,swipeThreshold:a,toastCount:c,viewport:o,onViewportChange:l,onToastAdd:T.useCallback(()=>u(p=>p+1),[]),onToastRemove:T.useCallback(()=>u(p=>p-1),[]),isFocusedToastEscapeKeyDownRef:d,isClosePausedRef:m,children:s})})};d6.displayName=d1;var m6="ToastViewport",IN=["F8"],tf="toast.viewportPause",nf="toast.viewportResume",h6=T.forwardRef((e,t)=>{const{__scopeToast:n,hotkey:r=IN,label:i="Notifications ({hotkey})",...a}=e,s=z0(m6,n),o=AN(n),l=T.useRef(null),c=T.useRef(null),u=T.useRef(null),d=T.useRef(null),m=yt(t,d,s.onViewportChange),p=r.join("+").replace(/Key/g,"").replace(/Digit/g,""),x=s.toastCount>0;T.useEffect(()=>{const w=v=>{var _;r.length!==0&&r.every(C=>v[C]||v.code===C)&&((_=d.current)==null||_.focus())};return document.addEventListener("keydown",w),()=>document.removeEventListener("keydown",w)},[r]),T.useEffect(()=>{const w=l.current,v=d.current;if(x&&w&&v){const $=()=>{if(!s.isClosePausedRef.current){const S=new CustomEvent(tf);v.dispatchEvent(S),s.isClosePausedRef.current=!0}},_=()=>{if(s.isClosePausedRef.current){const S=new CustomEvent(nf);v.dispatchEvent(S),s.isClosePausedRef.current=!1}},C=S=>{!w.contains(S.relatedTarget)&&_()},k=()=>{w.contains(document.activeElement)||_()};return w.addEventListener("focusin",$),w.addEventListener("focusout",C),w.addEventListener("pointermove",$),w.addEventListener("pointerleave",k),window.addEventListener("blur",$),window.addEventListener("focus",_),()=>{w.removeEventListener("focusin",$),w.removeEventListener("focusout",C),w.removeEventListener("pointermove",$),w.removeEventListener("pointerleave",k),window.removeEventListener("blur",$),window.removeEventListener("focus",_)}}},[x,s.isClosePausedRef]);const g=T.useCallback(({tabbingDirection:w})=>{const $=o().map(_=>{const C=_.ref.current,k=[C,...GN(C)];return w==="forwards"?k:k.reverse()});return(w==="forwards"?$.reverse():$).flat()},[o]);return T.useEffect(()=>{const w=d.current;if(w){const v=$=>{var k,S,L;const _=$.altKey||$.ctrlKey||$.metaKey;if($.key==="Tab"&&!_){const U=document.activeElement,F=$.shiftKey;if($.target===w&&F){(k=c.current)==null||k.focus();return}const H=g({tabbingDirection:F?"backwards":"forwards"}),ne=H.findIndex(K=>K===U);ih(H.slice(ne+1))?$.preventDefault():F?(S=c.current)==null||S.focus():(L=u.current)==null||L.focus()}};return w.addEventListener("keydown",v),()=>w.removeEventListener("keydown",v)}},[o,g]),h.jsxs(_N,{ref:l,role:"region","aria-label":i.replace("{hotkey}",p),tabIndex:-1,style:{pointerEvents:x?void 0:"none"},children:[x&&h.jsx(rf,{ref:c,onFocusFromOutsideViewport:()=>{const w=g({tabbingDirection:"forwards"});ih(w)}}),h.jsx(m1.Slot,{scope:n,children:h.jsx(We.ol,{tabIndex:-1,...a,ref:m})}),x&&h.jsx(rf,{ref:u,onFocusFromOutsideViewport:()=>{const w=g({tabbingDirection:"backwards"});ih(w)}})]})});h6.displayName=m6;var p6="ToastFocusProxy",rf=T.forwardRef((e,t)=>{const{__scopeToast:n,onFocusFromOutsideViewport:r,...i}=e,a=z0(p6,n);return h.jsx(jc,{"aria-hidden":!0,tabIndex:0,...i,ref:t,style:{position:"fixed"},onFocus:s=>{var c;const o=s.relatedTarget;!((c=a.viewport)!=null&&c.contains(o))&&r()}})});rf.displayName=p6;var B0="Toast",UN="toast.swipeStart",DN="toast.swipeMove",MN="toast.swipeCancel",RN="toast.swipeEnd",f6=T.forwardRef((e,t)=>{const{forceMount:n,open:r,defaultOpen:i,onOpenChange:a,...s}=e,[o=!0,l]=pc({prop:r,defaultProp:i,onChange:a});return h.jsx(O0,{present:n||o,children:h.jsx(BN,{open:o,...s,ref:t,onClose:()=>l(!1),onPause:Pn(e.onPause),onResume:Pn(e.onResume),onSwipeStart:Ae(e.onSwipeStart,c=>{c.currentTarget.setAttribute("data-swipe","start")}),onSwipeMove:Ae(e.onSwipeMove,c=>{const{x:u,y:d}=c.detail.delta;c.currentTarget.setAttribute("data-swipe","move"),c.currentTarget.style.setProperty("--radix-toast-swipe-move-x",`${u}px`),c.currentTarget.style.setProperty("--radix-toast-swipe-move-y",`${d}px`)}),onSwipeCancel:Ae(e.onSwipeCancel,c=>{c.currentTarget.setAttribute("data-swipe","cancel"),c.currentTarget.style.removeProperty("--radix-toast-swipe-move-x"),c.currentTarget.style.removeProperty("--radix-toast-swipe-move-y"),c.currentTarget.style.removeProperty("--radix-toast-swipe-end-x"),c.currentTarget.style.removeProperty("--radix-toast-swipe-end-y")}),onSwipeEnd:Ae(e.onSwipeEnd,c=>{const{x:u,y:d}=c.detail.delta;c.currentTarget.setAttribute("data-swipe","end"),c.currentTarget.style.removeProperty("--radix-toast-swipe-move-x"),c.currentTarget.style.removeProperty("--radix-toast-swipe-move-y"),c.currentTarget.style.setProperty("--radix-toast-swipe-end-x",`${u}px`),c.currentTarget.style.setProperty("--radix-toast-swipe-end-y",`${d}px`),l(!1)})})})});f6.displayName=B0;var[ON,zN]=u6(B0,{onClose(){}}),BN=T.forwardRef((e,t)=>{const{__scopeToast:n,type:r="foreground",duration:i,open:a,onClose:s,onEscapeKeyDown:o,onPause:l,onResume:c,onSwipeStart:u,onSwipeMove:d,onSwipeCancel:m,onSwipeEnd:p,...x}=e,g=z0(B0,n),[w,v]=T.useState(null),$=yt(t,K=>v(K)),_=T.useRef(null),C=T.useRef(null),k=i||g.duration,S=T.useRef(0),L=T.useRef(k),U=T.useRef(0),{onToastAdd:F,onToastRemove:q}=g,G=Pn(()=>{var te;(w==null?void 0:w.contains(document.activeElement))&&((te=g.viewport)==null||te.focus()),s()}),H=T.useCallback(K=>{!K||K===1/0||(window.clearTimeout(U.current),S.current=new Date().getTime(),U.current=window.setTimeout(G,K))},[G]);T.useEffect(()=>{const K=g.viewport;if(K){const te=()=>{H(L.current),c==null||c()},J=()=>{const ae=new Date().getTime()-S.current;L.current=L.current-ae,window.clearTimeout(U.current),l==null||l()};return K.addEventListener(tf,J),K.addEventListener(nf,te),()=>{K.removeEventListener(tf,J),K.removeEventListener(nf,te)}}},[g.viewport,k,l,c,H]),T.useEffect(()=>{a&&!g.isClosePausedRef.current&&H(k)},[a,k,g.isClosePausedRef,H]),T.useEffect(()=>(F(),()=>q()),[F,q]);const ne=T.useMemo(()=>w?_6(w):null,[w]);return g.viewport?h.jsxs(h.Fragment,{children:[ne&&h.jsx(FN,{__scopeToast:n,role:"status","aria-live":r==="foreground"?"assertive":"polite","aria-atomic":!0,children:ne}),h.jsx(ON,{scope:n,onClose:G,children:$s.createPortal(h.jsx(m1.ItemSlot,{scope:n,children:h.jsx(yN,{asChild:!0,onEscapeKeyDown:Ae(o,()=>{g.isFocusedToastEscapeKeyDownRef.current||G(),g.isFocusedToastEscapeKeyDownRef.current=!1}),children:h.jsx(We.li,{role:"status","aria-live":"off","aria-atomic":!0,tabIndex:0,"data-state":a?"open":"closed","data-swipe-direction":g.swipeDirection,...x,ref:$,style:{userSelect:"none",touchAction:"none",...e.style},onKeyDown:Ae(e.onKeyDown,K=>{K.key==="Escape"&&(o==null||o(K.nativeEvent),K.nativeEvent.defaultPrevented||(g.isFocusedToastEscapeKeyDownRef.current=!0,G()))}),onPointerDown:Ae(e.onPointerDown,K=>{K.button===0&&(_.current={x:K.clientX,y:K.clientY})}),onPointerMove:Ae(e.onPointerMove,K=>{if(!_.current)return;const te=K.clientX-_.current.x,J=K.clientY-_.current.y,ae=!!C.current,B=["left","right"].includes(g.swipeDirection),X=["left","up"].includes(g.swipeDirection)?Math.min:Math.max,P=B?X(0,te):0,se=B?0:X(0,J),he=K.pointerType==="touch"?10:2,D={x:P,y:se},Ee={originalEvent:K,delta:D};ae?(C.current=D,ku(DN,d,Ee,{discrete:!1})):Ev(D,g.swipeDirection,he)?(C.current=D,ku(UN,u,Ee,{discrete:!1}),K.target.setPointerCapture(K.pointerId)):(Math.abs(te)>he||Math.abs(J)>he)&&(_.current=null)}),onPointerUp:Ae(e.onPointerUp,K=>{const te=C.current,J=K.target;if(J.hasPointerCapture(K.pointerId)&&J.releasePointerCapture(K.pointerId),C.current=null,_.current=null,te){const ae=K.currentTarget,B={originalEvent:K,delta:te};Ev(te,g.swipeDirection,g.swipeThreshold)?ku(RN,p,B,{discrete:!0}):ku(MN,m,B,{discrete:!0}),ae.addEventListener("click",X=>X.preventDefault(),{once:!0})}})})})}),g.viewport)})]}):null}),FN=e=>{const{__scopeToast:t,children:n,...r}=e,i=z0(B0,t),[a,s]=T.useState(!1),[o,l]=T.useState(!1);return HN(()=>s(!0)),T.useEffect(()=>{const c=window.setTimeout(()=>l(!0),1e3);return()=>window.clearTimeout(c)},[]),o?null:h.jsx(u1,{asChild:!0,children:h.jsx(jc,{...r,children:a&&h.jsxs(h.Fragment,{children:[i.label," ",n]})})})},jN="ToastTitle",g6=T.forwardRef((e,t)=>{const{__scopeToast:n,...r}=e;return h.jsx(We.div,{...r,ref:t})});g6.displayName=jN;var VN="ToastDescription",b6=T.forwardRef((e,t)=>{const{__scopeToast:n,...r}=e;return h.jsx(We.div,{...r,ref:t})});b6.displayName=VN;var v6="ToastAction",x6=T.forwardRef((e,t)=>{const{altText:n,...r}=e;return n.trim()?h.jsx(y6,{altText:n,asChild:!0,children:h.jsx(h1,{...r,ref:t})}):(console.error(`Invalid prop \`altText\` supplied to \`${v6}\`. Expected non-empty \`string\`.`),null)});x6.displayName=v6;var $6="ToastClose",h1=T.forwardRef((e,t)=>{const{__scopeToast:n,...r}=e,i=zN($6,n);return h.jsx(y6,{asChild:!0,children:h.jsx(We.button,{type:"button",...r,ref:t,onClick:Ae(e.onClick,i.onClose)})})});h1.displayName=$6;var y6=T.forwardRef((e,t)=>{const{__scopeToast:n,altText:r,...i}=e;return h.jsx(We.div,{"data-radix-toast-announce-exclude":"","data-radix-toast-announce-alt":r||void 0,...i,ref:t})});function _6(e){const t=[];return Array.from(e.childNodes).forEach(r=>{if(r.nodeType===r.TEXT_NODE&&r.textContent&&t.push(r.textContent),qN(r)){const i=r.ariaHidden||r.hidden||r.style.display==="none",a=r.dataset.radixToastAnnounceExclude==="";if(!i)if(a){const s=r.dataset.radixToastAnnounceAlt;s&&t.push(s)}else t.push(..._6(r))}}),t}function ku(e,t,n,{discrete:r}){const i=n.originalEvent.currentTarget,a=new CustomEvent(e,{bubbles:!0,cancelable:!0,detail:n});t&&i.addEventListener(e,t,{once:!0}),r?s6(i,a):i.dispatchEvent(a)}var Ev=(e,t,n=0)=>{const r=Math.abs(e.x),i=Math.abs(e.y),a=r>i;return t==="left"||t==="right"?a&&r>n:!a&&i>n};function HN(e=()=>{}){const t=Pn(e);sn(()=>{let n=0,r=0;return n=window.requestAnimationFrame(()=>r=window.requestAnimationFrame(t)),()=>{window.cancelAnimationFrame(n),window.cancelAnimationFrame(r)}},[t])}function qN(e){return e.nodeType===e.ELEMENT_NODE}function GN(e){const t=[],n=document.createTreeWalker(e,NodeFilter.SHOW_ELEMENT,{acceptNode:r=>{const i=r.tagName==="INPUT"&&r.type==="hidden";return r.disabled||r.hidden||i?NodeFilter.FILTER_SKIP:r.tabIndex>=0?NodeFilter.FILTER_ACCEPT:NodeFilter.FILTER_SKIP}});for(;n.nextNode();)t.push(n.currentNode);return t}function ih(e){const t=document.activeElement;return e.some(n=>n===t?!0:(n.focus(),document.activeElement!==t))}var WN=d6,w6=h6,T6=f6,E6=g6,k6=b6,S6=x6,N6=h1;function C6(e){var t,n,r="";if(typeof e=="string"||typeof e=="number")r+=e;else if(typeof e=="object")if(Array.isArray(e)){var i=e.length;for(t=0;t<i;t++)e[t]&&(n=C6(e[t]))&&(r&&(r+=" "),r+=n)}else for(n in e)e[n]&&(r&&(r+=" "),r+=n);return r}function A6(){for(var e,t,n=0,r="",i=arguments.length;n<i;n++)(e=arguments[n])&&(t=C6(e))&&(r&&(r+=" "),r+=t);return r}const kv=e=>typeof e=="boolean"?`${e}`:e===0?"0":e,Sv=A6,p1=(e,t)=>n=>{var r;if((t==null?void 0:t.variants)==null)return Sv(e,n==null?void 0:n.class,n==null?void 0:n.className);const{variants:i,defaultVariants:a}=t,s=Object.keys(i).map(c=>{const u=n==null?void 0:n[c],d=a==null?void 0:a[c];if(u===null)return null;const m=kv(u)||kv(d);return i[c][m]}),o=n&&Object.entries(n).reduce((c,u)=>{let[d,m]=u;return m===void 0||(c[d]=m),c},{}),l=t==null||(r=t.compoundVariants)===null||r===void 0?void 0:r.reduce((c,u)=>{let{class:d,className:m,...p}=u;return Object.entries(p).every(x=>{let[g,w]=x;return Array.isArray(w)?w.includes({...a,...o}[g]):{...a,...o}[g]===w})?[...c,d,m]:c},[]);return Sv(e,s,l,n==null?void 0:n.class,n==null?void 0:n.className)};/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const KN=e=>e.replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase(),L6=(...e)=>e.filter((t,n,r)=>!!t&&t.trim()!==""&&r.indexOf(t)===n).join(" ").trim();/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */var YN={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const XN=T.forwardRef(({color:e="currentColor",size:t=24,strokeWidth:n=2,absoluteStrokeWidth:r,className:i="",children:a,iconNode:s,...o},l)=>T.createElement("svg",{ref:l,...YN,width:t,height:t,stroke:e,strokeWidth:r?Number(n)*24/Number(t):n,className:L6("lucide",i),...o},[...s.map(([c,u])=>T.createElement(c,u)),...Array.isArray(a)?a:[a]]));/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const Ye=(e,t)=>{const n=T.forwardRef(({className:r,...i},a)=>T.createElement(XN,{ref:a,iconNode:t,className:L6(`lucide-${KN(e)}`,r),...i}));return n.displayName=`${e}`,n};/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const QN=Ye("ArrowDown",[["path",{d:"M12 5v14",key:"s699le"}],["path",{d:"m19 12-7 7-7-7",key:"1idqje"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const P6=Ye("ArrowLeft",[["path",{d:"m12 19-7-7 7-7",key:"1l729n"}],["path",{d:"M19 12H5",key:"x3x0zl"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const F0=Ye("ArrowRight",[["path",{d:"M5 12h14",key:"1ays0h"}],["path",{d:"m12 5 7 7-7 7",key:"xquz4c"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const Nv=Ye("Award",[["path",{d:"m15.477 12.89 1.515 8.526a.5.5 0 0 1-.81.47l-3.58-2.687a1 1 0 0 0-1.197 0l-3.586 2.686a.5.5 0 0 1-.81-.469l1.514-8.526",key:"1yiouv"}],["circle",{cx:"12",cy:"8",r:"6",key:"1vp47v"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const ZN=Ye("BookOpenText",[["path",{d:"M12 7v14",key:"1akyts"}],["path",{d:"M16 12h2",key:"7q9ll5"}],["path",{d:"M16 8h2",key:"msurwy"}],["path",{d:"M3 18a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1h5a4 4 0 0 1 4 4 4 4 0 0 1 4-4h5a1 1 0 0 1 1 1v13a1 1 0 0 1-1 1h-6a3 3 0 0 0-3 3 3 3 0 0 0-3-3z",key:"ruj8y"}],["path",{d:"M6 12h2",key:"32wvfc"}],["path",{d:"M6 8h2",key:"30oboj"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const Cv=Ye("BookOpen",[["path",{d:"M12 7v14",key:"1akyts"}],["path",{d:"M3 18a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1h5a4 4 0 0 1 4 4 4 4 0 0 1 4-4h5a1 1 0 0 1 1 1v13a1 1 0 0 1-1 1h-6a3 3 0 0 0-3 3 3 3 0 0 0-3-3z",key:"ruj8y"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const JN=Ye("Briefcase",[["path",{d:"M16 20V4a2 2 0 0 0-2-2h-4a2 2 0 0 0-2 2v16",key:"jecpp"}],["rect",{width:"20",height:"14",x:"2",y:"6",rx:"2",key:"i6l2r4"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const Su=Ye("Calendar",[["path",{d:"M8 2v4",key:"1cmpym"}],["path",{d:"M16 2v4",key:"4m81vk"}],["rect",{width:"18",height:"18",x:"3",y:"4",rx:"2",key:"1hopcy"}],["path",{d:"M3 10h18",key:"8toen8"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const eC=Ye("Check",[["path",{d:"M20 6 9 17l-5-5",key:"1gmf2c"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const I6=Ye("ChevronDown",[["path",{d:"m6 9 6 6 6-6",key:"qrunsl"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const U6=Ye("ChevronLeft",[["path",{d:"m15 18-6-6 6-6",key:"1wnfg3"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const D6=Ye("ChevronRight",[["path",{d:"m9 18 6-6-6-6",key:"mthhwq"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const tC=Ye("ChevronUp",[["path",{d:"m18 15-6-6-6 6",key:"153udz"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const f1=Ye("Clock",[["circle",{cx:"12",cy:"12",r:"10",key:"1mglay"}],["polyline",{points:"12 6 12 12 16 14",key:"68esgv"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const Av=Ye("Code",[["polyline",{points:"16 18 22 12 16 6",key:"z7tu5w"}],["polyline",{points:"8 6 2 12 8 18",key:"1eg1df"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const nC=Ye("Ellipsis",[["circle",{cx:"12",cy:"12",r:"1",key:"41hilf"}],["circle",{cx:"19",cy:"12",r:"1",key:"1wjl8i"}],["circle",{cx:"5",cy:"12",r:"1",key:"1pcz8c"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const Lv=Ye("ExternalLink",[["path",{d:"M15 3h6v6",key:"1q9fwt"}],["path",{d:"M10 14 21 3",key:"gplh6r"}],["path",{d:"M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6",key:"a6xqqp"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const rC=Ye("FileText",[["path",{d:"M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z",key:"1rqfz7"}],["path",{d:"M14 2v4a2 2 0 0 0 2 2h4",key:"tnqrlb"}],["path",{d:"M10 9H8",key:"b1mrlr"}],["path",{d:"M16 13H8",key:"t4e002"}],["path",{d:"M16 17H8",key:"z1uh3a"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const iC=Ye("Flag",[["path",{d:"M4 15s1-1 4-1 5 2 8 2 4-1 4-1V3s-1 1-4 1-5-2-8-2-4 1-4 1z",key:"i9b6wo"}],["line",{x1:"4",x2:"4",y1:"22",y2:"15",key:"1cm3nv"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const Pv=Ye("Github",[["path",{d:"M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4",key:"tonef"}],["path",{d:"M9 18c-4.51 2-5-2-7-2",key:"9comsn"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const aC=Ye("GraduationCap",[["path",{d:"M21.42 10.922a1 1 0 0 0-.019-1.838L12.83 5.18a2 2 0 0 0-1.66 0L2.6 9.08a1 1 0 0 0 0 1.832l8.57 3.908a2 2 0 0 0 1.66 0z",key:"j76jl0"}],["path",{d:"M22 10v6",key:"1lu8f3"}],["path",{d:"M6 12.5V16a6 3 0 0 0 12 0v-3.5",key:"1r8lef"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const sC=Ye("House",[["path",{d:"M15 21v-8a1 1 0 0 0-1-1h-4a1 1 0 0 0-1 1v8",key:"5wwlr5"}],["path",{d:"M3 10a2 2 0 0 1 .709-1.528l7-5.999a2 2 0 0 1 2.582 0l7 5.999A2 2 0 0 1 21 10v9a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z",key:"1d0kgt"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const oC=Ye("Linkedin",[["path",{d:"M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z",key:"c2jq9f"}],["rect",{width:"4",height:"12",x:"2",y:"9",key:"mk3on5"}],["circle",{cx:"4",cy:"4",r:"2",key:"bt5ra8"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const M6=Ye("Mail",[["rect",{width:"20",height:"16",x:"2",y:"4",rx:"2",key:"18n3k1"}],["path",{d:"m22 7-8.97 5.7a1.94 1.94 0 0 1-2.06 0L2 7",key:"1ocrg3"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const md=Ye("MapPin",[["path",{d:"M20 10c0 4.993-5.539 10.193-7.399 11.799a1 1 0 0 1-1.202 0C9.539 20.193 4 14.993 4 10a8 8 0 0 1 16 0",key:"1r0f0z"}],["circle",{cx:"12",cy:"10",r:"3",key:"ilqhr7"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const Iv=Ye("Moon",[["path",{d:"M12 3a6 6 0 0 0 9 9 9 9 0 1 1-9-9Z",key:"a7tn18"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const R6=Ye("Search",[["circle",{cx:"11",cy:"11",r:"8",key:"4ej97u"}],["path",{d:"m21 21-4.3-4.3",key:"1qie3q"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const Uv=Ye("Sun",[["circle",{cx:"12",cy:"12",r:"4",key:"4exip2"}],["path",{d:"M12 2v2",key:"tus03m"}],["path",{d:"M12 20v2",key:"1lh1kg"}],["path",{d:"m4.93 4.93 1.41 1.41",key:"149t6j"}],["path",{d:"m17.66 17.66 1.41 1.41",key:"ptbguv"}],["path",{d:"M2 12h2",key:"1t8f8n"}],["path",{d:"M20 12h2",key:"1q8mjw"}],["path",{d:"m6.34 17.66-1.41 1.41",key:"1m8zz5"}],["path",{d:"m19.07 4.93-1.41 1.41",key:"1shlcs"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const g1=Ye("User",[["path",{d:"M19 21v-2a4 4 0 0 0-4-4H9a4 4 0 0 0-4 4v2",key:"975kel"}],["circle",{cx:"12",cy:"7",r:"4",key:"17ys0d"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const lC=Ye("Users",[["path",{d:"M16 21v-2a4 4 0 0 0-4-4H6a4 4 0 0 0-4 4v2",key:"1yyitq"}],["circle",{cx:"9",cy:"7",r:"4",key:"nufk8"}],["path",{d:"M22 21v-2a4 4 0 0 0-3-3.87",key:"kshegd"}],["path",{d:"M16 3.13a4 4 0 0 1 0 7.75",key:"1da9ce"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const O6=Ye("X",[["path",{d:"M18 6 6 18",key:"1bl5f8"}],["path",{d:"m6 6 12 12",key:"d8bk6v"}]]),b1="-",cC=e=>{const t=dC(e),{conflictingClassGroups:n,conflictingClassGroupModifiers:r}=e;return{getClassGroupId:s=>{const o=s.split(b1);return o[0]===""&&o.length!==1&&o.shift(),z6(o,t)||uC(s)},getConflictingClassGroupIds:(s,o)=>{const l=n[s]||[];return o&&r[s]?[...l,...r[s]]:l}}},z6=(e,t)=>{var s;if(e.length===0)return t.classGroupId;const n=e[0],r=t.nextPart.get(n),i=r?z6(e.slice(1),r):void 0;if(i)return i;if(t.validators.length===0)return;const a=e.join(b1);return(s=t.validators.find(({validator:o})=>o(a)))==null?void 0:s.classGroupId},Dv=/^\[(.+)\]$/,uC=e=>{if(Dv.test(e)){const t=Dv.exec(e)[1],n=t==null?void 0:t.substring(0,t.indexOf(":"));if(n)return"arbitrary.."+n}},dC=e=>{const{theme:t,prefix:n}=e,r={nextPart:new Map,validators:[]};return hC(Object.entries(e.classGroups),n).forEach(([a,s])=>{af(s,r,a,t)}),r},af=(e,t,n,r)=>{e.forEach(i=>{if(typeof i=="string"){const a=i===""?t:Mv(t,i);a.classGroupId=n;return}if(typeof i=="function"){if(mC(i)){af(i(r),t,n,r);return}t.validators.push({validator:i,classGroupId:n});return}Object.entries(i).forEach(([a,s])=>{af(s,Mv(t,a),n,r)})})},Mv=(e,t)=>{let n=e;return t.split(b1).forEach(r=>{n.nextPart.has(r)||n.nextPart.set(r,{nextPart:new Map,validators:[]}),n=n.nextPart.get(r)}),n},mC=e=>e.isThemeGetter,hC=(e,t)=>t?e.map(([n,r])=>{const i=r.map(a=>typeof a=="string"?t+a:typeof a=="object"?Object.fromEntries(Object.entries(a).map(([s,o])=>[t+s,o])):a);return[n,i]}):e,pC=e=>{if(e<1)return{get:()=>{},set:()=>{}};let t=0,n=new Map,r=new Map;const i=(a,s)=>{n.set(a,s),t++,t>e&&(t=0,r=n,n=new Map)};return{get(a){let s=n.get(a);if(s!==void 0)return s;if((s=r.get(a))!==void 0)return i(a,s),s},set(a,s){n.has(a)?n.set(a,s):i(a,s)}}},B6="!",fC=e=>{const{separator:t,experimentalParseClassName:n}=e,r=t.length===1,i=t[0],a=t.length,s=o=>{const l=[];let c=0,u=0,d;for(let w=0;w<o.length;w++){let v=o[w];if(c===0){if(v===i&&(r||o.slice(w,w+a)===t)){l.push(o.slice(u,w)),u=w+a;continue}if(v==="/"){d=w;continue}}v==="["?c++:v==="]"&&c--}const m=l.length===0?o:o.substring(u),p=m.startsWith(B6),x=p?m.substring(1):m,g=d&&d>u?d-u:void 0;return{modifiers:l,hasImportantModifier:p,baseClassName:x,maybePostfixModifierPosition:g}};return n?o=>n({className:o,parseClassName:s}):s},gC=e=>{if(e.length<=1)return e;const t=[];let n=[];return e.forEach(r=>{r[0]==="["?(t.push(...n.sort(),r),n=[]):n.push(r)}),t.push(...n.sort()),t},bC=e=>({cache:pC(e.cacheSize),parseClassName:fC(e),...cC(e)}),vC=/\s+/,xC=(e,t)=>{const{parseClassName:n,getClassGroupId:r,getConflictingClassGroupIds:i}=t,a=[],s=e.trim().split(vC);let o="";for(let l=s.length-1;l>=0;l-=1){const c=s[l],{modifiers:u,hasImportantModifier:d,baseClassName:m,maybePostfixModifierPosition:p}=n(c);let x=!!p,g=r(x?m.substring(0,p):m);if(!g){if(!x){o=c+(o.length>0?" "+o:o);continue}if(g=r(m),!g){o=c+(o.length>0?" "+o:o);continue}x=!1}const w=gC(u).join(":"),v=d?w+B6:w,$=v+g;if(a.includes($))continue;a.push($);const _=i(g,x);for(let C=0;C<_.length;++C){const k=_[C];a.push(v+k)}o=c+(o.length>0?" "+o:o)}return o};function $C(){let e=0,t,n,r="";for(;e<arguments.length;)(t=arguments[e++])&&(n=F6(t))&&(r&&(r+=" "),r+=n);return r}const F6=e=>{if(typeof e=="string")return e;let t,n="";for(let r=0;r<e.length;r++)e[r]&&(t=F6(e[r]))&&(n&&(n+=" "),n+=t);return n};function yC(e,...t){let n,r,i,a=s;function s(l){const c=t.reduce((u,d)=>d(u),e());return n=bC(c),r=n.cache.get,i=n.cache.set,a=o,o(l)}function o(l){const c=r(l);if(c)return c;const u=xC(l,n);return i(l,u),u}return function(){return a($C.apply(null,arguments))}}const lt=e=>{const t=n=>n[e]||[];return t.isThemeGetter=!0,t},j6=/^\[(?:([a-z-]+):)?(.+)\]$/i,_C=/^\d+\/\d+$/,wC=new Set(["px","full","screen"]),TC=/^(\d+(\.\d+)?)?(xs|sm|md|lg|xl)$/,EC=/\d+(%|px|r?em|[sdl]?v([hwib]|min|max)|pt|pc|in|cm|mm|cap|ch|ex|r?lh|cq(w|h|i|b|min|max))|\b(calc|min|max|clamp)\(.+\)|^0$/,kC=/^(rgba?|hsla?|hwb|(ok)?(lab|lch))\(.+\)$/,SC=/^(inset_)?-?((\d+)?\.?(\d+)[a-z]+|0)_-?((\d+)?\.?(\d+)[a-z]+|0)/,NC=/^(url|image|image-set|cross-fade|element|(repeating-)?(linear|radial|conic)-gradient)\(.+\)$/,gi=e=>ao(e)||wC.has(e)||_C.test(e),Hi=e=>Bo(e,"length",MC),ao=e=>!!e&&!Number.isNaN(Number(e)),ah=e=>Bo(e,"number",ao),dl=e=>!!e&&Number.isInteger(Number(e)),CC=e=>e.endsWith("%")&&ao(e.slice(0,-1)),De=e=>j6.test(e),qi=e=>TC.test(e),AC=new Set(["length","size","percentage"]),LC=e=>Bo(e,AC,V6),PC=e=>Bo(e,"position",V6),IC=new Set(["image","url"]),UC=e=>Bo(e,IC,OC),DC=e=>Bo(e,"",RC),ml=()=>!0,Bo=(e,t,n)=>{const r=j6.exec(e);return r?r[1]?typeof t=="string"?r[1]===t:t.has(r[1]):n(r[2]):!1},MC=e=>EC.test(e)&&!kC.test(e),V6=()=>!1,RC=e=>SC.test(e),OC=e=>NC.test(e),zC=()=>{const e=lt("colors"),t=lt("spacing"),n=lt("blur"),r=lt("brightness"),i=lt("borderColor"),a=lt("borderRadius"),s=lt("borderSpacing"),o=lt("borderWidth"),l=lt("contrast"),c=lt("grayscale"),u=lt("hueRotate"),d=lt("invert"),m=lt("gap"),p=lt("gradientColorStops"),x=lt("gradientColorStopPositions"),g=lt("inset"),w=lt("margin"),v=lt("opacity"),$=lt("padding"),_=lt("saturate"),C=lt("scale"),k=lt("sepia"),S=lt("skew"),L=lt("space"),U=lt("translate"),F=()=>["auto","contain","none"],q=()=>["auto","hidden","clip","visible","scroll"],G=()=>["auto",De,t],H=()=>[De,t],ne=()=>["",gi,Hi],K=()=>["auto",ao,De],te=()=>["bottom","center","left","left-bottom","left-top","right","right-bottom","right-top","top"],J=()=>["solid","dashed","dotted","double","none"],ae=()=>["normal","multiply","screen","overlay","darken","lighten","color-dodge","color-burn","hard-light","soft-light","difference","exclusion","hue","saturation","color","luminosity"],B=()=>["start","end","center","between","around","evenly","stretch"],X=()=>["","0",De],P=()=>["auto","avoid","all","avoid-page","page","left","right","column"],se=()=>[ao,De];return{cacheSize:500,separator:":",theme:{colors:[ml],spacing:[gi,Hi],blur:["none","",qi,De],brightness:se(),borderColor:[e],borderRadius:["none","","full",qi,De],borderSpacing:H(),borderWidth:ne(),contrast:se(),grayscale:X(),hueRotate:se(),invert:X(),gap:H(),gradientColorStops:[e],gradientColorStopPositions:[CC,Hi],inset:G(),margin:G(),opacity:se(),padding:H(),saturate:se(),scale:se(),sepia:X(),skew:se(),space:H(),translate:H()},classGroups:{aspect:[{aspect:["auto","square","video",De]}],container:["container"],columns:[{columns:[qi]}],"break-after":[{"break-after":P()}],"break-before":[{"break-before":P()}],"break-inside":[{"break-inside":["auto","avoid","avoid-page","avoid-column"]}],"box-decoration":[{"box-decoration":["slice","clone"]}],box:[{box:["border","content"]}],display:["block","inline-block","inline","flex","inline-flex","table","inline-table","table-caption","table-cell","table-column","table-column-group","table-footer-group","table-header-group","table-row-group","table-row","flow-root","grid","inline-grid","contents","list-item","hidden"],float:[{float:["right","left","none","start","end"]}],clear:[{clear:["left","right","both","none","start","end"]}],isolation:["isolate","isolation-auto"],"object-fit":[{object:["contain","cover","fill","none","scale-down"]}],"object-position":[{object:[...te(),De]}],overflow:[{overflow:q()}],"overflow-x":[{"overflow-x":q()}],"overflow-y":[{"overflow-y":q()}],overscroll:[{overscroll:F()}],"overscroll-x":[{"overscroll-x":F()}],"overscroll-y":[{"overscroll-y":F()}],position:["static","fixed","absolute","relative","sticky"],inset:[{inset:[g]}],"inset-x":[{"inset-x":[g]}],"inset-y":[{"inset-y":[g]}],start:[{start:[g]}],end:[{end:[g]}],top:[{top:[g]}],right:[{right:[g]}],bottom:[{bottom:[g]}],left:[{left:[g]}],visibility:["visible","invisible","collapse"],z:[{z:["auto",dl,De]}],basis:[{basis:G()}],"flex-direction":[{flex:["row","row-reverse","col","col-reverse"]}],"flex-wrap":[{flex:["wrap","wrap-reverse","nowrap"]}],flex:[{flex:["1","auto","initial","none",De]}],grow:[{grow:X()}],shrink:[{shrink:X()}],order:[{order:["first","last","none",dl,De]}],"grid-cols":[{"grid-cols":[ml]}],"col-start-end":[{col:["auto",{span:["full",dl,De]},De]}],"col-start":[{"col-start":K()}],"col-end":[{"col-end":K()}],"grid-rows":[{"grid-rows":[ml]}],"row-start-end":[{row:["auto",{span:[dl,De]},De]}],"row-start":[{"row-start":K()}],"row-end":[{"row-end":K()}],"grid-flow":[{"grid-flow":["row","col","dense","row-dense","col-dense"]}],"auto-cols":[{"auto-cols":["auto","min","max","fr",De]}],"auto-rows":[{"auto-rows":["auto","min","max","fr",De]}],gap:[{gap:[m]}],"gap-x":[{"gap-x":[m]}],"gap-y":[{"gap-y":[m]}],"justify-content":[{justify:["normal",...B()]}],"justify-items":[{"justify-items":["start","end","center","stretch"]}],"justify-self":[{"justify-self":["auto","start","end","center","stretch"]}],"align-content":[{content:["normal",...B(),"baseline"]}],"align-items":[{items:["start","end","center","baseline","stretch"]}],"align-self":[{self:["auto","start","end","center","stretch","baseline"]}],"place-content":[{"place-content":[...B(),"baseline"]}],"place-items":[{"place-items":["start","end","center","baseline","stretch"]}],"place-self":[{"place-self":["auto","start","end","center","stretch"]}],p:[{p:[$]}],px:[{px:[$]}],py:[{py:[$]}],ps:[{ps:[$]}],pe:[{pe:[$]}],pt:[{pt:[$]}],pr:[{pr:[$]}],pb:[{pb:[$]}],pl:[{pl:[$]}],m:[{m:[w]}],mx:[{mx:[w]}],my:[{my:[w]}],ms:[{ms:[w]}],me:[{me:[w]}],mt:[{mt:[w]}],mr:[{mr:[w]}],mb:[{mb:[w]}],ml:[{ml:[w]}],"space-x":[{"space-x":[L]}],"space-x-reverse":["space-x-reverse"],"space-y":[{"space-y":[L]}],"space-y-reverse":["space-y-reverse"],w:[{w:["auto","min","max","fit","svw","lvw","dvw",De,t]}],"min-w":[{"min-w":[De,t,"min","max","fit"]}],"max-w":[{"max-w":[De,t,"none","full","min","max","fit","prose",{screen:[qi]},qi]}],h:[{h:[De,t,"auto","min","max","fit","svh","lvh","dvh"]}],"min-h":[{"min-h":[De,t,"min","max","fit","svh","lvh","dvh"]}],"max-h":[{"max-h":[De,t,"min","max","fit","svh","lvh","dvh"]}],size:[{size:[De,t,"auto","min","max","fit"]}],"font-size":[{text:["base",qi,Hi]}],"font-smoothing":["antialiased","subpixel-antialiased"],"font-style":["italic","not-italic"],"font-weight":[{font:["thin","extralight","light","normal","medium","semibold","bold","extrabold","black",ah]}],"font-family":[{font:[ml]}],"fvn-normal":["normal-nums"],"fvn-ordinal":["ordinal"],"fvn-slashed-zero":["slashed-zero"],"fvn-figure":["lining-nums","oldstyle-nums"],"fvn-spacing":["proportional-nums","tabular-nums"],"fvn-fraction":["diagonal-fractions","stacked-fractons"],tracking:[{tracking:["tighter","tight","normal","wide","wider","widest",De]}],"line-clamp":[{"line-clamp":["none",ao,ah]}],leading:[{leading:["none","tight","snug","normal","relaxed","loose",gi,De]}],"list-image":[{"list-image":["none",De]}],"list-style-type":[{list:["none","disc","decimal",De]}],"list-style-position":[{list:["inside","outside"]}],"placeholder-color":[{placeholder:[e]}],"placeholder-opacity":[{"placeholder-opacity":[v]}],"text-alignment":[{text:["left","center","right","justify","start","end"]}],"text-color":[{text:[e]}],"text-opacity":[{"text-opacity":[v]}],"text-decoration":["underline","overline","line-through","no-underline"],"text-decoration-style":[{decoration:[...J(),"wavy"]}],"text-decoration-thickness":[{decoration:["auto","from-font",gi,Hi]}],"underline-offset":[{"underline-offset":["auto",gi,De]}],"text-decoration-color":[{decoration:[e]}],"text-transform":["uppercase","lowercase","capitalize","normal-case"],"text-overflow":["truncate","text-ellipsis","text-clip"],"text-wrap":[{text:["wrap","nowrap","balance","pretty"]}],indent:[{indent:H()}],"vertical-align":[{align:["baseline","top","middle","bottom","text-top","text-bottom","sub","super",De]}],whitespace:[{whitespace:["normal","nowrap","pre","pre-line","pre-wrap","break-spaces"]}],break:[{break:["normal","words","all","keep"]}],hyphens:[{hyphens:["none","manual","auto"]}],content:[{content:["none",De]}],"bg-attachment":[{bg:["fixed","local","scroll"]}],"bg-clip":[{"bg-clip":["border","padding","content","text"]}],"bg-opacity":[{"bg-opacity":[v]}],"bg-origin":[{"bg-origin":["border","padding","content"]}],"bg-position":[{bg:[...te(),PC]}],"bg-repeat":[{bg:["no-repeat",{repeat:["","x","y","round","space"]}]}],"bg-size":[{bg:["auto","cover","contain",LC]}],"bg-image":[{bg:["none",{"gradient-to":["t","tr","r","br","b","bl","l","tl"]},UC]}],"bg-color":[{bg:[e]}],"gradient-from-pos":[{from:[x]}],"gradient-via-pos":[{via:[x]}],"gradient-to-pos":[{to:[x]}],"gradient-from":[{from:[p]}],"gradient-via":[{via:[p]}],"gradient-to":[{to:[p]}],rounded:[{rounded:[a]}],"rounded-s":[{"rounded-s":[a]}],"rounded-e":[{"rounded-e":[a]}],"rounded-t":[{"rounded-t":[a]}],"rounded-r":[{"rounded-r":[a]}],"rounded-b":[{"rounded-b":[a]}],"rounded-l":[{"rounded-l":[a]}],"rounded-ss":[{"rounded-ss":[a]}],"rounded-se":[{"rounded-se":[a]}],"rounded-ee":[{"rounded-ee":[a]}],"rounded-es":[{"rounded-es":[a]}],"rounded-tl":[{"rounded-tl":[a]}],"rounded-tr":[{"rounded-tr":[a]}],"rounded-br":[{"rounded-br":[a]}],"rounded-bl":[{"rounded-bl":[a]}],"border-w":[{border:[o]}],"border-w-x":[{"border-x":[o]}],"border-w-y":[{"border-y":[o]}],"border-w-s":[{"border-s":[o]}],"border-w-e":[{"border-e":[o]}],"border-w-t":[{"border-t":[o]}],"border-w-r":[{"border-r":[o]}],"border-w-b":[{"border-b":[o]}],"border-w-l":[{"border-l":[o]}],"border-opacity":[{"border-opacity":[v]}],"border-style":[{border:[...J(),"hidden"]}],"divide-x":[{"divide-x":[o]}],"divide-x-reverse":["divide-x-reverse"],"divide-y":[{"divide-y":[o]}],"divide-y-reverse":["divide-y-reverse"],"divide-opacity":[{"divide-opacity":[v]}],"divide-style":[{divide:J()}],"border-color":[{border:[i]}],"border-color-x":[{"border-x":[i]}],"border-color-y":[{"border-y":[i]}],"border-color-s":[{"border-s":[i]}],"border-color-e":[{"border-e":[i]}],"border-color-t":[{"border-t":[i]}],"border-color-r":[{"border-r":[i]}],"border-color-b":[{"border-b":[i]}],"border-color-l":[{"border-l":[i]}],"divide-color":[{divide:[i]}],"outline-style":[{outline:["",...J()]}],"outline-offset":[{"outline-offset":[gi,De]}],"outline-w":[{outline:[gi,Hi]}],"outline-color":[{outline:[e]}],"ring-w":[{ring:ne()}],"ring-w-inset":["ring-inset"],"ring-color":[{ring:[e]}],"ring-opacity":[{"ring-opacity":[v]}],"ring-offset-w":[{"ring-offset":[gi,Hi]}],"ring-offset-color":[{"ring-offset":[e]}],shadow:[{shadow:["","inner","none",qi,DC]}],"shadow-color":[{shadow:[ml]}],opacity:[{opacity:[v]}],"mix-blend":[{"mix-blend":[...ae(),"plus-lighter","plus-darker"]}],"bg-blend":[{"bg-blend":ae()}],filter:[{filter:["","none"]}],blur:[{blur:[n]}],brightness:[{brightness:[r]}],contrast:[{contrast:[l]}],"drop-shadow":[{"drop-shadow":["","none",qi,De]}],grayscale:[{grayscale:[c]}],"hue-rotate":[{"hue-rotate":[u]}],invert:[{invert:[d]}],saturate:[{saturate:[_]}],sepia:[{sepia:[k]}],"backdrop-filter":[{"backdrop-filter":["","none"]}],"backdrop-blur":[{"backdrop-blur":[n]}],"backdrop-brightness":[{"backdrop-brightness":[r]}],"backdrop-contrast":[{"backdrop-contrast":[l]}],"backdrop-grayscale":[{"backdrop-grayscale":[c]}],"backdrop-hue-rotate":[{"backdrop-hue-rotate":[u]}],"backdrop-invert":[{"backdrop-invert":[d]}],"backdrop-opacity":[{"backdrop-opacity":[v]}],"backdrop-saturate":[{"backdrop-saturate":[_]}],"backdrop-sepia":[{"backdrop-sepia":[k]}],"border-collapse":[{border:["collapse","separate"]}],"border-spacing":[{"border-spacing":[s]}],"border-spacing-x":[{"border-spacing-x":[s]}],"border-spacing-y":[{"border-spacing-y":[s]}],"table-layout":[{table:["auto","fixed"]}],caption:[{caption:["top","bottom"]}],transition:[{transition:["none","all","","colors","opacity","shadow","transform",De]}],duration:[{duration:se()}],ease:[{ease:["linear","in","out","in-out",De]}],delay:[{delay:se()}],animate:[{animate:["none","spin","ping","pulse","bounce",De]}],transform:[{transform:["","gpu","none"]}],scale:[{scale:[C]}],"scale-x":[{"scale-x":[C]}],"scale-y":[{"scale-y":[C]}],rotate:[{rotate:[dl,De]}],"translate-x":[{"translate-x":[U]}],"translate-y":[{"translate-y":[U]}],"skew-x":[{"skew-x":[S]}],"skew-y":[{"skew-y":[S]}],"transform-origin":[{origin:["center","top","top-right","right","bottom-right","bottom","bottom-left","left","top-left",De]}],accent:[{accent:["auto",e]}],appearance:[{appearance:["none","auto"]}],cursor:[{cursor:["auto","default","pointer","wait","text","move","help","not-allowed","none","context-menu","progress","cell","crosshair","vertical-text","alias","copy","no-drop","grab","grabbing","all-scroll","col-resize","row-resize","n-resize","e-resize","s-resize","w-resize","ne-resize","nw-resize","se-resize","sw-resize","ew-resize","ns-resize","nesw-resize","nwse-resize","zoom-in","zoom-out",De]}],"caret-color":[{caret:[e]}],"pointer-events":[{"pointer-events":["none","auto"]}],resize:[{resize:["none","y","x",""]}],"scroll-behavior":[{scroll:["auto","smooth"]}],"scroll-m":[{"scroll-m":H()}],"scroll-mx":[{"scroll-mx":H()}],"scroll-my":[{"scroll-my":H()}],"scroll-ms":[{"scroll-ms":H()}],"scroll-me":[{"scroll-me":H()}],"scroll-mt":[{"scroll-mt":H()}],"scroll-mr":[{"scroll-mr":H()}],"scroll-mb":[{"scroll-mb":H()}],"scroll-ml":[{"scroll-ml":H()}],"scroll-p":[{"scroll-p":H()}],"scroll-px":[{"scroll-px":H()}],"scroll-py":[{"scroll-py":H()}],"scroll-ps":[{"scroll-ps":H()}],"scroll-pe":[{"scroll-pe":H()}],"scroll-pt":[{"scroll-pt":H()}],"scroll-pr":[{"scroll-pr":H()}],"scroll-pb":[{"scroll-pb":H()}],"scroll-pl":[{"scroll-pl":H()}],"snap-align":[{snap:["start","end","center","align-none"]}],"snap-stop":[{snap:["normal","always"]}],"snap-type":[{snap:["none","x","y","both"]}],"snap-strictness":[{snap:["mandatory","proximity"]}],touch:[{touch:["auto","none","manipulation"]}],"touch-x":[{"touch-pan":["x","left","right"]}],"touch-y":[{"touch-pan":["y","up","down"]}],"touch-pz":["touch-pinch-zoom"],select:[{select:["none","text","all","auto"]}],"will-change":[{"will-change":["auto","scroll","contents","transform",De]}],fill:[{fill:[e,"none"]}],"stroke-w":[{stroke:[gi,Hi,ah]}],stroke:[{stroke:[e,"none"]}],sr:["sr-only","not-sr-only"],"forced-color-adjust":[{"forced-color-adjust":["auto","none"]}]},conflictingClassGroups:{overflow:["overflow-x","overflow-y"],overscroll:["overscroll-x","overscroll-y"],inset:["inset-x","inset-y","start","end","top","right","bottom","left"],"inset-x":["right","left"],"inset-y":["top","bottom"],flex:["basis","grow","shrink"],gap:["gap-x","gap-y"],p:["px","py","ps","pe","pt","pr","pb","pl"],px:["pr","pl"],py:["pt","pb"],m:["mx","my","ms","me","mt","mr","mb","ml"],mx:["mr","ml"],my:["mt","mb"],size:["w","h"],"font-size":["leading"],"fvn-normal":["fvn-ordinal","fvn-slashed-zero","fvn-figure","fvn-spacing","fvn-fraction"],"fvn-ordinal":["fvn-normal"],"fvn-slashed-zero":["fvn-normal"],"fvn-figure":["fvn-normal"],"fvn-spacing":["fvn-normal"],"fvn-fraction":["fvn-normal"],"line-clamp":["display","overflow"],rounded:["rounded-s","rounded-e","rounded-t","rounded-r","rounded-b","rounded-l","rounded-ss","rounded-se","rounded-ee","rounded-es","rounded-tl","rounded-tr","rounded-br","rounded-bl"],"rounded-s":["rounded-ss","rounded-es"],"rounded-e":["rounded-se","rounded-ee"],"rounded-t":["rounded-tl","rounded-tr"],"rounded-r":["rounded-tr","rounded-br"],"rounded-b":["rounded-br","rounded-bl"],"rounded-l":["rounded-tl","rounded-bl"],"border-spacing":["border-spacing-x","border-spacing-y"],"border-w":["border-w-s","border-w-e","border-w-t","border-w-r","border-w-b","border-w-l"],"border-w-x":["border-w-r","border-w-l"],"border-w-y":["border-w-t","border-w-b"],"border-color":["border-color-s","border-color-e","border-color-t","border-color-r","border-color-b","border-color-l"],"border-color-x":["border-color-r","border-color-l"],"border-color-y":["border-color-t","border-color-b"],"scroll-m":["scroll-mx","scroll-my","scroll-ms","scroll-me","scroll-mt","scroll-mr","scroll-mb","scroll-ml"],"scroll-mx":["scroll-mr","scroll-ml"],"scroll-my":["scroll-mt","scroll-mb"],"scroll-p":["scroll-px","scroll-py","scroll-ps","scroll-pe","scroll-pt","scroll-pr","scroll-pb","scroll-pl"],"scroll-px":["scroll-pr","scroll-pl"],"scroll-py":["scroll-pt","scroll-pb"],touch:["touch-x","touch-y","touch-pz"],"touch-x":["touch"],"touch-y":["touch"],"touch-pz":["touch"]},conflictingClassGroupModifiers:{"font-size":["leading"]}}},BC=yC(zC);function Ue(...e){return BC(A6(e))}const FC=WN,H6=T.forwardRef(({className:e,...t},n)=>h.jsx(w6,{ref:n,className:Ue("fixed top-0 z-[100] flex max-h-screen w-full flex-col-reverse p-4 sm:bottom-0 sm:right-0 sm:top-auto sm:flex-col md:max-w-[420px]",e),...t}));H6.displayName=w6.displayName;const jC=p1("group pointer-events-auto relative flex w-full items-center justify-between space-x-4 overflow-hidden rounded-md border p-6 pr-8 shadow-lg transition-all data-[swipe=cancel]:translate-x-0 data-[swipe=end]:translate-x-[var(--radix-toast-swipe-end-x)] data-[swipe=move]:translate-x-[var(--radix-toast-swipe-move-x)] data-[swipe=move]:transition-none data-[state=open]:animate-in data-[state=closed]:animate-out data-[swipe=end]:animate-out data-[state=closed]:fade-out-80 data-[state=closed]:slide-out-to-right-full data-[state=open]:slide-in-from-top-full data-[state=open]:sm:slide-in-from-bottom-full",{variants:{variant:{default:"border bg-background text-foreground",destructive:"destructive group border-destructive bg-destructive text-destructive-foreground"}},defaultVariants:{variant:"default"}}),q6=T.forwardRef(({className:e,variant:t,...n},r)=>h.jsx(T6,{ref:r,className:Ue(jC({variant:t}),e),...n}));q6.displayName=T6.displayName;const VC=T.forwardRef(({className:e,...t},n)=>h.jsx(S6,{ref:n,className:Ue("inline-flex h-8 shrink-0 items-center justify-center rounded-md border bg-transparent px-3 text-sm font-medium ring-offset-background transition-colors hover:bg-secondary focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 group-[.destructive]:border-muted/40 group-[.destructive]:hover:border-destructive/30 group-[.destructive]:hover:bg-destructive group-[.destructive]:hover:text-destructive-foreground group-[.destructive]:focus:ring-destructive",e),...t}));VC.displayName=S6.displayName;const G6=T.forwardRef(({className:e,...t},n)=>h.jsx(N6,{ref:n,className:Ue("absolute right-2 top-2 rounded-md p-1 text-foreground/50 opacity-0 transition-opacity hover:text-foreground focus:opacity-100 focus:outline-none focus:ring-2 group-hover:opacity-100 group-[.destructive]:text-red-300 group-[.destructive]:hover:text-red-50 group-[.destructive]:focus:ring-red-400 group-[.destructive]:focus:ring-offset-red-600",e),"toast-close":"",...t,children:h.jsx(O6,{className:"h-4 w-4"})}));G6.displayName=N6.displayName;const W6=T.forwardRef(({className:e,...t},n)=>h.jsx(E6,{ref:n,className:Ue("text-sm font-semibold",e),...t}));W6.displayName=E6.displayName;const K6=T.forwardRef(({className:e,...t},n)=>h.jsx(k6,{ref:n,className:Ue("text-sm opacity-90",e),...t}));K6.displayName=k6.displayName;function HC(){const{toasts:e}=r6();return h.jsxs(FC,{children:[e.map(function({id:t,title:n,description:r,action:i,...a}){return h.jsxs(q6,{...a,children:[h.jsxs("div",{className:"grid gap-1",children:[n&&h.jsx(W6,{children:n}),r&&h.jsx(K6,{children:r})]}),i,h.jsx(G6,{})]},t)}),h.jsx(H6,{})]})}var Rv=["light","dark"],qC="(prefers-color-scheme: dark)",GC=T.createContext(void 0),WC={setTheme:e=>{},themes:[]},KC=()=>{var e;return(e=T.useContext(GC))!=null?e:WC};T.memo(({forcedTheme:e,storageKey:t,attribute:n,enableSystem:r,enableColorScheme:i,defaultTheme:a,value:s,attrs:o,nonce:l})=>{let c=a==="system",u=n==="class"?`var d=document.documentElement,c=d.classList;${`c.remove(${o.map(x=>`'${x}'`).join(",")})`};`:`var d=document.documentElement,n='${n}',s='setAttribute';`,d=i?Rv.includes(a)&&a?`if(e==='light'||e==='dark'||!e)d.style.colorScheme=e||'${a}'`:"if(e==='light'||e==='dark')d.style.colorScheme=e":"",m=(x,g=!1,w=!0)=>{let v=s?s[x]:x,$=g?x+"|| ''":`'${v}'`,_="";return i&&w&&!g&&Rv.includes(x)&&(_+=`d.style.colorScheme = '${x}';`),n==="class"?g||v?_+=`c.add(${$})`:_+="null":v&&(_+=`d[s](n,${$})`),_},p=e?`!function(){${u}${m(e)}}()`:r?`!function(){try{${u}var e=localStorage.getItem('${t}');if('system'===e||(!e&&${c})){var t='${qC}',m=window.matchMedia(t);if(m.media!==t||m.matches){${m("dark")}}else{${m("light")}}}else if(e){${s?`var x=${JSON.stringify(s)};`:""}${m(s?"x[e]":"e",!0)}}${c?"":"else{"+m(a,!1,!1)+"}"}${d}}catch(e){}}()`:`!function(){try{${u}var e=localStorage.getItem('${t}');if(e){${s?`var x=${JSON.stringify(s)};`:""}${m(s?"x[e]":"e",!0)}}else{${m(a,!1,!1)};}${d}}catch(t){}}();`;return T.createElement("script",{nonce:l,dangerouslySetInnerHTML:{__html:p}})});var YC=e=>{switch(e){case"success":return ZC;case"info":return eA;case"warning":return JC;case"error":return tA;default:return null}},XC=Array(12).fill(0),QC=({visible:e})=>pe.createElement("div",{className:"sonner-loading-wrapper","data-visible":e},pe.createElement("div",{className:"sonner-spinner"},XC.map((t,n)=>pe.createElement("div",{className:"sonner-loading-bar",key:`spinner-bar-${n}`})))),ZC=pe.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 20 20",fill:"currentColor",height:"20",width:"20"},pe.createElement("path",{fillRule:"evenodd",d:"M10 18a8 8 0 100-16 8 8 0 000 16zm3.857-9.809a.75.75 0 00-1.214-.882l-3.483 4.79-1.88-1.88a.75.75 0 10-1.06 1.061l2.5 2.5a.75.75 0 001.137-.089l4-5.5z",clipRule:"evenodd"})),JC=pe.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 24 24",fill:"currentColor",height:"20",width:"20"},pe.createElement("path",{fillRule:"evenodd",d:"M9.401 3.003c1.155-2 4.043-2 5.197 0l7.355 12.748c1.154 2-.29 4.5-2.599 4.5H4.645c-2.309 0-3.752-2.5-2.598-4.5L9.4 3.003zM12 8.25a.75.75 0 01.75.75v3.75a.75.75 0 01-1.5 0V9a.75.75 0 01.75-.75zm0 8.25a.75.75 0 100-1.5.75.75 0 000 1.5z",clipRule:"evenodd"})),eA=pe.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 20 20",fill:"currentColor",height:"20",width:"20"},pe.createElement("path",{fillRule:"evenodd",d:"M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-7-4a1 1 0 11-2 0 1 1 0 012 0zM9 9a.75.75 0 000 1.5h.253a.25.25 0 01.244.304l-.459 2.066A1.75 1.75 0 0010.747 15H11a.75.75 0 000-1.5h-.253a.25.25 0 01-.244-.304l.459-2.066A1.75 1.75 0 009.253 9H9z",clipRule:"evenodd"})),tA=pe.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 20 20",fill:"currentColor",height:"20",width:"20"},pe.createElement("path",{fillRule:"evenodd",d:"M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-8-5a.75.75 0 01.75.75v4.5a.75.75 0 01-1.5 0v-4.5A.75.75 0 0110 5zm0 10a1 1 0 100-2 1 1 0 000 2z",clipRule:"evenodd"})),nA=()=>{let[e,t]=pe.useState(document.hidden);return pe.useEffect(()=>{let n=()=>{t(document.hidden)};return document.addEventListener("visibilitychange",n),()=>window.removeEventListener("visibilitychange",n)},[]),e},sf=1,rA=class{constructor(){this.subscribe=e=>(this.subscribers.push(e),()=>{let t=this.subscribers.indexOf(e);this.subscribers.splice(t,1)}),this.publish=e=>{this.subscribers.forEach(t=>t(e))},this.addToast=e=>{this.publish(e),this.toasts=[...this.toasts,e]},this.create=e=>{var t;let{message:n,...r}=e,i=typeof(e==null?void 0:e.id)=="number"||((t=e.id)==null?void 0:t.length)>0?e.id:sf++,a=this.toasts.find(o=>o.id===i),s=e.dismissible===void 0?!0:e.dismissible;return a?this.toasts=this.toasts.map(o=>o.id===i?(this.publish({...o,...e,id:i,title:n}),{...o,...e,id:i,dismissible:s,title:n}):o):this.addToast({title:n,...r,dismissible:s,id:i}),i},this.dismiss=e=>(e||this.toasts.forEach(t=>{this.subscribers.forEach(n=>n({id:t.id,dismiss:!0}))}),this.subscribers.forEach(t=>t({id:e,dismiss:!0})),e),this.message=(e,t)=>this.create({...t,message:e}),this.error=(e,t)=>this.create({...t,message:e,type:"error"}),this.success=(e,t)=>this.create({...t,type:"success",message:e}),this.info=(e,t)=>this.create({...t,type:"info",message:e}),this.warning=(e,t)=>this.create({...t,type:"warning",message:e}),this.loading=(e,t)=>this.create({...t,type:"loading",message:e}),this.promise=(e,t)=>{if(!t)return;let n;t.loading!==void 0&&(n=this.create({...t,promise:e,type:"loading",message:t.loading,description:typeof t.description!="function"?t.description:void 0}));let r=e instanceof Promise?e:e(),i=n!==void 0;return r.then(async a=>{if(aA(a)&&!a.ok){i=!1;let s=typeof t.error=="function"?await t.error(`HTTP error! status: ${a.status}`):t.error,o=typeof t.description=="function"?await t.description(`HTTP error! status: ${a.status}`):t.description;this.create({id:n,type:"error",message:s,description:o})}else if(t.success!==void 0){i=!1;let s=typeof t.success=="function"?await t.success(a):t.success,o=typeof t.description=="function"?await t.description(a):t.description;this.create({id:n,type:"success",message:s,description:o})}}).catch(async a=>{if(t.error!==void 0){i=!1;let s=typeof t.error=="function"?await t.error(a):t.error,o=typeof t.description=="function"?await t.description(a):t.description;this.create({id:n,type:"error",message:s,description:o})}}).finally(()=>{var a;i&&(this.dismiss(n),n=void 0),(a=t.finally)==null||a.call(t)}),n},this.custom=(e,t)=>{let n=(t==null?void 0:t.id)||sf++;return this.create({jsx:e(n),id:n,...t}),n},this.subscribers=[],this.toasts=[]}},zn=new rA,iA=(e,t)=>{let n=(t==null?void 0:t.id)||sf++;return zn.addToast({title:e,...t,id:n}),n},aA=e=>e&&typeof e=="object"&&"ok"in e&&typeof e.ok=="boolean"&&"status"in e&&typeof e.status=="number",sA=iA,oA=()=>zn.toasts;Object.assign(sA,{success:zn.success,info:zn.info,warning:zn.warning,error:zn.error,custom:zn.custom,message:zn.message,promise:zn.promise,dismiss:zn.dismiss,loading:zn.loading},{getHistory:oA});function lA(e,{insertAt:t}={}){if(typeof document>"u")return;let n=document.head||document.getElementsByTagName("head")[0],r=document.createElement("style");r.type="text/css",t==="top"&&n.firstChild?n.insertBefore(r,n.firstChild):n.appendChild(r),r.styleSheet?r.styleSheet.cssText=e:r.appendChild(document.createTextNode(e))}lA(`:where(html[dir="ltr"]),:where([data-sonner-toaster][dir="ltr"]){--toast-icon-margin-start: -3px;--toast-icon-margin-end: 4px;--toast-svg-margin-start: -1px;--toast-svg-margin-end: 0px;--toast-button-margin-start: auto;--toast-button-margin-end: 0;--toast-close-button-start: 0;--toast-close-button-end: unset;--toast-close-button-transform: translate(-35%, -35%)}:where(html[dir="rtl"]),:where([data-sonner-toaster][dir="rtl"]){--toast-icon-margin-start: 4px;--toast-icon-margin-end: -3px;--toast-svg-margin-start: 0px;--toast-svg-margin-end: -1px;--toast-button-margin-start: 0;--toast-button-margin-end: auto;--toast-close-button-start: unset;--toast-close-button-end: 0;--toast-close-button-transform: translate(35%, -35%)}:where([data-sonner-toaster]){position:fixed;width:var(--width);font-family:ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji;--gray1: hsl(0, 0%, 99%);--gray2: hsl(0, 0%, 97.3%);--gray3: hsl(0, 0%, 95.1%);--gray4: hsl(0, 0%, 93%);--gray5: hsl(0, 0%, 90.9%);--gray6: hsl(0, 0%, 88.7%);--gray7: hsl(0, 0%, 85.8%);--gray8: hsl(0, 0%, 78%);--gray9: hsl(0, 0%, 56.1%);--gray10: hsl(0, 0%, 52.3%);--gray11: hsl(0, 0%, 43.5%);--gray12: hsl(0, 0%, 9%);--border-radius: 8px;box-sizing:border-box;padding:0;margin:0;list-style:none;outline:none;z-index:999999999}:where([data-sonner-toaster][data-x-position="right"]){right:max(var(--offset),env(safe-area-inset-right))}:where([data-sonner-toaster][data-x-position="left"]){left:max(var(--offset),env(safe-area-inset-left))}:where([data-sonner-toaster][data-x-position="center"]){left:50%;transform:translate(-50%)}:where([data-sonner-toaster][data-y-position="top"]){top:max(var(--offset),env(safe-area-inset-top))}:where([data-sonner-toaster][data-y-position="bottom"]){bottom:max(var(--offset),env(safe-area-inset-bottom))}:where([data-sonner-toast]){--y: translateY(100%);--lift-amount: calc(var(--lift) * var(--gap));z-index:var(--z-index);position:absolute;opacity:0;transform:var(--y);filter:blur(0);touch-action:none;transition:transform .4s,opacity .4s,height .4s,box-shadow .2s;box-sizing:border-box;outline:none;overflow-wrap:anywhere}:where([data-sonner-toast][data-styled="true"]){padding:16px;background:var(--normal-bg);border:1px solid var(--normal-border);color:var(--normal-text);border-radius:var(--border-radius);box-shadow:0 4px 12px #0000001a;width:var(--width);font-size:13px;display:flex;align-items:center;gap:6px}:where([data-sonner-toast]:focus-visible){box-shadow:0 4px 12px #0000001a,0 0 0 2px #0003}:where([data-sonner-toast][data-y-position="top"]){top:0;--y: translateY(-100%);--lift: 1;--lift-amount: calc(1 * var(--gap))}:where([data-sonner-toast][data-y-position="bottom"]){bottom:0;--y: translateY(100%);--lift: -1;--lift-amount: calc(var(--lift) * var(--gap))}:where([data-sonner-toast]) :where([data-description]){font-weight:400;line-height:1.4;color:inherit}:where([data-sonner-toast]) :where([data-title]){font-weight:500;line-height:1.5;color:inherit}:where([data-sonner-toast]) :where([data-icon]){display:flex;height:16px;width:16px;position:relative;justify-content:flex-start;align-items:center;flex-shrink:0;margin-left:var(--toast-icon-margin-start);margin-right:var(--toast-icon-margin-end)}:where([data-sonner-toast][data-promise="true"]) :where([data-icon])>svg{opacity:0;transform:scale(.8);transform-origin:center;animation:sonner-fade-in .3s ease forwards}:where([data-sonner-toast]) :where([data-icon])>*{flex-shrink:0}:where([data-sonner-toast]) :where([data-icon]) svg{margin-left:var(--toast-svg-margin-start);margin-right:var(--toast-svg-margin-end)}:where([data-sonner-toast]) :where([data-content]){display:flex;flex-direction:column;gap:2px}[data-sonner-toast][data-styled=true] [data-button]{border-radius:4px;padding-left:8px;padding-right:8px;height:24px;font-size:12px;color:var(--normal-bg);background:var(--normal-text);margin-left:var(--toast-button-margin-start);margin-right:var(--toast-button-margin-end);border:none;cursor:pointer;outline:none;display:flex;align-items:center;flex-shrink:0;transition:opacity .4s,box-shadow .2s}:where([data-sonner-toast]) :where([data-button]):focus-visible{box-shadow:0 0 0 2px #0006}:where([data-sonner-toast]) :where([data-button]):first-of-type{margin-left:var(--toast-button-margin-start);margin-right:var(--toast-button-margin-end)}:where([data-sonner-toast]) :where([data-cancel]){color:var(--normal-text);background:rgba(0,0,0,.08)}:where([data-sonner-toast][data-theme="dark"]) :where([data-cancel]){background:rgba(255,255,255,.3)}:where([data-sonner-toast]) :where([data-close-button]){position:absolute;left:var(--toast-close-button-start);right:var(--toast-close-button-end);top:0;height:20px;width:20px;display:flex;justify-content:center;align-items:center;padding:0;background:var(--gray1);color:var(--gray12);border:1px solid var(--gray4);transform:var(--toast-close-button-transform);border-radius:50%;cursor:pointer;z-index:1;transition:opacity .1s,background .2s,border-color .2s}:where([data-sonner-toast]) :where([data-close-button]):focus-visible{box-shadow:0 4px 12px #0000001a,0 0 0 2px #0003}:where([data-sonner-toast]) :where([data-disabled="true"]){cursor:not-allowed}:where([data-sonner-toast]):hover :where([data-close-button]):hover{background:var(--gray2);border-color:var(--gray5)}:where([data-sonner-toast][data-swiping="true"]):before{content:"";position:absolute;left:0;right:0;height:100%;z-index:-1}:where([data-sonner-toast][data-y-position="top"][data-swiping="true"]):before{bottom:50%;transform:scaleY(3) translateY(50%)}:where([data-sonner-toast][data-y-position="bottom"][data-swiping="true"]):before{top:50%;transform:scaleY(3) translateY(-50%)}:where([data-sonner-toast][data-swiping="false"][data-removed="true"]):before{content:"";position:absolute;inset:0;transform:scaleY(2)}:where([data-sonner-toast]):after{content:"";position:absolute;left:0;height:calc(var(--gap) + 1px);bottom:100%;width:100%}:where([data-sonner-toast][data-mounted="true"]){--y: translateY(0);opacity:1}:where([data-sonner-toast][data-expanded="false"][data-front="false"]){--scale: var(--toasts-before) * .05 + 1;--y: translateY(calc(var(--lift-amount) * var(--toasts-before))) scale(calc(-1 * var(--scale)));height:var(--front-toast-height)}:where([data-sonner-toast])>*{transition:opacity .4s}:where([data-sonner-toast][data-expanded="false"][data-front="false"][data-styled="true"])>*{opacity:0}:where([data-sonner-toast][data-visible="false"]){opacity:0;pointer-events:none}:where([data-sonner-toast][data-mounted="true"][data-expanded="true"]){--y: translateY(calc(var(--lift) * var(--offset)));height:var(--initial-height)}:where([data-sonner-toast][data-removed="true"][data-front="true"][data-swipe-out="false"]){--y: translateY(calc(var(--lift) * -100%));opacity:0}:where([data-sonner-toast][data-removed="true"][data-front="false"][data-swipe-out="false"][data-expanded="true"]){--y: translateY(calc(var(--lift) * var(--offset) + var(--lift) * -100%));opacity:0}:where([data-sonner-toast][data-removed="true"][data-front="false"][data-swipe-out="false"][data-expanded="false"]){--y: translateY(40%);opacity:0;transition:transform .5s,opacity .2s}:where([data-sonner-toast][data-removed="true"][data-front="false"]):before{height:calc(var(--initial-height) + 20%)}[data-sonner-toast][data-swiping=true]{transform:var(--y) translateY(var(--swipe-amount, 0px));transition:none}[data-sonner-toast][data-swipe-out=true][data-y-position=bottom],[data-sonner-toast][data-swipe-out=true][data-y-position=top]{animation:swipe-out .2s ease-out forwards}@keyframes swipe-out{0%{transform:translateY(calc(var(--lift) * var(--offset) + var(--swipe-amount)));opacity:1}to{transform:translateY(calc(var(--lift) * var(--offset) + var(--swipe-amount) + var(--lift) * -100%));opacity:0}}@media (max-width: 600px){[data-sonner-toaster]{position:fixed;--mobile-offset: 16px;right:var(--mobile-offset);left:var(--mobile-offset);width:100%}[data-sonner-toaster] [data-sonner-toast]{left:0;right:0;width:calc(100% - var(--mobile-offset) * 2)}[data-sonner-toaster][data-x-position=left]{left:var(--mobile-offset)}[data-sonner-toaster][data-y-position=bottom]{bottom:20px}[data-sonner-toaster][data-y-position=top]{top:20px}[data-sonner-toaster][data-x-position=center]{left:var(--mobile-offset);right:var(--mobile-offset);transform:none}}[data-sonner-toaster][data-theme=light]{--normal-bg: #fff;--normal-border: var(--gray4);--normal-text: var(--gray12);--success-bg: hsl(143, 85%, 96%);--success-border: hsl(145, 92%, 91%);--success-text: hsl(140, 100%, 27%);--info-bg: hsl(208, 100%, 97%);--info-border: hsl(221, 91%, 91%);--info-text: hsl(210, 92%, 45%);--warning-bg: hsl(49, 100%, 97%);--warning-border: hsl(49, 91%, 91%);--warning-text: hsl(31, 92%, 45%);--error-bg: hsl(359, 100%, 97%);--error-border: hsl(359, 100%, 94%);--error-text: hsl(360, 100%, 45%)}[data-sonner-toaster][data-theme=light] [data-sonner-toast][data-invert=true]{--normal-bg: #000;--normal-border: hsl(0, 0%, 20%);--normal-text: var(--gray1)}[data-sonner-toaster][data-theme=dark] [data-sonner-toast][data-invert=true]{--normal-bg: #fff;--normal-border: var(--gray3);--normal-text: var(--gray12)}[data-sonner-toaster][data-theme=dark]{--normal-bg: #000;--normal-border: hsl(0, 0%, 20%);--normal-text: var(--gray1);--success-bg: hsl(150, 100%, 6%);--success-border: hsl(147, 100%, 12%);--success-text: hsl(150, 86%, 65%);--info-bg: hsl(215, 100%, 6%);--info-border: hsl(223, 100%, 12%);--info-text: hsl(216, 87%, 65%);--warning-bg: hsl(64, 100%, 6%);--warning-border: hsl(60, 100%, 12%);--warning-text: hsl(46, 87%, 65%);--error-bg: hsl(358, 76%, 10%);--error-border: hsl(357, 89%, 16%);--error-text: hsl(358, 100%, 81%)}[data-rich-colors=true][data-sonner-toast][data-type=success],[data-rich-colors=true][data-sonner-toast][data-type=success] [data-close-button]{background:var(--success-bg);border-color:var(--success-border);color:var(--success-text)}[data-rich-colors=true][data-sonner-toast][data-type=info],[data-rich-colors=true][data-sonner-toast][data-type=info] [data-close-button]{background:var(--info-bg);border-color:var(--info-border);color:var(--info-text)}[data-rich-colors=true][data-sonner-toast][data-type=warning],[data-rich-colors=true][data-sonner-toast][data-type=warning] [data-close-button]{background:var(--warning-bg);border-color:var(--warning-border);color:var(--warning-text)}[data-rich-colors=true][data-sonner-toast][data-type=error],[data-rich-colors=true][data-sonner-toast][data-type=error] [data-close-button]{background:var(--error-bg);border-color:var(--error-border);color:var(--error-text)}.sonner-loading-wrapper{--size: 16px;height:var(--size);width:var(--size);position:absolute;inset:0;z-index:10}.sonner-loading-wrapper[data-visible=false]{transform-origin:center;animation:sonner-fade-out .2s ease forwards}.sonner-spinner{position:relative;top:50%;left:50%;height:var(--size);width:var(--size)}.sonner-loading-bar{animation:sonner-spin 1.2s linear infinite;background:var(--gray11);border-radius:6px;height:8%;left:-10%;position:absolute;top:-3.9%;width:24%}.sonner-loading-bar:nth-child(1){animation-delay:-1.2s;transform:rotate(.0001deg) translate(146%)}.sonner-loading-bar:nth-child(2){animation-delay:-1.1s;transform:rotate(30deg) translate(146%)}.sonner-loading-bar:nth-child(3){animation-delay:-1s;transform:rotate(60deg) translate(146%)}.sonner-loading-bar:nth-child(4){animation-delay:-.9s;transform:rotate(90deg) translate(146%)}.sonner-loading-bar:nth-child(5){animation-delay:-.8s;transform:rotate(120deg) translate(146%)}.sonner-loading-bar:nth-child(6){animation-delay:-.7s;transform:rotate(150deg) translate(146%)}.sonner-loading-bar:nth-child(7){animation-delay:-.6s;transform:rotate(180deg) translate(146%)}.sonner-loading-bar:nth-child(8){animation-delay:-.5s;transform:rotate(210deg) translate(146%)}.sonner-loading-bar:nth-child(9){animation-delay:-.4s;transform:rotate(240deg) translate(146%)}.sonner-loading-bar:nth-child(10){animation-delay:-.3s;transform:rotate(270deg) translate(146%)}.sonner-loading-bar:nth-child(11){animation-delay:-.2s;transform:rotate(300deg) translate(146%)}.sonner-loading-bar:nth-child(12){animation-delay:-.1s;transform:rotate(330deg) translate(146%)}@keyframes sonner-fade-in{0%{opacity:0;transform:scale(.8)}to{opacity:1;transform:scale(1)}}@keyframes sonner-fade-out{0%{opacity:1;transform:scale(1)}to{opacity:0;transform:scale(.8)}}@keyframes sonner-spin{0%{opacity:1}to{opacity:.15}}@media (prefers-reduced-motion){[data-sonner-toast],[data-sonner-toast]>*,.sonner-loading-bar{transition:none!important;animation:none!important}}.sonner-loader{position:absolute;top:50%;left:50%;transform:translate(-50%,-50%);transform-origin:center;transition:opacity .2s,transform .2s}.sonner-loader[data-visible=false]{opacity:0;transform:scale(.8) translate(-50%,-50%)}
`);function Nu(e){return e.label!==void 0}var cA=3,uA="32px",dA=4e3,mA=356,hA=14,pA=20,fA=200;function gA(...e){return e.filter(Boolean).join(" ")}var bA=e=>{var t,n,r,i,a,s,o,l,c,u;let{invert:d,toast:m,unstyled:p,interacting:x,setHeights:g,visibleToasts:w,heights:v,index:$,toasts:_,expanded:C,removeToast:k,defaultRichColors:S,closeButton:L,style:U,cancelButtonStyle:F,actionButtonStyle:q,className:G="",descriptionClassName:H="",duration:ne,position:K,gap:te,loadingIcon:J,expandByDefault:ae,classNames:B,icons:X,closeButtonAriaLabel:P="Close toast",pauseWhenPageIsHidden:se,cn:he}=e,[D,Ee]=pe.useState(!1),[je,xe]=pe.useState(!1),[be,Ne]=pe.useState(!1),[He,Le]=pe.useState(!1),[Be,Ve]=pe.useState(0),[Dt,Mt]=pe.useState(0),xn=pe.useRef(null),Ft=pe.useRef(null),$n=$===0,Ur=$+1<=w,st=m.type,pi=m.dismissible!==!1,km=m.className||"",Sm=m.descriptionClassName||"",Ss=pe.useMemo(()=>v.findIndex(Te=>Te.toastId===m.id)||0,[v,m.id]),Nm=pe.useMemo(()=>{var Te;return(Te=m.closeButton)!=null?Te:L},[m.closeButton,L]),au=pe.useMemo(()=>m.duration||ne||dA,[m.duration,ne]),Y=pe.useRef(0),de=pe.useRef(0),Ce=pe.useRef(0),Pe=pe.useRef(null),[Xe,Dn]=K.split("-"),Dr=pe.useMemo(()=>v.reduce((Te,ot,tt)=>tt>=Ss?Te:Te+ot.height,0),[v,Ss]),Zn=nA(),ja=m.invert||d,fi=st==="loading";de.current=pe.useMemo(()=>Ss*te+Dr,[Ss,Dr]),pe.useEffect(()=>{Ee(!0)},[]),pe.useLayoutEffect(()=>{if(!D)return;let Te=Ft.current,ot=Te.style.height;Te.style.height="auto";let tt=Te.getBoundingClientRect().height;Te.style.height=ot,Mt(tt),g(Mr=>Mr.find(Rr=>Rr.toastId===m.id)?Mr.map(Rr=>Rr.toastId===m.id?{...Rr,height:tt}:Rr):[{toastId:m.id,height:tt,position:m.position},...Mr])},[D,m.title,m.description,g,m.id]);let _t=pe.useCallback(()=>{xe(!0),Ve(de.current),g(Te=>Te.filter(ot=>ot.toastId!==m.id)),setTimeout(()=>{k(m)},fA)},[m,k,g,de]);pe.useEffect(()=>{if(m.promise&&st==="loading"||m.duration===1/0||m.type==="loading")return;let Te,ot=au;return C||x||se&&Zn?(()=>{if(Ce.current<Y.current){let tt=new Date().getTime()-Y.current;ot=ot-tt}Ce.current=new Date().getTime()})():ot!==1/0&&(Y.current=new Date().getTime(),Te=setTimeout(()=>{var tt;(tt=m.onAutoClose)==null||tt.call(m,m),_t()},ot)),()=>clearTimeout(Te)},[C,x,ae,m,au,_t,m.promise,st,se,Zn]),pe.useEffect(()=>{let Te=Ft.current;if(Te){let ot=Te.getBoundingClientRect().height;return Mt(ot),g(tt=>[{toastId:m.id,height:ot,position:m.position},...tt]),()=>g(tt=>tt.filter(Mr=>Mr.toastId!==m.id))}},[g,m.id]),pe.useEffect(()=>{m.delete&&_t()},[_t,m.delete]);function ji(){return X!=null&&X.loading?pe.createElement("div",{className:"sonner-loader","data-visible":st==="loading"},X.loading):J?pe.createElement("div",{className:"sonner-loader","data-visible":st==="loading"},J):pe.createElement(QC,{visible:st==="loading"})}return pe.createElement("li",{"aria-live":m.important?"assertive":"polite","aria-atomic":"true",role:"status",tabIndex:0,ref:Ft,className:he(G,km,B==null?void 0:B.toast,(t=m==null?void 0:m.classNames)==null?void 0:t.toast,B==null?void 0:B.default,B==null?void 0:B[st],(n=m==null?void 0:m.classNames)==null?void 0:n[st]),"data-sonner-toast":"","data-rich-colors":(r=m.richColors)!=null?r:S,"data-styled":!(m.jsx||m.unstyled||p),"data-mounted":D,"data-promise":!!m.promise,"data-removed":je,"data-visible":Ur,"data-y-position":Xe,"data-x-position":Dn,"data-index":$,"data-front":$n,"data-swiping":be,"data-dismissible":pi,"data-type":st,"data-invert":ja,"data-swipe-out":He,"data-expanded":!!(C||ae&&D),style:{"--index":$,"--toasts-before":$,"--z-index":_.length-$,"--offset":`${je?Be:de.current}px`,"--initial-height":ae?"auto":`${Dt}px`,...U,...m.style},onPointerDown:Te=>{fi||!pi||(xn.current=new Date,Ve(de.current),Te.target.setPointerCapture(Te.pointerId),Te.target.tagName!=="BUTTON"&&(Ne(!0),Pe.current={x:Te.clientX,y:Te.clientY}))},onPointerUp:()=>{var Te,ot,tt,Mr;if(He||!pi)return;Pe.current=null;let Rr=Number(((Te=Ft.current)==null?void 0:Te.style.getPropertyValue("--swipe-amount").replace("px",""))||0),su=new Date().getTime()-((ot=xn.current)==null?void 0:ot.getTime()),kE=Math.abs(Rr)/su;if(Math.abs(Rr)>=pA||kE>.11){Ve(de.current),(tt=m.onDismiss)==null||tt.call(m,m),_t(),Le(!0);return}(Mr=Ft.current)==null||Mr.style.setProperty("--swipe-amount","0px"),Ne(!1)},onPointerMove:Te=>{var ot;if(!Pe.current||!pi)return;let tt=Te.clientY-Pe.current.y,Mr=Te.clientX-Pe.current.x,Rr=(Xe==="top"?Math.min:Math.max)(0,tt),su=Te.pointerType==="touch"?10:2;Math.abs(Rr)>su?(ot=Ft.current)==null||ot.style.setProperty("--swipe-amount",`${tt}px`):Math.abs(Mr)>su&&(Pe.current=null)}},Nm&&!m.jsx?pe.createElement("button",{"aria-label":P,"data-disabled":fi,"data-close-button":!0,onClick:fi||!pi?()=>{}:()=>{var Te;_t(),(Te=m.onDismiss)==null||Te.call(m,m)},className:he(B==null?void 0:B.closeButton,(i=m==null?void 0:m.classNames)==null?void 0:i.closeButton)},pe.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",width:"12",height:"12",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"1.5",strokeLinecap:"round",strokeLinejoin:"round"},pe.createElement("line",{x1:"18",y1:"6",x2:"6",y2:"18"}),pe.createElement("line",{x1:"6",y1:"6",x2:"18",y2:"18"}))):null,m.jsx||pe.isValidElement(m.title)?m.jsx||m.title:pe.createElement(pe.Fragment,null,st||m.icon||m.promise?pe.createElement("div",{"data-icon":"",className:he(B==null?void 0:B.icon,(a=m==null?void 0:m.classNames)==null?void 0:a.icon)},m.promise||m.type==="loading"&&!m.icon?m.icon||ji():null,m.type!=="loading"?m.icon||(X==null?void 0:X[st])||YC(st):null):null,pe.createElement("div",{"data-content":"",className:he(B==null?void 0:B.content,(s=m==null?void 0:m.classNames)==null?void 0:s.content)},pe.createElement("div",{"data-title":"",className:he(B==null?void 0:B.title,(o=m==null?void 0:m.classNames)==null?void 0:o.title)},m.title),m.description?pe.createElement("div",{"data-description":"",className:he(H,Sm,B==null?void 0:B.description,(l=m==null?void 0:m.classNames)==null?void 0:l.description)},m.description):null),pe.isValidElement(m.cancel)?m.cancel:m.cancel&&Nu(m.cancel)?pe.createElement("button",{"data-button":!0,"data-cancel":!0,style:m.cancelButtonStyle||F,onClick:Te=>{var ot,tt;Nu(m.cancel)&&pi&&((tt=(ot=m.cancel).onClick)==null||tt.call(ot,Te),_t())},className:he(B==null?void 0:B.cancelButton,(c=m==null?void 0:m.classNames)==null?void 0:c.cancelButton)},m.cancel.label):null,pe.isValidElement(m.action)?m.action:m.action&&Nu(m.action)?pe.createElement("button",{"data-button":!0,"data-action":!0,style:m.actionButtonStyle||q,onClick:Te=>{var ot,tt;Nu(m.action)&&(Te.defaultPrevented||((tt=(ot=m.action).onClick)==null||tt.call(ot,Te),_t()))},className:he(B==null?void 0:B.actionButton,(u=m==null?void 0:m.classNames)==null?void 0:u.actionButton)},m.action.label):null))};function Ov(){if(typeof window>"u"||typeof document>"u")return"ltr";let e=document.documentElement.getAttribute("dir");return e==="auto"||!e?window.getComputedStyle(document.documentElement).direction:e}var vA=e=>{let{invert:t,position:n="bottom-right",hotkey:r=["altKey","KeyT"],expand:i,closeButton:a,className:s,offset:o,theme:l="light",richColors:c,duration:u,style:d,visibleToasts:m=cA,toastOptions:p,dir:x=Ov(),gap:g=hA,loadingIcon:w,icons:v,containerAriaLabel:$="Notifications",pauseWhenPageIsHidden:_,cn:C=gA}=e,[k,S]=pe.useState([]),L=pe.useMemo(()=>Array.from(new Set([n].concat(k.filter(se=>se.position).map(se=>se.position)))),[k,n]),[U,F]=pe.useState([]),[q,G]=pe.useState(!1),[H,ne]=pe.useState(!1),[K,te]=pe.useState(l!=="system"?l:typeof window<"u"&&window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"),J=pe.useRef(null),ae=r.join("+").replace(/Key/g,"").replace(/Digit/g,""),B=pe.useRef(null),X=pe.useRef(!1),P=pe.useCallback(se=>{var he;(he=k.find(D=>D.id===se.id))!=null&&he.delete||zn.dismiss(se.id),S(D=>D.filter(({id:Ee})=>Ee!==se.id))},[k]);return pe.useEffect(()=>zn.subscribe(se=>{if(se.dismiss){S(he=>he.map(D=>D.id===se.id?{...D,delete:!0}:D));return}setTimeout(()=>{t6.flushSync(()=>{S(he=>{let D=he.findIndex(Ee=>Ee.id===se.id);return D!==-1?[...he.slice(0,D),{...he[D],...se},...he.slice(D+1)]:[se,...he]})})})}),[]),pe.useEffect(()=>{if(l!=="system"){te(l);return}l==="system"&&(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?te("dark"):te("light")),typeof window<"u"&&window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",({matches:se})=>{te(se?"dark":"light")})},[l]),pe.useEffect(()=>{k.length<=1&&G(!1)},[k]),pe.useEffect(()=>{let se=he=>{var D,Ee;r.every(je=>he[je]||he.code===je)&&(G(!0),(D=J.current)==null||D.focus()),he.code==="Escape"&&(document.activeElement===J.current||(Ee=J.current)!=null&&Ee.contains(document.activeElement))&&G(!1)};return document.addEventListener("keydown",se),()=>document.removeEventListener("keydown",se)},[r]),pe.useEffect(()=>{if(J.current)return()=>{B.current&&(B.current.focus({preventScroll:!0}),B.current=null,X.current=!1)}},[J.current]),k.length?pe.createElement("section",{"aria-label":`${$} ${ae}`,tabIndex:-1},L.map((se,he)=>{var D;let[Ee,je]=se.split("-");return pe.createElement("ol",{key:se,dir:x==="auto"?Ov():x,tabIndex:-1,ref:J,className:s,"data-sonner-toaster":!0,"data-theme":K,"data-y-position":Ee,"data-x-position":je,style:{"--front-toast-height":`${((D=U[0])==null?void 0:D.height)||0}px`,"--offset":typeof o=="number"?`${o}px`:o||uA,"--width":`${mA}px`,"--gap":`${g}px`,...d},onBlur:xe=>{X.current&&!xe.currentTarget.contains(xe.relatedTarget)&&(X.current=!1,B.current&&(B.current.focus({preventScroll:!0}),B.current=null))},onFocus:xe=>{xe.target instanceof HTMLElement&&xe.target.dataset.dismissible==="false"||X.current||(X.current=!0,B.current=xe.relatedTarget)},onMouseEnter:()=>G(!0),onMouseMove:()=>G(!0),onMouseLeave:()=>{H||G(!1)},onPointerDown:xe=>{xe.target instanceof HTMLElement&&xe.target.dataset.dismissible==="false"||ne(!0)},onPointerUp:()=>ne(!1)},k.filter(xe=>!xe.position&&he===0||xe.position===se).map((xe,be)=>{var Ne,He;return pe.createElement(bA,{key:xe.id,icons:v,index:be,toast:xe,defaultRichColors:c,duration:(Ne=p==null?void 0:p.duration)!=null?Ne:u,className:p==null?void 0:p.className,descriptionClassName:p==null?void 0:p.descriptionClassName,invert:t,visibleToasts:m,closeButton:(He=p==null?void 0:p.closeButton)!=null?He:a,interacting:H,position:se,style:p==null?void 0:p.style,unstyled:p==null?void 0:p.unstyled,classNames:p==null?void 0:p.classNames,cancelButtonStyle:p==null?void 0:p.cancelButtonStyle,actionButtonStyle:p==null?void 0:p.actionButtonStyle,removeToast:P,toasts:k.filter(Le=>Le.position==xe.position),heights:U.filter(Le=>Le.position==xe.position),setHeights:F,expandByDefault:i,gap:g,loadingIcon:w,expanded:q,pauseWhenPageIsHidden:_,cn:C})}))})):null};const xA=({...e})=>{const{theme:t="system"}=KC();return h.jsx(vA,{theme:t,className:"toaster group",toastOptions:{classNames:{toast:"group toast group-[.toaster]:bg-background group-[.toaster]:text-foreground group-[.toaster]:border-border group-[.toaster]:shadow-lg",description:"group-[.toast]:text-muted-foreground",actionButton:"group-[.toast]:bg-primary group-[.toast]:text-primary-foreground",cancelButton:"group-[.toast]:bg-muted group-[.toast]:text-muted-foreground"}},...e})};var $A=Z$.useId||(()=>{}),yA=0;function Vc(e){const[t,n]=T.useState($A());return sn(()=>{e||n(r=>r??String(yA++))},[e]),e||(t?`radix-${t}`:"")}const _A=["top","right","bottom","left"],Ea=Math.min,jn=Math.max,Qd=Math.round,Cu=Math.floor,ka=e=>({x:e,y:e}),wA={left:"right",right:"left",bottom:"top",top:"bottom"},TA={start:"end",end:"start"};function of(e,t,n){return jn(e,Ea(t,n))}function Ii(e,t){return typeof e=="function"?e(t):e}function Ui(e){return e.split("-")[0]}function Fo(e){return e.split("-")[1]}function v1(e){return e==="x"?"y":"x"}function x1(e){return e==="y"?"height":"width"}function Sa(e){return["top","bottom"].includes(Ui(e))?"y":"x"}function $1(e){return v1(Sa(e))}function EA(e,t,n){n===void 0&&(n=!1);const r=Fo(e),i=$1(e),a=x1(i);let s=i==="x"?r===(n?"end":"start")?"right":"left":r==="start"?"bottom":"top";return t.reference[a]>t.floating[a]&&(s=Zd(s)),[s,Zd(s)]}function kA(e){const t=Zd(e);return[lf(e),t,lf(t)]}function lf(e){return e.replace(/start|end/g,t=>TA[t])}function SA(e,t,n){const r=["left","right"],i=["right","left"],a=["top","bottom"],s=["bottom","top"];switch(e){case"top":case"bottom":return n?t?i:r:t?r:i;case"left":case"right":return t?a:s;default:return[]}}function NA(e,t,n,r){const i=Fo(e);let a=SA(Ui(e),n==="start",r);return i&&(a=a.map(s=>s+"-"+i),t&&(a=a.concat(a.map(lf)))),a}function Zd(e){return e.replace(/left|right|bottom|top/g,t=>wA[t])}function CA(e){return{top:0,right:0,bottom:0,left:0,...e}}function Y6(e){return typeof e!="number"?CA(e):{top:e,right:e,bottom:e,left:e}}function Jd(e){const{x:t,y:n,width:r,height:i}=e;return{width:r,height:i,top:n,left:t,right:t+r,bottom:n+i,x:t,y:n}}function zv(e,t,n){let{reference:r,floating:i}=e;const a=Sa(t),s=$1(t),o=x1(s),l=Ui(t),c=a==="y",u=r.x+r.width/2-i.width/2,d=r.y+r.height/2-i.height/2,m=r[o]/2-i[o]/2;let p;switch(l){case"top":p={x:u,y:r.y-i.height};break;case"bottom":p={x:u,y:r.y+r.height};break;case"right":p={x:r.x+r.width,y:d};break;case"left":p={x:r.x-i.width,y:d};break;default:p={x:r.x,y:r.y}}switch(Fo(t)){case"start":p[s]-=m*(n&&c?-1:1);break;case"end":p[s]+=m*(n&&c?-1:1);break}return p}const AA=async(e,t,n)=>{const{placement:r="bottom",strategy:i="absolute",middleware:a=[],platform:s}=n,o=a.filter(Boolean),l=await(s.isRTL==null?void 0:s.isRTL(t));let c=await s.getElementRects({reference:e,floating:t,strategy:i}),{x:u,y:d}=zv(c,r,l),m=r,p={},x=0;for(let g=0;g<o.length;g++){const{name:w,fn:v}=o[g],{x:$,y:_,data:C,reset:k}=await v({x:u,y:d,initialPlacement:r,placement:m,strategy:i,middlewareData:p,rects:c,platform:s,elements:{reference:e,floating:t}});u=$??u,d=_??d,p={...p,[w]:{...p[w],...C}},k&&x<=50&&(x++,typeof k=="object"&&(k.placement&&(m=k.placement),k.rects&&(c=k.rects===!0?await s.getElementRects({reference:e,floating:t,strategy:i}):k.rects),{x:u,y:d}=zv(c,m,l)),g=-1)}return{x:u,y:d,placement:m,strategy:i,middlewareData:p}};async function fc(e,t){var n;t===void 0&&(t={});const{x:r,y:i,platform:a,rects:s,elements:o,strategy:l}=e,{boundary:c="clippingAncestors",rootBoundary:u="viewport",elementContext:d="floating",altBoundary:m=!1,padding:p=0}=Ii(t,e),x=Y6(p),w=o[m?d==="floating"?"reference":"floating":d],v=Jd(await a.getClippingRect({element:(n=await(a.isElement==null?void 0:a.isElement(w)))==null||n?w:w.contextElement||await(a.getDocumentElement==null?void 0:a.getDocumentElement(o.floating)),boundary:c,rootBoundary:u,strategy:l})),$=d==="floating"?{x:r,y:i,width:s.floating.width,height:s.floating.height}:s.reference,_=await(a.getOffsetParent==null?void 0:a.getOffsetParent(o.floating)),C=await(a.isElement==null?void 0:a.isElement(_))?await(a.getScale==null?void 0:a.getScale(_))||{x:1,y:1}:{x:1,y:1},k=Jd(a.convertOffsetParentRelativeRectToViewportRelativeRect?await a.convertOffsetParentRelativeRectToViewportRelativeRect({elements:o,rect:$,offsetParent:_,strategy:l}):$);return{top:(v.top-k.top+x.top)/C.y,bottom:(k.bottom-v.bottom+x.bottom)/C.y,left:(v.left-k.left+x.left)/C.x,right:(k.right-v.right+x.right)/C.x}}const LA=e=>({name:"arrow",options:e,async fn(t){const{x:n,y:r,placement:i,rects:a,platform:s,elements:o,middlewareData:l}=t,{element:c,padding:u=0}=Ii(e,t)||{};if(c==null)return{};const d=Y6(u),m={x:n,y:r},p=$1(i),x=x1(p),g=await s.getDimensions(c),w=p==="y",v=w?"top":"left",$=w?"bottom":"right",_=w?"clientHeight":"clientWidth",C=a.reference[x]+a.reference[p]-m[p]-a.floating[x],k=m[p]-a.reference[p],S=await(s.getOffsetParent==null?void 0:s.getOffsetParent(c));let L=S?S[_]:0;(!L||!await(s.isElement==null?void 0:s.isElement(S)))&&(L=o.floating[_]||a.floating[x]);const U=C/2-k/2,F=L/2-g[x]/2-1,q=Ea(d[v],F),G=Ea(d[$],F),H=q,ne=L-g[x]-G,K=L/2-g[x]/2+U,te=of(H,K,ne),J=!l.arrow&&Fo(i)!=null&&K!==te&&a.reference[x]/2-(K<H?q:G)-g[x]/2<0,ae=J?K<H?K-H:K-ne:0;return{[p]:m[p]+ae,data:{[p]:te,centerOffset:K-te-ae,...J&&{alignmentOffset:ae}},reset:J}}}),PA=function(e){return e===void 0&&(e={}),{name:"flip",options:e,async fn(t){var n,r;const{placement:i,middlewareData:a,rects:s,initialPlacement:o,platform:l,elements:c}=t,{mainAxis:u=!0,crossAxis:d=!0,fallbackPlacements:m,fallbackStrategy:p="bestFit",fallbackAxisSideDirection:x="none",flipAlignment:g=!0,...w}=Ii(e,t);if((n=a.arrow)!=null&&n.alignmentOffset)return{};const v=Ui(i),$=Sa(o),_=Ui(o)===o,C=await(l.isRTL==null?void 0:l.isRTL(c.floating)),k=m||(_||!g?[Zd(o)]:kA(o)),S=x!=="none";!m&&S&&k.push(...NA(o,g,x,C));const L=[o,...k],U=await fc(t,w),F=[];let q=((r=a.flip)==null?void 0:r.overflows)||[];if(u&&F.push(U[v]),d){const K=EA(i,s,C);F.push(U[K[0]],U[K[1]])}if(q=[...q,{placement:i,overflows:F}],!F.every(K=>K<=0)){var G,H;const K=(((G=a.flip)==null?void 0:G.index)||0)+1,te=L[K];if(te)return{data:{index:K,overflows:q},reset:{placement:te}};let J=(H=q.filter(ae=>ae.overflows[0]<=0).sort((ae,B)=>ae.overflows[1]-B.overflows[1])[0])==null?void 0:H.placement;if(!J)switch(p){case"bestFit":{var ne;const ae=(ne=q.filter(B=>{if(S){const X=Sa(B.placement);return X===$||X==="y"}return!0}).map(B=>[B.placement,B.overflows.filter(X=>X>0).reduce((X,P)=>X+P,0)]).sort((B,X)=>B[1]-X[1])[0])==null?void 0:ne[0];ae&&(J=ae);break}case"initialPlacement":J=o;break}if(i!==J)return{reset:{placement:J}}}return{}}}};function Bv(e,t){return{top:e.top-t.height,right:e.right-t.width,bottom:e.bottom-t.height,left:e.left-t.width}}function Fv(e){return _A.some(t=>e[t]>=0)}const IA=function(e){return e===void 0&&(e={}),{name:"hide",options:e,async fn(t){const{rects:n}=t,{strategy:r="referenceHidden",...i}=Ii(e,t);switch(r){case"referenceHidden":{const a=await fc(t,{...i,elementContext:"reference"}),s=Bv(a,n.reference);return{data:{referenceHiddenOffsets:s,referenceHidden:Fv(s)}}}case"escaped":{const a=await fc(t,{...i,altBoundary:!0}),s=Bv(a,n.floating);return{data:{escapedOffsets:s,escaped:Fv(s)}}}default:return{}}}}};async function UA(e,t){const{placement:n,platform:r,elements:i}=e,a=await(r.isRTL==null?void 0:r.isRTL(i.floating)),s=Ui(n),o=Fo(n),l=Sa(n)==="y",c=["left","top"].includes(s)?-1:1,u=a&&l?-1:1,d=Ii(t,e);let{mainAxis:m,crossAxis:p,alignmentAxis:x}=typeof d=="number"?{mainAxis:d,crossAxis:0,alignmentAxis:null}:{mainAxis:d.mainAxis||0,crossAxis:d.crossAxis||0,alignmentAxis:d.alignmentAxis};return o&&typeof x=="number"&&(p=o==="end"?x*-1:x),l?{x:p*u,y:m*c}:{x:m*c,y:p*u}}const DA=function(e){return e===void 0&&(e=0),{name:"offset",options:e,async fn(t){var n,r;const{x:i,y:a,placement:s,middlewareData:o}=t,l=await UA(t,e);return s===((n=o.offset)==null?void 0:n.placement)&&(r=o.arrow)!=null&&r.alignmentOffset?{}:{x:i+l.x,y:a+l.y,data:{...l,placement:s}}}}},MA=function(e){return e===void 0&&(e={}),{name:"shift",options:e,async fn(t){const{x:n,y:r,placement:i}=t,{mainAxis:a=!0,crossAxis:s=!1,limiter:o={fn:w=>{let{x:v,y:$}=w;return{x:v,y:$}}},...l}=Ii(e,t),c={x:n,y:r},u=await fc(t,l),d=Sa(Ui(i)),m=v1(d);let p=c[m],x=c[d];if(a){const w=m==="y"?"top":"left",v=m==="y"?"bottom":"right",$=p+u[w],_=p-u[v];p=of($,p,_)}if(s){const w=d==="y"?"top":"left",v=d==="y"?"bottom":"right",$=x+u[w],_=x-u[v];x=of($,x,_)}const g=o.fn({...t,[m]:p,[d]:x});return{...g,data:{x:g.x-n,y:g.y-r,enabled:{[m]:a,[d]:s}}}}}},RA=function(e){return e===void 0&&(e={}),{options:e,fn(t){const{x:n,y:r,placement:i,rects:a,middlewareData:s}=t,{offset:o=0,mainAxis:l=!0,crossAxis:c=!0}=Ii(e,t),u={x:n,y:r},d=Sa(i),m=v1(d);let p=u[m],x=u[d];const g=Ii(o,t),w=typeof g=="number"?{mainAxis:g,crossAxis:0}:{mainAxis:0,crossAxis:0,...g};if(l){const _=m==="y"?"height":"width",C=a.reference[m]-a.floating[_]+w.mainAxis,k=a.reference[m]+a.reference[_]-w.mainAxis;p<C?p=C:p>k&&(p=k)}if(c){var v,$;const _=m==="y"?"width":"height",C=["top","left"].includes(Ui(i)),k=a.reference[d]-a.floating[_]+(C&&((v=s.offset)==null?void 0:v[d])||0)+(C?0:w.crossAxis),S=a.reference[d]+a.reference[_]+(C?0:(($=s.offset)==null?void 0:$[d])||0)-(C?w.crossAxis:0);x<k?x=k:x>S&&(x=S)}return{[m]:p,[d]:x}}}},OA=function(e){return e===void 0&&(e={}),{name:"size",options:e,async fn(t){var n,r;const{placement:i,rects:a,platform:s,elements:o}=t,{apply:l=()=>{},...c}=Ii(e,t),u=await fc(t,c),d=Ui(i),m=Fo(i),p=Sa(i)==="y",{width:x,height:g}=a.floating;let w,v;d==="top"||d==="bottom"?(w=d,v=m===(await(s.isRTL==null?void 0:s.isRTL(o.floating))?"start":"end")?"left":"right"):(v=d,w=m==="end"?"top":"bottom");const $=g-u.top-u.bottom,_=x-u.left-u.right,C=Ea(g-u[w],$),k=Ea(x-u[v],_),S=!t.middlewareData.shift;let L=C,U=k;if((n=t.middlewareData.shift)!=null&&n.enabled.x&&(U=_),(r=t.middlewareData.shift)!=null&&r.enabled.y&&(L=$),S&&!m){const q=jn(u.left,0),G=jn(u.right,0),H=jn(u.top,0),ne=jn(u.bottom,0);p?U=x-2*(q!==0||G!==0?q+G:jn(u.left,u.right)):L=g-2*(H!==0||ne!==0?H+ne:jn(u.top,u.bottom))}await l({...t,availableWidth:U,availableHeight:L});const F=await s.getDimensions(o.floating);return x!==F.width||g!==F.height?{reset:{rects:!0}}:{}}}};function j0(){return typeof window<"u"}function jo(e){return X6(e)?(e.nodeName||"").toLowerCase():"#document"}function Wn(e){var t;return(e==null||(t=e.ownerDocument)==null?void 0:t.defaultView)||window}function ci(e){var t;return(t=(X6(e)?e.ownerDocument:e.document)||window.document)==null?void 0:t.documentElement}function X6(e){return j0()?e instanceof Node||e instanceof Wn(e).Node:!1}function Lr(e){return j0()?e instanceof Element||e instanceof Wn(e).Element:!1}function oi(e){return j0()?e instanceof HTMLElement||e instanceof Wn(e).HTMLElement:!1}function jv(e){return!j0()||typeof ShadowRoot>"u"?!1:e instanceof ShadowRoot||e instanceof Wn(e).ShadowRoot}function Hc(e){const{overflow:t,overflowX:n,overflowY:r,display:i}=Pr(e);return/auto|scroll|overlay|hidden|clip/.test(t+r+n)&&!["inline","contents"].includes(i)}function zA(e){return["table","td","th"].includes(jo(e))}function V0(e){return[":popover-open",":modal"].some(t=>{try{return e.matches(t)}catch{return!1}})}function y1(e){const t=_1(),n=Lr(e)?Pr(e):e;return n.transform!=="none"||n.perspective!=="none"||(n.containerType?n.containerType!=="normal":!1)||!t&&(n.backdropFilter?n.backdropFilter!=="none":!1)||!t&&(n.filter?n.filter!=="none":!1)||["transform","perspective","filter"].some(r=>(n.willChange||"").includes(r))||["paint","layout","strict","content"].some(r=>(n.contain||"").includes(r))}function BA(e){let t=Na(e);for(;oi(t)&&!Ao(t);){if(y1(t))return t;if(V0(t))return null;t=Na(t)}return null}function _1(){return typeof CSS>"u"||!CSS.supports?!1:CSS.supports("-webkit-backdrop-filter","none")}function Ao(e){return["html","body","#document"].includes(jo(e))}function Pr(e){return Wn(e).getComputedStyle(e)}function H0(e){return Lr(e)?{scrollLeft:e.scrollLeft,scrollTop:e.scrollTop}:{scrollLeft:e.scrollX,scrollTop:e.scrollY}}function Na(e){if(jo(e)==="html")return e;const t=e.assignedSlot||e.parentNode||jv(e)&&e.host||ci(e);return jv(t)?t.host:t}function Q6(e){const t=Na(e);return Ao(t)?e.ownerDocument?e.ownerDocument.body:e.body:oi(t)&&Hc(t)?t:Q6(t)}function gc(e,t,n){var r;t===void 0&&(t=[]),n===void 0&&(n=!0);const i=Q6(e),a=i===((r=e.ownerDocument)==null?void 0:r.body),s=Wn(i);if(a){const o=cf(s);return t.concat(s,s.visualViewport||[],Hc(i)?i:[],o&&n?gc(o):[])}return t.concat(i,gc(i,[],n))}function cf(e){return e.parent&&Object.getPrototypeOf(e.parent)?e.frameElement:null}function Z6(e){const t=Pr(e);let n=parseFloat(t.width)||0,r=parseFloat(t.height)||0;const i=oi(e),a=i?e.offsetWidth:n,s=i?e.offsetHeight:r,o=Qd(n)!==a||Qd(r)!==s;return o&&(n=a,r=s),{width:n,height:r,$:o}}function w1(e){return Lr(e)?e:e.contextElement}function so(e){const t=w1(e);if(!oi(t))return ka(1);const n=t.getBoundingClientRect(),{width:r,height:i,$:a}=Z6(t);let s=(a?Qd(n.width):n.width)/r,o=(a?Qd(n.height):n.height)/i;return(!s||!Number.isFinite(s))&&(s=1),(!o||!Number.isFinite(o))&&(o=1),{x:s,y:o}}const FA=ka(0);function J6(e){const t=Wn(e);return!_1()||!t.visualViewport?FA:{x:t.visualViewport.offsetLeft,y:t.visualViewport.offsetTop}}function jA(e,t,n){return t===void 0&&(t=!1),!n||t&&n!==Wn(e)?!1:t}function gs(e,t,n,r){t===void 0&&(t=!1),n===void 0&&(n=!1);const i=e.getBoundingClientRect(),a=w1(e);let s=ka(1);t&&(r?Lr(r)&&(s=so(r)):s=so(e));const o=jA(a,n,r)?J6(a):ka(0);let l=(i.left+o.x)/s.x,c=(i.top+o.y)/s.y,u=i.width/s.x,d=i.height/s.y;if(a){const m=Wn(a),p=r&&Lr(r)?Wn(r):r;let x=m,g=cf(x);for(;g&&r&&p!==x;){const w=so(g),v=g.getBoundingClientRect(),$=Pr(g),_=v.left+(g.clientLeft+parseFloat($.paddingLeft))*w.x,C=v.top+(g.clientTop+parseFloat($.paddingTop))*w.y;l*=w.x,c*=w.y,u*=w.x,d*=w.y,l+=_,c+=C,x=Wn(g),g=cf(x)}}return Jd({width:u,height:d,x:l,y:c})}function VA(e){let{elements:t,rect:n,offsetParent:r,strategy:i}=e;const a=i==="fixed",s=ci(r),o=t?V0(t.floating):!1;if(r===s||o&&a)return n;let l={scrollLeft:0,scrollTop:0},c=ka(1);const u=ka(0),d=oi(r);if((d||!d&&!a)&&((jo(r)!=="body"||Hc(s))&&(l=H0(r)),oi(r))){const m=gs(r);c=so(r),u.x=m.x+r.clientLeft,u.y=m.y+r.clientTop}return{width:n.width*c.x,height:n.height*c.y,x:n.x*c.x-l.scrollLeft*c.x+u.x,y:n.y*c.y-l.scrollTop*c.y+u.y}}function HA(e){return Array.from(e.getClientRects())}function uf(e,t){const n=H0(e).scrollLeft;return t?t.left+n:gs(ci(e)).left+n}function qA(e){const t=ci(e),n=H0(e),r=e.ownerDocument.body,i=jn(t.scrollWidth,t.clientWidth,r.scrollWidth,r.clientWidth),a=jn(t.scrollHeight,t.clientHeight,r.scrollHeight,r.clientHeight);let s=-n.scrollLeft+uf(e);const o=-n.scrollTop;return Pr(r).direction==="rtl"&&(s+=jn(t.clientWidth,r.clientWidth)-i),{width:i,height:a,x:s,y:o}}function GA(e,t){const n=Wn(e),r=ci(e),i=n.visualViewport;let a=r.clientWidth,s=r.clientHeight,o=0,l=0;if(i){a=i.width,s=i.height;const c=_1();(!c||c&&t==="fixed")&&(o=i.offsetLeft,l=i.offsetTop)}return{width:a,height:s,x:o,y:l}}function WA(e,t){const n=gs(e,!0,t==="fixed"),r=n.top+e.clientTop,i=n.left+e.clientLeft,a=oi(e)?so(e):ka(1),s=e.clientWidth*a.x,o=e.clientHeight*a.y,l=i*a.x,c=r*a.y;return{width:s,height:o,x:l,y:c}}function Vv(e,t,n){let r;if(t==="viewport")r=GA(e,n);else if(t==="document")r=qA(ci(e));else if(Lr(t))r=WA(t,n);else{const i=J6(e);r={...t,x:t.x-i.x,y:t.y-i.y}}return Jd(r)}function e5(e,t){const n=Na(e);return n===t||!Lr(n)||Ao(n)?!1:Pr(n).position==="fixed"||e5(n,t)}function KA(e,t){const n=t.get(e);if(n)return n;let r=gc(e,[],!1).filter(o=>Lr(o)&&jo(o)!=="body"),i=null;const a=Pr(e).position==="fixed";let s=a?Na(e):e;for(;Lr(s)&&!Ao(s);){const o=Pr(s),l=y1(s);!l&&o.position==="fixed"&&(i=null),(a?!l&&!i:!l&&o.position==="static"&&!!i&&["absolute","fixed"].includes(i.position)||Hc(s)&&!l&&e5(e,s))?r=r.filter(u=>u!==s):i=o,s=Na(s)}return t.set(e,r),r}function YA(e){let{element:t,boundary:n,rootBoundary:r,strategy:i}=e;const s=[...n==="clippingAncestors"?V0(t)?[]:KA(t,this._c):[].concat(n),r],o=s[0],l=s.reduce((c,u)=>{const d=Vv(t,u,i);return c.top=jn(d.top,c.top),c.right=Ea(d.right,c.right),c.bottom=Ea(d.bottom,c.bottom),c.left=jn(d.left,c.left),c},Vv(t,o,i));return{width:l.right-l.left,height:l.bottom-l.top,x:l.left,y:l.top}}function XA(e){const{width:t,height:n}=Z6(e);return{width:t,height:n}}function QA(e,t,n){const r=oi(t),i=ci(t),a=n==="fixed",s=gs(e,!0,a,t);let o={scrollLeft:0,scrollTop:0};const l=ka(0);if(r||!r&&!a)if((jo(t)!=="body"||Hc(i))&&(o=H0(t)),r){const p=gs(t,!0,a,t);l.x=p.x+t.clientLeft,l.y=p.y+t.clientTop}else i&&(l.x=uf(i));let c=0,u=0;if(i&&!r&&!a){const p=i.getBoundingClientRect();u=p.top+o.scrollTop,c=p.left+o.scrollLeft-uf(i,p)}const d=s.left+o.scrollLeft-l.x-c,m=s.top+o.scrollTop-l.y-u;return{x:d,y:m,width:s.width,height:s.height}}function sh(e){return Pr(e).position==="static"}function Hv(e,t){if(!oi(e)||Pr(e).position==="fixed")return null;if(t)return t(e);let n=e.offsetParent;return ci(e)===n&&(n=n.ownerDocument.body),n}function t5(e,t){const n=Wn(e);if(V0(e))return n;if(!oi(e)){let i=Na(e);for(;i&&!Ao(i);){if(Lr(i)&&!sh(i))return i;i=Na(i)}return n}let r=Hv(e,t);for(;r&&zA(r)&&sh(r);)r=Hv(r,t);return r&&Ao(r)&&sh(r)&&!y1(r)?n:r||BA(e)||n}const ZA=async function(e){const t=this.getOffsetParent||t5,n=this.getDimensions,r=await n(e.floating);return{reference:QA(e.reference,await t(e.floating),e.strategy),floating:{x:0,y:0,width:r.width,height:r.height}}};function JA(e){return Pr(e).direction==="rtl"}const eL={convertOffsetParentRelativeRectToViewportRelativeRect:VA,getDocumentElement:ci,getClippingRect:YA,getOffsetParent:t5,getElementRects:ZA,getClientRects:HA,getDimensions:XA,getScale:so,isElement:Lr,isRTL:JA};function tL(e,t){let n=null,r;const i=ci(e);function a(){var o;clearTimeout(r),(o=n)==null||o.disconnect(),n=null}function s(o,l){o===void 0&&(o=!1),l===void 0&&(l=1),a();const{left:c,top:u,width:d,height:m}=e.getBoundingClientRect();if(o||t(),!d||!m)return;const p=Cu(u),x=Cu(i.clientWidth-(c+d)),g=Cu(i.clientHeight-(u+m)),w=Cu(c),$={rootMargin:-p+"px "+-x+"px "+-g+"px "+-w+"px",threshold:jn(0,Ea(1,l))||1};let _=!0;function C(k){const S=k[0].intersectionRatio;if(S!==l){if(!_)return s();S?s(!1,S):r=setTimeout(()=>{s(!1,1e-7)},1e3)}_=!1}try{n=new IntersectionObserver(C,{...$,root:i.ownerDocument})}catch{n=new IntersectionObserver(C,$)}n.observe(e)}return s(!0),a}function nL(e,t,n,r){r===void 0&&(r={});const{ancestorScroll:i=!0,ancestorResize:a=!0,elementResize:s=typeof ResizeObserver=="function",layoutShift:o=typeof IntersectionObserver=="function",animationFrame:l=!1}=r,c=w1(e),u=i||a?[...c?gc(c):[],...gc(t)]:[];u.forEach(v=>{i&&v.addEventListener("scroll",n,{passive:!0}),a&&v.addEventListener("resize",n)});const d=c&&o?tL(c,n):null;let m=-1,p=null;s&&(p=new ResizeObserver(v=>{let[$]=v;$&&$.target===c&&p&&(p.unobserve(t),cancelAnimationFrame(m),m=requestAnimationFrame(()=>{var _;(_=p)==null||_.observe(t)})),n()}),c&&!l&&p.observe(c),p.observe(t));let x,g=l?gs(e):null;l&&w();function w(){const v=gs(e);g&&(v.x!==g.x||v.y!==g.y||v.width!==g.width||v.height!==g.height)&&n(),g=v,x=requestAnimationFrame(w)}return n(),()=>{var v;u.forEach($=>{i&&$.removeEventListener("scroll",n),a&&$.removeEventListener("resize",n)}),d==null||d(),(v=p)==null||v.disconnect(),p=null,l&&cancelAnimationFrame(x)}}const rL=DA,iL=MA,aL=PA,sL=OA,oL=IA,qv=LA,lL=RA,cL=(e,t,n)=>{const r=new Map,i={platform:eL,...n},a={...i.platform,_c:r};return AA(e,t,{...i,platform:a})};var hd=typeof document<"u"?T.useLayoutEffect:T.useEffect;function e0(e,t){if(e===t)return!0;if(typeof e!=typeof t)return!1;if(typeof e=="function"&&e.toString()===t.toString())return!0;let n,r,i;if(e&&t&&typeof e=="object"){if(Array.isArray(e)){if(n=e.length,n!==t.length)return!1;for(r=n;r--!==0;)if(!e0(e[r],t[r]))return!1;return!0}if(i=Object.keys(e),n=i.length,n!==Object.keys(t).length)return!1;for(r=n;r--!==0;)if(!{}.hasOwnProperty.call(t,i[r]))return!1;for(r=n;r--!==0;){const a=i[r];if(!(a==="_owner"&&e.$$typeof)&&!e0(e[a],t[a]))return!1}return!0}return e!==e&&t!==t}function n5(e){return typeof window>"u"?1:(e.ownerDocument.defaultView||window).devicePixelRatio||1}function Gv(e,t){const n=n5(e);return Math.round(t*n)/n}function oh(e){const t=T.useRef(e);return hd(()=>{t.current=e}),t}function uL(e){e===void 0&&(e={});const{placement:t="bottom",strategy:n="absolute",middleware:r=[],platform:i,elements:{reference:a,floating:s}={},transform:o=!0,whileElementsMounted:l,open:c}=e,[u,d]=T.useState({x:0,y:0,strategy:n,placement:t,middlewareData:{},isPositioned:!1}),[m,p]=T.useState(r);e0(m,r)||p(r);const[x,g]=T.useState(null),[w,v]=T.useState(null),$=T.useCallback(B=>{B!==S.current&&(S.current=B,g(B))},[]),_=T.useCallback(B=>{B!==L.current&&(L.current=B,v(B))},[]),C=a||x,k=s||w,S=T.useRef(null),L=T.useRef(null),U=T.useRef(u),F=l!=null,q=oh(l),G=oh(i),H=oh(c),ne=T.useCallback(()=>{if(!S.current||!L.current)return;const B={placement:t,strategy:n,middleware:m};G.current&&(B.platform=G.current),cL(S.current,L.current,B).then(X=>{const P={...X,isPositioned:H.current!==!1};K.current&&!e0(U.current,P)&&(U.current=P,$s.flushSync(()=>{d(P)}))})},[m,t,n,G,H]);hd(()=>{c===!1&&U.current.isPositioned&&(U.current.isPositioned=!1,d(B=>({...B,isPositioned:!1})))},[c]);const K=T.useRef(!1);hd(()=>(K.current=!0,()=>{K.current=!1}),[]),hd(()=>{if(C&&(S.current=C),k&&(L.current=k),C&&k){if(q.current)return q.current(C,k,ne);ne()}},[C,k,ne,q,F]);const te=T.useMemo(()=>({reference:S,floating:L,setReference:$,setFloating:_}),[$,_]),J=T.useMemo(()=>({reference:C,floating:k}),[C,k]),ae=T.useMemo(()=>{const B={position:n,left:0,top:0};if(!J.floating)return B;const X=Gv(J.floating,u.x),P=Gv(J.floating,u.y);return o?{...B,transform:"translate("+X+"px, "+P+"px)",...n5(J.floating)>=1.5&&{willChange:"transform"}}:{position:n,left:X,top:P}},[n,o,J.floating,u.x,u.y]);return T.useMemo(()=>({...u,update:ne,refs:te,elements:J,floatingStyles:ae}),[u,ne,te,J,ae])}const dL=e=>{function t(n){return{}.hasOwnProperty.call(n,"current")}return{name:"arrow",options:e,fn(n){const{element:r,padding:i}=typeof e=="function"?e(n):e;return r&&t(r)?r.current!=null?qv({element:r.current,padding:i}).fn(n):{}:r?qv({element:r,padding:i}).fn(n):{}}}},mL=(e,t)=>({...rL(e),options:[e,t]}),hL=(e,t)=>({...iL(e),options:[e,t]}),pL=(e,t)=>({...lL(e),options:[e,t]}),fL=(e,t)=>({...aL(e),options:[e,t]}),gL=(e,t)=>({...sL(e),options:[e,t]}),bL=(e,t)=>({...oL(e),options:[e,t]}),vL=(e,t)=>({...dL(e),options:[e,t]});var xL="Arrow",r5=T.forwardRef((e,t)=>{const{children:n,width:r=10,height:i=5,...a}=e;return h.jsx(We.svg,{...a,ref:t,width:r,height:i,viewBox:"0 0 30 10",preserveAspectRatio:"none",children:e.asChild?n:h.jsx("polygon",{points:"0,0 30,0 15,10"})})});r5.displayName=xL;var $L=r5;function yL(e,t=[]){let n=[];function r(a,s){const o=T.createContext(s),l=n.length;n=[...n,s];function c(d){const{scope:m,children:p,...x}=d,g=(m==null?void 0:m[e][l])||o,w=T.useMemo(()=>x,Object.values(x));return h.jsx(g.Provider,{value:w,children:p})}function u(d,m){const p=(m==null?void 0:m[e][l])||o,x=T.useContext(p);if(x)return x;if(s!==void 0)return s;throw new Error(`\`${d}\` must be used within \`${a}\``)}return c.displayName=a+"Provider",[c,u]}const i=()=>{const a=n.map(s=>T.createContext(s));return function(o){const l=(o==null?void 0:o[e])||a;return T.useMemo(()=>({[`__scope${e}`]:{...o,[e]:l}}),[o,l])}};return i.scopeName=e,[r,_L(i,...t)]}function _L(...e){const t=e[0];if(e.length===1)return t;const n=()=>{const r=e.map(i=>({useScope:i(),scopeName:i.scopeName}));return function(a){const s=r.reduce((o,{useScope:l,scopeName:c})=>{const d=l(a)[`__scope${c}`];return{...o,...d}},{});return T.useMemo(()=>({[`__scope${t.scopeName}`]:s}),[s])}};return n.scopeName=t.scopeName,n}function wL(e){const[t,n]=T.useState(void 0);return sn(()=>{if(e){n({width:e.offsetWidth,height:e.offsetHeight});const r=new ResizeObserver(i=>{if(!Array.isArray(i)||!i.length)return;const a=i[0];let s,o;if("borderBoxSize"in a){const l=a.borderBoxSize,c=Array.isArray(l)?l[0]:l;s=c.inlineSize,o=c.blockSize}else s=e.offsetWidth,o=e.offsetHeight;n({width:s,height:o})});return r.observe(e,{box:"border-box"}),()=>r.unobserve(e)}else n(void 0)},[e]),t}var T1="Popper",[i5,q0]=yL(T1),[TL,a5]=i5(T1),s5=e=>{const{__scopePopper:t,children:n}=e,[r,i]=T.useState(null);return h.jsx(TL,{scope:t,anchor:r,onAnchorChange:i,children:n})};s5.displayName=T1;var o5="PopperAnchor",l5=T.forwardRef((e,t)=>{const{__scopePopper:n,virtualRef:r,...i}=e,a=a5(o5,n),s=T.useRef(null),o=yt(t,s);return T.useEffect(()=>{a.onAnchorChange((r==null?void 0:r.current)||s.current)}),r?null:h.jsx(We.div,{...i,ref:o})});l5.displayName=o5;var E1="PopperContent",[EL,kL]=i5(E1),c5=T.forwardRef((e,t)=>{var be,Ne,He,Le,Be,Ve;const{__scopePopper:n,side:r="bottom",sideOffset:i=0,align:a="center",alignOffset:s=0,arrowPadding:o=0,avoidCollisions:l=!0,collisionBoundary:c=[],collisionPadding:u=0,sticky:d="partial",hideWhenDetached:m=!1,updatePositionStrategy:p="optimized",onPlaced:x,...g}=e,w=a5(E1,n),[v,$]=T.useState(null),_=yt(t,Dt=>$(Dt)),[C,k]=T.useState(null),S=wL(C),L=(S==null?void 0:S.width)??0,U=(S==null?void 0:S.height)??0,F=r+(a!=="center"?"-"+a:""),q=typeof u=="number"?u:{top:0,right:0,bottom:0,left:0,...u},G=Array.isArray(c)?c:[c],H=G.length>0,ne={padding:q,boundary:G.filter(NL),altBoundary:H},{refs:K,floatingStyles:te,placement:J,isPositioned:ae,middlewareData:B}=uL({strategy:"fixed",placement:F,whileElementsMounted:(...Dt)=>nL(...Dt,{animationFrame:p==="always"}),elements:{reference:w.anchor},middleware:[mL({mainAxis:i+U,alignmentAxis:s}),l&&hL({mainAxis:!0,crossAxis:!1,limiter:d==="partial"?pL():void 0,...ne}),l&&fL({...ne}),gL({...ne,apply:({elements:Dt,rects:Mt,availableWidth:xn,availableHeight:Ft})=>{const{width:$n,height:Ur}=Mt.reference,st=Dt.floating.style;st.setProperty("--radix-popper-available-width",`${xn}px`),st.setProperty("--radix-popper-available-height",`${Ft}px`),st.setProperty("--radix-popper-anchor-width",`${$n}px`),st.setProperty("--radix-popper-anchor-height",`${Ur}px`)}}),C&&vL({element:C,padding:o}),CL({arrowWidth:L,arrowHeight:U}),m&&bL({strategy:"referenceHidden",...ne})]}),[X,P]=m5(J),se=Pn(x);sn(()=>{ae&&(se==null||se())},[ae,se]);const he=(be=B.arrow)==null?void 0:be.x,D=(Ne=B.arrow)==null?void 0:Ne.y,Ee=((He=B.arrow)==null?void 0:He.centerOffset)!==0,[je,xe]=T.useState();return sn(()=>{v&&xe(window.getComputedStyle(v).zIndex)},[v]),h.jsx("div",{ref:K.setFloating,"data-radix-popper-content-wrapper":"",style:{...te,transform:ae?te.transform:"translate(0, -200%)",minWidth:"max-content",zIndex:je,"--radix-popper-transform-origin":[(Le=B.transformOrigin)==null?void 0:Le.x,(Be=B.transformOrigin)==null?void 0:Be.y].join(" "),...((Ve=B.hide)==null?void 0:Ve.referenceHidden)&&{visibility:"hidden",pointerEvents:"none"}},dir:e.dir,children:h.jsx(EL,{scope:n,placedSide:X,onArrowChange:k,arrowX:he,arrowY:D,shouldHideArrow:Ee,children:h.jsx(We.div,{"data-side":X,"data-align":P,...g,ref:_,style:{...g.style,animation:ae?void 0:"none"}})})})});c5.displayName=E1;var u5="PopperArrow",SL={top:"bottom",right:"left",bottom:"top",left:"right"},d5=T.forwardRef(function(t,n){const{__scopePopper:r,...i}=t,a=kL(u5,r),s=SL[a.placedSide];return h.jsx("span",{ref:a.onArrowChange,style:{position:"absolute",left:a.arrowX,top:a.arrowY,[s]:0,transformOrigin:{top:"",right:"0 0",bottom:"center 0",left:"100% 0"}[a.placedSide],transform:{top:"translateY(100%)",right:"translateY(50%) rotate(90deg) translateX(-50%)",bottom:"rotate(180deg)",left:"translateY(50%) rotate(-90deg) translateX(50%)"}[a.placedSide],visibility:a.shouldHideArrow?"hidden":void 0},children:h.jsx($L,{...i,ref:n,style:{...i.style,display:"block"}})})});d5.displayName=u5;function NL(e){return e!==null}var CL=e=>({name:"transformOrigin",options:e,fn(t){var w,v,$;const{placement:n,rects:r,middlewareData:i}=t,s=((w=i.arrow)==null?void 0:w.centerOffset)!==0,o=s?0:e.arrowWidth,l=s?0:e.arrowHeight,[c,u]=m5(n),d={start:"0%",center:"50%",end:"100%"}[u],m=(((v=i.arrow)==null?void 0:v.x)??0)+o/2,p=((($=i.arrow)==null?void 0:$.y)??0)+l/2;let x="",g="";return c==="bottom"?(x=s?d:`${m}px`,g=`${-l}px`):c==="top"?(x=s?d:`${m}px`,g=`${r.floating.height+l}px`):c==="right"?(x=`${-l}px`,g=s?d:`${p}px`):c==="left"&&(x=`${r.floating.width+l}px`,g=s?d:`${p}px`),{data:{x,y:g}}}});function m5(e){const[t,n="center"]=e.split("-");return[t,n]}var AL=s5,h5=l5,p5=c5,f5=d5,[G0,dZ]=M0("Tooltip",[q0]),k1=q0(),g5="TooltipProvider",LL=700,Wv="tooltip.open",[PL,b5]=G0(g5),v5=e=>{const{__scopeTooltip:t,delayDuration:n=LL,skipDelayDuration:r=300,disableHoverableContent:i=!1,children:a}=e,[s,o]=T.useState(!0),l=T.useRef(!1),c=T.useRef(0);return T.useEffect(()=>{const u=c.current;return()=>window.clearTimeout(u)},[]),h.jsx(PL,{scope:t,isOpenDelayed:s,delayDuration:n,onOpen:T.useCallback(()=>{window.clearTimeout(c.current),o(!1)},[]),onClose:T.useCallback(()=>{window.clearTimeout(c.current),c.current=window.setTimeout(()=>o(!0),r)},[r]),isPointerInTransitRef:l,onPointerInTransitChange:T.useCallback(u=>{l.current=u},[]),disableHoverableContent:i,children:a})};v5.displayName=g5;var x5="Tooltip",[mZ,W0]=G0(x5),df="TooltipTrigger",IL=T.forwardRef((e,t)=>{const{__scopeTooltip:n,...r}=e,i=W0(df,n),a=b5(df,n),s=k1(n),o=T.useRef(null),l=yt(t,o,i.onTriggerChange),c=T.useRef(!1),u=T.useRef(!1),d=T.useCallback(()=>c.current=!1,[]);return T.useEffect(()=>()=>document.removeEventListener("pointerup",d),[d]),h.jsx(h5,{asChild:!0,...s,children:h.jsx(We.button,{"aria-describedby":i.open?i.contentId:void 0,"data-state":i.stateAttribute,...r,ref:l,onPointerMove:Ae(e.onPointerMove,m=>{m.pointerType!=="touch"&&!u.current&&!a.isPointerInTransitRef.current&&(i.onTriggerEnter(),u.current=!0)}),onPointerLeave:Ae(e.onPointerLeave,()=>{i.onTriggerLeave(),u.current=!1}),onPointerDown:Ae(e.onPointerDown,()=>{c.current=!0,document.addEventListener("pointerup",d,{once:!0})}),onFocus:Ae(e.onFocus,()=>{c.current||i.onOpen()}),onBlur:Ae(e.onBlur,i.onClose),onClick:Ae(e.onClick,i.onClose)})})});IL.displayName=df;var UL="TooltipPortal",[hZ,DL]=G0(UL,{forceMount:void 0}),Lo="TooltipContent",$5=T.forwardRef((e,t)=>{const n=DL(Lo,e.__scopeTooltip),{forceMount:r=n.forceMount,side:i="top",...a}=e,s=W0(Lo,e.__scopeTooltip);return h.jsx(O0,{present:r||s.open,children:s.disableHoverableContent?h.jsx(y5,{side:i,...a,ref:t}):h.jsx(ML,{side:i,...a,ref:t})})}),ML=T.forwardRef((e,t)=>{const n=W0(Lo,e.__scopeTooltip),r=b5(Lo,e.__scopeTooltip),i=T.useRef(null),a=yt(t,i),[s,o]=T.useState(null),{trigger:l,onClose:c}=n,u=i.current,{onPointerInTransitChange:d}=r,m=T.useCallback(()=>{o(null),d(!1)},[d]),p=T.useCallback((x,g)=>{const w=x.currentTarget,v={x:x.clientX,y:x.clientY},$=BL(v,w.getBoundingClientRect()),_=FL(v,$),C=jL(g.getBoundingClientRect()),k=HL([..._,...C]);o(k),d(!0)},[d]);return T.useEffect(()=>()=>m(),[m]),T.useEffect(()=>{if(l&&u){const x=w=>p(w,u),g=w=>p(w,l);return l.addEventListener("pointerleave",x),u.addEventListener("pointerleave",g),()=>{l.removeEventListener("pointerleave",x),u.removeEventListener("pointerleave",g)}}},[l,u,p,m]),T.useEffect(()=>{if(s){const x=g=>{const w=g.target,v={x:g.clientX,y:g.clientY},$=(l==null?void 0:l.contains(w))||(u==null?void 0:u.contains(w)),_=!VL(v,s);$?m():_&&(m(),c())};return document.addEventListener("pointermove",x),()=>document.removeEventListener("pointermove",x)}},[l,u,s,c,m]),h.jsx(y5,{...e,ref:a})}),[RL,OL]=G0(x5,{isInside:!1}),y5=T.forwardRef((e,t)=>{const{__scopeTooltip:n,children:r,"aria-label":i,onEscapeKeyDown:a,onPointerDownOutside:s,...o}=e,l=W0(Lo,n),c=k1(n),{onClose:u}=l;return T.useEffect(()=>(document.addEventListener(Wv,u),()=>document.removeEventListener(Wv,u)),[u]),T.useEffect(()=>{if(l.trigger){const d=m=>{const p=m.target;p!=null&&p.contains(l.trigger)&&u()};return window.addEventListener("scroll",d,{capture:!0}),()=>window.removeEventListener("scroll",d,{capture:!0})}},[l.trigger,u]),h.jsx(R0,{asChild:!0,disableOutsidePointerEvents:!1,onEscapeKeyDown:a,onPointerDownOutside:s,onFocusOutside:d=>d.preventDefault(),onDismiss:u,children:h.jsxs(p5,{"data-state":l.stateAttribute,...c,...o,ref:t,style:{...o.style,"--radix-tooltip-content-transform-origin":"var(--radix-popper-transform-origin)","--radix-tooltip-content-available-width":"var(--radix-popper-available-width)","--radix-tooltip-content-available-height":"var(--radix-popper-available-height)","--radix-tooltip-trigger-width":"var(--radix-popper-anchor-width)","--radix-tooltip-trigger-height":"var(--radix-popper-anchor-height)"},children:[h.jsx(a6,{children:r}),h.jsx(RL,{scope:n,isInside:!0,children:h.jsx(CN,{id:l.contentId,role:"tooltip",children:i||r})})]})})});$5.displayName=Lo;var _5="TooltipArrow",zL=T.forwardRef((e,t)=>{const{__scopeTooltip:n,...r}=e,i=k1(n);return OL(_5,n).isInside?null:h.jsx(f5,{...i,...r,ref:t})});zL.displayName=_5;function BL(e,t){const n=Math.abs(t.top-e.y),r=Math.abs(t.bottom-e.y),i=Math.abs(t.right-e.x),a=Math.abs(t.left-e.x);switch(Math.min(n,r,i,a)){case a:return"left";case i:return"right";case n:return"top";case r:return"bottom";default:throw new Error("unreachable")}}function FL(e,t,n=5){const r=[];switch(t){case"top":r.push({x:e.x-n,y:e.y+n},{x:e.x+n,y:e.y+n});break;case"bottom":r.push({x:e.x-n,y:e.y-n},{x:e.x+n,y:e.y-n});break;case"left":r.push({x:e.x+n,y:e.y-n},{x:e.x+n,y:e.y+n});break;case"right":r.push({x:e.x-n,y:e.y-n},{x:e.x-n,y:e.y+n});break}return r}function jL(e){const{top:t,right:n,bottom:r,left:i}=e;return[{x:i,y:t},{x:n,y:t},{x:n,y:r},{x:i,y:r}]}function VL(e,t){const{x:n,y:r}=e;let i=!1;for(let a=0,s=t.length-1;a<t.length;s=a++){const o=t[a].x,l=t[a].y,c=t[s].x,u=t[s].y;l>r!=u>r&&n<(c-o)*(r-l)/(u-l)+o&&(i=!i)}return i}function HL(e){const t=e.slice();return t.sort((n,r)=>n.x<r.x?-1:n.x>r.x?1:n.y<r.y?-1:n.y>r.y?1:0),qL(t)}function qL(e){if(e.length<=1)return e.slice();const t=[];for(let r=0;r<e.length;r++){const i=e[r];for(;t.length>=2;){const a=t[t.length-1],s=t[t.length-2];if((a.x-s.x)*(i.y-s.y)>=(a.y-s.y)*(i.x-s.x))t.pop();else break}t.push(i)}t.pop();const n=[];for(let r=e.length-1;r>=0;r--){const i=e[r];for(;n.length>=2;){const a=n[n.length-1],s=n[n.length-2];if((a.x-s.x)*(i.y-s.y)>=(a.y-s.y)*(i.x-s.x))n.pop();else break}n.push(i)}return n.pop(),t.length===1&&n.length===1&&t[0].x===n[0].x&&t[0].y===n[0].y?t:t.concat(n)}var GL=v5,w5=$5;const WL=GL,KL=T.forwardRef(({className:e,sideOffset:t=4,...n},r)=>h.jsx(w5,{ref:r,sideOffset:t,className:Ue("z-50 overflow-hidden rounded-md border bg-popover px-3 py-1.5 text-sm text-popover-foreground shadow-md animate-in fade-in-0 zoom-in-95 data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=closed]:zoom-out-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",e),...n}));KL.displayName=w5.displayName;var K0=class{constructor(){this.listeners=new Set,this.subscribe=this.subscribe.bind(this)}subscribe(e){return this.listeners.add(e),this.onSubscribe(),()=>{this.listeners.delete(e),this.onUnsubscribe()}}hasListeners(){return this.listeners.size>0}onSubscribe(){}onUnsubscribe(){}},Y0=typeof window>"u"||"Deno"in globalThis;function _r(){}function YL(e,t){return typeof e=="function"?e(t):e}function XL(e){return typeof e=="number"&&e>=0&&e!==1/0}function QL(e,t){return Math.max(e+(t||0)-Date.now(),0)}function Kv(e,t){return typeof e=="function"?e(t):e}function ZL(e,t){return typeof e=="function"?e(t):e}function Yv(e,t){const{type:n="all",exact:r,fetchStatus:i,predicate:a,queryKey:s,stale:o}=e;if(s){if(r){if(t.queryHash!==S1(s,t.options))return!1}else if(!vc(t.queryKey,s))return!1}if(n!=="all"){const l=t.isActive();if(n==="active"&&!l||n==="inactive"&&l)return!1}return!(typeof o=="boolean"&&t.isStale()!==o||i&&i!==t.state.fetchStatus||a&&!a(t))}function Xv(e,t){const{exact:n,status:r,predicate:i,mutationKey:a}=e;if(a){if(!t.options.mutationKey)return!1;if(n){if(bc(t.options.mutationKey)!==bc(a))return!1}else if(!vc(t.options.mutationKey,a))return!1}return!(r&&t.state.status!==r||i&&!i(t))}function S1(e,t){return((t==null?void 0:t.queryKeyHashFn)||bc)(e)}function bc(e){return JSON.stringify(e,(t,n)=>mf(n)?Object.keys(n).sort().reduce((r,i)=>(r[i]=n[i],r),{}):n)}function vc(e,t){return e===t?!0:typeof e!=typeof t?!1:e&&t&&typeof e=="object"&&typeof t=="object"?!Object.keys(t).some(n=>!vc(e[n],t[n])):!1}function T5(e,t){if(e===t)return e;const n=Qv(e)&&Qv(t);if(n||mf(e)&&mf(t)){const r=n?e:Object.keys(e),i=r.length,a=n?t:Object.keys(t),s=a.length,o=n?[]:{};let l=0;for(let c=0;c<s;c++){const u=n?c:a[c];(!n&&r.includes(u)||n)&&e[u]===void 0&&t[u]===void 0?(o[u]=void 0,l++):(o[u]=T5(e[u],t[u]),o[u]===e[u]&&e[u]!==void 0&&l++)}return i===s&&l===i?e:o}return t}function Qv(e){return Array.isArray(e)&&e.length===Object.keys(e).length}function mf(e){if(!Zv(e))return!1;const t=e.constructor;if(t===void 0)return!0;const n=t.prototype;return!(!Zv(n)||!n.hasOwnProperty("isPrototypeOf")||Object.getPrototypeOf(e)!==Object.prototype)}function Zv(e){return Object.prototype.toString.call(e)==="[object Object]"}function JL(e){return new Promise(t=>{setTimeout(t,e)})}function eP(e,t,n){return typeof n.structuralSharing=="function"?n.structuralSharing(e,t):n.structuralSharing!==!1?T5(e,t):t}function tP(e,t,n=0){const r=[...e,t];return n&&r.length>n?r.slice(1):r}function nP(e,t,n=0){const r=[t,...e];return n&&r.length>n?r.slice(0,-1):r}var N1=Symbol();function E5(e,t){return!e.queryFn&&(t!=null&&t.initialPromise)?()=>t.initialPromise:!e.queryFn||e.queryFn===N1?()=>Promise.reject(new Error(`Missing queryFn: '${e.queryHash}'`)):e.queryFn}var ts,ia,ho,U$,rP=(U$=class extends K0{constructor(){super();Qe(this,ts);Qe(this,ia);Qe(this,ho);Oe(this,ho,t=>{if(!Y0&&window.addEventListener){const n=()=>t();return window.addEventListener("visibilitychange",n,!1),()=>{window.removeEventListener("visibilitychange",n)}}})}onSubscribe(){Q(this,ia)||this.setEventListener(Q(this,ho))}onUnsubscribe(){var t;this.hasListeners()||((t=Q(this,ia))==null||t.call(this),Oe(this,ia,void 0))}setEventListener(t){var n;Oe(this,ho,t),(n=Q(this,ia))==null||n.call(this),Oe(this,ia,t(r=>{typeof r=="boolean"?this.setFocused(r):this.onFocus()}))}setFocused(t){Q(this,ts)!==t&&(Oe(this,ts,t),this.onFocus())}onFocus(){const t=this.isFocused();this.listeners.forEach(n=>{n(t)})}isFocused(){var t;return typeof Q(this,ts)=="boolean"?Q(this,ts):((t=globalThis.document)==null?void 0:t.visibilityState)!=="hidden"}},ts=new WeakMap,ia=new WeakMap,ho=new WeakMap,U$),k5=new rP,po,aa,fo,D$,iP=(D$=class extends K0{constructor(){super();Qe(this,po,!0);Qe(this,aa);Qe(this,fo);Oe(this,fo,t=>{if(!Y0&&window.addEventListener){const n=()=>t(!0),r=()=>t(!1);return window.addEventListener("online",n,!1),window.addEventListener("offline",r,!1),()=>{window.removeEventListener("online",n),window.removeEventListener("offline",r)}}})}onSubscribe(){Q(this,aa)||this.setEventListener(Q(this,fo))}onUnsubscribe(){var t;this.hasListeners()||((t=Q(this,aa))==null||t.call(this),Oe(this,aa,void 0))}setEventListener(t){var n;Oe(this,fo,t),(n=Q(this,aa))==null||n.call(this),Oe(this,aa,t(this.setOnline.bind(this)))}setOnline(t){Q(this,po)!==t&&(Oe(this,po,t),this.listeners.forEach(r=>{r(t)}))}isOnline(){return Q(this,po)}},po=new WeakMap,aa=new WeakMap,fo=new WeakMap,D$),t0=new iP;function aP(){let e,t;const n=new Promise((i,a)=>{e=i,t=a});n.status="pending",n.catch(()=>{});function r(i){Object.assign(n,i),delete n.resolve,delete n.reject}return n.resolve=i=>{r({status:"fulfilled",value:i}),e(i)},n.reject=i=>{r({status:"rejected",reason:i}),t(i)},n}function sP(e){return Math.min(1e3*2**e,3e4)}function S5(e){return(e??"online")==="online"?t0.isOnline():!0}var N5=class extends Error{constructor(e){super("CancelledError"),this.revert=e==null?void 0:e.revert,this.silent=e==null?void 0:e.silent}};function lh(e){return e instanceof N5}function C5(e){let t=!1,n=0,r=!1,i;const a=aP(),s=g=>{var w;r||(m(new N5(g)),(w=e.abort)==null||w.call(e))},o=()=>{t=!0},l=()=>{t=!1},c=()=>k5.isFocused()&&(e.networkMode==="always"||t0.isOnline())&&e.canRun(),u=()=>S5(e.networkMode)&&e.canRun(),d=g=>{var w;r||(r=!0,(w=e.onSuccess)==null||w.call(e,g),i==null||i(),a.resolve(g))},m=g=>{var w;r||(r=!0,(w=e.onError)==null||w.call(e,g),i==null||i(),a.reject(g))},p=()=>new Promise(g=>{var w;i=v=>{(r||c())&&g(v)},(w=e.onPause)==null||w.call(e)}).then(()=>{var g;i=void 0,r||(g=e.onContinue)==null||g.call(e)}),x=()=>{if(r)return;let g;const w=n===0?e.initialPromise:void 0;try{g=w??e.fn()}catch(v){g=Promise.reject(v)}Promise.resolve(g).then(d).catch(v=>{var S;if(r)return;const $=e.retry??(Y0?0:3),_=e.retryDelay??sP,C=typeof _=="function"?_(n,v):_,k=$===!0||typeof $=="number"&&n<$||typeof $=="function"&&$(n,v);if(t||!k){m(v);return}n++,(S=e.onFail)==null||S.call(e,n,v),JL(C).then(()=>c()?void 0:p()).then(()=>{t?m(v):x()})})};return{promise:a,cancel:s,continue:()=>(i==null||i(),a),cancelRetry:o,continueRetry:l,canStart:u,start:()=>(u()?x():p().then(x),a)}}function oP(){let e=[],t=0,n=o=>{o()},r=o=>{o()},i=o=>setTimeout(o,0);const a=o=>{t?e.push(o):i(()=>{n(o)})},s=()=>{const o=e;e=[],o.length&&i(()=>{r(()=>{o.forEach(l=>{n(l)})})})};return{batch:o=>{let l;t++;try{l=o()}finally{t--,t||s()}return l},batchCalls:o=>(...l)=>{a(()=>{o(...l)})},schedule:a,setNotifyFunction:o=>{n=o},setBatchNotifyFunction:o=>{r=o},setScheduler:o=>{i=o}}}var mn=oP(),ns,M$,A5=(M$=class{constructor(){Qe(this,ns)}destroy(){this.clearGcTimeout()}scheduleGc(){this.clearGcTimeout(),XL(this.gcTime)&&Oe(this,ns,setTimeout(()=>{this.optionalRemove()},this.gcTime))}updateGcTime(e){this.gcTime=Math.max(this.gcTime||0,e??(Y0?1/0:5*60*1e3))}clearGcTimeout(){Q(this,ns)&&(clearTimeout(Q(this,ns)),Oe(this,ns,void 0))}},ns=new WeakMap,M$),go,bo,tr,en,Dc,rs,Tr,vi,R$,lP=(R$=class extends A5{constructor(t){super();Qe(this,Tr);Qe(this,go);Qe(this,bo);Qe(this,tr);Qe(this,en);Qe(this,Dc);Qe(this,rs);Oe(this,rs,!1),Oe(this,Dc,t.defaultOptions),this.setOptions(t.options),this.observers=[],Oe(this,tr,t.cache),this.queryKey=t.queryKey,this.queryHash=t.queryHash,Oe(this,go,uP(this.options)),this.state=t.state??Q(this,go),this.scheduleGc()}get meta(){return this.options.meta}get promise(){var t;return(t=Q(this,en))==null?void 0:t.promise}setOptions(t){this.options={...Q(this,Dc),...t},this.updateGcTime(this.options.gcTime)}optionalRemove(){!this.observers.length&&this.state.fetchStatus==="idle"&&Q(this,tr).remove(this)}setData(t,n){const r=eP(this.state.data,t,this.options);return Qt(this,Tr,vi).call(this,{data:r,type:"success",dataUpdatedAt:n==null?void 0:n.updatedAt,manual:n==null?void 0:n.manual}),r}setState(t,n){Qt(this,Tr,vi).call(this,{type:"setState",state:t,setStateOptions:n})}cancel(t){var r,i;const n=(r=Q(this,en))==null?void 0:r.promise;return(i=Q(this,en))==null||i.cancel(t),n?n.then(_r).catch(_r):Promise.resolve()}destroy(){super.destroy(),this.cancel({silent:!0})}reset(){this.destroy(),this.setState(Q(this,go))}isActive(){return this.observers.some(t=>ZL(t.options.enabled,this)!==!1)}isDisabled(){return this.getObserversCount()>0?!this.isActive():this.options.queryFn===N1||this.state.dataUpdateCount+this.state.errorUpdateCount===0}isStale(){return this.state.isInvalidated?!0:this.getObserversCount()>0?this.observers.some(t=>t.getCurrentResult().isStale):this.state.data===void 0}isStaleByTime(t=0){return this.state.isInvalidated||this.state.data===void 0||!QL(this.state.dataUpdatedAt,t)}onFocus(){var n;const t=this.observers.find(r=>r.shouldFetchOnWindowFocus());t==null||t.refetch({cancelRefetch:!1}),(n=Q(this,en))==null||n.continue()}onOnline(){var n;const t=this.observers.find(r=>r.shouldFetchOnReconnect());t==null||t.refetch({cancelRefetch:!1}),(n=Q(this,en))==null||n.continue()}addObserver(t){this.observers.includes(t)||(this.observers.push(t),this.clearGcTimeout(),Q(this,tr).notify({type:"observerAdded",query:this,observer:t}))}removeObserver(t){this.observers.includes(t)&&(this.observers=this.observers.filter(n=>n!==t),this.observers.length||(Q(this,en)&&(Q(this,rs)?Q(this,en).cancel({revert:!0}):Q(this,en).cancelRetry()),this.scheduleGc()),Q(this,tr).notify({type:"observerRemoved",query:this,observer:t}))}getObserversCount(){return this.observers.length}invalidate(){this.state.isInvalidated||Qt(this,Tr,vi).call(this,{type:"invalidate"})}fetch(t,n){var l,c,u;if(this.state.fetchStatus!=="idle"){if(this.state.data!==void 0&&(n!=null&&n.cancelRefetch))this.cancel({silent:!0});else if(Q(this,en))return Q(this,en).continueRetry(),Q(this,en).promise}if(t&&this.setOptions(t),!this.options.queryFn){const d=this.observers.find(m=>m.options.queryFn);d&&this.setOptions(d.options)}const r=new AbortController,i=d=>{Object.defineProperty(d,"signal",{enumerable:!0,get:()=>(Oe(this,rs,!0),r.signal)})},a=()=>{const d=E5(this.options,n),m={queryKey:this.queryKey,meta:this.meta};return i(m),Oe(this,rs,!1),this.options.persister?this.options.persister(d,m,this):d(m)},s={fetchOptions:n,options:this.options,queryKey:this.queryKey,state:this.state,fetchFn:a};i(s),(l=this.options.behavior)==null||l.onFetch(s,this),Oe(this,bo,this.state),(this.state.fetchStatus==="idle"||this.state.fetchMeta!==((c=s.fetchOptions)==null?void 0:c.meta))&&Qt(this,Tr,vi).call(this,{type:"fetch",meta:(u=s.fetchOptions)==null?void 0:u.meta});const o=d=>{var m,p,x,g;lh(d)&&d.silent||Qt(this,Tr,vi).call(this,{type:"error",error:d}),lh(d)||((p=(m=Q(this,tr).config).onError)==null||p.call(m,d,this),(g=(x=Q(this,tr).config).onSettled)==null||g.call(x,this.state.data,d,this)),this.scheduleGc()};return Oe(this,en,C5({initialPromise:n==null?void 0:n.initialPromise,fn:s.fetchFn,abort:r.abort.bind(r),onSuccess:d=>{var m,p,x,g;if(d===void 0){o(new Error(`${this.queryHash} data is undefined`));return}try{this.setData(d)}catch(w){o(w);return}(p=(m=Q(this,tr).config).onSuccess)==null||p.call(m,d,this),(g=(x=Q(this,tr).config).onSettled)==null||g.call(x,d,this.state.error,this),this.scheduleGc()},onError:o,onFail:(d,m)=>{Qt(this,Tr,vi).call(this,{type:"failed",failureCount:d,error:m})},onPause:()=>{Qt(this,Tr,vi).call(this,{type:"pause"})},onContinue:()=>{Qt(this,Tr,vi).call(this,{type:"continue"})},retry:s.options.retry,retryDelay:s.options.retryDelay,networkMode:s.options.networkMode,canRun:()=>!0})),Q(this,en).start()}},go=new WeakMap,bo=new WeakMap,tr=new WeakMap,en=new WeakMap,Dc=new WeakMap,rs=new WeakMap,Tr=new WeakSet,vi=function(t){const n=r=>{switch(t.type){case"failed":return{...r,fetchFailureCount:t.failureCount,fetchFailureReason:t.error};case"pause":return{...r,fetchStatus:"paused"};case"continue":return{...r,fetchStatus:"fetching"};case"fetch":return{...r,...cP(r.data,this.options),fetchMeta:t.meta??null};case"success":return{...r,data:t.data,dataUpdateCount:r.dataUpdateCount+1,dataUpdatedAt:t.dataUpdatedAt??Date.now(),error:null,isInvalidated:!1,status:"success",...!t.manual&&{fetchStatus:"idle",fetchFailureCount:0,fetchFailureReason:null}};case"error":const i=t.error;return lh(i)&&i.revert&&Q(this,bo)?{...Q(this,bo),fetchStatus:"idle"}:{...r,error:i,errorUpdateCount:r.errorUpdateCount+1,errorUpdatedAt:Date.now(),fetchFailureCount:r.fetchFailureCount+1,fetchFailureReason:i,fetchStatus:"idle",status:"error"};case"invalidate":return{...r,isInvalidated:!0};case"setState":return{...r,...t.state}}};this.state=n(this.state),mn.batch(()=>{this.observers.forEach(r=>{r.onQueryUpdate()}),Q(this,tr).notify({query:this,type:"updated",action:t})})},R$);function cP(e,t){return{fetchFailureCount:0,fetchFailureReason:null,fetchStatus:S5(t.networkMode)?"fetching":"paused",...e===void 0&&{error:null,status:"pending"}}}function uP(e){const t=typeof e.initialData=="function"?e.initialData():e.initialData,n=t!==void 0,r=n?typeof e.initialDataUpdatedAt=="function"?e.initialDataUpdatedAt():e.initialDataUpdatedAt:0;return{data:t,dataUpdateCount:0,dataUpdatedAt:n?r??Date.now():0,error:null,errorUpdateCount:0,errorUpdatedAt:0,fetchFailureCount:0,fetchFailureReason:null,fetchMeta:null,isInvalidated:!1,status:n?"success":"pending",fetchStatus:"idle"}}var qr,O$,dP=(O$=class extends K0{constructor(t={}){super();Qe(this,qr);this.config=t,Oe(this,qr,new Map)}build(t,n,r){const i=n.queryKey,a=n.queryHash??S1(i,n);let s=this.get(a);return s||(s=new lP({cache:this,queryKey:i,queryHash:a,options:t.defaultQueryOptions(n),state:r,defaultOptions:t.getQueryDefaults(i)}),this.add(s)),s}add(t){Q(this,qr).has(t.queryHash)||(Q(this,qr).set(t.queryHash,t),this.notify({type:"added",query:t}))}remove(t){const n=Q(this,qr).get(t.queryHash);n&&(t.destroy(),n===t&&Q(this,qr).delete(t.queryHash),this.notify({type:"removed",query:t}))}clear(){mn.batch(()=>{this.getAll().forEach(t=>{this.remove(t)})})}get(t){return Q(this,qr).get(t)}getAll(){return[...Q(this,qr).values()]}find(t){const n={exact:!0,...t};return this.getAll().find(r=>Yv(n,r))}findAll(t={}){const n=this.getAll();return Object.keys(t).length>0?n.filter(r=>Yv(t,r)):n}notify(t){mn.batch(()=>{this.listeners.forEach(n=>{n(t)})})}onFocus(){mn.batch(()=>{this.getAll().forEach(t=>{t.onFocus()})})}onOnline(){mn.batch(()=>{this.getAll().forEach(t=>{t.onOnline()})})}},qr=new WeakMap,O$),Gr,un,is,Wr,Yi,z$,mP=(z$=class extends A5{constructor(t){super();Qe(this,Wr);Qe(this,Gr);Qe(this,un);Qe(this,is);this.mutationId=t.mutationId,Oe(this,un,t.mutationCache),Oe(this,Gr,[]),this.state=t.state||hP(),this.setOptions(t.options),this.scheduleGc()}setOptions(t){this.options=t,this.updateGcTime(this.options.gcTime)}get meta(){return this.options.meta}addObserver(t){Q(this,Gr).includes(t)||(Q(this,Gr).push(t),this.clearGcTimeout(),Q(this,un).notify({type:"observerAdded",mutation:this,observer:t}))}removeObserver(t){Oe(this,Gr,Q(this,Gr).filter(n=>n!==t)),this.scheduleGc(),Q(this,un).notify({type:"observerRemoved",mutation:this,observer:t})}optionalRemove(){Q(this,Gr).length||(this.state.status==="pending"?this.scheduleGc():Q(this,un).remove(this))}continue(){var t;return((t=Q(this,is))==null?void 0:t.continue())??this.execute(this.state.variables)}async execute(t){var i,a,s,o,l,c,u,d,m,p,x,g,w,v,$,_,C,k,S,L;Oe(this,is,C5({fn:()=>this.options.mutationFn?this.options.mutationFn(t):Promise.reject(new Error("No mutationFn found")),onFail:(U,F)=>{Qt(this,Wr,Yi).call(this,{type:"failed",failureCount:U,error:F})},onPause:()=>{Qt(this,Wr,Yi).call(this,{type:"pause"})},onContinue:()=>{Qt(this,Wr,Yi).call(this,{type:"continue"})},retry:this.options.retry??0,retryDelay:this.options.retryDelay,networkMode:this.options.networkMode,canRun:()=>Q(this,un).canRun(this)}));const n=this.state.status==="pending",r=!Q(this,is).canStart();try{if(!n){Qt(this,Wr,Yi).call(this,{type:"pending",variables:t,isPaused:r}),await((a=(i=Q(this,un).config).onMutate)==null?void 0:a.call(i,t,this));const F=await((o=(s=this.options).onMutate)==null?void 0:o.call(s,t));F!==this.state.context&&Qt(this,Wr,Yi).call(this,{type:"pending",context:F,variables:t,isPaused:r})}const U=await Q(this,is).start();return await((c=(l=Q(this,un).config).onSuccess)==null?void 0:c.call(l,U,t,this.state.context,this)),await((d=(u=this.options).onSuccess)==null?void 0:d.call(u,U,t,this.state.context)),await((p=(m=Q(this,un).config).onSettled)==null?void 0:p.call(m,U,null,this.state.variables,this.state.context,this)),await((g=(x=this.options).onSettled)==null?void 0:g.call(x,U,null,t,this.state.context)),Qt(this,Wr,Yi).call(this,{type:"success",data:U}),U}catch(U){try{throw await((v=(w=Q(this,un).config).onError)==null?void 0:v.call(w,U,t,this.state.context,this)),await((_=($=this.options).onError)==null?void 0:_.call($,U,t,this.state.context)),await((k=(C=Q(this,un).config).onSettled)==null?void 0:k.call(C,void 0,U,this.state.variables,this.state.context,this)),await((L=(S=this.options).onSettled)==null?void 0:L.call(S,void 0,U,t,this.state.context)),U}finally{Qt(this,Wr,Yi).call(this,{type:"error",error:U})}}finally{Q(this,un).runNext(this)}}},Gr=new WeakMap,un=new WeakMap,is=new WeakMap,Wr=new WeakSet,Yi=function(t){const n=r=>{switch(t.type){case"failed":return{...r,failureCount:t.failureCount,failureReason:t.error};case"pause":return{...r,isPaused:!0};case"continue":return{...r,isPaused:!1};case"pending":return{...r,context:t.context,data:void 0,failureCount:0,failureReason:null,error:null,isPaused:t.isPaused,status:"pending",variables:t.variables,submittedAt:Date.now()};case"success":return{...r,data:t.data,failureCount:0,failureReason:null,error:null,status:"success",isPaused:!1};case"error":return{...r,data:void 0,error:t.error,failureCount:r.failureCount+1,failureReason:t.error,isPaused:!1,status:"error"}}};this.state=n(this.state),mn.batch(()=>{Q(this,Gr).forEach(r=>{r.onMutationUpdate(t)}),Q(this,un).notify({mutation:this,type:"updated",action:t})})},z$);function hP(){return{context:void 0,data:void 0,error:null,failureCount:0,failureReason:null,isPaused:!1,status:"idle",variables:void 0,submittedAt:0}}var On,Mc,B$,pP=(B$=class extends K0{constructor(t={}){super();Qe(this,On);Qe(this,Mc);this.config=t,Oe(this,On,new Map),Oe(this,Mc,Date.now())}build(t,n,r){const i=new mP({mutationCache:this,mutationId:++ou(this,Mc)._,options:t.defaultMutationOptions(n),state:r});return this.add(i),i}add(t){const n=Au(t),r=Q(this,On).get(n)??[];r.push(t),Q(this,On).set(n,r),this.notify({type:"added",mutation:t})}remove(t){var r;const n=Au(t);if(Q(this,On).has(n)){const i=(r=Q(this,On).get(n))==null?void 0:r.filter(a=>a!==t);i&&(i.length===0?Q(this,On).delete(n):Q(this,On).set(n,i))}this.notify({type:"removed",mutation:t})}canRun(t){var r;const n=(r=Q(this,On).get(Au(t)))==null?void 0:r.find(i=>i.state.status==="pending");return!n||n===t}runNext(t){var r;const n=(r=Q(this,On).get(Au(t)))==null?void 0:r.find(i=>i!==t&&i.state.isPaused);return(n==null?void 0:n.continue())??Promise.resolve()}clear(){mn.batch(()=>{this.getAll().forEach(t=>{this.remove(t)})})}getAll(){return[...Q(this,On).values()].flat()}find(t){const n={exact:!0,...t};return this.getAll().find(r=>Xv(n,r))}findAll(t={}){return this.getAll().filter(n=>Xv(t,n))}notify(t){mn.batch(()=>{this.listeners.forEach(n=>{n(t)})})}resumePausedMutations(){const t=this.getAll().filter(n=>n.state.isPaused);return mn.batch(()=>Promise.all(t.map(n=>n.continue().catch(_r))))}},On=new WeakMap,Mc=new WeakMap,B$);function Au(e){var t;return((t=e.options.scope)==null?void 0:t.id)??String(e.mutationId)}function Jv(e){return{onFetch:(t,n)=>{var u,d,m,p,x;const r=t.options,i=(m=(d=(u=t.fetchOptions)==null?void 0:u.meta)==null?void 0:d.fetchMore)==null?void 0:m.direction,a=((p=t.state.data)==null?void 0:p.pages)||[],s=((x=t.state.data)==null?void 0:x.pageParams)||[];let o={pages:[],pageParams:[]},l=0;const c=async()=>{let g=!1;const w=_=>{Object.defineProperty(_,"signal",{enumerable:!0,get:()=>(t.signal.aborted?g=!0:t.signal.addEventListener("abort",()=>{g=!0}),t.signal)})},v=E5(t.options,t.fetchOptions),$=async(_,C,k)=>{if(g)return Promise.reject();if(C==null&&_.pages.length)return Promise.resolve(_);const S={queryKey:t.queryKey,pageParam:C,direction:k?"backward":"forward",meta:t.options.meta};w(S);const L=await v(S),{maxPages:U}=t.options,F=k?nP:tP;return{pages:F(_.pages,L,U),pageParams:F(_.pageParams,C,U)}};if(i&&a.length){const _=i==="backward",C=_?fP:e4,k={pages:a,pageParams:s},S=C(r,k);o=await $(k,S,_)}else{const _=e??a.length;do{const C=l===0?s[0]??r.initialPageParam:e4(r,o);if(l>0&&C==null)break;o=await $(o,C),l++}while(l<_)}return o};t.options.persister?t.fetchFn=()=>{var g,w;return(w=(g=t.options).persister)==null?void 0:w.call(g,c,{queryKey:t.queryKey,meta:t.options.meta,signal:t.signal},n)}:t.fetchFn=c}}}function e4(e,{pages:t,pageParams:n}){const r=t.length-1;return t.length>0?e.getNextPageParam(t[r],t,n[r],n):void 0}function fP(e,{pages:t,pageParams:n}){var r;return t.length>0?(r=e.getPreviousPageParam)==null?void 0:r.call(e,t[0],t,n[0],n):void 0}var Tt,sa,oa,vo,xo,la,$o,yo,F$,gP=(F$=class{constructor(e={}){Qe(this,Tt);Qe(this,sa);Qe(this,oa);Qe(this,vo);Qe(this,xo);Qe(this,la);Qe(this,$o);Qe(this,yo);Oe(this,Tt,e.queryCache||new dP),Oe(this,sa,e.mutationCache||new pP),Oe(this,oa,e.defaultOptions||{}),Oe(this,vo,new Map),Oe(this,xo,new Map),Oe(this,la,0)}mount(){ou(this,la)._++,Q(this,la)===1&&(Oe(this,$o,k5.subscribe(async e=>{e&&(await this.resumePausedMutations(),Q(this,Tt).onFocus())})),Oe(this,yo,t0.subscribe(async e=>{e&&(await this.resumePausedMutations(),Q(this,Tt).onOnline())})))}unmount(){var e,t;ou(this,la)._--,Q(this,la)===0&&((e=Q(this,$o))==null||e.call(this),Oe(this,$o,void 0),(t=Q(this,yo))==null||t.call(this),Oe(this,yo,void 0))}isFetching(e){return Q(this,Tt).findAll({...e,fetchStatus:"fetching"}).length}isMutating(e){return Q(this,sa).findAll({...e,status:"pending"}).length}getQueryData(e){var n;const t=this.defaultQueryOptions({queryKey:e});return(n=Q(this,Tt).get(t.queryHash))==null?void 0:n.state.data}ensureQueryData(e){const t=this.getQueryData(e.queryKey);if(t===void 0)return this.fetchQuery(e);{const n=this.defaultQueryOptions(e),r=Q(this,Tt).build(this,n);return e.revalidateIfStale&&r.isStaleByTime(Kv(n.staleTime,r))&&this.prefetchQuery(n),Promise.resolve(t)}}getQueriesData(e){return Q(this,Tt).findAll(e).map(({queryKey:t,state:n})=>{const r=n.data;return[t,r]})}setQueryData(e,t,n){const r=this.defaultQueryOptions({queryKey:e}),i=Q(this,Tt).get(r.queryHash),a=i==null?void 0:i.state.data,s=YL(t,a);if(s!==void 0)return Q(this,Tt).build(this,r).setData(s,{...n,manual:!0})}setQueriesData(e,t,n){return mn.batch(()=>Q(this,Tt).findAll(e).map(({queryKey:r})=>[r,this.setQueryData(r,t,n)]))}getQueryState(e){var n;const t=this.defaultQueryOptions({queryKey:e});return(n=Q(this,Tt).get(t.queryHash))==null?void 0:n.state}removeQueries(e){const t=Q(this,Tt);mn.batch(()=>{t.findAll(e).forEach(n=>{t.remove(n)})})}resetQueries(e,t){const n=Q(this,Tt),r={type:"active",...e};return mn.batch(()=>(n.findAll(e).forEach(i=>{i.reset()}),this.refetchQueries(r,t)))}cancelQueries(e={},t={}){const n={revert:!0,...t},r=mn.batch(()=>Q(this,Tt).findAll(e).map(i=>i.cancel(n)));return Promise.all(r).then(_r).catch(_r)}invalidateQueries(e={},t={}){return mn.batch(()=>{if(Q(this,Tt).findAll(e).forEach(r=>{r.invalidate()}),e.refetchType==="none")return Promise.resolve();const n={...e,type:e.refetchType??e.type??"active"};return this.refetchQueries(n,t)})}refetchQueries(e={},t){const n={...t,cancelRefetch:(t==null?void 0:t.cancelRefetch)??!0},r=mn.batch(()=>Q(this,Tt).findAll(e).filter(i=>!i.isDisabled()).map(i=>{let a=i.fetch(void 0,n);return n.throwOnError||(a=a.catch(_r)),i.state.fetchStatus==="paused"?Promise.resolve():a}));return Promise.all(r).then(_r)}fetchQuery(e){const t=this.defaultQueryOptions(e);t.retry===void 0&&(t.retry=!1);const n=Q(this,Tt).build(this,t);return n.isStaleByTime(Kv(t.staleTime,n))?n.fetch(t):Promise.resolve(n.state.data)}prefetchQuery(e){return this.fetchQuery(e).then(_r).catch(_r)}fetchInfiniteQuery(e){return e.behavior=Jv(e.pages),this.fetchQuery(e)}prefetchInfiniteQuery(e){return this.fetchInfiniteQuery(e).then(_r).catch(_r)}ensureInfiniteQueryData(e){return e.behavior=Jv(e.pages),this.ensureQueryData(e)}resumePausedMutations(){return t0.isOnline()?Q(this,sa).resumePausedMutations():Promise.resolve()}getQueryCache(){return Q(this,Tt)}getMutationCache(){return Q(this,sa)}getDefaultOptions(){return Q(this,oa)}setDefaultOptions(e){Oe(this,oa,e)}setQueryDefaults(e,t){Q(this,vo).set(bc(e),{queryKey:e,defaultOptions:t})}getQueryDefaults(e){const t=[...Q(this,vo).values()];let n={};return t.forEach(r=>{vc(e,r.queryKey)&&(n={...n,...r.defaultOptions})}),n}setMutationDefaults(e,t){Q(this,xo).set(bc(e),{mutationKey:e,defaultOptions:t})}getMutationDefaults(e){const t=[...Q(this,xo).values()];let n={};return t.forEach(r=>{vc(e,r.mutationKey)&&(n={...n,...r.defaultOptions})}),n}defaultQueryOptions(e){if(e._defaulted)return e;const t={...Q(this,oa).queries,...this.getQueryDefaults(e.queryKey),...e,_defaulted:!0};return t.queryHash||(t.queryHash=S1(t.queryKey,t)),t.refetchOnReconnect===void 0&&(t.refetchOnReconnect=t.networkMode!=="always"),t.throwOnError===void 0&&(t.throwOnError=!!t.suspense),!t.networkMode&&t.persister&&(t.networkMode="offlineFirst"),t.enabled!==!0&&t.queryFn===N1&&(t.enabled=!1),t}defaultMutationOptions(e){return e!=null&&e._defaulted?e:{...Q(this,oa).mutations,...(e==null?void 0:e.mutationKey)&&this.getMutationDefaults(e.mutationKey),...e,_defaulted:!0}}clear(){Q(this,Tt).clear(),Q(this,sa).clear()}},Tt=new WeakMap,sa=new WeakMap,oa=new WeakMap,vo=new WeakMap,xo=new WeakMap,la=new WeakMap,$o=new WeakMap,yo=new WeakMap,F$),bP=T.createContext(void 0),vP=({client:e,children:t})=>(T.useEffect(()=>(e.mount(),()=>{e.unmount()}),[e]),h.jsx(bP.Provider,{value:e,children:t}));/**
 * @remix-run/router v1.20.0
 *
 * Copyright (c) Remix Software Inc.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE.md file in the root directory of this source tree.
 *
 * @license MIT
 */function xc(){return xc=Object.assign?Object.assign.bind():function(e){for(var t=1;t<arguments.length;t++){var n=arguments[t];for(var r in n)Object.prototype.hasOwnProperty.call(n,r)&&(e[r]=n[r])}return e},xc.apply(this,arguments)}var da;(function(e){e.Pop="POP",e.Push="PUSH",e.Replace="REPLACE"})(da||(da={}));const t4="popstate";function xP(e){e===void 0&&(e={});function t(r,i){let{pathname:a,search:s,hash:o}=r.location;return hf("",{pathname:a,search:s,hash:o},i.state&&i.state.usr||null,i.state&&i.state.key||"default")}function n(r,i){return typeof i=="string"?i:P5(i)}return yP(t,n,null,e)}function zt(e,t){if(e===!1||e===null||typeof e>"u")throw new Error(t)}function L5(e,t){if(!e){typeof console<"u"&&console.warn(t);try{throw new Error(t)}catch{}}}function $P(){return Math.random().toString(36).substr(2,8)}function n4(e,t){return{usr:e.state,key:e.key,idx:t}}function hf(e,t,n,r){return n===void 0&&(n=null),xc({pathname:typeof e=="string"?e:e.pathname,search:"",hash:""},typeof t=="string"?Vo(t):t,{state:n,key:t&&t.key||r||$P()})}function P5(e){let{pathname:t="/",search:n="",hash:r=""}=e;return n&&n!=="?"&&(t+=n.charAt(0)==="?"?n:"?"+n),r&&r!=="#"&&(t+=r.charAt(0)==="#"?r:"#"+r),t}function Vo(e){let t={};if(e){let n=e.indexOf("#");n>=0&&(t.hash=e.substr(n),e=e.substr(0,n));let r=e.indexOf("?");r>=0&&(t.search=e.substr(r),e=e.substr(0,r)),e&&(t.pathname=e)}return t}function yP(e,t,n,r){r===void 0&&(r={});let{window:i=document.defaultView,v5Compat:a=!1}=r,s=i.history,o=da.Pop,l=null,c=u();c==null&&(c=0,s.replaceState(xc({},s.state,{idx:c}),""));function u(){return(s.state||{idx:null}).idx}function d(){o=da.Pop;let w=u(),v=w==null?null:w-c;c=w,l&&l({action:o,location:g.location,delta:v})}function m(w,v){o=da.Push;let $=hf(g.location,w,v);c=u()+1;let _=n4($,c),C=g.createHref($);try{s.pushState(_,"",C)}catch(k){if(k instanceof DOMException&&k.name==="DataCloneError")throw k;i.location.assign(C)}a&&l&&l({action:o,location:g.location,delta:1})}function p(w,v){o=da.Replace;let $=hf(g.location,w,v);c=u();let _=n4($,c),C=g.createHref($);s.replaceState(_,"",C),a&&l&&l({action:o,location:g.location,delta:0})}function x(w){let v=i.location.origin!=="null"?i.location.origin:i.location.href,$=typeof w=="string"?w:P5(w);return $=$.replace(/ $/,"%20"),zt(v,"No window.location.(origin|href) available to create URL for href: "+$),new URL($,v)}let g={get action(){return o},get location(){return e(i,s)},listen(w){if(l)throw new Error("A history only accepts one active listener");return i.addEventListener(t4,d),l=w,()=>{i.removeEventListener(t4,d),l=null}},createHref(w){return t(i,w)},createURL:x,encodeLocation(w){let v=x(w);return{pathname:v.pathname,search:v.search,hash:v.hash}},push:m,replace:p,go(w){return s.go(w)}};return g}var r4;(function(e){e.data="data",e.deferred="deferred",e.redirect="redirect",e.error="error"})(r4||(r4={}));function _P(e,t,n){return n===void 0&&(n="/"),wP(e,t,n,!1)}function wP(e,t,n,r){let i=typeof t=="string"?Vo(t):t,a=D5(i.pathname||"/",n);if(a==null)return null;let s=I5(e);TP(s);let o=null;for(let l=0;o==null&&l<s.length;++l){let c=DP(a);o=IP(s[l],c,r)}return o}function I5(e,t,n,r){t===void 0&&(t=[]),n===void 0&&(n=[]),r===void 0&&(r="");let i=(a,s,o)=>{let l={relativePath:o===void 0?a.path||"":o,caseSensitive:a.caseSensitive===!0,childrenIndex:s,route:a};l.relativePath.startsWith("/")&&(zt(l.relativePath.startsWith(r),'Absolute route path "'+l.relativePath+'" nested under path '+('"'+r+'" is not valid. An absolute child route path ')+"must start with the combined path of all its parent routes."),l.relativePath=l.relativePath.slice(r.length));let c=os([r,l.relativePath]),u=n.concat(l);a.children&&a.children.length>0&&(zt(a.index!==!0,"Index routes must not have child routes. Please remove "+('all child routes from route path "'+c+'".')),I5(a.children,t,u,c)),!(a.path==null&&!a.index)&&t.push({path:c,score:LP(c,a.index),routesMeta:u})};return e.forEach((a,s)=>{var o;if(a.path===""||!((o=a.path)!=null&&o.includes("?")))i(a,s);else for(let l of U5(a.path))i(a,s,l)}),t}function U5(e){let t=e.split("/");if(t.length===0)return[];let[n,...r]=t,i=n.endsWith("?"),a=n.replace(/\?$/,"");if(r.length===0)return i?[a,""]:[a];let s=U5(r.join("/")),o=[];return o.push(...s.map(l=>l===""?a:[a,l].join("/"))),i&&o.push(...s),o.map(l=>e.startsWith("/")&&l===""?"/":l)}function TP(e){e.sort((t,n)=>t.score!==n.score?n.score-t.score:PP(t.routesMeta.map(r=>r.childrenIndex),n.routesMeta.map(r=>r.childrenIndex)))}const EP=/^:[\w-]+$/,kP=3,SP=2,NP=1,CP=10,AP=-2,i4=e=>e==="*";function LP(e,t){let n=e.split("/"),r=n.length;return n.some(i4)&&(r+=AP),t&&(r+=SP),n.filter(i=>!i4(i)).reduce((i,a)=>i+(EP.test(a)?kP:a===""?NP:CP),r)}function PP(e,t){return e.length===t.length&&e.slice(0,-1).every((r,i)=>r===t[i])?e[e.length-1]-t[t.length-1]:0}function IP(e,t,n){let{routesMeta:r}=e,i={},a="/",s=[];for(let o=0;o<r.length;++o){let l=r[o],c=o===r.length-1,u=a==="/"?t:t.slice(a.length)||"/",d=a4({path:l.relativePath,caseSensitive:l.caseSensitive,end:c},u),m=l.route;if(!d&&c&&n&&!r[r.length-1].route.index&&(d=a4({path:l.relativePath,caseSensitive:l.caseSensitive,end:!1},u)),!d)return null;Object.assign(i,d.params),s.push({params:i,pathname:os([a,d.pathname]),pathnameBase:FP(os([a,d.pathnameBase])),route:m}),d.pathnameBase!=="/"&&(a=os([a,d.pathnameBase]))}return s}function a4(e,t){typeof e=="string"&&(e={path:e,caseSensitive:!1,end:!0});let[n,r]=UP(e.path,e.caseSensitive,e.end),i=t.match(n);if(!i)return null;let a=i[0],s=a.replace(/(.)\/+$/,"$1"),o=i.slice(1);return{params:r.reduce((c,u,d)=>{let{paramName:m,isOptional:p}=u;if(m==="*"){let g=o[d]||"";s=a.slice(0,a.length-g.length).replace(/(.)\/+$/,"$1")}const x=o[d];return p&&!x?c[m]=void 0:c[m]=(x||"").replace(/%2F/g,"/"),c},{}),pathname:a,pathnameBase:s,pattern:e}}function UP(e,t,n){t===void 0&&(t=!1),n===void 0&&(n=!0),L5(e==="*"||!e.endsWith("*")||e.endsWith("/*"),'Route path "'+e+'" will be treated as if it were '+('"'+e.replace(/\*$/,"/*")+'" because the `*` character must ')+"always follow a `/` in the pattern. To get rid of this warning, "+('please change the route path to "'+e.replace(/\*$/,"/*")+'".'));let r=[],i="^"+e.replace(/\/*\*?$/,"").replace(/^\/*/,"/").replace(/[\\.*+^${}|()[\]]/g,"\\$&").replace(/\/:([\w-]+)(\?)?/g,(s,o,l)=>(r.push({paramName:o,isOptional:l!=null}),l?"/?([^\\/]+)?":"/([^\\/]+)"));return e.endsWith("*")?(r.push({paramName:"*"}),i+=e==="*"||e==="/*"?"(.*)$":"(?:\\/(.+)|\\/*)$"):n?i+="\\/*$":e!==""&&e!=="/"&&(i+="(?:(?=\\/|$))"),[new RegExp(i,t?void 0:"i"),r]}function DP(e){try{return e.split("/").map(t=>decodeURIComponent(t).replace(/\//g,"%2F")).join("/")}catch(t){return L5(!1,'The URL path "'+e+'" could not be decoded because it is is a malformed URL segment. This is probably due to a bad percent '+("encoding ("+t+").")),e}}function D5(e,t){if(t==="/")return e;if(!e.toLowerCase().startsWith(t.toLowerCase()))return null;let n=t.endsWith("/")?t.length-1:t.length,r=e.charAt(n);return r&&r!=="/"?null:e.slice(n)||"/"}function MP(e,t){t===void 0&&(t="/");let{pathname:n,search:r="",hash:i=""}=typeof e=="string"?Vo(e):e;return{pathname:n?n.startsWith("/")?n:RP(n,t):t,search:jP(r),hash:VP(i)}}function RP(e,t){let n=t.replace(/\/+$/,"").split("/");return e.split("/").forEach(i=>{i===".."?n.length>1&&n.pop():i!=="."&&n.push(i)}),n.length>1?n.join("/"):"/"}function ch(e,t,n,r){return"Cannot include a '"+e+"' character in a manually specified "+("`to."+t+"` field ["+JSON.stringify(r)+"].  Please separate it out to the ")+("`to."+n+"` field. Alternatively you may provide the full path as ")+'a string in <Link to="..."> and the router will parse it for you.'}function OP(e){return e.filter((t,n)=>n===0||t.route.path&&t.route.path.length>0)}function zP(e,t){let n=OP(e);return t?n.map((r,i)=>i===n.length-1?r.pathname:r.pathnameBase):n.map(r=>r.pathnameBase)}function BP(e,t,n,r){r===void 0&&(r=!1);let i;typeof e=="string"?i=Vo(e):(i=xc({},e),zt(!i.pathname||!i.pathname.includes("?"),ch("?","pathname","search",i)),zt(!i.pathname||!i.pathname.includes("#"),ch("#","pathname","hash",i)),zt(!i.search||!i.search.includes("#"),ch("#","search","hash",i)));let a=e===""||i.pathname==="",s=a?"/":i.pathname,o;if(s==null)o=n;else{let d=t.length-1;if(!r&&s.startsWith("..")){let m=s.split("/");for(;m[0]==="..";)m.shift(),d-=1;i.pathname=m.join("/")}o=d>=0?t[d]:"/"}let l=MP(i,o),c=s&&s!=="/"&&s.endsWith("/"),u=(a||s===".")&&n.endsWith("/");return!l.pathname.endsWith("/")&&(c||u)&&(l.pathname+="/"),l}const os=e=>e.join("/").replace(/\/\/+/g,"/"),FP=e=>e.replace(/\/+$/,"").replace(/^\/*/,"/"),jP=e=>!e||e==="?"?"":e.startsWith("?")?e:"?"+e,VP=e=>!e||e==="#"?"":e.startsWith("#")?e:"#"+e;function HP(e){return e!=null&&typeof e.status=="number"&&typeof e.statusText=="string"&&typeof e.internal=="boolean"&&"data"in e}const M5=["post","put","patch","delete"];new Set(M5);const qP=["get",...M5];new Set(qP);/**
 * React Router v6.27.0
 *
 * Copyright (c) Remix Software Inc.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE.md file in the root directory of this source tree.
 *
 * @license MIT
 */function $c(){return $c=Object.assign?Object.assign.bind():function(e){for(var t=1;t<arguments.length;t++){var n=arguments[t];for(var r in n)Object.prototype.hasOwnProperty.call(n,r)&&(e[r]=n[r])}return e},$c.apply(this,arguments)}const C1=T.createContext(null),GP=T.createContext(null),X0=T.createContext(null),Q0=T.createContext(null),ys=T.createContext({outlet:null,matches:[],isDataRoute:!1}),R5=T.createContext(null);function Z0(){return T.useContext(Q0)!=null}function J0(){return Z0()||zt(!1),T.useContext(Q0).location}function O5(e){T.useContext(X0).static||T.useLayoutEffect(e)}function em(){let{isDataRoute:e}=T.useContext(ys);return e?sI():WP()}function WP(){Z0()||zt(!1);let e=T.useContext(C1),{basename:t,future:n,navigator:r}=T.useContext(X0),{matches:i}=T.useContext(ys),{pathname:a}=J0(),s=JSON.stringify(zP(i,n.v7_relativeSplatPath)),o=T.useRef(!1);return O5(()=>{o.current=!0}),T.useCallback(function(c,u){if(u===void 0&&(u={}),!o.current)return;if(typeof c=="number"){r.go(c);return}let d=BP(c,JSON.parse(s),a,u.relative==="path");e==null&&t!=="/"&&(d.pathname=d.pathname==="/"?t:os([t,d.pathname])),(u.replace?r.replace:r.push)(d,u.state,u)},[t,r,s,a,e])}function KP(){let{matches:e}=T.useContext(ys),t=e[e.length-1];return t?t.params:{}}function YP(e,t){return XP(e,t)}function XP(e,t,n,r){Z0()||zt(!1);let{navigator:i}=T.useContext(X0),{matches:a}=T.useContext(ys),s=a[a.length-1],o=s?s.params:{};s&&s.pathname;let l=s?s.pathnameBase:"/";s&&s.route;let c=J0(),u;if(t){var d;let w=typeof t=="string"?Vo(t):t;l==="/"||(d=w.pathname)!=null&&d.startsWith(l)||zt(!1),u=w}else u=c;let m=u.pathname||"/",p=m;if(l!=="/"){let w=l.replace(/^\//,"").split("/");p="/"+m.replace(/^\//,"").split("/").slice(w.length).join("/")}let x=_P(e,{pathname:p}),g=tI(x&&x.map(w=>Object.assign({},w,{params:Object.assign({},o,w.params),pathname:os([l,i.encodeLocation?i.encodeLocation(w.pathname).pathname:w.pathname]),pathnameBase:w.pathnameBase==="/"?l:os([l,i.encodeLocation?i.encodeLocation(w.pathnameBase).pathname:w.pathnameBase])})),a,n,r);return t&&g?T.createElement(Q0.Provider,{value:{location:$c({pathname:"/",search:"",hash:"",state:null,key:"default"},u),navigationType:da.Pop}},g):g}function QP(){let e=aI(),t=HP(e)?e.status+" "+e.statusText:e instanceof Error?e.message:JSON.stringify(e),n=e instanceof Error?e.stack:null,i={padding:"0.5rem",backgroundColor:"rgba(200,200,200, 0.5)"};return T.createElement(T.Fragment,null,T.createElement("h2",null,"Unexpected Application Error!"),T.createElement("h3",{style:{fontStyle:"italic"}},t),n?T.createElement("pre",{style:i},n):null,null)}const ZP=T.createElement(QP,null);class JP extends T.Component{constructor(t){super(t),this.state={location:t.location,revalidation:t.revalidation,error:t.error}}static getDerivedStateFromError(t){return{error:t}}static getDerivedStateFromProps(t,n){return n.location!==t.location||n.revalidation!=="idle"&&t.revalidation==="idle"?{error:t.error,location:t.location,revalidation:t.revalidation}:{error:t.error!==void 0?t.error:n.error,location:n.location,revalidation:t.revalidation||n.revalidation}}componentDidCatch(t,n){console.error("React Router caught the following error during render",t,n)}render(){return this.state.error!==void 0?T.createElement(ys.Provider,{value:this.props.routeContext},T.createElement(R5.Provider,{value:this.state.error,children:this.props.component})):this.props.children}}function eI(e){let{routeContext:t,match:n,children:r}=e,i=T.useContext(C1);return i&&i.static&&i.staticContext&&(n.route.errorElement||n.route.ErrorBoundary)&&(i.staticContext._deepestRenderedBoundaryId=n.route.id),T.createElement(ys.Provider,{value:t},r)}function tI(e,t,n,r){var i;if(t===void 0&&(t=[]),n===void 0&&(n=null),r===void 0&&(r=null),e==null){var a;if(!n)return null;if(n.errors)e=n.matches;else if((a=r)!=null&&a.v7_partialHydration&&t.length===0&&!n.initialized&&n.matches.length>0)e=n.matches;else return null}let s=e,o=(i=n)==null?void 0:i.errors;if(o!=null){let u=s.findIndex(d=>d.route.id&&(o==null?void 0:o[d.route.id])!==void 0);u>=0||zt(!1),s=s.slice(0,Math.min(s.length,u+1))}let l=!1,c=-1;if(n&&r&&r.v7_partialHydration)for(let u=0;u<s.length;u++){let d=s[u];if((d.route.HydrateFallback||d.route.hydrateFallbackElement)&&(c=u),d.route.id){let{loaderData:m,errors:p}=n,x=d.route.loader&&m[d.route.id]===void 0&&(!p||p[d.route.id]===void 0);if(d.route.lazy||x){l=!0,c>=0?s=s.slice(0,c+1):s=[s[0]];break}}}return s.reduceRight((u,d,m)=>{let p,x=!1,g=null,w=null;n&&(p=o&&d.route.id?o[d.route.id]:void 0,g=d.route.errorElement||ZP,l&&(c<0&&m===0?(x=!0,w=null):c===m&&(x=!0,w=d.route.hydrateFallbackElement||null)));let v=t.concat(s.slice(0,m+1)),$=()=>{let _;return p?_=g:x?_=w:d.route.Component?_=T.createElement(d.route.Component,null):d.route.element?_=d.route.element:_=u,T.createElement(eI,{match:d,routeContext:{outlet:u,matches:v,isDataRoute:n!=null},children:_})};return n&&(d.route.ErrorBoundary||d.route.errorElement||m===0)?T.createElement(JP,{location:n.location,revalidation:n.revalidation,component:g,error:p,children:$(),routeContext:{outlet:null,matches:v,isDataRoute:!0}}):$()},null)}var z5=function(e){return e.UseBlocker="useBlocker",e.UseRevalidator="useRevalidator",e.UseNavigateStable="useNavigate",e}(z5||{}),n0=function(e){return e.UseBlocker="useBlocker",e.UseLoaderData="useLoaderData",e.UseActionData="useActionData",e.UseRouteError="useRouteError",e.UseNavigation="useNavigation",e.UseRouteLoaderData="useRouteLoaderData",e.UseMatches="useMatches",e.UseRevalidator="useRevalidator",e.UseNavigateStable="useNavigate",e.UseRouteId="useRouteId",e}(n0||{});function nI(e){let t=T.useContext(C1);return t||zt(!1),t}function rI(e){let t=T.useContext(GP);return t||zt(!1),t}function iI(e){let t=T.useContext(ys);return t||zt(!1),t}function B5(e){let t=iI(),n=t.matches[t.matches.length-1];return n.route.id||zt(!1),n.route.id}function aI(){var e;let t=T.useContext(R5),n=rI(n0.UseRouteError),r=B5(n0.UseRouteError);return t!==void 0?t:(e=n.errors)==null?void 0:e[r]}function sI(){let{router:e}=nI(z5.UseNavigateStable),t=B5(n0.UseNavigateStable),n=T.useRef(!1);return O5(()=>{n.current=!0}),T.useCallback(function(i,a){a===void 0&&(a={}),n.current&&(typeof i=="number"?e.navigate(i):e.navigate(i,$c({fromRouteId:t},a)))},[e,t])}function Us(e){zt(!1)}function oI(e){let{basename:t="/",children:n=null,location:r,navigationType:i=da.Pop,navigator:a,static:s=!1,future:o}=e;Z0()&&zt(!1);let l=t.replace(/^\/*/,"/"),c=T.useMemo(()=>({basename:l,navigator:a,static:s,future:$c({v7_relativeSplatPath:!1},o)}),[l,o,a,s]);typeof r=="string"&&(r=Vo(r));let{pathname:u="/",search:d="",hash:m="",state:p=null,key:x="default"}=r,g=T.useMemo(()=>{let w=D5(u,l);return w==null?null:{location:{pathname:w,search:d,hash:m,state:p,key:x},navigationType:i}},[l,u,d,m,p,x,i]);return g==null?null:T.createElement(X0.Provider,{value:c},T.createElement(Q0.Provider,{children:n,value:g}))}function lI(e){let{children:t,location:n}=e;return YP(pf(t),n)}new Promise(()=>{});function pf(e,t){t===void 0&&(t=[]);let n=[];return T.Children.forEach(e,(r,i)=>{if(!T.isValidElement(r))return;let a=[...t,i];if(r.type===T.Fragment){n.push.apply(n,pf(r.props.children,a));return}r.type!==Us&&zt(!1),!r.props.index||!r.props.children||zt(!1);let s={id:r.props.id||a.join("-"),caseSensitive:r.props.caseSensitive,element:r.props.element,Component:r.props.Component,index:r.props.index,path:r.props.path,loader:r.props.loader,action:r.props.action,errorElement:r.props.errorElement,ErrorBoundary:r.props.ErrorBoundary,hasErrorBoundary:r.props.ErrorBoundary!=null||r.props.errorElement!=null,shouldRevalidate:r.props.shouldRevalidate,handle:r.props.handle,lazy:r.props.lazy};r.props.children&&(s.children=pf(r.props.children,a)),n.push(s)}),n}/**
 * React Router DOM v6.27.0
 *
 * Copyright (c) Remix Software Inc.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE.md file in the root directory of this source tree.
 *
 * @license MIT
 */const cI="6";try{window.__reactRouterVersion=cI}catch{}const uI="startTransition",s4=Z$[uI];function dI(e){let{basename:t,children:n,future:r,window:i}=e,a=T.useRef();a.current==null&&(a.current=xP({window:i,v5Compat:!0}));let s=a.current,[o,l]=T.useState({action:s.action,location:s.location}),{v7_startTransition:c}=r||{},u=T.useCallback(d=>{c&&s4?s4(()=>l(d)):l(d)},[l,c]);return T.useLayoutEffect(()=>s.listen(u),[s,u]),T.createElement(oI,{basename:t,children:n,location:o.location,navigationType:o.action,navigator:s,future:r})}var o4;(function(e){e.UseScrollRestoration="useScrollRestoration",e.UseSubmit="useSubmit",e.UseSubmitFetcher="useSubmitFetcher",e.UseFetcher="useFetcher",e.useViewTransitionState="useViewTransitionState"})(o4||(o4={}));var l4;(function(e){e.UseFetcher="useFetcher",e.UseFetchers="useFetchers",e.UseScrollRestoration="useScrollRestoration"})(l4||(l4={}));const F5=(e=.1)=>{const[t,n]=T.useState(!1),r=T.useRef(null);return T.useEffect(()=>{const i=new IntersectionObserver(([a])=>{a.isIntersecting&&n(!0)},{threshold:e});return r.current&&i.observe(r.current),()=>i.disconnect()},[e]),{ref:r,isVisible:t}},mI=()=>{const[e,t]=T.useState(0);return T.useEffect(()=>{const n=()=>t(window.scrollY);return window.addEventListener("scroll",n),()=>window.removeEventListener("scroll",n)},[]),e},j5=p1("inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg]:size-4 [&_svg]:shrink-0",{variants:{variant:{default:"bg-primary text-primary-foreground hover:bg-primary/90",destructive:"bg-destructive text-destructive-foreground hover:bg-destructive/90",outline:"border border-input bg-background hover:bg-accent hover:text-accent-foreground",secondary:"bg-secondary text-secondary-foreground hover:bg-secondary/80",ghost:"hover:bg-accent hover:text-accent-foreground",link:"text-primary underline-offset-4 hover:underline"},size:{default:"h-10 px-4 py-2",sm:"h-9 rounded-md px-3",lg:"h-11 rounded-md px-8",icon:"h-10 w-10"}},defaultVariants:{variant:"default",size:"default"}}),sr=T.forwardRef(({className:e,variant:t,size:n,asChild:r=!1,...i},a)=>{const s=r?Co:"button";return h.jsx(s,{className:Ue(j5({variant:t,size:n,className:e})),ref:a,...i})});sr.displayName="Button";const hI=()=>{const t=mI()*.3,n=["AI RESEARCH","AI ENGINEERING","BE DEVELOPER","MLOPS / AIOPS","DEEP LEARNING","COMPUTER VISION","NATURAL LANGUAGE","DEEP LEARNING","MACHINE LEARNING","VIDEO PROCESSING","AUDIO PROCESSING"];return h.jsxs("section",{className:"relative min-h-screen flex items-center justify-center overflow-hidden bg-background",children:[h.jsx("style",{children:`
        @keyframes float-up { 0%,100%{transform:translateY(0)} 50%{transform:translateY(-10px)} }
        .hero-float { animation: float-up 6s ease-in-out infinite; }

        /* Glass text writing effect */
        @keyframes write-text {
          0% {
            clip-path: inset(0 100% 0 0);
          }
          100% {
            clip-path: inset(0 0 0 0);
          }
        }
        
        .glass-welcome {
          font-family: 'Dancing Script', cursive;
          color: hsl(var(--foreground));
          filter: drop-shadow(0 0 20px hsl(var(--foreground)/0.3));
          animation: write-text 2s ease-out forwards;
        }

        /* -------- Text Styling -------- */
        .text-line{
          position:relative;
          font-size:5rem;
          font-weight:900;
          letter-spacing:.05em;
          line-height:0.8;
          padding:1rem 0;              /* 좌우 패딩은 marquee가 담당 */
          cursor:default;
          user-select:none;
          white-space:nowrap;
          overflow:hidden;              /* 중요: 트랙이 흘러나가지 않도록 */
        }

        /* 홀수 줄 (채워진 텍스트) */
        .text-line.filled {
          color: hsl(var(--foreground));
          -webkit-text-fill-color: hsl(var(--foreground));
          -webkit-text-stroke: 1.5px hsl(var(--foreground));
          text-stroke: 1.5px hsl(var(--foreground));
        }

        /* 짝수 줄 (윤곽선만 더 진하게) */
        .text-line.outlined {
          color: transparent;
          -webkit-text-fill-color: transparent;
          -webkit-text-stroke: 2px hsl(var(--foreground));
          text-stroke: 2px hsl(var(--foreground));
        }

        .text-line.left-align{text-align:left}
        .text-line.right-align{text-align:right}

        /* --- 무한 마퀴 공통 --- */
        .marquee {
          position: relative;
          width: 100%;
          overflow: hidden;
        }
        .marquee-track {
          display: flex;
          width: max-content;
          will-change: transform;
          gap: 2rem;                 /* 복제된 아이템 사이 간격 */
          padding: 0 2rem;           /* 좌우 여백 */
          animation: scroll-left var(--speed, 24s) linear infinite;
        }
        .marquee.right .marquee-track {
          animation-name: scroll-right;
        }
        .marquee-item {
          display: inline-block;
          white-space: nowrap;
        }

        @keyframes scroll-left {
          0%   { transform: translateX(0); }
          100% { transform: translateX(-30%); }  /* 두 배로 복제했으므로 -50%면 완전 무한 */
        }
        @keyframes scroll-right {
          0%   { transform: translateX(-30%); }
          100% { transform: translateX(0); }
        }

        @media (max-width:768px){
          .text-line{font-size:2.5rem;padding:.5rem 0}
        }

        @media (prefers-reduced-motion: reduce) {
          .hero-float, .scroll-hint, .marquee-track { animation: none; }
        }

        .hero-content{ margin-top:0 }
        @media (max-height:750px){ .hero-content{margin-top:2rem} }
        @media (max-height:650px){ .hero-content{margin-top:4rem} }
        @media (max-height:550px){ .hero-content{margin-top:6rem} }

        @keyframes scroll-bounce { 0%,100%{transform:translateY(0);opacity:.7} 50%{transform:translateY(6px);opacity:1} }
        .scroll-hint { animation: scroll-bounce 1.8s ease-in-out infinite; }
      `}),h.jsx("div",{className:"absolute inset-0 overflow-hidden transition-transform duration-1000 ease-out",style:{transform:`translateY(${t}px)`},children:h.jsx("div",{className:"absolute inset-0 flex flex-col justify-center py-8",children:n.map((r,i)=>{const a=i%2===0,s=25+i%5*2;return h.jsx("div",{className:`text-line ${a?"filled left-align":"outlined right-align"}`,children:h.jsx("div",{className:`marquee ${a?"left":"right"}`,style:{"--speed":`${s}s`},children:h.jsx("div",{className:"marquee-track",children:Array.from({length:8}).map((o,l)=>h.jsx("span",{className:"marquee-item","aria-hidden":l!==0,children:r},l))})})},i)})})}),h.jsx("div",{className:"absolute inset-0 bg-gradient-to-br from-background/70 via-background/50 to-background/70"}),h.jsxs("div",{className:"relative z-10 text-center px-6 max-w-5xl mx-auto hero-content",children:[h.jsxs("div",{className:"animate-fade-in",children:[h.jsx("div",{className:"hero-float inline-block mb-8",style:{animationDelay:"0.3s"},children:h.jsx("h2",{className:"text-8xl sm:text-9xl font-bold glass-welcome",children:"Welcome"})}),h.jsxs("div",{className:"flex flex-col sm:flex-row gap-4 justify-center items-center mb-12 hero-float",style:{animationDelay:"0.6s"},children:[h.jsx(sr,{asChild:!0,size:"lg",className:`h-14 px-8 sm:px-10 text-lg font-semibold rounded-2xl gap-3
                         bg-foreground text-background hover:bg-foreground/90
                         focus-visible:ring-2 focus-visible:ring-offset-2 focus-visible:ring-foreground`,"aria-label":"Explore my work",children:h.jsxs("a",{href:"#about",children:[h.jsx(ZN,{className:"w-6 h-6","aria-hidden":"true"}),"Explore My Work",h.jsx(F0,{className:"w-6 h-6 ml-1","aria-hidden":"true"})]})}),h.jsx(sr,{asChild:!0,size:"lg",variant:"outline",className:`h-14 px-8 sm:px-10 text-lg font-semibold rounded-2xl gap-3
                         border-border hover:bg-accent hover:text-accent-foreground
                         focus-visible:ring-2 focus-visible:ring-offset-2 focus-visible:ring-border`,"aria-label":"View publications",children:h.jsxs("a",{href:"#publications",children:[h.jsx(rC,{className:"w-6 h-6","aria-hidden":"true"}),"View Publications"]})})]})]}),h.jsx("button",{type:"button",onClick:()=>{const r=document.querySelector("#about");r==null||r.scrollIntoView({behavior:"smooth",block:"start"})},className:`group scroll-hint inline-flex items-center justify-center rounded-full border border-border/60
                     bg-background/60 backdrop-blur px-3 py-3 mx-auto
                     hover:border-border hover:bg-background/80 transition-colors`,"aria-label":"Scroll to About",children:h.jsx(QN,{className:"w-6 h-6 text-foreground/70 group-hover:text-foreground"})})]})]})},A1=T.createContext({});function L1(e){const t=T.useRef(null);return t.current===null&&(t.current=e()),t.current}const P1=typeof window<"u",V5=P1?T.useLayoutEffect:T.useEffect,tm=T.createContext(null);function I1(e,t){e.indexOf(t)===-1&&e.push(t)}function U1(e,t){const n=e.indexOf(t);n>-1&&e.splice(n,1)}const Di=(e,t,n)=>n>t?t:n<e?e:n;let r0=()=>{};const Mi={},H5=e=>/^-?(?:\d+(?:\.\d+)?|\.\d+)$/u.test(e);function q5(e){return typeof e=="object"&&e!==null}const G5=e=>/^0[^.\s]+$/u.test(e);function D1(e){let t;return()=>(t===void 0&&(t=e()),t)}const ur=e=>e,pI=(e,t)=>n=>t(e(n)),qc=(...e)=>e.reduce(pI),yc=(e,t,n)=>{const r=t-e;return r===0?1:(n-e)/r};class M1{constructor(){this.subscriptions=[]}add(t){return I1(this.subscriptions,t),()=>U1(this.subscriptions,t)}notify(t,n,r){const i=this.subscriptions.length;if(i)if(i===1)this.subscriptions[0](t,n,r);else for(let a=0;a<i;a++){const s=this.subscriptions[a];s&&s(t,n,r)}}getSize(){return this.subscriptions.length}clear(){this.subscriptions.length=0}}const ii=e=>e*1e3,ai=e=>e/1e3;function W5(e,t){return t?e*(1e3/t):0}const K5=(e,t,n)=>(((1-3*n+3*t)*e+(3*n-6*t))*e+3*t)*e,fI=1e-7,gI=12;function bI(e,t,n,r,i){let a,s,o=0;do s=t+(n-t)/2,a=K5(s,r,i)-e,a>0?n=s:t=s;while(Math.abs(a)>fI&&++o<gI);return s}function Gc(e,t,n,r){if(e===t&&n===r)return ur;const i=a=>bI(a,0,1,e,n);return a=>a===0||a===1?a:K5(i(a),t,r)}const Y5=e=>t=>t<=.5?e(2*t)/2:(2-e(2*(1-t)))/2,X5=e=>t=>1-e(1-t),Q5=Gc(.33,1.53,.69,.99),R1=X5(Q5),Z5=Y5(R1),J5=e=>(e*=2)<1?.5*R1(e):.5*(2-Math.pow(2,-10*(e-1))),O1=e=>1-Math.sin(Math.acos(e)),e9=X5(O1),t9=Y5(O1),vI=Gc(.42,0,1,1),xI=Gc(0,0,.58,1),n9=Gc(.42,0,.58,1),$I=e=>Array.isArray(e)&&typeof e[0]!="number",r9=e=>Array.isArray(e)&&typeof e[0]=="number",c4={linear:ur,easeIn:vI,easeInOut:n9,easeOut:xI,circIn:O1,circInOut:t9,circOut:e9,backIn:R1,backInOut:Z5,backOut:Q5,anticipate:J5},yI=e=>typeof e=="string",u4=e=>{if(r9(e)){r0(e.length===4);const[t,n,r,i]=e;return Gc(t,n,r,i)}else if(yI(e))return r0(c4[e]!==void 0),c4[e];return e},Lu=["setup","read","resolveKeyframes","preUpdate","update","preRender","render","postRender"],d4={value:null,addProjectionMetrics:null};function _I(e,t){let n=new Set,r=new Set,i=!1,a=!1;const s=new WeakSet;let o={delta:0,timestamp:0,isProcessing:!1},l=0;function c(d){s.has(d)&&(u.schedule(d),e()),l++,d(o)}const u={schedule:(d,m=!1,p=!1)=>{const g=p&&i?n:r;return m&&s.add(d),g.has(d)||g.add(d),d},cancel:d=>{r.delete(d),s.delete(d)},process:d=>{if(o=d,i){a=!0;return}i=!0,[n,r]=[r,n],n.forEach(c),t&&d4.value&&d4.value.frameloop[t].push(l),l=0,n.clear(),i=!1,a&&(a=!1,u.process(d))}};return u}const wI=40;function i9(e,t){let n=!1,r=!0;const i={delta:0,timestamp:0,isProcessing:!1},a=()=>n=!0,s=Lu.reduce((_,C)=>(_[C]=_I(a,t?C:void 0),_),{}),{setup:o,read:l,resolveKeyframes:c,preUpdate:u,update:d,preRender:m,render:p,postRender:x}=s,g=()=>{const _=Mi.useManualTiming?i.timestamp:performance.now();n=!1,Mi.useManualTiming||(i.delta=r?1e3/60:Math.max(Math.min(_-i.timestamp,wI),1)),i.timestamp=_,i.isProcessing=!0,o.process(i),l.process(i),c.process(i),u.process(i),d.process(i),m.process(i),p.process(i),x.process(i),i.isProcessing=!1,n&&t&&(r=!1,e(g))},w=()=>{n=!0,r=!0,i.isProcessing||e(g)};return{schedule:Lu.reduce((_,C)=>{const k=s[C];return _[C]=(S,L=!1,U=!1)=>(n||w(),k.schedule(S,L,U)),_},{}),cancel:_=>{for(let C=0;C<Lu.length;C++)s[Lu[C]].cancel(_)},state:i,steps:s}}const{schedule:xt,cancel:Ca,state:Wt,steps:uh}=i9(typeof requestAnimationFrame<"u"?requestAnimationFrame:ur,!0);let pd;function TI(){pd=void 0}const Sn={now:()=>(pd===void 0&&Sn.set(Wt.isProcessing||Mi.useManualTiming?Wt.timestamp:performance.now()),pd),set:e=>{pd=e,queueMicrotask(TI)}},a9=e=>t=>typeof t=="string"&&t.startsWith(e),z1=a9("--"),EI=a9("var(--"),B1=e=>EI(e)?kI.test(e.split("/*")[0].trim()):!1,kI=/var\(--(?:[\w-]+\s*|[\w-]+\s*,(?:\s*[^)(\s]|\s*\((?:[^)(]|\([^)(]*\))*\))+\s*)\)$/iu,Ho={test:e=>typeof e=="number",parse:parseFloat,transform:e=>e},_c={...Ho,transform:e=>Di(0,1,e)},Pu={...Ho,default:1},Rl=e=>Math.round(e*1e5)/1e5,F1=/-?(?:\d+(?:\.\d+)?|\.\d+)/gu;function SI(e){return e==null}const NI=/^(?:#[\da-f]{3,8}|(?:rgb|hsl)a?\((?:-?[\d.]+%?[,\s]+){2}-?[\d.]+%?\s*(?:[,/]\s*)?(?:\b\d+(?:\.\d+)?|\.\d+)?%?\))$/iu,j1=(e,t)=>n=>!!(typeof n=="string"&&NI.test(n)&&n.startsWith(e)||t&&!SI(n)&&Object.prototype.hasOwnProperty.call(n,t)),s9=(e,t,n)=>r=>{if(typeof r!="string")return r;const[i,a,s,o]=r.match(F1);return{[e]:parseFloat(i),[t]:parseFloat(a),[n]:parseFloat(s),alpha:o!==void 0?parseFloat(o):1}},CI=e=>Di(0,255,e),dh={...Ho,transform:e=>Math.round(CI(e))},Ja={test:j1("rgb","red"),parse:s9("red","green","blue"),transform:({red:e,green:t,blue:n,alpha:r=1})=>"rgba("+dh.transform(e)+", "+dh.transform(t)+", "+dh.transform(n)+", "+Rl(_c.transform(r))+")"};function AI(e){let t="",n="",r="",i="";return e.length>5?(t=e.substring(1,3),n=e.substring(3,5),r=e.substring(5,7),i=e.substring(7,9)):(t=e.substring(1,2),n=e.substring(2,3),r=e.substring(3,4),i=e.substring(4,5),t+=t,n+=n,r+=r,i+=i),{red:parseInt(t,16),green:parseInt(n,16),blue:parseInt(r,16),alpha:i?parseInt(i,16)/255:1}}const ff={test:j1("#"),parse:AI,transform:Ja.transform},Wc=e=>({test:t=>typeof t=="string"&&t.endsWith(e)&&t.split(" ").length===1,parse:parseFloat,transform:t=>`${t}${e}`}),Xi=Wc("deg"),si=Wc("%"),Se=Wc("px"),LI=Wc("vh"),PI=Wc("vw"),m4={...si,parse:e=>si.parse(e)/100,transform:e=>si.transform(e*100)},Ws={test:j1("hsl","hue"),parse:s9("hue","saturation","lightness"),transform:({hue:e,saturation:t,lightness:n,alpha:r=1})=>"hsla("+Math.round(e)+", "+si.transform(Rl(t))+", "+si.transform(Rl(n))+", "+Rl(_c.transform(r))+")"},tn={test:e=>Ja.test(e)||ff.test(e)||Ws.test(e),parse:e=>Ja.test(e)?Ja.parse(e):Ws.test(e)?Ws.parse(e):ff.parse(e),transform:e=>typeof e=="string"?e:e.hasOwnProperty("red")?Ja.transform(e):Ws.transform(e)},II=/(?:#[\da-f]{3,8}|(?:rgb|hsl)a?\((?:-?[\d.]+%?[,\s]+){2}-?[\d.]+%?\s*(?:[,/]\s*)?(?:\b\d+(?:\.\d+)?|\.\d+)?%?\))/giu;function UI(e){var t,n;return isNaN(e)&&typeof e=="string"&&(((t=e.match(F1))==null?void 0:t.length)||0)+(((n=e.match(II))==null?void 0:n.length)||0)>0}const o9="number",l9="color",DI="var",MI="var(",h4="${}",RI=/var\s*\(\s*--(?:[\w-]+\s*|[\w-]+\s*,(?:\s*[^)(\s]|\s*\((?:[^)(]|\([^)(]*\))*\))+\s*)\)|#[\da-f]{3,8}|(?:rgb|hsl)a?\((?:-?[\d.]+%?[,\s]+){2}-?[\d.]+%?\s*(?:[,/]\s*)?(?:\b\d+(?:\.\d+)?|\.\d+)?%?\)|-?(?:\d+(?:\.\d+)?|\.\d+)/giu;function wc(e){const t=e.toString(),n=[],r={color:[],number:[],var:[]},i=[];let a=0;const o=t.replace(RI,l=>(tn.test(l)?(r.color.push(a),i.push(l9),n.push(tn.parse(l))):l.startsWith(MI)?(r.var.push(a),i.push(DI),n.push(l)):(r.number.push(a),i.push(o9),n.push(parseFloat(l))),++a,h4)).split(h4);return{values:n,split:o,indexes:r,types:i}}function c9(e){return wc(e).values}function u9(e){const{split:t,types:n}=wc(e),r=t.length;return i=>{let a="";for(let s=0;s<r;s++)if(a+=t[s],i[s]!==void 0){const o=n[s];o===o9?a+=Rl(i[s]):o===l9?a+=tn.transform(i[s]):a+=i[s]}return a}}const OI=e=>typeof e=="number"?0:e;function zI(e){const t=c9(e);return u9(e)(t.map(OI))}const Aa={test:UI,parse:c9,createTransformer:u9,getAnimatableNone:zI};function mh(e,t,n){return n<0&&(n+=1),n>1&&(n-=1),n<1/6?e+(t-e)*6*n:n<1/2?t:n<2/3?e+(t-e)*(2/3-n)*6:e}function BI({hue:e,saturation:t,lightness:n,alpha:r}){e/=360,t/=100,n/=100;let i=0,a=0,s=0;if(!t)i=a=s=n;else{const o=n<.5?n*(1+t):n+t-n*t,l=2*n-o;i=mh(l,o,e+1/3),a=mh(l,o,e),s=mh(l,o,e-1/3)}return{red:Math.round(i*255),green:Math.round(a*255),blue:Math.round(s*255),alpha:r}}function i0(e,t){return n=>n>0?t:e}const bt=(e,t,n)=>e+(t-e)*n,hh=(e,t,n)=>{const r=e*e,i=n*(t*t-r)+r;return i<0?0:Math.sqrt(i)},FI=[ff,Ja,Ws],jI=e=>FI.find(t=>t.test(e));function p4(e){const t=jI(e);if(!t)return!1;let n=t.parse(e);return t===Ws&&(n=BI(n)),n}const f4=(e,t)=>{const n=p4(e),r=p4(t);if(!n||!r)return i0(e,t);const i={...n};return a=>(i.red=hh(n.red,r.red,a),i.green=hh(n.green,r.green,a),i.blue=hh(n.blue,r.blue,a),i.alpha=bt(n.alpha,r.alpha,a),Ja.transform(i))},gf=new Set(["none","hidden"]);function VI(e,t){return gf.has(e)?n=>n<=0?e:t:n=>n>=1?t:e}function HI(e,t){return n=>bt(e,t,n)}function V1(e){return typeof e=="number"?HI:typeof e=="string"?B1(e)?i0:tn.test(e)?f4:WI:Array.isArray(e)?d9:typeof e=="object"?tn.test(e)?f4:qI:i0}function d9(e,t){const n=[...e],r=n.length,i=e.map((a,s)=>V1(a)(a,t[s]));return a=>{for(let s=0;s<r;s++)n[s]=i[s](a);return n}}function qI(e,t){const n={...e,...t},r={};for(const i in n)e[i]!==void 0&&t[i]!==void 0&&(r[i]=V1(e[i])(e[i],t[i]));return i=>{for(const a in r)n[a]=r[a](i);return n}}function GI(e,t){const n=[],r={color:0,var:0,number:0};for(let i=0;i<t.values.length;i++){const a=t.types[i],s=e.indexes[a][r[a]],o=e.values[s]??0;n[i]=o,r[a]++}return n}const WI=(e,t)=>{const n=Aa.createTransformer(t),r=wc(e),i=wc(t);return r.indexes.var.length===i.indexes.var.length&&r.indexes.color.length===i.indexes.color.length&&r.indexes.number.length>=i.indexes.number.length?gf.has(e)&&!i.values.length||gf.has(t)&&!r.values.length?VI(e,t):qc(d9(GI(r,i),i.values),n):i0(e,t)};function m9(e,t,n){return typeof e=="number"&&typeof t=="number"&&typeof n=="number"?bt(e,t,n):V1(e)(e,t)}const KI=e=>{const t=({timestamp:n})=>e(n);return{start:(n=!0)=>xt.update(t,n),stop:()=>Ca(t),now:()=>Wt.isProcessing?Wt.timestamp:Sn.now()}},h9=(e,t,n=10)=>{let r="";const i=Math.max(Math.round(t/n),2);for(let a=0;a<i;a++)r+=e(a/(i-1))+", ";return`linear(${r.substring(0,r.length-2)})`},a0=2e4;function H1(e){let t=0;const n=50;let r=e.next(t);for(;!r.done&&t<a0;)t+=n,r=e.next(t);return t>=a0?1/0:t}function YI(e,t=100,n){const r=n({...e,keyframes:[0,t]}),i=Math.min(H1(r),a0);return{type:"keyframes",ease:a=>r.next(i*a).value/t,duration:ai(i)}}const XI=5;function p9(e,t,n){const r=Math.max(t-XI,0);return W5(n-e(r),t-r)}const Et={stiffness:100,damping:10,mass:1,velocity:0,duration:800,bounce:.3,visualDuration:.3,restSpeed:{granular:.01,default:2},restDelta:{granular:.005,default:.5},minDuration:.01,maxDuration:10,minDamping:.05,maxDamping:1},ph=.001;function QI({duration:e=Et.duration,bounce:t=Et.bounce,velocity:n=Et.velocity,mass:r=Et.mass}){let i,a,s=1-t;s=Di(Et.minDamping,Et.maxDamping,s),e=Di(Et.minDuration,Et.maxDuration,ai(e)),s<1?(i=c=>{const u=c*s,d=u*e,m=u-n,p=bf(c,s),x=Math.exp(-d);return ph-m/p*x},a=c=>{const d=c*s*e,m=d*n+n,p=Math.pow(s,2)*Math.pow(c,2)*e,x=Math.exp(-d),g=bf(Math.pow(c,2),s);return(-i(c)+ph>0?-1:1)*((m-p)*x)/g}):(i=c=>{const u=Math.exp(-c*e),d=(c-n)*e+1;return-ph+u*d},a=c=>{const u=Math.exp(-c*e),d=(n-c)*(e*e);return u*d});const o=5/e,l=JI(i,a,o);if(e=ii(e),isNaN(l))return{stiffness:Et.stiffness,damping:Et.damping,duration:e};{const c=Math.pow(l,2)*r;return{stiffness:c,damping:s*2*Math.sqrt(r*c),duration:e}}}const ZI=12;function JI(e,t,n){let r=n;for(let i=1;i<ZI;i++)r=r-e(r)/t(r);return r}function bf(e,t){return e*Math.sqrt(1-t*t)}const eU=["duration","bounce"],tU=["stiffness","damping","mass"];function g4(e,t){return t.some(n=>e[n]!==void 0)}function nU(e){let t={velocity:Et.velocity,stiffness:Et.stiffness,damping:Et.damping,mass:Et.mass,isResolvedFromDuration:!1,...e};if(!g4(e,tU)&&g4(e,eU))if(e.visualDuration){const n=e.visualDuration,r=2*Math.PI/(n*1.2),i=r*r,a=2*Di(.05,1,1-(e.bounce||0))*Math.sqrt(i);t={...t,mass:Et.mass,stiffness:i,damping:a}}else{const n=QI(e);t={...t,...n,mass:Et.mass},t.isResolvedFromDuration=!0}return t}function s0(e=Et.visualDuration,t=Et.bounce){const n=typeof e!="object"?{visualDuration:e,keyframes:[0,1],bounce:t}:e;let{restSpeed:r,restDelta:i}=n;const a=n.keyframes[0],s=n.keyframes[n.keyframes.length-1],o={done:!1,value:a},{stiffness:l,damping:c,mass:u,duration:d,velocity:m,isResolvedFromDuration:p}=nU({...n,velocity:-ai(n.velocity||0)}),x=m||0,g=c/(2*Math.sqrt(l*u)),w=s-a,v=ai(Math.sqrt(l/u)),$=Math.abs(w)<5;r||(r=$?Et.restSpeed.granular:Et.restSpeed.default),i||(i=$?Et.restDelta.granular:Et.restDelta.default);let _;if(g<1){const k=bf(v,g);_=S=>{const L=Math.exp(-g*v*S);return s-L*((x+g*v*w)/k*Math.sin(k*S)+w*Math.cos(k*S))}}else if(g===1)_=k=>s-Math.exp(-v*k)*(w+(x+v*w)*k);else{const k=v*Math.sqrt(g*g-1);_=S=>{const L=Math.exp(-g*v*S),U=Math.min(k*S,300);return s-L*((x+g*v*w)*Math.sinh(U)+k*w*Math.cosh(U))/k}}const C={calculatedDuration:p&&d||null,next:k=>{const S=_(k);if(p)o.done=k>=d;else{let L=k===0?x:0;g<1&&(L=k===0?ii(x):p9(_,k,S));const U=Math.abs(L)<=r,F=Math.abs(s-S)<=i;o.done=U&&F}return o.value=o.done?s:S,o},toString:()=>{const k=Math.min(H1(C),a0),S=h9(L=>C.next(k*L).value,k,30);return k+"ms "+S},toTransition:()=>{}};return C}s0.applyToOptions=e=>{const t=YI(e,100,s0);return e.ease=t.ease,e.duration=ii(t.duration),e.type="keyframes",e};function vf({keyframes:e,velocity:t=0,power:n=.8,timeConstant:r=325,bounceDamping:i=10,bounceStiffness:a=500,modifyTarget:s,min:o,max:l,restDelta:c=.5,restSpeed:u}){const d=e[0],m={done:!1,value:d},p=U=>o!==void 0&&U<o||l!==void 0&&U>l,x=U=>o===void 0?l:l===void 0||Math.abs(o-U)<Math.abs(l-U)?o:l;let g=n*t;const w=d+g,v=s===void 0?w:s(w);v!==w&&(g=v-d);const $=U=>-g*Math.exp(-U/r),_=U=>v+$(U),C=U=>{const F=$(U),q=_(U);m.done=Math.abs(F)<=c,m.value=m.done?v:q};let k,S;const L=U=>{p(m.value)&&(k=U,S=s0({keyframes:[m.value,x(m.value)],velocity:p9(_,U,m.value),damping:i,stiffness:a,restDelta:c,restSpeed:u}))};return L(0),{calculatedDuration:null,next:U=>{let F=!1;return!S&&k===void 0&&(F=!0,C(U),L(U)),k!==void 0&&U>=k?S.next(U-k):(!F&&C(U),m)}}}function rU(e,t,n){const r=[],i=n||Mi.mix||m9,a=e.length-1;for(let s=0;s<a;s++){let o=i(e[s],e[s+1]);if(t){const l=Array.isArray(t)?t[s]||ur:t;o=qc(l,o)}r.push(o)}return r}function iU(e,t,{clamp:n=!0,ease:r,mixer:i}={}){const a=e.length;if(r0(a===t.length),a===1)return()=>t[0];if(a===2&&t[0]===t[1])return()=>t[1];const s=e[0]===e[1];e[0]>e[a-1]&&(e=[...e].reverse(),t=[...t].reverse());const o=rU(t,r,i),l=o.length,c=u=>{if(s&&u<e[0])return t[0];let d=0;if(l>1)for(;d<e.length-2&&!(u<e[d+1]);d++);const m=yc(e[d],e[d+1],u);return o[d](m)};return n?u=>c(Di(e[0],e[a-1],u)):c}function aU(e,t){const n=e[e.length-1];for(let r=1;r<=t;r++){const i=yc(0,t,r);e.push(bt(n,1,i))}}function sU(e){const t=[0];return aU(t,e.length-1),t}function oU(e,t){return e.map(n=>n*t)}function lU(e,t){return e.map(()=>t||n9).splice(0,e.length-1)}function Ol({duration:e=300,keyframes:t,times:n,ease:r="easeInOut"}){const i=$I(r)?r.map(u4):u4(r),a={done:!1,value:t[0]},s=oU(n&&n.length===t.length?n:sU(t),e),o=iU(s,t,{ease:Array.isArray(i)?i:lU(t,i)});return{calculatedDuration:e,next:l=>(a.value=o(l),a.done=l>=e,a)}}const cU=e=>e!==null;function q1(e,{repeat:t,repeatType:n="loop"},r,i=1){const a=e.filter(cU),o=i<0||t&&n!=="loop"&&t%2===1?0:a.length-1;return!o||r===void 0?a[o]:r}const uU={decay:vf,inertia:vf,tween:Ol,keyframes:Ol,spring:s0};function f9(e){typeof e.type=="string"&&(e.type=uU[e.type])}class G1{constructor(){this.updateFinished()}get finished(){return this._finished}updateFinished(){this._finished=new Promise(t=>{this.resolve=t})}notifyFinished(){this.resolve()}then(t,n){return this.finished.then(t,n)}}const dU=e=>e/100;class W1 extends G1{constructor(t){super(),this.state="idle",this.startTime=null,this.isStopped=!1,this.currentTime=0,this.holdTime=null,this.playbackSpeed=1,this.stop=()=>{var r,i;const{motionValue:n}=this.options;n&&n.updatedAt!==Sn.now()&&this.tick(Sn.now()),this.isStopped=!0,this.state!=="idle"&&(this.teardown(),(i=(r=this.options).onStop)==null||i.call(r))},this.options=t,this.initAnimation(),this.play(),t.autoplay===!1&&this.pause()}initAnimation(){const{options:t}=this;f9(t);const{type:n=Ol,repeat:r=0,repeatDelay:i=0,repeatType:a,velocity:s=0}=t;let{keyframes:o}=t;const l=n||Ol;l!==Ol&&typeof o[0]!="number"&&(this.mixKeyframes=qc(dU,m9(o[0],o[1])),o=[0,100]);const c=l({...t,keyframes:o});a==="mirror"&&(this.mirroredGenerator=l({...t,keyframes:[...o].reverse(),velocity:-s})),c.calculatedDuration===null&&(c.calculatedDuration=H1(c));const{calculatedDuration:u}=c;this.calculatedDuration=u,this.resolvedDuration=u+i,this.totalDuration=this.resolvedDuration*(r+1)-i,this.generator=c}updateTime(t){const n=Math.round(t-this.startTime)*this.playbackSpeed;this.holdTime!==null?this.currentTime=this.holdTime:this.currentTime=n}tick(t,n=!1){const{generator:r,totalDuration:i,mixKeyframes:a,mirroredGenerator:s,resolvedDuration:o,calculatedDuration:l}=this;if(this.startTime===null)return r.next(0);const{delay:c=0,keyframes:u,repeat:d,repeatType:m,repeatDelay:p,type:x,onUpdate:g,finalKeyframe:w}=this.options;this.speed>0?this.startTime=Math.min(this.startTime,t):this.speed<0&&(this.startTime=Math.min(t-i/this.speed,this.startTime)),n?this.currentTime=t:this.updateTime(t);const v=this.currentTime-c*(this.playbackSpeed>=0?1:-1),$=this.playbackSpeed>=0?v<0:v>i;this.currentTime=Math.max(v,0),this.state==="finished"&&this.holdTime===null&&(this.currentTime=i);let _=this.currentTime,C=r;if(d){const U=Math.min(this.currentTime,i)/o;let F=Math.floor(U),q=U%1;!q&&U>=1&&(q=1),q===1&&F--,F=Math.min(F,d+1),!!(F%2)&&(m==="reverse"?(q=1-q,p&&(q-=p/o)):m==="mirror"&&(C=s)),_=Di(0,1,q)*o}const k=$?{done:!1,value:u[0]}:C.next(_);a&&(k.value=a(k.value));let{done:S}=k;!$&&l!==null&&(S=this.playbackSpeed>=0?this.currentTime>=i:this.currentTime<=0);const L=this.holdTime===null&&(this.state==="finished"||this.state==="running"&&S);return L&&x!==vf&&(k.value=q1(u,this.options,w,this.speed)),g&&g(k.value),L&&this.finish(),k}then(t,n){return this.finished.then(t,n)}get duration(){return ai(this.calculatedDuration)}get time(){return ai(this.currentTime)}set time(t){var n;t=ii(t),this.currentTime=t,this.startTime===null||this.holdTime!==null||this.playbackSpeed===0?this.holdTime=t:this.driver&&(this.startTime=this.driver.now()-t/this.playbackSpeed),(n=this.driver)==null||n.start(!1)}get speed(){return this.playbackSpeed}set speed(t){this.updateTime(Sn.now());const n=this.playbackSpeed!==t;this.playbackSpeed=t,n&&(this.time=ai(this.currentTime))}play(){var i,a;if(this.isStopped)return;const{driver:t=KI,startTime:n}=this.options;this.driver||(this.driver=t(s=>this.tick(s))),(a=(i=this.options).onPlay)==null||a.call(i);const r=this.driver.now();this.state==="finished"?(this.updateFinished(),this.startTime=r):this.holdTime!==null?this.startTime=r-this.holdTime:this.startTime||(this.startTime=n??r),this.state==="finished"&&this.speed<0&&(this.startTime+=this.calculatedDuration),this.holdTime=null,this.state="running",this.driver.start()}pause(){this.state="paused",this.updateTime(Sn.now()),this.holdTime=this.currentTime}complete(){this.state!=="running"&&this.play(),this.state="finished",this.holdTime=null}finish(){var t,n;this.notifyFinished(),this.teardown(),this.state="finished",(n=(t=this.options).onComplete)==null||n.call(t)}cancel(){var t,n;this.holdTime=null,this.startTime=0,this.tick(0),this.teardown(),(n=(t=this.options).onCancel)==null||n.call(t)}teardown(){this.state="idle",this.stopDriver(),this.startTime=this.holdTime=null}stopDriver(){this.driver&&(this.driver.stop(),this.driver=void 0)}sample(t){return this.startTime=0,this.tick(t,!0)}attachTimeline(t){var n;return this.options.allowFlatten&&(this.options.type="keyframes",this.options.ease="linear",this.initAnimation()),(n=this.driver)==null||n.stop(),t.observe(this)}}function mU(e){for(let t=1;t<e.length;t++)e[t]??(e[t]=e[t-1])}const es=e=>e*180/Math.PI,xf=e=>{const t=es(Math.atan2(e[1],e[0]));return $f(t)},hU={x:4,y:5,translateX:4,translateY:5,scaleX:0,scaleY:3,scale:e=>(Math.abs(e[0])+Math.abs(e[3]))/2,rotate:xf,rotateZ:xf,skewX:e=>es(Math.atan(e[1])),skewY:e=>es(Math.atan(e[2])),skew:e=>(Math.abs(e[1])+Math.abs(e[2]))/2},$f=e=>(e=e%360,e<0&&(e+=360),e),b4=xf,v4=e=>Math.sqrt(e[0]*e[0]+e[1]*e[1]),x4=e=>Math.sqrt(e[4]*e[4]+e[5]*e[5]),pU={x:12,y:13,z:14,translateX:12,translateY:13,translateZ:14,scaleX:v4,scaleY:x4,scale:e=>(v4(e)+x4(e))/2,rotateX:e=>$f(es(Math.atan2(e[6],e[5]))),rotateY:e=>$f(es(Math.atan2(-e[2],e[0]))),rotateZ:b4,rotate:b4,skewX:e=>es(Math.atan(e[4])),skewY:e=>es(Math.atan(e[1])),skew:e=>(Math.abs(e[1])+Math.abs(e[4]))/2};function yf(e){return e.includes("scale")?1:0}function _f(e,t){if(!e||e==="none")return yf(t);const n=e.match(/^matrix3d\(([-\d.e\s,]+)\)$/u);let r,i;if(n)r=pU,i=n;else{const o=e.match(/^matrix\(([-\d.e\s,]+)\)$/u);r=hU,i=o}if(!i)return yf(t);const a=r[t],s=i[1].split(",").map(gU);return typeof a=="function"?a(s):s[a]}const fU=(e,t)=>{const{transform:n="none"}=getComputedStyle(e);return _f(n,t)};function gU(e){return parseFloat(e.trim())}const qo=["transformPerspective","x","y","z","translateX","translateY","translateZ","scale","scaleX","scaleY","rotate","rotateX","rotateY","rotateZ","skew","skewX","skewY"],Go=new Set(qo),$4=e=>e===Ho||e===Se,bU=new Set(["x","y","z"]),vU=qo.filter(e=>!bU.has(e));function xU(e){const t=[];return vU.forEach(n=>{const r=e.getValue(n);r!==void 0&&(t.push([n,r.get()]),r.set(n.startsWith("scale")?1:0))}),t}const ls={width:({x:e},{paddingLeft:t="0",paddingRight:n="0"})=>e.max-e.min-parseFloat(t)-parseFloat(n),height:({y:e},{paddingTop:t="0",paddingBottom:n="0"})=>e.max-e.min-parseFloat(t)-parseFloat(n),top:(e,{top:t})=>parseFloat(t),left:(e,{left:t})=>parseFloat(t),bottom:({y:e},{top:t})=>parseFloat(t)+(e.max-e.min),right:({x:e},{left:t})=>parseFloat(t)+(e.max-e.min),x:(e,{transform:t})=>_f(t,"x"),y:(e,{transform:t})=>_f(t,"y")};ls.translateX=ls.x;ls.translateY=ls.y;const cs=new Set;let wf=!1,Tf=!1,Ef=!1;function g9(){if(Tf){const e=Array.from(cs).filter(r=>r.needsMeasurement),t=new Set(e.map(r=>r.element)),n=new Map;t.forEach(r=>{const i=xU(r);i.length&&(n.set(r,i),r.render())}),e.forEach(r=>r.measureInitialState()),t.forEach(r=>{r.render();const i=n.get(r);i&&i.forEach(([a,s])=>{var o;(o=r.getValue(a))==null||o.set(s)})}),e.forEach(r=>r.measureEndState()),e.forEach(r=>{r.suspendedScrollY!==void 0&&window.scrollTo(0,r.suspendedScrollY)})}Tf=!1,wf=!1,cs.forEach(e=>e.complete(Ef)),cs.clear()}function b9(){cs.forEach(e=>{e.readKeyframes(),e.needsMeasurement&&(Tf=!0)})}function $U(){Ef=!0,b9(),g9(),Ef=!1}class K1{constructor(t,n,r,i,a,s=!1){this.state="pending",this.isAsync=!1,this.needsMeasurement=!1,this.unresolvedKeyframes=[...t],this.onComplete=n,this.name=r,this.motionValue=i,this.element=a,this.isAsync=s}scheduleResolve(){this.state="scheduled",this.isAsync?(cs.add(this),wf||(wf=!0,xt.read(b9),xt.resolveKeyframes(g9))):(this.readKeyframes(),this.complete())}readKeyframes(){const{unresolvedKeyframes:t,name:n,element:r,motionValue:i}=this;if(t[0]===null){const a=i==null?void 0:i.get(),s=t[t.length-1];if(a!==void 0)t[0]=a;else if(r&&n){const o=r.readValue(n,s);o!=null&&(t[0]=o)}t[0]===void 0&&(t[0]=s),i&&a===void 0&&i.set(t[0])}mU(t)}setFinalKeyframe(){}measureInitialState(){}renderEndStyles(){}measureEndState(){}complete(t=!1){this.state="complete",this.onComplete(this.unresolvedKeyframes,this.finalKeyframe,t),cs.delete(this)}cancel(){this.state==="scheduled"&&(cs.delete(this),this.state="pending")}resume(){this.state==="pending"&&this.scheduleResolve()}}const yU=e=>e.startsWith("--");function _U(e,t,n){yU(t)?e.style.setProperty(t,n):e.style[t]=n}const wU=D1(()=>window.ScrollTimeline!==void 0),TU={};function EU(e,t){const n=D1(e);return()=>TU[t]??n()}const v9=EU(()=>{try{document.createElement("div").animate({opacity:0},{easing:"linear(0, 1)"})}catch{return!1}return!0},"linearEasing"),_l=([e,t,n,r])=>`cubic-bezier(${e}, ${t}, ${n}, ${r})`,y4={linear:"linear",ease:"ease",easeIn:"ease-in",easeOut:"ease-out",easeInOut:"ease-in-out",circIn:_l([0,.65,.55,1]),circOut:_l([.55,0,1,.45]),backIn:_l([.31,.01,.66,-.59]),backOut:_l([.33,1.53,.69,.99])};function x9(e,t){if(e)return typeof e=="function"?v9()?h9(e,t):"ease-out":r9(e)?_l(e):Array.isArray(e)?e.map(n=>x9(n,t)||y4.easeOut):y4[e]}function kU(e,t,n,{delay:r=0,duration:i=300,repeat:a=0,repeatType:s="loop",ease:o="easeOut",times:l}={},c=void 0){const u={[t]:n};l&&(u.offset=l);const d=x9(o,i);Array.isArray(d)&&(u.easing=d);const m={delay:r,duration:i,easing:Array.isArray(d)?"linear":d,fill:"both",iterations:a+1,direction:s==="reverse"?"alternate":"normal"};return c&&(m.pseudoElement=c),e.animate(u,m)}function $9(e){return typeof e=="function"&&"applyToOptions"in e}function SU({type:e,...t}){return $9(e)&&v9()?e.applyToOptions(t):(t.duration??(t.duration=300),t.ease??(t.ease="easeOut"),t)}class NU extends G1{constructor(t){if(super(),this.finishedTime=null,this.isStopped=!1,!t)return;const{element:n,name:r,keyframes:i,pseudoElement:a,allowFlatten:s=!1,finalKeyframe:o,onComplete:l}=t;this.isPseudoElement=!!a,this.allowFlatten=s,this.options=t,r0(typeof t.type!="string");const c=SU(t);this.animation=kU(n,r,i,c,a),c.autoplay===!1&&this.animation.pause(),this.animation.onfinish=()=>{if(this.finishedTime=this.time,!a){const u=q1(i,this.options,o,this.speed);this.updateMotionValue?this.updateMotionValue(u):_U(n,r,u),this.animation.cancel()}l==null||l(),this.notifyFinished()}}play(){this.isStopped||(this.animation.play(),this.state==="finished"&&this.updateFinished())}pause(){this.animation.pause()}complete(){var t,n;(n=(t=this.animation).finish)==null||n.call(t)}cancel(){try{this.animation.cancel()}catch{}}stop(){if(this.isStopped)return;this.isStopped=!0;const{state:t}=this;t==="idle"||t==="finished"||(this.updateMotionValue?this.updateMotionValue():this.commitStyles(),this.isPseudoElement||this.cancel())}commitStyles(){var t,n;this.isPseudoElement||(n=(t=this.animation).commitStyles)==null||n.call(t)}get duration(){var n,r;const t=((r=(n=this.animation.effect)==null?void 0:n.getComputedTiming)==null?void 0:r.call(n).duration)||0;return ai(Number(t))}get time(){return ai(Number(this.animation.currentTime)||0)}set time(t){this.finishedTime=null,this.animation.currentTime=ii(t)}get speed(){return this.animation.playbackRate}set speed(t){t<0&&(this.finishedTime=null),this.animation.playbackRate=t}get state(){return this.finishedTime!==null?"finished":this.animation.playState}get startTime(){return Number(this.animation.startTime)}set startTime(t){this.animation.startTime=t}attachTimeline({timeline:t,observe:n}){var r;return this.allowFlatten&&((r=this.animation.effect)==null||r.updateTiming({easing:"linear"})),this.animation.onfinish=null,t&&wU()?(this.animation.timeline=t,ur):n(this)}}const y9={anticipate:J5,backInOut:Z5,circInOut:t9};function CU(e){return e in y9}function AU(e){typeof e.ease=="string"&&CU(e.ease)&&(e.ease=y9[e.ease])}const _4=10;class LU extends NU{constructor(t){AU(t),f9(t),super(t),t.startTime&&(this.startTime=t.startTime),this.options=t}updateMotionValue(t){const{motionValue:n,onUpdate:r,onComplete:i,element:a,...s}=this.options;if(!n)return;if(t!==void 0){n.set(t);return}const o=new W1({...s,autoplay:!1}),l=ii(this.finishedTime??this.time);n.setWithVelocity(o.sample(l-_4).value,o.sample(l).value,_4),o.stop()}}const w4=(e,t)=>t==="zIndex"?!1:!!(typeof e=="number"||Array.isArray(e)||typeof e=="string"&&(Aa.test(e)||e==="0")&&!e.startsWith("url("));function PU(e){const t=e[0];if(e.length===1)return!0;for(let n=0;n<e.length;n++)if(e[n]!==t)return!0}function IU(e,t,n,r){const i=e[0];if(i===null)return!1;if(t==="display"||t==="visibility")return!0;const a=e[e.length-1],s=w4(i,t),o=w4(a,t);return!s||!o?!1:PU(e)||(n==="spring"||$9(n))&&r}function Y1(e){return q5(e)&&"offsetHeight"in e}const UU=new Set(["opacity","clipPath","filter","transform"]),DU=D1(()=>Object.hasOwnProperty.call(Element.prototype,"animate"));function MU(e){var c;const{motionValue:t,name:n,repeatDelay:r,repeatType:i,damping:a,type:s}=e;if(!Y1((c=t==null?void 0:t.owner)==null?void 0:c.current))return!1;const{onUpdate:o,transformTemplate:l}=t.owner.getProps();return DU()&&n&&UU.has(n)&&(n!=="transform"||!l)&&!o&&!r&&i!=="mirror"&&a!==0&&s!=="inertia"}const RU=40;class OU extends G1{constructor({autoplay:t=!0,delay:n=0,type:r="keyframes",repeat:i=0,repeatDelay:a=0,repeatType:s="loop",keyframes:o,name:l,motionValue:c,element:u,...d}){var x;super(),this.stop=()=>{var g,w;this._animation&&(this._animation.stop(),(g=this.stopTimeline)==null||g.call(this)),(w=this.keyframeResolver)==null||w.cancel()},this.createdAt=Sn.now();const m={autoplay:t,delay:n,type:r,repeat:i,repeatDelay:a,repeatType:s,name:l,motionValue:c,element:u,...d},p=(u==null?void 0:u.KeyframeResolver)||K1;this.keyframeResolver=new p(o,(g,w,v)=>this.onKeyframesResolved(g,w,m,!v),l,c,u),(x=this.keyframeResolver)==null||x.scheduleResolve()}onKeyframesResolved(t,n,r,i){this.keyframeResolver=void 0;const{name:a,type:s,velocity:o,delay:l,isHandoff:c,onUpdate:u}=r;this.resolvedAt=Sn.now(),IU(t,a,s,o)||((Mi.instantAnimations||!l)&&(u==null||u(q1(t,r,n))),t[0]=t[t.length-1],r.duration=0,r.repeat=0);const m={startTime:i?this.resolvedAt?this.resolvedAt-this.createdAt>RU?this.resolvedAt:this.createdAt:this.createdAt:void 0,finalKeyframe:n,...r,keyframes:t},p=!c&&MU(m)?new LU({...m,element:m.motionValue.owner.current}):new W1(m);p.finished.then(()=>this.notifyFinished()).catch(ur),this.pendingTimeline&&(this.stopTimeline=p.attachTimeline(this.pendingTimeline),this.pendingTimeline=void 0),this._animation=p}get finished(){return this._animation?this.animation.finished:this._finished}then(t,n){return this.finished.finally(t).then(()=>{})}get animation(){var t;return this._animation||((t=this.keyframeResolver)==null||t.resume(),$U()),this._animation}get duration(){return this.animation.duration}get time(){return this.animation.time}set time(t){this.animation.time=t}get speed(){return this.animation.speed}get state(){return this.animation.state}set speed(t){this.animation.speed=t}get startTime(){return this.animation.startTime}attachTimeline(t){return this._animation?this.stopTimeline=this.animation.attachTimeline(t):this.pendingTimeline=t,()=>this.stop()}play(){this.animation.play()}pause(){this.animation.pause()}complete(){this.animation.complete()}cancel(){var t;this._animation&&this.animation.cancel(),(t=this.keyframeResolver)==null||t.cancel()}}const zU=/^var\(--(?:([\w-]+)|([\w-]+), ?([a-zA-Z\d ()%#.,-]+))\)/u;function BU(e){const t=zU.exec(e);if(!t)return[,];const[,n,r,i]=t;return[`--${n??r}`,i]}function _9(e,t,n=1){const[r,i]=BU(e);if(!r)return;const a=window.getComputedStyle(t).getPropertyValue(r);if(a){const s=a.trim();return H5(s)?parseFloat(s):s}return B1(i)?_9(i,t,n+1):i}function X1(e,t){return(e==null?void 0:e[t])??(e==null?void 0:e.default)??e}const w9=new Set(["width","height","top","left","right","bottom",...qo]),FU={test:e=>e==="auto",parse:e=>e},T9=e=>t=>t.test(e),E9=[Ho,Se,si,Xi,PI,LI,FU],T4=e=>E9.find(T9(e));function jU(e){return typeof e=="number"?e===0:e!==null?e==="none"||e==="0"||G5(e):!0}const VU=new Set(["brightness","contrast","saturate","opacity"]);function HU(e){const[t,n]=e.slice(0,-1).split("(");if(t==="drop-shadow")return e;const[r]=n.match(F1)||[];if(!r)return e;const i=n.replace(r,"");let a=VU.has(t)?1:0;return r!==n&&(a*=100),t+"("+a+i+")"}const qU=/\b([a-z-]*)\(.*?\)/gu,kf={...Aa,getAnimatableNone:e=>{const t=e.match(qU);return t?t.map(HU).join(" "):e}},E4={...Ho,transform:Math.round},GU={rotate:Xi,rotateX:Xi,rotateY:Xi,rotateZ:Xi,scale:Pu,scaleX:Pu,scaleY:Pu,scaleZ:Pu,skew:Xi,skewX:Xi,skewY:Xi,distance:Se,translateX:Se,translateY:Se,translateZ:Se,x:Se,y:Se,z:Se,perspective:Se,transformPerspective:Se,opacity:_c,originX:m4,originY:m4,originZ:Se},Q1={borderWidth:Se,borderTopWidth:Se,borderRightWidth:Se,borderBottomWidth:Se,borderLeftWidth:Se,borderRadius:Se,radius:Se,borderTopLeftRadius:Se,borderTopRightRadius:Se,borderBottomRightRadius:Se,borderBottomLeftRadius:Se,width:Se,maxWidth:Se,height:Se,maxHeight:Se,top:Se,right:Se,bottom:Se,left:Se,padding:Se,paddingTop:Se,paddingRight:Se,paddingBottom:Se,paddingLeft:Se,margin:Se,marginTop:Se,marginRight:Se,marginBottom:Se,marginLeft:Se,backgroundPositionX:Se,backgroundPositionY:Se,...GU,zIndex:E4,fillOpacity:_c,strokeOpacity:_c,numOctaves:E4},WU={...Q1,color:tn,backgroundColor:tn,outlineColor:tn,fill:tn,stroke:tn,borderColor:tn,borderTopColor:tn,borderRightColor:tn,borderBottomColor:tn,borderLeftColor:tn,filter:kf,WebkitFilter:kf},k9=e=>WU[e];function S9(e,t){let n=k9(e);return n!==kf&&(n=Aa),n.getAnimatableNone?n.getAnimatableNone(t):void 0}const KU=new Set(["auto","none","0"]);function YU(e,t,n){let r=0,i;for(;r<e.length&&!i;){const a=e[r];typeof a=="string"&&!KU.has(a)&&wc(a).values.length&&(i=e[r]),r++}if(i&&n)for(const a of t)e[a]=S9(n,i)}class XU extends K1{constructor(t,n,r,i,a){super(t,n,r,i,a,!0)}readKeyframes(){const{unresolvedKeyframes:t,element:n,name:r}=this;if(!n||!n.current)return;super.readKeyframes();for(let l=0;l<t.length;l++){let c=t[l];if(typeof c=="string"&&(c=c.trim(),B1(c))){const u=_9(c,n.current);u!==void 0&&(t[l]=u),l===t.length-1&&(this.finalKeyframe=c)}}if(this.resolveNoneKeyframes(),!w9.has(r)||t.length!==2)return;const[i,a]=t,s=T4(i),o=T4(a);if(s!==o)if($4(s)&&$4(o))for(let l=0;l<t.length;l++){const c=t[l];typeof c=="string"&&(t[l]=parseFloat(c))}else ls[r]&&(this.needsMeasurement=!0)}resolveNoneKeyframes(){const{unresolvedKeyframes:t,name:n}=this,r=[];for(let i=0;i<t.length;i++)(t[i]===null||jU(t[i]))&&r.push(i);r.length&&YU(t,r,n)}measureInitialState(){const{element:t,unresolvedKeyframes:n,name:r}=this;if(!t||!t.current)return;r==="height"&&(this.suspendedScrollY=window.pageYOffset),this.measuredOrigin=ls[r](t.measureViewportBox(),window.getComputedStyle(t.current)),n[0]=this.measuredOrigin;const i=n[n.length-1];i!==void 0&&t.getValue(r,i).jump(i,!1)}measureEndState(){var o;const{element:t,name:n,unresolvedKeyframes:r}=this;if(!t||!t.current)return;const i=t.getValue(n);i&&i.jump(this.measuredOrigin,!1);const a=r.length-1,s=r[a];r[a]=ls[n](t.measureViewportBox(),window.getComputedStyle(t.current)),s!==null&&this.finalKeyframe===void 0&&(this.finalKeyframe=s),(o=this.removedTransforms)!=null&&o.length&&this.removedTransforms.forEach(([l,c])=>{t.getValue(l).set(c)}),this.resolveNoneKeyframes()}}const k4=30,QU=e=>!isNaN(parseFloat(e));class ZU{constructor(t,n={}){this.canTrackVelocity=null,this.events={},this.updateAndNotify=(r,i=!0)=>{var s,o;const a=Sn.now();if(this.updatedAt!==a&&this.setPrevFrameValue(),this.prev=this.current,this.setCurrent(r),this.current!==this.prev&&((s=this.events.change)==null||s.notify(this.current),this.dependents))for(const l of this.dependents)l.dirty();i&&((o=this.events.renderRequest)==null||o.notify(this.current))},this.hasAnimated=!1,this.setCurrent(t),this.owner=n.owner}setCurrent(t){this.current=t,this.updatedAt=Sn.now(),this.canTrackVelocity===null&&t!==void 0&&(this.canTrackVelocity=QU(this.current))}setPrevFrameValue(t=this.current){this.prevFrameValue=t,this.prevUpdatedAt=this.updatedAt}onChange(t){return this.on("change",t)}on(t,n){this.events[t]||(this.events[t]=new M1);const r=this.events[t].add(n);return t==="change"?()=>{r(),xt.read(()=>{this.events.change.getSize()||this.stop()})}:r}clearListeners(){for(const t in this.events)this.events[t].clear()}attach(t,n){this.passiveEffect=t,this.stopPassiveEffect=n}set(t,n=!0){!n||!this.passiveEffect?this.updateAndNotify(t,n):this.passiveEffect(t,this.updateAndNotify)}setWithVelocity(t,n,r){this.set(n),this.prev=void 0,this.prevFrameValue=t,this.prevUpdatedAt=this.updatedAt-r}jump(t,n=!0){this.updateAndNotify(t),this.prev=t,this.prevUpdatedAt=this.prevFrameValue=void 0,n&&this.stop(),this.stopPassiveEffect&&this.stopPassiveEffect()}dirty(){var t;(t=this.events.change)==null||t.notify(this.current)}addDependent(t){this.dependents||(this.dependents=new Set),this.dependents.add(t)}removeDependent(t){this.dependents&&this.dependents.delete(t)}get(){return this.current}getPrevious(){return this.prev}getVelocity(){const t=Sn.now();if(!this.canTrackVelocity||this.prevFrameValue===void 0||t-this.updatedAt>k4)return 0;const n=Math.min(this.updatedAt-this.prevUpdatedAt,k4);return W5(parseFloat(this.current)-parseFloat(this.prevFrameValue),n)}start(t){return this.stop(),new Promise(n=>{this.hasAnimated=!0,this.animation=t(n),this.events.animationStart&&this.events.animationStart.notify()}).then(()=>{this.events.animationComplete&&this.events.animationComplete.notify(),this.clearAnimation()})}stop(){this.animation&&(this.animation.stop(),this.events.animationCancel&&this.events.animationCancel.notify()),this.clearAnimation()}isAnimating(){return!!this.animation}clearAnimation(){delete this.animation}destroy(){var t,n;(t=this.dependents)==null||t.clear(),(n=this.events.destroy)==null||n.notify(),this.clearListeners(),this.stop(),this.stopPassiveEffect&&this.stopPassiveEffect()}}function Po(e,t){return new ZU(e,t)}function JU(e,t,n){if(e instanceof EventTarget)return[e];if(typeof e=="string"){const i=document.querySelectorAll(e);return i?Array.from(i):[]}return Array.from(e)}const N9=(e,t)=>t&&typeof e=="number"?t.transform(e):e,{schedule:Z1,cancel:pZ}=i9(queueMicrotask,!1),xr={x:!1,y:!1};function C9(){return xr.x||xr.y}function eD(e){return e==="x"||e==="y"?xr[e]?null:(xr[e]=!0,()=>{xr[e]=!1}):xr.x||xr.y?null:(xr.x=xr.y=!0,()=>{xr.x=xr.y=!1})}function A9(e,t){const n=JU(e),r=new AbortController,i={passive:!0,...t,signal:r.signal};return[n,i,()=>r.abort()]}function S4(e){return!(e.pointerType==="touch"||C9())}function tD(e,t,n={}){const[r,i,a]=A9(e,n),s=o=>{if(!S4(o))return;const{target:l}=o,c=t(l,o);if(typeof c!="function"||!l)return;const u=d=>{S4(d)&&(c(d),l.removeEventListener("pointerleave",u))};l.addEventListener("pointerleave",u,i)};return r.forEach(o=>{o.addEventListener("pointerenter",s,i)}),a}const L9=(e,t)=>t?e===t?!0:L9(e,t.parentElement):!1,J1=e=>e.pointerType==="mouse"?typeof e.button!="number"||e.button<=0:e.isPrimary!==!1,nD=new Set(["BUTTON","INPUT","SELECT","TEXTAREA","A"]);function rD(e){return nD.has(e.tagName)||e.tabIndex!==-1}const fd=new WeakSet;function N4(e){return t=>{t.key==="Enter"&&e(t)}}function fh(e,t){e.dispatchEvent(new PointerEvent("pointer"+t,{isPrimary:!0,bubbles:!0}))}const iD=(e,t)=>{const n=e.currentTarget;if(!n)return;const r=N4(()=>{if(fd.has(n))return;fh(n,"down");const i=N4(()=>{fh(n,"up")}),a=()=>fh(n,"cancel");n.addEventListener("keyup",i,t),n.addEventListener("blur",a,t)});n.addEventListener("keydown",r,t),n.addEventListener("blur",()=>n.removeEventListener("keydown",r),t)};function C4(e){return J1(e)&&!C9()}function aD(e,t,n={}){const[r,i,a]=A9(e,n),s=o=>{const l=o.currentTarget;if(!C4(o))return;fd.add(l);const c=t(l,o),u=(p,x)=>{window.removeEventListener("pointerup",d),window.removeEventListener("pointercancel",m),fd.has(l)&&fd.delete(l),C4(p)&&typeof c=="function"&&c(p,{success:x})},d=p=>{u(p,l===window||l===document||n.useGlobalTarget||L9(l,p.target))},m=p=>{u(p,!1)};window.addEventListener("pointerup",d,i),window.addEventListener("pointercancel",m,i)};return r.forEach(o=>{(n.useGlobalTarget?window:o).addEventListener("pointerdown",s,i),Y1(o)&&(o.addEventListener("focus",c=>iD(c,i)),!rD(o)&&!o.hasAttribute("tabindex")&&(o.tabIndex=0))}),a}function P9(e){return q5(e)&&"ownerSVGElement"in e}function sD(e){return P9(e)&&e.tagName==="svg"}const rn=e=>!!(e&&e.getVelocity),oD=[...E9,tn,Aa],lD=e=>oD.find(T9(e)),e2=T.createContext({transformPagePoint:e=>e,isStatic:!1,reducedMotion:"never"});class cD extends T.Component{getSnapshotBeforeUpdate(t){const n=this.props.childRef.current;if(n&&t.isPresent&&!this.props.isPresent){const r=n.offsetParent,i=Y1(r)&&r.offsetWidth||0,a=this.props.sizeRef.current;a.height=n.offsetHeight||0,a.width=n.offsetWidth||0,a.top=n.offsetTop,a.left=n.offsetLeft,a.right=i-a.width-a.left}return null}componentDidUpdate(){}render(){return this.props.children}}function uD({children:e,isPresent:t,anchorX:n}){const r=T.useId(),i=T.useRef(null),a=T.useRef({width:0,height:0,top:0,left:0,right:0}),{nonce:s}=T.useContext(e2);return T.useInsertionEffect(()=>{const{width:o,height:l,top:c,left:u,right:d}=a.current;if(t||!i.current||!o||!l)return;const m=n==="left"?`left: ${u}`:`right: ${d}`;i.current.dataset.motionPopId=r;const p=document.createElement("style");return s&&(p.nonce=s),document.head.appendChild(p),p.sheet&&p.sheet.insertRule(`
          [data-motion-pop-id="${r}"] {
            position: absolute !important;
            width: ${o}px !important;
            height: ${l}px !important;
            ${m}px !important;
            top: ${c}px !important;
          }
        `),()=>{document.head.contains(p)&&document.head.removeChild(p)}},[t]),h.jsx(cD,{isPresent:t,childRef:i,sizeRef:a,children:T.cloneElement(e,{ref:i})})}const dD=({children:e,initial:t,isPresent:n,onExitComplete:r,custom:i,presenceAffectsLayout:a,mode:s,anchorX:o})=>{const l=L1(mD),c=T.useId();let u=!0,d=T.useMemo(()=>(u=!1,{id:c,initial:t,isPresent:n,custom:i,onExitComplete:m=>{l.set(m,!0);for(const p of l.values())if(!p)return;r&&r()},register:m=>(l.set(m,!1),()=>l.delete(m))}),[n,l,r]);return a&&u&&(d={...d}),T.useMemo(()=>{l.forEach((m,p)=>l.set(p,!1))},[n]),T.useEffect(()=>{!n&&!l.size&&r&&r()},[n]),s==="popLayout"&&(e=h.jsx(uD,{isPresent:n,anchorX:o,children:e})),h.jsx(tm.Provider,{value:d,children:e})};function mD(){return new Map}function I9(e=!0){const t=T.useContext(tm);if(t===null)return[!0,null];const{isPresent:n,onExitComplete:r,register:i}=t,a=T.useId();T.useEffect(()=>{if(e)return i(a)},[e]);const s=T.useCallback(()=>e&&r&&r(a),[a,r,e]);return!n&&r?[!1,s]:[!0]}const Iu=e=>e.key||"";function A4(e){const t=[];return T.Children.forEach(e,n=>{T.isValidElement(n)&&t.push(n)}),t}const hD=({children:e,custom:t,initial:n=!0,onExitComplete:r,presenceAffectsLayout:i=!0,mode:a="sync",propagate:s=!1,anchorX:o="left"})=>{const[l,c]=I9(s),u=T.useMemo(()=>A4(e),[e]),d=s&&!l?[]:u.map(Iu),m=T.useRef(!0),p=T.useRef(u),x=L1(()=>new Map),[g,w]=T.useState(u),[v,$]=T.useState(u);V5(()=>{m.current=!1,p.current=u;for(let k=0;k<v.length;k++){const S=Iu(v[k]);d.includes(S)?x.delete(S):x.get(S)!==!0&&x.set(S,!1)}},[v,d.length,d.join("-")]);const _=[];if(u!==g){let k=[...u];for(let S=0;S<v.length;S++){const L=v[S],U=Iu(L);d.includes(U)||(k.splice(S,0,L),_.push(L))}return a==="wait"&&_.length&&(k=_),$(A4(k)),w(u),null}const{forceRender:C}=T.useContext(A1);return h.jsx(h.Fragment,{children:v.map(k=>{const S=Iu(k),L=s&&!l?!1:u===v||d.includes(S),U=()=>{if(x.has(S))x.set(S,!0);else return;let F=!0;x.forEach(q=>{q||(F=!1)}),F&&(C==null||C(),$(p.current),s&&(c==null||c()),r&&r())};return h.jsx(dD,{isPresent:L,initial:!m.current||n?void 0:!1,custom:t,presenceAffectsLayout:i,mode:a,onExitComplete:L?void 0:U,anchorX:o,children:k},S)})})},U9=T.createContext({strict:!1}),L4={animation:["animate","variants","whileHover","whileTap","exit","whileInView","whileFocus","whileDrag"],exit:["exit"],drag:["drag","dragControls"],focus:["whileFocus"],hover:["whileHover","onHoverStart","onHoverEnd"],tap:["whileTap","onTap","onTapStart","onTapCancel"],pan:["onPan","onPanStart","onPanSessionStart","onPanEnd"],inView:["whileInView","onViewportEnter","onViewportLeave"],layout:["layout","layoutId"]},Io={};for(const e in L4)Io[e]={isEnabled:t=>L4[e].some(n=>!!t[n])};function pD(e){for(const t in e)Io[t]={...Io[t],...e[t]}}const fD=new Set(["animate","exit","variants","initial","style","values","variants","transition","transformTemplate","custom","inherit","onBeforeLayoutMeasure","onAnimationStart","onAnimationComplete","onUpdate","onDragStart","onDrag","onDragEnd","onMeasureDragConstraints","onDirectionLock","onDragTransitionEnd","_dragX","_dragY","onHoverStart","onHoverEnd","onViewportEnter","onViewportLeave","globalTapTarget","ignoreStrict","viewport"]);function o0(e){return e.startsWith("while")||e.startsWith("drag")&&e!=="draggable"||e.startsWith("layout")||e.startsWith("onTap")||e.startsWith("onPan")||e.startsWith("onLayout")||fD.has(e)}let D9=e=>!o0(e);function gD(e){e&&(D9=t=>t.startsWith("on")?!o0(t):e(t))}try{gD(require("@emotion/is-prop-valid").default)}catch{}function bD(e,t,n){const r={};for(const i in e)i==="values"&&typeof e.values=="object"||(D9(i)||n===!0&&o0(i)||!t&&!o0(i)||e.draggable&&i.startsWith("onDrag"))&&(r[i]=e[i]);return r}function vD(e){if(typeof Proxy>"u")return e;const t=new Map,n=(...r)=>e(...r);return new Proxy(n,{get:(r,i)=>i==="create"?e:(t.has(i)||t.set(i,e(i)),t.get(i))})}const nm=T.createContext({});function rm(e){return e!==null&&typeof e=="object"&&typeof e.start=="function"}function Tc(e){return typeof e=="string"||Array.isArray(e)}const t2=["animate","whileInView","whileFocus","whileHover","whileTap","whileDrag","exit"],n2=["initial",...t2];function im(e){return rm(e.animate)||n2.some(t=>Tc(e[t]))}function M9(e){return!!(im(e)||e.variants)}function xD(e,t){if(im(e)){const{initial:n,animate:r}=e;return{initial:n===!1||Tc(n)?n:void 0,animate:Tc(r)?r:void 0}}return e.inherit!==!1?t:{}}function $D(e){const{initial:t,animate:n}=xD(e,T.useContext(nm));return T.useMemo(()=>({initial:t,animate:n}),[P4(t),P4(n)])}function P4(e){return Array.isArray(e)?e.join(" "):e}const yD=Symbol.for("motionComponentSymbol");function Ks(e){return e&&typeof e=="object"&&Object.prototype.hasOwnProperty.call(e,"current")}function _D(e,t,n){return T.useCallback(r=>{r&&e.onMount&&e.onMount(r),t&&(r?t.mount(r):t.unmount()),n&&(typeof n=="function"?n(r):Ks(n)&&(n.current=r))},[t])}const r2=e=>e.replace(/([a-z])([A-Z])/gu,"$1-$2").toLowerCase(),wD="framerAppearId",R9="data-"+r2(wD),O9=T.createContext({});function TD(e,t,n,r,i){var g,w;const{visualElement:a}=T.useContext(nm),s=T.useContext(U9),o=T.useContext(tm),l=T.useContext(e2).reducedMotion,c=T.useRef(null);r=r||s.renderer,!c.current&&r&&(c.current=r(e,{visualState:t,parent:a,props:n,presenceContext:o,blockInitialAnimation:o?o.initial===!1:!1,reducedMotionConfig:l}));const u=c.current,d=T.useContext(O9);u&&!u.projection&&i&&(u.type==="html"||u.type==="svg")&&ED(c.current,n,i,d);const m=T.useRef(!1);T.useInsertionEffect(()=>{u&&m.current&&u.update(n,o)});const p=n[R9],x=T.useRef(!!p&&!((g=window.MotionHandoffIsComplete)!=null&&g.call(window,p))&&((w=window.MotionHasOptimisedAnimation)==null?void 0:w.call(window,p)));return V5(()=>{u&&(m.current=!0,window.MotionIsMounted=!0,u.updateFeatures(),Z1.render(u.render),x.current&&u.animationState&&u.animationState.animateChanges())}),T.useEffect(()=>{u&&(!x.current&&u.animationState&&u.animationState.animateChanges(),x.current&&(queueMicrotask(()=>{var v;(v=window.MotionHandoffMarkAsComplete)==null||v.call(window,p)}),x.current=!1))}),u}function ED(e,t,n,r){const{layoutId:i,layout:a,drag:s,dragConstraints:o,layoutScroll:l,layoutRoot:c,layoutCrossfade:u}=t;e.projection=new n(e.latestValues,t["data-framer-portal-id"]?void 0:z9(e.parent)),e.projection.setOptions({layoutId:i,layout:a,alwaysMeasureLayout:!!s||o&&Ks(o),visualElement:e,animationType:typeof a=="string"?a:"both",initialPromotionConfig:r,crossfade:u,layoutScroll:l,layoutRoot:c})}function z9(e){if(e)return e.options.allowProjection!==!1?e.projection:z9(e.parent)}function kD({preloadedFeatures:e,createVisualElement:t,useRender:n,useVisualState:r,Component:i}){e&&pD(e);function a(o,l){let c;const u={...T.useContext(e2),...o,layoutId:SD(o)},{isStatic:d}=u,m=$D(o),p=r(o,d);if(!d&&P1){ND();const x=CD(u);c=x.MeasureLayout,m.visualElement=TD(i,p,u,t,x.ProjectionNode)}return h.jsxs(nm.Provider,{value:m,children:[c&&m.visualElement?h.jsx(c,{visualElement:m.visualElement,...u}):null,n(i,o,_D(p,m.visualElement,l),p,d,m.visualElement)]})}a.displayName=`motion.${typeof i=="string"?i:`create(${i.displayName??i.name??""})`}`;const s=T.forwardRef(a);return s[yD]=i,s}function SD({layoutId:e}){const t=T.useContext(A1).id;return t&&e!==void 0?t+"-"+e:e}function ND(e,t){T.useContext(U9).strict}function CD(e){const{drag:t,layout:n}=Io;if(!t&&!n)return{};const r={...t,...n};return{MeasureLayout:t!=null&&t.isEnabled(e)||n!=null&&n.isEnabled(e)?r.MeasureLayout:void 0,ProjectionNode:r.ProjectionNode}}const Ec={};function AD(e){for(const t in e)Ec[t]=e[t],z1(t)&&(Ec[t].isCSSVariable=!0)}function B9(e,{layout:t,layoutId:n}){return Go.has(e)||e.startsWith("origin")||(t||n!==void 0)&&(!!Ec[e]||e==="opacity")}const LD={x:"translateX",y:"translateY",z:"translateZ",transformPerspective:"perspective"},PD=qo.length;function ID(e,t,n){let r="",i=!0;for(let a=0;a<PD;a++){const s=qo[a],o=e[s];if(o===void 0)continue;let l=!0;if(typeof o=="number"?l=o===(s.startsWith("scale")?1:0):l=parseFloat(o)===0,!l||n){const c=N9(o,Q1[s]);if(!l){i=!1;const u=LD[s]||s;r+=`${u}(${c}) `}n&&(t[s]=c)}}return r=r.trim(),n?r=n(t,i?"":r):i&&(r="none"),r}function i2(e,t,n){const{style:r,vars:i,transformOrigin:a}=e;let s=!1,o=!1;for(const l in t){const c=t[l];if(Go.has(l)){s=!0;continue}else if(z1(l)){i[l]=c;continue}else{const u=N9(c,Q1[l]);l.startsWith("origin")?(o=!0,a[l]=u):r[l]=u}}if(t.transform||(s||n?r.transform=ID(t,e.transform,n):r.transform&&(r.transform="none")),o){const{originX:l="50%",originY:c="50%",originZ:u=0}=a;r.transformOrigin=`${l} ${c} ${u}`}}const a2=()=>({style:{},transform:{},transformOrigin:{},vars:{}});function F9(e,t,n){for(const r in t)!rn(t[r])&&!B9(r,n)&&(e[r]=t[r])}function UD({transformTemplate:e},t){return T.useMemo(()=>{const n=a2();return i2(n,t,e),Object.assign({},n.vars,n.style)},[t])}function DD(e,t){const n=e.style||{},r={};return F9(r,n,e),Object.assign(r,UD(e,t)),r}function MD(e,t){const n={},r=DD(e,t);return e.drag&&e.dragListener!==!1&&(n.draggable=!1,r.userSelect=r.WebkitUserSelect=r.WebkitTouchCallout="none",r.touchAction=e.drag===!0?"none":`pan-${e.drag==="x"?"y":"x"}`),e.tabIndex===void 0&&(e.onTap||e.onTapStart||e.whileTap)&&(n.tabIndex=0),n.style=r,n}const RD={offset:"stroke-dashoffset",array:"stroke-dasharray"},OD={offset:"strokeDashoffset",array:"strokeDasharray"};function zD(e,t,n=1,r=0,i=!0){e.pathLength=1;const a=i?RD:OD;e[a.offset]=Se.transform(-r);const s=Se.transform(t),o=Se.transform(n);e[a.array]=`${s} ${o}`}function j9(e,{attrX:t,attrY:n,attrScale:r,pathLength:i,pathSpacing:a=1,pathOffset:s=0,...o},l,c,u){if(i2(e,o,c),l){e.style.viewBox&&(e.attrs.viewBox=e.style.viewBox);return}e.attrs=e.style,e.style={};const{attrs:d,style:m}=e;d.transform&&(m.transform=d.transform,delete d.transform),(m.transform||d.transformOrigin)&&(m.transformOrigin=d.transformOrigin??"50% 50%",delete d.transformOrigin),m.transform&&(m.transformBox=(u==null?void 0:u.transformBox)??"fill-box",delete d.transformBox),t!==void 0&&(d.x=t),n!==void 0&&(d.y=n),r!==void 0&&(d.scale=r),i!==void 0&&zD(d,i,a,s,!1)}const V9=()=>({...a2(),attrs:{}}),H9=e=>typeof e=="string"&&e.toLowerCase()==="svg";function BD(e,t,n,r){const i=T.useMemo(()=>{const a=V9();return j9(a,t,H9(r),e.transformTemplate,e.style),{...a.attrs,style:{...a.style}}},[t]);if(e.style){const a={};F9(a,e.style,e),i.style={...a,...i.style}}return i}const FD=["animate","circle","defs","desc","ellipse","g","image","line","filter","marker","mask","metadata","path","pattern","polygon","polyline","rect","stop","switch","symbol","svg","text","tspan","use","view"];function s2(e){return typeof e!="string"||e.includes("-")?!1:!!(FD.indexOf(e)>-1||/[A-Z]/u.test(e))}function jD(e=!1){return(n,r,i,{latestValues:a},s)=>{const l=(s2(n)?BD:MD)(r,a,s,n),c=bD(r,typeof n=="string",e),u=n!==T.Fragment?{...c,...l,ref:i}:{},{children:d}=r,m=T.useMemo(()=>rn(d)?d.get():d,[d]);return T.createElement(n,{...u,children:m})}}function I4(e){const t=[{},{}];return e==null||e.values.forEach((n,r)=>{t[0][r]=n.get(),t[1][r]=n.getVelocity()}),t}function o2(e,t,n,r){if(typeof t=="function"){const[i,a]=I4(r);t=t(n!==void 0?n:e.custom,i,a)}if(typeof t=="string"&&(t=e.variants&&e.variants[t]),typeof t=="function"){const[i,a]=I4(r);t=t(n!==void 0?n:e.custom,i,a)}return t}function gd(e){return rn(e)?e.get():e}function VD({scrapeMotionValuesFromProps:e,createRenderState:t},n,r,i){return{latestValues:HD(n,r,i,e),renderState:t()}}const q9=e=>(t,n)=>{const r=T.useContext(nm),i=T.useContext(tm),a=()=>VD(e,t,r,i);return n?a():L1(a)};function HD(e,t,n,r){const i={},a=r(e,{});for(const m in a)i[m]=gd(a[m]);let{initial:s,animate:o}=e;const l=im(e),c=M9(e);t&&c&&!l&&e.inherit!==!1&&(s===void 0&&(s=t.initial),o===void 0&&(o=t.animate));let u=n?n.initial===!1:!1;u=u||s===!1;const d=u?o:s;if(d&&typeof d!="boolean"&&!rm(d)){const m=Array.isArray(d)?d:[d];for(let p=0;p<m.length;p++){const x=o2(e,m[p]);if(x){const{transitionEnd:g,transition:w,...v}=x;for(const $ in v){let _=v[$];if(Array.isArray(_)){const C=u?_.length-1:0;_=_[C]}_!==null&&(i[$]=_)}for(const $ in g)i[$]=g[$]}}}return i}function l2(e,t,n){var a;const{style:r}=e,i={};for(const s in r)(rn(r[s])||t.style&&rn(t.style[s])||B9(s,e)||((a=n==null?void 0:n.getValue(s))==null?void 0:a.liveStyle)!==void 0)&&(i[s]=r[s]);return i}const qD={useVisualState:q9({scrapeMotionValuesFromProps:l2,createRenderState:a2})};function G9(e,t,n){const r=l2(e,t,n);for(const i in e)if(rn(e[i])||rn(t[i])){const a=qo.indexOf(i)!==-1?"attr"+i.charAt(0).toUpperCase()+i.substring(1):i;r[a]=e[i]}return r}const GD={useVisualState:q9({scrapeMotionValuesFromProps:G9,createRenderState:V9})};function WD(e,t){return function(r,{forwardMotionProps:i}={forwardMotionProps:!1}){const s={...s2(r)?GD:qD,preloadedFeatures:e,useRender:jD(i),createVisualElement:t,Component:r};return kD(s)}}function kc(e,t,n){const r=e.getProps();return o2(r,t,n!==void 0?n:r.custom,e)}const Sf=e=>Array.isArray(e);function KD(e,t,n){e.hasValue(t)?e.getValue(t).set(n):e.addValue(t,Po(n))}function YD(e){return Sf(e)?e[e.length-1]||0:e}function XD(e,t){const n=kc(e,t);let{transitionEnd:r={},transition:i={},...a}=n||{};a={...a,...r};for(const s in a){const o=YD(a[s]);KD(e,s,o)}}function QD(e){return!!(rn(e)&&e.add)}function Nf(e,t){const n=e.getValue("willChange");if(QD(n))return n.add(t);if(!n&&Mi.WillChange){const r=new Mi.WillChange("auto");e.addValue("willChange",r),r.add(t)}}function W9(e){return e.props[R9]}const ZD=e=>e!==null;function JD(e,{repeat:t,repeatType:n="loop"},r){const i=e.filter(ZD),a=t&&n!=="loop"&&t%2===1?0:i.length-1;return!a||r===void 0?i[a]:r}const eM={type:"spring",stiffness:500,damping:25,restSpeed:10},tM=e=>({type:"spring",stiffness:550,damping:e===0?2*Math.sqrt(550):30,restSpeed:10}),nM={type:"keyframes",duration:.8},rM={type:"keyframes",ease:[.25,.1,.35,1],duration:.3},iM=(e,{keyframes:t})=>t.length>2?nM:Go.has(e)?e.startsWith("scale")?tM(t[1]):eM:rM;function aM({when:e,delay:t,delayChildren:n,staggerChildren:r,staggerDirection:i,repeat:a,repeatType:s,repeatDelay:o,from:l,elapsed:c,...u}){return!!Object.keys(u).length}const c2=(e,t,n,r={},i,a)=>s=>{const o=X1(r,e)||{},l=o.delay||r.delay||0;let{elapsed:c=0}=r;c=c-ii(l);const u={keyframes:Array.isArray(n)?n:[null,n],ease:"easeOut",velocity:t.getVelocity(),...o,delay:-c,onUpdate:m=>{t.set(m),o.onUpdate&&o.onUpdate(m)},onComplete:()=>{s(),o.onComplete&&o.onComplete()},name:e,motionValue:t,element:a?void 0:i};aM(o)||Object.assign(u,iM(e,u)),u.duration&&(u.duration=ii(u.duration)),u.repeatDelay&&(u.repeatDelay=ii(u.repeatDelay)),u.from!==void 0&&(u.keyframes[0]=u.from);let d=!1;if((u.type===!1||u.duration===0&&!u.repeatDelay)&&(u.duration=0,u.delay===0&&(d=!0)),(Mi.instantAnimations||Mi.skipAnimations)&&(d=!0,u.duration=0,u.delay=0),u.allowFlatten=!o.type&&!o.ease,d&&!a&&t.get()!==void 0){const m=JD(u.keyframes,o);if(m!==void 0){xt.update(()=>{u.onUpdate(m),u.onComplete()});return}}return o.isSync?new W1(u):new OU(u)};function sM({protectedKeys:e,needsAnimating:t},n){const r=e.hasOwnProperty(n)&&t[n]!==!0;return t[n]=!1,r}function K9(e,t,{delay:n=0,transitionOverride:r,type:i}={}){let{transition:a=e.getDefaultTransition(),transitionEnd:s,...o}=t;r&&(a=r);const l=[],c=i&&e.animationState&&e.animationState.getState()[i];for(const u in o){const d=e.getValue(u,e.latestValues[u]??null),m=o[u];if(m===void 0||c&&sM(c,u))continue;const p={delay:n,...X1(a||{},u)},x=d.get();if(x!==void 0&&!d.isAnimating&&!Array.isArray(m)&&m===x&&!p.velocity)continue;let g=!1;if(window.MotionHandoffAnimation){const v=W9(e);if(v){const $=window.MotionHandoffAnimation(v,u,xt);$!==null&&(p.startTime=$,g=!0)}}Nf(e,u),d.start(c2(u,d,m,e.shouldReduceMotion&&w9.has(u)?{type:!1}:p,e,g));const w=d.animation;w&&l.push(w)}return s&&Promise.all(l).then(()=>{xt.update(()=>{s&&XD(e,s)})}),l}function Cf(e,t,n={}){var l;const r=kc(e,t,n.type==="exit"?(l=e.presenceContext)==null?void 0:l.custom:void 0);let{transition:i=e.getDefaultTransition()||{}}=r||{};n.transitionOverride&&(i=n.transitionOverride);const a=r?()=>Promise.all(K9(e,r,n)):()=>Promise.resolve(),s=e.variantChildren&&e.variantChildren.size?(c=0)=>{const{delayChildren:u=0,staggerChildren:d,staggerDirection:m}=i;return oM(e,t,u+c,d,m,n)}:()=>Promise.resolve(),{when:o}=i;if(o){const[c,u]=o==="beforeChildren"?[a,s]:[s,a];return c().then(()=>u())}else return Promise.all([a(),s(n.delay)])}function oM(e,t,n=0,r=0,i=1,a){const s=[],o=(e.variantChildren.size-1)*r,l=i===1?(c=0)=>c*r:(c=0)=>o-c*r;return Array.from(e.variantChildren).sort(lM).forEach((c,u)=>{c.notify("AnimationStart",t),s.push(Cf(c,t,{...a,delay:n+l(u)}).then(()=>c.notify("AnimationComplete",t)))}),Promise.all(s)}function lM(e,t){return e.sortNodePosition(t)}function cM(e,t,n={}){e.notify("AnimationStart",t);let r;if(Array.isArray(t)){const i=t.map(a=>Cf(e,a,n));r=Promise.all(i)}else if(typeof t=="string")r=Cf(e,t,n);else{const i=typeof t=="function"?kc(e,t,n.custom):t;r=Promise.all(K9(e,i,n))}return r.then(()=>{e.notify("AnimationComplete",t)})}function Y9(e,t){if(!Array.isArray(t))return!1;const n=t.length;if(n!==e.length)return!1;for(let r=0;r<n;r++)if(t[r]!==e[r])return!1;return!0}const uM=n2.length;function X9(e){if(!e)return;if(!e.isControllingVariants){const n=e.parent?X9(e.parent)||{}:{};return e.props.initial!==void 0&&(n.initial=e.props.initial),n}const t={};for(let n=0;n<uM;n++){const r=n2[n],i=e.props[r];(Tc(i)||i===!1)&&(t[r]=i)}return t}const dM=[...t2].reverse(),mM=t2.length;function hM(e){return t=>Promise.all(t.map(({animation:n,options:r})=>cM(e,n,r)))}function pM(e){let t=hM(e),n=U4(),r=!0;const i=l=>(c,u)=>{var m;const d=kc(e,u,l==="exit"?(m=e.presenceContext)==null?void 0:m.custom:void 0);if(d){const{transition:p,transitionEnd:x,...g}=d;c={...c,...g,...x}}return c};function a(l){t=l(e)}function s(l){const{props:c}=e,u=X9(e.parent)||{},d=[],m=new Set;let p={},x=1/0;for(let w=0;w<mM;w++){const v=dM[w],$=n[v],_=c[v]!==void 0?c[v]:u[v],C=Tc(_),k=v===l?$.isActive:null;k===!1&&(x=w);let S=_===u[v]&&_!==c[v]&&C;if(S&&r&&e.manuallyAnimateOnMount&&(S=!1),$.protectedKeys={...p},!$.isActive&&k===null||!_&&!$.prevProp||rm(_)||typeof _=="boolean")continue;const L=fM($.prevProp,_);let U=L||v===l&&$.isActive&&!S&&C||w>x&&C,F=!1;const q=Array.isArray(_)?_:[_];let G=q.reduce(i(v),{});k===!1&&(G={});const{prevResolvedValues:H={}}=$,ne={...H,...G},K=ae=>{U=!0,m.has(ae)&&(F=!0,m.delete(ae)),$.needsAnimating[ae]=!0;const B=e.getValue(ae);B&&(B.liveStyle=!1)};for(const ae in ne){const B=G[ae],X=H[ae];if(p.hasOwnProperty(ae))continue;let P=!1;Sf(B)&&Sf(X)?P=!Y9(B,X):P=B!==X,P?B!=null?K(ae):m.add(ae):B!==void 0&&m.has(ae)?K(ae):$.protectedKeys[ae]=!0}$.prevProp=_,$.prevResolvedValues=G,$.isActive&&(p={...p,...G}),r&&e.blockInitialAnimation&&(U=!1),U&&(!(S&&L)||F)&&d.push(...q.map(ae=>({animation:ae,options:{type:v}})))}if(m.size){const w={};if(typeof c.initial!="boolean"){const v=kc(e,Array.isArray(c.initial)?c.initial[0]:c.initial);v&&v.transition&&(w.transition=v.transition)}m.forEach(v=>{const $=e.getBaseTarget(v),_=e.getValue(v);_&&(_.liveStyle=!0),w[v]=$??null}),d.push({animation:w})}let g=!!d.length;return r&&(c.initial===!1||c.initial===c.animate)&&!e.manuallyAnimateOnMount&&(g=!1),r=!1,g?t(d):Promise.resolve()}function o(l,c){var d;if(n[l].isActive===c)return Promise.resolve();(d=e.variantChildren)==null||d.forEach(m=>{var p;return(p=m.animationState)==null?void 0:p.setActive(l,c)}),n[l].isActive=c;const u=s(l);for(const m in n)n[m].protectedKeys={};return u}return{animateChanges:s,setActive:o,setAnimateFunction:a,getState:()=>n,reset:()=>{n=U4(),r=!0}}}function fM(e,t){return typeof t=="string"?t!==e:Array.isArray(t)?!Y9(t,e):!1}function Va(e=!1){return{isActive:e,protectedKeys:{},needsAnimating:{},prevResolvedValues:{}}}function U4(){return{animate:Va(!0),whileInView:Va(),whileHover:Va(),whileTap:Va(),whileDrag:Va(),whileFocus:Va(),exit:Va()}}class Ra{constructor(t){this.isMounted=!1,this.node=t}update(){}}class gM extends Ra{constructor(t){super(t),t.animationState||(t.animationState=pM(t))}updateAnimationControlsSubscription(){const{animate:t}=this.node.getProps();rm(t)&&(this.unmountControls=t.subscribe(this.node))}mount(){this.updateAnimationControlsSubscription()}update(){const{animate:t}=this.node.getProps(),{animate:n}=this.node.prevProps||{};t!==n&&this.updateAnimationControlsSubscription()}unmount(){var t;this.node.animationState.reset(),(t=this.unmountControls)==null||t.call(this)}}let bM=0;class vM extends Ra{constructor(){super(...arguments),this.id=bM++}update(){if(!this.node.presenceContext)return;const{isPresent:t,onExitComplete:n}=this.node.presenceContext,{isPresent:r}=this.node.prevPresenceContext||{};if(!this.node.animationState||t===r)return;const i=this.node.animationState.setActive("exit",!t);n&&!t&&i.then(()=>{n(this.id)})}mount(){const{register:t,onExitComplete:n}=this.node.presenceContext||{};n&&n(this.id),t&&(this.unmount=t(this.id))}unmount(){}}const xM={animation:{Feature:gM},exit:{Feature:vM}};function Sc(e,t,n,r={passive:!0}){return e.addEventListener(t,n,r),()=>e.removeEventListener(t,n)}function Kc(e){return{point:{x:e.pageX,y:e.pageY}}}const $M=e=>t=>J1(t)&&e(t,Kc(t));function zl(e,t,n,r){return Sc(e,t,$M(n),r)}function Q9({top:e,left:t,right:n,bottom:r}){return{x:{min:t,max:n},y:{min:e,max:r}}}function yM({x:e,y:t}){return{top:t.min,right:e.max,bottom:t.max,left:e.min}}function _M(e,t){if(!t)return e;const n=t({x:e.left,y:e.top}),r=t({x:e.right,y:e.bottom});return{top:n.y,left:n.x,bottom:r.y,right:r.x}}const Z9=1e-4,wM=1-Z9,TM=1+Z9,J9=.01,EM=0-J9,kM=0+J9;function hn(e){return e.max-e.min}function SM(e,t,n){return Math.abs(e-t)<=n}function D4(e,t,n,r=.5){e.origin=r,e.originPoint=bt(t.min,t.max,e.origin),e.scale=hn(n)/hn(t),e.translate=bt(n.min,n.max,e.origin)-e.originPoint,(e.scale>=wM&&e.scale<=TM||isNaN(e.scale))&&(e.scale=1),(e.translate>=EM&&e.translate<=kM||isNaN(e.translate))&&(e.translate=0)}function Bl(e,t,n,r){D4(e.x,t.x,n.x,r?r.originX:void 0),D4(e.y,t.y,n.y,r?r.originY:void 0)}function M4(e,t,n){e.min=n.min+t.min,e.max=e.min+hn(t)}function NM(e,t,n){M4(e.x,t.x,n.x),M4(e.y,t.y,n.y)}function R4(e,t,n){e.min=t.min-n.min,e.max=e.min+hn(t)}function Fl(e,t,n){R4(e.x,t.x,n.x),R4(e.y,t.y,n.y)}const O4=()=>({translate:0,scale:1,origin:0,originPoint:0}),Ys=()=>({x:O4(),y:O4()}),z4=()=>({min:0,max:0}),St=()=>({x:z4(),y:z4()});function er(e){return[e("x"),e("y")]}function gh(e){return e===void 0||e===1}function Af({scale:e,scaleX:t,scaleY:n}){return!gh(e)||!gh(t)||!gh(n)}function Ka(e){return Af(e)||e7(e)||e.z||e.rotate||e.rotateX||e.rotateY||e.skewX||e.skewY}function e7(e){return B4(e.x)||B4(e.y)}function B4(e){return e&&e!=="0%"}function l0(e,t,n){const r=e-n,i=t*r;return n+i}function F4(e,t,n,r,i){return i!==void 0&&(e=l0(e,i,r)),l0(e,n,r)+t}function Lf(e,t=0,n=1,r,i){e.min=F4(e.min,t,n,r,i),e.max=F4(e.max,t,n,r,i)}function t7(e,{x:t,y:n}){Lf(e.x,t.translate,t.scale,t.originPoint),Lf(e.y,n.translate,n.scale,n.originPoint)}const j4=.999999999999,V4=1.0000000000001;function CM(e,t,n,r=!1){const i=n.length;if(!i)return;t.x=t.y=1;let a,s;for(let o=0;o<i;o++){a=n[o],s=a.projectionDelta;const{visualElement:l}=a.options;l&&l.props.style&&l.props.style.display==="contents"||(r&&a.options.layoutScroll&&a.scroll&&a!==a.root&&Qs(e,{x:-a.scroll.offset.x,y:-a.scroll.offset.y}),s&&(t.x*=s.x.scale,t.y*=s.y.scale,t7(e,s)),r&&Ka(a.latestValues)&&Qs(e,a.latestValues))}t.x<V4&&t.x>j4&&(t.x=1),t.y<V4&&t.y>j4&&(t.y=1)}function Xs(e,t){e.min=e.min+t,e.max=e.max+t}function H4(e,t,n,r,i=.5){const a=bt(e.min,e.max,i);Lf(e,t,n,a,r)}function Qs(e,t){H4(e.x,t.x,t.scaleX,t.scale,t.originX),H4(e.y,t.y,t.scaleY,t.scale,t.originY)}function n7(e,t){return Q9(_M(e.getBoundingClientRect(),t))}function AM(e,t,n){const r=n7(e,n),{scroll:i}=t;return i&&(Xs(r.x,i.offset.x),Xs(r.y,i.offset.y)),r}const r7=({current:e})=>e?e.ownerDocument.defaultView:null,q4=(e,t)=>Math.abs(e-t);function LM(e,t){const n=q4(e.x,t.x),r=q4(e.y,t.y);return Math.sqrt(n**2+r**2)}class i7{constructor(t,n,{transformPagePoint:r,contextWindow:i,dragSnapToOrigin:a=!1}={}){if(this.startEvent=null,this.lastMoveEvent=null,this.lastMoveEventInfo=null,this.handlers={},this.contextWindow=window,this.updatePoint=()=>{if(!(this.lastMoveEvent&&this.lastMoveEventInfo))return;const d=vh(this.lastMoveEventInfo,this.history),m=this.startEvent!==null,p=LM(d.offset,{x:0,y:0})>=3;if(!m&&!p)return;const{point:x}=d,{timestamp:g}=Wt;this.history.push({...x,timestamp:g});const{onStart:w,onMove:v}=this.handlers;m||(w&&w(this.lastMoveEvent,d),this.startEvent=this.lastMoveEvent),v&&v(this.lastMoveEvent,d)},this.handlePointerMove=(d,m)=>{this.lastMoveEvent=d,this.lastMoveEventInfo=bh(m,this.transformPagePoint),xt.update(this.updatePoint,!0)},this.handlePointerUp=(d,m)=>{this.end();const{onEnd:p,onSessionEnd:x,resumeAnimation:g}=this.handlers;if(this.dragSnapToOrigin&&g&&g(),!(this.lastMoveEvent&&this.lastMoveEventInfo))return;const w=vh(d.type==="pointercancel"?this.lastMoveEventInfo:bh(m,this.transformPagePoint),this.history);this.startEvent&&p&&p(d,w),x&&x(d,w)},!J1(t))return;this.dragSnapToOrigin=a,this.handlers=n,this.transformPagePoint=r,this.contextWindow=i||window;const s=Kc(t),o=bh(s,this.transformPagePoint),{point:l}=o,{timestamp:c}=Wt;this.history=[{...l,timestamp:c}];const{onSessionStart:u}=n;u&&u(t,vh(o,this.history)),this.removeListeners=qc(zl(this.contextWindow,"pointermove",this.handlePointerMove),zl(this.contextWindow,"pointerup",this.handlePointerUp),zl(this.contextWindow,"pointercancel",this.handlePointerUp))}updateHandlers(t){this.handlers=t}end(){this.removeListeners&&this.removeListeners(),Ca(this.updatePoint)}}function bh(e,t){return t?{point:t(e.point)}:e}function G4(e,t){return{x:e.x-t.x,y:e.y-t.y}}function vh({point:e},t){return{point:e,delta:G4(e,a7(t)),offset:G4(e,PM(t)),velocity:IM(t,.1)}}function PM(e){return e[0]}function a7(e){return e[e.length-1]}function IM(e,t){if(e.length<2)return{x:0,y:0};let n=e.length-1,r=null;const i=a7(e);for(;n>=0&&(r=e[n],!(i.timestamp-r.timestamp>ii(t)));)n--;if(!r)return{x:0,y:0};const a=ai(i.timestamp-r.timestamp);if(a===0)return{x:0,y:0};const s={x:(i.x-r.x)/a,y:(i.y-r.y)/a};return s.x===1/0&&(s.x=0),s.y===1/0&&(s.y=0),s}function UM(e,{min:t,max:n},r){return t!==void 0&&e<t?e=r?bt(t,e,r.min):Math.max(e,t):n!==void 0&&e>n&&(e=r?bt(n,e,r.max):Math.min(e,n)),e}function W4(e,t,n){return{min:t!==void 0?e.min+t:void 0,max:n!==void 0?e.max+n-(e.max-e.min):void 0}}function DM(e,{top:t,left:n,bottom:r,right:i}){return{x:W4(e.x,n,i),y:W4(e.y,t,r)}}function K4(e,t){let n=t.min-e.min,r=t.max-e.max;return t.max-t.min<e.max-e.min&&([n,r]=[r,n]),{min:n,max:r}}function MM(e,t){return{x:K4(e.x,t.x),y:K4(e.y,t.y)}}function RM(e,t){let n=.5;const r=hn(e),i=hn(t);return i>r?n=yc(t.min,t.max-r,e.min):r>i&&(n=yc(e.min,e.max-i,t.min)),Di(0,1,n)}function OM(e,t){const n={};return t.min!==void 0&&(n.min=t.min-e.min),t.max!==void 0&&(n.max=t.max-e.min),n}const Pf=.35;function zM(e=Pf){return e===!1?e=0:e===!0&&(e=Pf),{x:Y4(e,"left","right"),y:Y4(e,"top","bottom")}}function Y4(e,t,n){return{min:X4(e,t),max:X4(e,n)}}function X4(e,t){return typeof e=="number"?e:e[t]||0}const BM=new WeakMap;class FM{constructor(t){this.openDragLock=null,this.isDragging=!1,this.currentDirection=null,this.originPoint={x:0,y:0},this.constraints=!1,this.hasMutatedConstraints=!1,this.elastic=St(),this.visualElement=t}start(t,{snapToCursor:n=!1}={}){const{presenceContext:r}=this.visualElement;if(r&&r.isPresent===!1)return;const i=u=>{const{dragSnapToOrigin:d}=this.getProps();d?this.pauseAnimation():this.stopAnimation(),n&&this.snapToCursor(Kc(u).point)},a=(u,d)=>{const{drag:m,dragPropagation:p,onDragStart:x}=this.getProps();if(m&&!p&&(this.openDragLock&&this.openDragLock(),this.openDragLock=eD(m),!this.openDragLock))return;this.isDragging=!0,this.currentDirection=null,this.resolveConstraints(),this.visualElement.projection&&(this.visualElement.projection.isAnimationBlocked=!0,this.visualElement.projection.target=void 0),er(w=>{let v=this.getAxisMotionValue(w).get()||0;if(si.test(v)){const{projection:$}=this.visualElement;if($&&$.layout){const _=$.layout.layoutBox[w];_&&(v=hn(_)*(parseFloat(v)/100))}}this.originPoint[w]=v}),x&&xt.postRender(()=>x(u,d)),Nf(this.visualElement,"transform");const{animationState:g}=this.visualElement;g&&g.setActive("whileDrag",!0)},s=(u,d)=>{const{dragPropagation:m,dragDirectionLock:p,onDirectionLock:x,onDrag:g}=this.getProps();if(!m&&!this.openDragLock)return;const{offset:w}=d;if(p&&this.currentDirection===null){this.currentDirection=jM(w),this.currentDirection!==null&&x&&x(this.currentDirection);return}this.updateAxis("x",d.point,w),this.updateAxis("y",d.point,w),this.visualElement.render(),g&&g(u,d)},o=(u,d)=>this.stop(u,d),l=()=>er(u=>{var d;return this.getAnimationState(u)==="paused"&&((d=this.getAxisMotionValue(u).animation)==null?void 0:d.play())}),{dragSnapToOrigin:c}=this.getProps();this.panSession=new i7(t,{onSessionStart:i,onStart:a,onMove:s,onSessionEnd:o,resumeAnimation:l},{transformPagePoint:this.visualElement.getTransformPagePoint(),dragSnapToOrigin:c,contextWindow:r7(this.visualElement)})}stop(t,n){const r=this.isDragging;if(this.cancel(),!r)return;const{velocity:i}=n;this.startAnimation(i);const{onDragEnd:a}=this.getProps();a&&xt.postRender(()=>a(t,n))}cancel(){this.isDragging=!1;const{projection:t,animationState:n}=this.visualElement;t&&(t.isAnimationBlocked=!1),this.panSession&&this.panSession.end(),this.panSession=void 0;const{dragPropagation:r}=this.getProps();!r&&this.openDragLock&&(this.openDragLock(),this.openDragLock=null),n&&n.setActive("whileDrag",!1)}updateAxis(t,n,r){const{drag:i}=this.getProps();if(!r||!Uu(t,i,this.currentDirection))return;const a=this.getAxisMotionValue(t);let s=this.originPoint[t]+r[t];this.constraints&&this.constraints[t]&&(s=UM(s,this.constraints[t],this.elastic[t])),a.set(s)}resolveConstraints(){var a;const{dragConstraints:t,dragElastic:n}=this.getProps(),r=this.visualElement.projection&&!this.visualElement.projection.layout?this.visualElement.projection.measure(!1):(a=this.visualElement.projection)==null?void 0:a.layout,i=this.constraints;t&&Ks(t)?this.constraints||(this.constraints=this.resolveRefConstraints()):t&&r?this.constraints=DM(r.layoutBox,t):this.constraints=!1,this.elastic=zM(n),i!==this.constraints&&r&&this.constraints&&!this.hasMutatedConstraints&&er(s=>{this.constraints!==!1&&this.getAxisMotionValue(s)&&(this.constraints[s]=OM(r.layoutBox[s],this.constraints[s]))})}resolveRefConstraints(){const{dragConstraints:t,onMeasureDragConstraints:n}=this.getProps();if(!t||!Ks(t))return!1;const r=t.current,{projection:i}=this.visualElement;if(!i||!i.layout)return!1;const a=AM(r,i.root,this.visualElement.getTransformPagePoint());let s=MM(i.layout.layoutBox,a);if(n){const o=n(yM(s));this.hasMutatedConstraints=!!o,o&&(s=Q9(o))}return s}startAnimation(t){const{drag:n,dragMomentum:r,dragElastic:i,dragTransition:a,dragSnapToOrigin:s,onDragTransitionEnd:o}=this.getProps(),l=this.constraints||{},c=er(u=>{if(!Uu(u,n,this.currentDirection))return;let d=l&&l[u]||{};s&&(d={min:0,max:0});const m=i?200:1e6,p=i?40:1e7,x={type:"inertia",velocity:r?t[u]:0,bounceStiffness:m,bounceDamping:p,timeConstant:750,restDelta:1,restSpeed:10,...a,...d};return this.startAxisValueAnimation(u,x)});return Promise.all(c).then(o)}startAxisValueAnimation(t,n){const r=this.getAxisMotionValue(t);return Nf(this.visualElement,t),r.start(c2(t,r,0,n,this.visualElement,!1))}stopAnimation(){er(t=>this.getAxisMotionValue(t).stop())}pauseAnimation(){er(t=>{var n;return(n=this.getAxisMotionValue(t).animation)==null?void 0:n.pause()})}getAnimationState(t){var n;return(n=this.getAxisMotionValue(t).animation)==null?void 0:n.state}getAxisMotionValue(t){const n=`_drag${t.toUpperCase()}`,r=this.visualElement.getProps(),i=r[n];return i||this.visualElement.getValue(t,(r.initial?r.initial[t]:void 0)||0)}snapToCursor(t){er(n=>{const{drag:r}=this.getProps();if(!Uu(n,r,this.currentDirection))return;const{projection:i}=this.visualElement,a=this.getAxisMotionValue(n);if(i&&i.layout){const{min:s,max:o}=i.layout.layoutBox[n];a.set(t[n]-bt(s,o,.5))}})}scalePositionWithinConstraints(){if(!this.visualElement.current)return;const{drag:t,dragConstraints:n}=this.getProps(),{projection:r}=this.visualElement;if(!Ks(n)||!r||!this.constraints)return;this.stopAnimation();const i={x:0,y:0};er(s=>{const o=this.getAxisMotionValue(s);if(o&&this.constraints!==!1){const l=o.get();i[s]=RM({min:l,max:l},this.constraints[s])}});const{transformTemplate:a}=this.visualElement.getProps();this.visualElement.current.style.transform=a?a({},""):"none",r.root&&r.root.updateScroll(),r.updateLayout(),this.resolveConstraints(),er(s=>{if(!Uu(s,t,null))return;const o=this.getAxisMotionValue(s),{min:l,max:c}=this.constraints[s];o.set(bt(l,c,i[s]))})}addListeners(){if(!this.visualElement.current)return;BM.set(this.visualElement,this);const t=this.visualElement.current,n=zl(t,"pointerdown",l=>{const{drag:c,dragListener:u=!0}=this.getProps();c&&u&&this.start(l)}),r=()=>{const{dragConstraints:l}=this.getProps();Ks(l)&&l.current&&(this.constraints=this.resolveRefConstraints())},{projection:i}=this.visualElement,a=i.addEventListener("measure",r);i&&!i.layout&&(i.root&&i.root.updateScroll(),i.updateLayout()),xt.read(r);const s=Sc(window,"resize",()=>this.scalePositionWithinConstraints()),o=i.addEventListener("didUpdate",({delta:l,hasLayoutChanged:c})=>{this.isDragging&&c&&(er(u=>{const d=this.getAxisMotionValue(u);d&&(this.originPoint[u]+=l[u].translate,d.set(d.get()+l[u].translate))}),this.visualElement.render())});return()=>{s(),n(),a(),o&&o()}}getProps(){const t=this.visualElement.getProps(),{drag:n=!1,dragDirectionLock:r=!1,dragPropagation:i=!1,dragConstraints:a=!1,dragElastic:s=Pf,dragMomentum:o=!0}=t;return{...t,drag:n,dragDirectionLock:r,dragPropagation:i,dragConstraints:a,dragElastic:s,dragMomentum:o}}}function Uu(e,t,n){return(t===!0||t===e)&&(n===null||n===e)}function jM(e,t=10){let n=null;return Math.abs(e.y)>t?n="y":Math.abs(e.x)>t&&(n="x"),n}class VM extends Ra{constructor(t){super(t),this.removeGroupControls=ur,this.removeListeners=ur,this.controls=new FM(t)}mount(){const{dragControls:t}=this.node.getProps();t&&(this.removeGroupControls=t.subscribe(this.controls)),this.removeListeners=this.controls.addListeners()||ur}unmount(){this.removeGroupControls(),this.removeListeners()}}const Q4=e=>(t,n)=>{e&&xt.postRender(()=>e(t,n))};class HM extends Ra{constructor(){super(...arguments),this.removePointerDownListener=ur}onPointerDown(t){this.session=new i7(t,this.createPanHandlers(),{transformPagePoint:this.node.getTransformPagePoint(),contextWindow:r7(this.node)})}createPanHandlers(){const{onPanSessionStart:t,onPanStart:n,onPan:r,onPanEnd:i}=this.node.getProps();return{onSessionStart:Q4(t),onStart:Q4(n),onMove:r,onEnd:(a,s)=>{delete this.session,i&&xt.postRender(()=>i(a,s))}}}mount(){this.removePointerDownListener=zl(this.node.current,"pointerdown",t=>this.onPointerDown(t))}update(){this.session&&this.session.updateHandlers(this.createPanHandlers())}unmount(){this.removePointerDownListener(),this.session&&this.session.end()}}const bd={hasAnimatedSinceResize:!0,hasEverUpdated:!1};function Z4(e,t){return t.max===t.min?0:e/(t.max-t.min)*100}const hl={correct:(e,t)=>{if(!t.target)return e;if(typeof e=="string")if(Se.test(e))e=parseFloat(e);else return e;const n=Z4(e,t.target.x),r=Z4(e,t.target.y);return`${n}% ${r}%`}},qM={correct:(e,{treeScale:t,projectionDelta:n})=>{const r=e,i=Aa.parse(e);if(i.length>5)return r;const a=Aa.createTransformer(e),s=typeof i[0]!="number"?1:0,o=n.x.scale*t.x,l=n.y.scale*t.y;i[0+s]/=o,i[1+s]/=l;const c=bt(o,l,.5);return typeof i[2+s]=="number"&&(i[2+s]/=c),typeof i[3+s]=="number"&&(i[3+s]/=c),a(i)}};class GM extends T.Component{componentDidMount(){const{visualElement:t,layoutGroup:n,switchLayoutGroup:r,layoutId:i}=this.props,{projection:a}=t;AD(WM),a&&(n.group&&n.group.add(a),r&&r.register&&i&&r.register(a),a.root.didUpdate(),a.addEventListener("animationComplete",()=>{this.safeToRemove()}),a.setOptions({...a.options,onExitComplete:()=>this.safeToRemove()})),bd.hasEverUpdated=!0}getSnapshotBeforeUpdate(t){const{layoutDependency:n,visualElement:r,drag:i,isPresent:a}=this.props,{projection:s}=r;return s&&(s.isPresent=a,i||t.layoutDependency!==n||n===void 0||t.isPresent!==a?s.willUpdate():this.safeToRemove(),t.isPresent!==a&&(a?s.promote():s.relegate()||xt.postRender(()=>{const o=s.getStack();(!o||!o.members.length)&&this.safeToRemove()}))),null}componentDidUpdate(){const{projection:t}=this.props.visualElement;t&&(t.root.didUpdate(),Z1.postRender(()=>{!t.currentAnimation&&t.isLead()&&this.safeToRemove()}))}componentWillUnmount(){const{visualElement:t,layoutGroup:n,switchLayoutGroup:r}=this.props,{projection:i}=t;i&&(i.scheduleCheckAfterUnmount(),n&&n.group&&n.group.remove(i),r&&r.deregister&&r.deregister(i))}safeToRemove(){const{safeToRemove:t}=this.props;t&&t()}render(){return null}}function s7(e){const[t,n]=I9(),r=T.useContext(A1);return h.jsx(GM,{...e,layoutGroup:r,switchLayoutGroup:T.useContext(O9),isPresent:t,safeToRemove:n})}const WM={borderRadius:{...hl,applyTo:["borderTopLeftRadius","borderTopRightRadius","borderBottomLeftRadius","borderBottomRightRadius"]},borderTopLeftRadius:hl,borderTopRightRadius:hl,borderBottomLeftRadius:hl,borderBottomRightRadius:hl,boxShadow:qM};function KM(e,t,n){const r=rn(e)?e:Po(e);return r.start(c2("",r,t,n)),r.animation}const YM=(e,t)=>e.depth-t.depth;class XM{constructor(){this.children=[],this.isDirty=!1}add(t){I1(this.children,t),this.isDirty=!0}remove(t){U1(this.children,t),this.isDirty=!0}forEach(t){this.isDirty&&this.children.sort(YM),this.isDirty=!1,this.children.forEach(t)}}function QM(e,t){const n=Sn.now(),r=({timestamp:i})=>{const a=i-n;a>=t&&(Ca(r),e(a-t))};return xt.setup(r,!0),()=>Ca(r)}const o7=["TopLeft","TopRight","BottomLeft","BottomRight"],ZM=o7.length,J4=e=>typeof e=="string"?parseFloat(e):e,ex=e=>typeof e=="number"||Se.test(e);function JM(e,t,n,r,i,a){i?(e.opacity=bt(0,n.opacity??1,eR(r)),e.opacityExit=bt(t.opacity??1,0,tR(r))):a&&(e.opacity=bt(t.opacity??1,n.opacity??1,r));for(let s=0;s<ZM;s++){const o=`border${o7[s]}Radius`;let l=tx(t,o),c=tx(n,o);if(l===void 0&&c===void 0)continue;l||(l=0),c||(c=0),l===0||c===0||ex(l)===ex(c)?(e[o]=Math.max(bt(J4(l),J4(c),r),0),(si.test(c)||si.test(l))&&(e[o]+="%")):e[o]=c}(t.rotate||n.rotate)&&(e.rotate=bt(t.rotate||0,n.rotate||0,r))}function tx(e,t){return e[t]!==void 0?e[t]:e.borderRadius}const eR=l7(0,.5,e9),tR=l7(.5,.95,ur);function l7(e,t,n){return r=>r<e?0:r>t?1:n(yc(e,t,r))}function nx(e,t){e.min=t.min,e.max=t.max}function Jn(e,t){nx(e.x,t.x),nx(e.y,t.y)}function rx(e,t){e.translate=t.translate,e.scale=t.scale,e.originPoint=t.originPoint,e.origin=t.origin}function ix(e,t,n,r,i){return e-=t,e=l0(e,1/n,r),i!==void 0&&(e=l0(e,1/i,r)),e}function nR(e,t=0,n=1,r=.5,i,a=e,s=e){if(si.test(t)&&(t=parseFloat(t),t=bt(s.min,s.max,t/100)-s.min),typeof t!="number")return;let o=bt(a.min,a.max,r);e===a&&(o-=t),e.min=ix(e.min,t,n,o,i),e.max=ix(e.max,t,n,o,i)}function ax(e,t,[n,r,i],a,s){nR(e,t[n],t[r],t[i],t.scale,a,s)}const rR=["x","scaleX","originX"],iR=["y","scaleY","originY"];function sx(e,t,n,r){ax(e.x,t,rR,n?n.x:void 0,r?r.x:void 0),ax(e.y,t,iR,n?n.y:void 0,r?r.y:void 0)}function ox(e){return e.translate===0&&e.scale===1}function c7(e){return ox(e.x)&&ox(e.y)}function lx(e,t){return e.min===t.min&&e.max===t.max}function aR(e,t){return lx(e.x,t.x)&&lx(e.y,t.y)}function cx(e,t){return Math.round(e.min)===Math.round(t.min)&&Math.round(e.max)===Math.round(t.max)}function u7(e,t){return cx(e.x,t.x)&&cx(e.y,t.y)}function ux(e){return hn(e.x)/hn(e.y)}function dx(e,t){return e.translate===t.translate&&e.scale===t.scale&&e.originPoint===t.originPoint}class sR{constructor(){this.members=[]}add(t){I1(this.members,t),t.scheduleRender()}remove(t){if(U1(this.members,t),t===this.prevLead&&(this.prevLead=void 0),t===this.lead){const n=this.members[this.members.length-1];n&&this.promote(n)}}relegate(t){const n=this.members.findIndex(i=>t===i);if(n===0)return!1;let r;for(let i=n;i>=0;i--){const a=this.members[i];if(a.isPresent!==!1){r=a;break}}return r?(this.promote(r),!0):!1}promote(t,n){const r=this.lead;if(t!==r&&(this.prevLead=r,this.lead=t,t.show(),r)){r.instance&&r.scheduleRender(),t.scheduleRender(),t.resumeFrom=r,n&&(t.resumeFrom.preserveOpacity=!0),r.snapshot&&(t.snapshot=r.snapshot,t.snapshot.latestValues=r.animationValues||r.latestValues),t.root&&t.root.isUpdating&&(t.isLayoutDirty=!0);const{crossfade:i}=t.options;i===!1&&r.hide()}}exitAnimationComplete(){this.members.forEach(t=>{const{options:n,resumingFrom:r}=t;n.onExitComplete&&n.onExitComplete(),r&&r.options.onExitComplete&&r.options.onExitComplete()})}scheduleRender(){this.members.forEach(t=>{t.instance&&t.scheduleRender(!1)})}removeLeadSnapshot(){this.lead&&this.lead.snapshot&&(this.lead.snapshot=void 0)}}function oR(e,t,n){let r="";const i=e.x.translate/t.x,a=e.y.translate/t.y,s=(n==null?void 0:n.z)||0;if((i||a||s)&&(r=`translate3d(${i}px, ${a}px, ${s}px) `),(t.x!==1||t.y!==1)&&(r+=`scale(${1/t.x}, ${1/t.y}) `),n){const{transformPerspective:c,rotate:u,rotateX:d,rotateY:m,skewX:p,skewY:x}=n;c&&(r=`perspective(${c}px) ${r}`),u&&(r+=`rotate(${u}deg) `),d&&(r+=`rotateX(${d}deg) `),m&&(r+=`rotateY(${m}deg) `),p&&(r+=`skewX(${p}deg) `),x&&(r+=`skewY(${x}deg) `)}const o=e.x.scale*t.x,l=e.y.scale*t.y;return(o!==1||l!==1)&&(r+=`scale(${o}, ${l})`),r||"none"}const xh=["","X","Y","Z"],lR={visibility:"hidden"},cR=1e3;let uR=0;function $h(e,t,n,r){const{latestValues:i}=t;i[e]&&(n[e]=i[e],t.setStaticValue(e,0),r&&(r[e]=0))}function d7(e){if(e.hasCheckedOptimisedAppear=!0,e.root===e)return;const{visualElement:t}=e.options;if(!t)return;const n=W9(t);if(window.MotionHasOptimisedAnimation(n,"transform")){const{layout:i,layoutId:a}=e.options;window.MotionCancelOptimisedAnimation(n,"transform",xt,!(i||a))}const{parent:r}=e;r&&!r.hasCheckedOptimisedAppear&&d7(r)}function m7({attachResizeListener:e,defaultParent:t,measureScroll:n,checkIsScrollRoot:r,resetTransform:i}){return class{constructor(s={},o=t==null?void 0:t()){this.id=uR++,this.animationId=0,this.children=new Set,this.options={},this.isTreeAnimating=!1,this.isAnimationBlocked=!1,this.isLayoutDirty=!1,this.isProjectionDirty=!1,this.isSharedProjectionDirty=!1,this.isTransformDirty=!1,this.updateManuallyBlocked=!1,this.updateBlockedByResize=!1,this.isUpdating=!1,this.isSVG=!1,this.needsReset=!1,this.shouldResetTransform=!1,this.hasCheckedOptimisedAppear=!1,this.treeScale={x:1,y:1},this.eventHandlers=new Map,this.hasTreeAnimated=!1,this.updateScheduled=!1,this.scheduleUpdate=()=>this.update(),this.projectionUpdateScheduled=!1,this.checkUpdateFailed=()=>{this.isUpdating&&(this.isUpdating=!1,this.clearAllSnapshots())},this.updateProjection=()=>{this.projectionUpdateScheduled=!1,this.nodes.forEach(hR),this.nodes.forEach(vR),this.nodes.forEach(xR),this.nodes.forEach(pR)},this.resolvedRelativeTargetAt=0,this.hasProjected=!1,this.isVisible=!0,this.animationProgress=0,this.sharedNodes=new Map,this.latestValues=s,this.root=o?o.root||o:this,this.path=o?[...o.path,o]:[],this.parent=o,this.depth=o?o.depth+1:0;for(let l=0;l<this.path.length;l++)this.path[l].shouldResetTransform=!0;this.root===this&&(this.nodes=new XM)}addEventListener(s,o){return this.eventHandlers.has(s)||this.eventHandlers.set(s,new M1),this.eventHandlers.get(s).add(o)}notifyListeners(s,...o){const l=this.eventHandlers.get(s);l&&l.notify(...o)}hasListeners(s){return this.eventHandlers.has(s)}mount(s){if(this.instance)return;this.isSVG=P9(s)&&!sD(s),this.instance=s;const{layoutId:o,layout:l,visualElement:c}=this.options;if(c&&!c.current&&c.mount(s),this.root.nodes.add(this),this.parent&&this.parent.children.add(this),this.root.hasTreeAnimated&&(l||o)&&(this.isLayoutDirty=!0),e){let u;const d=()=>this.root.updateBlockedByResize=!1;e(s,()=>{this.root.updateBlockedByResize=!0,u&&u(),u=QM(d,250),bd.hasAnimatedSinceResize&&(bd.hasAnimatedSinceResize=!1,this.nodes.forEach(hx))})}o&&this.root.registerSharedNode(o,this),this.options.animate!==!1&&c&&(o||l)&&this.addEventListener("didUpdate",({delta:u,hasLayoutChanged:d,hasRelativeLayoutChanged:m,layout:p})=>{if(this.isTreeAnimationBlocked()){this.target=void 0,this.relativeTarget=void 0;return}const x=this.options.transition||c.getDefaultTransition()||TR,{onLayoutAnimationStart:g,onLayoutAnimationComplete:w}=c.getProps(),v=!this.targetLayout||!u7(this.targetLayout,p),$=!d&&m;if(this.options.layoutRoot||this.resumeFrom||$||d&&(v||!this.currentAnimation)){this.resumeFrom&&(this.resumingFrom=this.resumeFrom,this.resumingFrom.resumingFrom=void 0);const _={...X1(x,"layout"),onPlay:g,onComplete:w};(c.shouldReduceMotion||this.options.layoutRoot)&&(_.delay=0,_.type=!1),this.startAnimation(_),this.setAnimationOrigin(u,$)}else d||hx(this),this.isLead()&&this.options.onExitComplete&&this.options.onExitComplete();this.targetLayout=p})}unmount(){this.options.layoutId&&this.willUpdate(),this.root.nodes.remove(this);const s=this.getStack();s&&s.remove(this),this.parent&&this.parent.children.delete(this),this.instance=void 0,this.eventHandlers.clear(),Ca(this.updateProjection)}blockUpdate(){this.updateManuallyBlocked=!0}unblockUpdate(){this.updateManuallyBlocked=!1}isUpdateBlocked(){return this.updateManuallyBlocked||this.updateBlockedByResize}isTreeAnimationBlocked(){return this.isAnimationBlocked||this.parent&&this.parent.isTreeAnimationBlocked()||!1}startUpdate(){this.isUpdateBlocked()||(this.isUpdating=!0,this.nodes&&this.nodes.forEach($R),this.animationId++)}getTransformTemplate(){const{visualElement:s}=this.options;return s&&s.getProps().transformTemplate}willUpdate(s=!0){if(this.root.hasTreeAnimated=!0,this.root.isUpdateBlocked()){this.options.onExitComplete&&this.options.onExitComplete();return}if(window.MotionCancelOptimisedAnimation&&!this.hasCheckedOptimisedAppear&&d7(this),!this.root.isUpdating&&this.root.startUpdate(),this.isLayoutDirty)return;this.isLayoutDirty=!0;for(let u=0;u<this.path.length;u++){const d=this.path[u];d.shouldResetTransform=!0,d.updateScroll("snapshot"),d.options.layoutRoot&&d.willUpdate(!1)}const{layoutId:o,layout:l}=this.options;if(o===void 0&&!l)return;const c=this.getTransformTemplate();this.prevTransformTemplateValue=c?c(this.latestValues,""):void 0,this.updateSnapshot(),s&&this.notifyListeners("willUpdate")}update(){if(this.updateScheduled=!1,this.isUpdateBlocked()){this.unblockUpdate(),this.clearAllSnapshots(),this.nodes.forEach(mx);return}this.isUpdating||this.nodes.forEach(gR),this.isUpdating=!1,this.nodes.forEach(bR),this.nodes.forEach(dR),this.nodes.forEach(mR),this.clearAllSnapshots();const o=Sn.now();Wt.delta=Di(0,1e3/60,o-Wt.timestamp),Wt.timestamp=o,Wt.isProcessing=!0,uh.update.process(Wt),uh.preRender.process(Wt),uh.render.process(Wt),Wt.isProcessing=!1}didUpdate(){this.updateScheduled||(this.updateScheduled=!0,Z1.read(this.scheduleUpdate))}clearAllSnapshots(){this.nodes.forEach(fR),this.sharedNodes.forEach(yR)}scheduleUpdateProjection(){this.projectionUpdateScheduled||(this.projectionUpdateScheduled=!0,xt.preRender(this.updateProjection,!1,!0))}scheduleCheckAfterUnmount(){xt.postRender(()=>{this.isLayoutDirty?this.root.didUpdate():this.root.checkUpdateFailed()})}updateSnapshot(){this.snapshot||!this.instance||(this.snapshot=this.measure(),this.snapshot&&!hn(this.snapshot.measuredBox.x)&&!hn(this.snapshot.measuredBox.y)&&(this.snapshot=void 0))}updateLayout(){if(!this.instance||(this.updateScroll(),!(this.options.alwaysMeasureLayout&&this.isLead())&&!this.isLayoutDirty))return;if(this.resumeFrom&&!this.resumeFrom.instance)for(let l=0;l<this.path.length;l++)this.path[l].updateScroll();const s=this.layout;this.layout=this.measure(!1),this.layoutCorrected=St(),this.isLayoutDirty=!1,this.projectionDelta=void 0,this.notifyListeners("measure",this.layout.layoutBox);const{visualElement:o}=this.options;o&&o.notify("LayoutMeasure",this.layout.layoutBox,s?s.layoutBox:void 0)}updateScroll(s="measure"){let o=!!(this.options.layoutScroll&&this.instance);if(this.scroll&&this.scroll.animationId===this.root.animationId&&this.scroll.phase===s&&(o=!1),o&&this.instance){const l=r(this.instance);this.scroll={animationId:this.root.animationId,phase:s,isRoot:l,offset:n(this.instance),wasRoot:this.scroll?this.scroll.isRoot:l}}}resetTransform(){if(!i)return;const s=this.isLayoutDirty||this.shouldResetTransform||this.options.alwaysMeasureLayout,o=this.projectionDelta&&!c7(this.projectionDelta),l=this.getTransformTemplate(),c=l?l(this.latestValues,""):void 0,u=c!==this.prevTransformTemplateValue;s&&this.instance&&(o||Ka(this.latestValues)||u)&&(i(this.instance,c),this.shouldResetTransform=!1,this.scheduleRender())}measure(s=!0){const o=this.measurePageBox();let l=this.removeElementScroll(o);return s&&(l=this.removeTransform(l)),ER(l),{animationId:this.root.animationId,measuredBox:o,layoutBox:l,latestValues:{},source:this.id}}measurePageBox(){var c;const{visualElement:s}=this.options;if(!s)return St();const o=s.measureViewportBox();if(!(((c=this.scroll)==null?void 0:c.wasRoot)||this.path.some(kR))){const{scroll:u}=this.root;u&&(Xs(o.x,u.offset.x),Xs(o.y,u.offset.y))}return o}removeElementScroll(s){var l;const o=St();if(Jn(o,s),(l=this.scroll)!=null&&l.wasRoot)return o;for(let c=0;c<this.path.length;c++){const u=this.path[c],{scroll:d,options:m}=u;u!==this.root&&d&&m.layoutScroll&&(d.wasRoot&&Jn(o,s),Xs(o.x,d.offset.x),Xs(o.y,d.offset.y))}return o}applyTransform(s,o=!1){const l=St();Jn(l,s);for(let c=0;c<this.path.length;c++){const u=this.path[c];!o&&u.options.layoutScroll&&u.scroll&&u!==u.root&&Qs(l,{x:-u.scroll.offset.x,y:-u.scroll.offset.y}),Ka(u.latestValues)&&Qs(l,u.latestValues)}return Ka(this.latestValues)&&Qs(l,this.latestValues),l}removeTransform(s){const o=St();Jn(o,s);for(let l=0;l<this.path.length;l++){const c=this.path[l];if(!c.instance||!Ka(c.latestValues))continue;Af(c.latestValues)&&c.updateSnapshot();const u=St(),d=c.measurePageBox();Jn(u,d),sx(o,c.latestValues,c.snapshot?c.snapshot.layoutBox:void 0,u)}return Ka(this.latestValues)&&sx(o,this.latestValues),o}setTargetDelta(s){this.targetDelta=s,this.root.scheduleUpdateProjection(),this.isProjectionDirty=!0}setOptions(s){this.options={...this.options,...s,crossfade:s.crossfade!==void 0?s.crossfade:!0}}clearMeasurements(){this.scroll=void 0,this.layout=void 0,this.snapshot=void 0,this.prevTransformTemplateValue=void 0,this.targetDelta=void 0,this.target=void 0,this.isLayoutDirty=!1}forceRelativeParentToResolveTarget(){this.relativeParent&&this.relativeParent.resolvedRelativeTargetAt!==Wt.timestamp&&this.relativeParent.resolveTargetDelta(!0)}resolveTargetDelta(s=!1){var m;const o=this.getLead();this.isProjectionDirty||(this.isProjectionDirty=o.isProjectionDirty),this.isTransformDirty||(this.isTransformDirty=o.isTransformDirty),this.isSharedProjectionDirty||(this.isSharedProjectionDirty=o.isSharedProjectionDirty);const l=!!this.resumingFrom||this!==o;if(!(s||l&&this.isSharedProjectionDirty||this.isProjectionDirty||(m=this.parent)!=null&&m.isProjectionDirty||this.attemptToResolveRelativeTarget||this.root.updateBlockedByResize))return;const{layout:u,layoutId:d}=this.options;if(!(!this.layout||!(u||d))){if(this.resolvedRelativeTargetAt=Wt.timestamp,!this.targetDelta&&!this.relativeTarget){const p=this.getClosestProjectingParent();p&&p.layout&&this.animationProgress!==1?(this.relativeParent=p,this.forceRelativeParentToResolveTarget(),this.relativeTarget=St(),this.relativeTargetOrigin=St(),Fl(this.relativeTargetOrigin,this.layout.layoutBox,p.layout.layoutBox),Jn(this.relativeTarget,this.relativeTargetOrigin)):this.relativeParent=this.relativeTarget=void 0}if(!(!this.relativeTarget&&!this.targetDelta)&&(this.target||(this.target=St(),this.targetWithTransforms=St()),this.relativeTarget&&this.relativeTargetOrigin&&this.relativeParent&&this.relativeParent.target?(this.forceRelativeParentToResolveTarget(),NM(this.target,this.relativeTarget,this.relativeParent.target)):this.targetDelta?(this.resumingFrom?this.target=this.applyTransform(this.layout.layoutBox):Jn(this.target,this.layout.layoutBox),t7(this.target,this.targetDelta)):Jn(this.target,this.layout.layoutBox),this.attemptToResolveRelativeTarget)){this.attemptToResolveRelativeTarget=!1;const p=this.getClosestProjectingParent();p&&!!p.resumingFrom==!!this.resumingFrom&&!p.options.layoutScroll&&p.target&&this.animationProgress!==1?(this.relativeParent=p,this.forceRelativeParentToResolveTarget(),this.relativeTarget=St(),this.relativeTargetOrigin=St(),Fl(this.relativeTargetOrigin,this.target,p.target),Jn(this.relativeTarget,this.relativeTargetOrigin)):this.relativeParent=this.relativeTarget=void 0}}}getClosestProjectingParent(){if(!(!this.parent||Af(this.parent.latestValues)||e7(this.parent.latestValues)))return this.parent.isProjecting()?this.parent:this.parent.getClosestProjectingParent()}isProjecting(){return!!((this.relativeTarget||this.targetDelta||this.options.layoutRoot)&&this.layout)}calcProjection(){var x;const s=this.getLead(),o=!!this.resumingFrom||this!==s;let l=!0;if((this.isProjectionDirty||(x=this.parent)!=null&&x.isProjectionDirty)&&(l=!1),o&&(this.isSharedProjectionDirty||this.isTransformDirty)&&(l=!1),this.resolvedRelativeTargetAt===Wt.timestamp&&(l=!1),l)return;const{layout:c,layoutId:u}=this.options;if(this.isTreeAnimating=!!(this.parent&&this.parent.isTreeAnimating||this.currentAnimation||this.pendingAnimation),this.isTreeAnimating||(this.targetDelta=this.relativeTarget=void 0),!this.layout||!(c||u))return;Jn(this.layoutCorrected,this.layout.layoutBox);const d=this.treeScale.x,m=this.treeScale.y;CM(this.layoutCorrected,this.treeScale,this.path,o),s.layout&&!s.target&&(this.treeScale.x!==1||this.treeScale.y!==1)&&(s.target=s.layout.layoutBox,s.targetWithTransforms=St());const{target:p}=s;if(!p){this.prevProjectionDelta&&(this.createProjectionDeltas(),this.scheduleRender());return}!this.projectionDelta||!this.prevProjectionDelta?this.createProjectionDeltas():(rx(this.prevProjectionDelta.x,this.projectionDelta.x),rx(this.prevProjectionDelta.y,this.projectionDelta.y)),Bl(this.projectionDelta,this.layoutCorrected,p,this.latestValues),(this.treeScale.x!==d||this.treeScale.y!==m||!dx(this.projectionDelta.x,this.prevProjectionDelta.x)||!dx(this.projectionDelta.y,this.prevProjectionDelta.y))&&(this.hasProjected=!0,this.scheduleRender(),this.notifyListeners("projectionUpdate",p))}hide(){this.isVisible=!1}show(){this.isVisible=!0}scheduleRender(s=!0){var o;if((o=this.options.visualElement)==null||o.scheduleRender(),s){const l=this.getStack();l&&l.scheduleRender()}this.resumingFrom&&!this.resumingFrom.instance&&(this.resumingFrom=void 0)}createProjectionDeltas(){this.prevProjectionDelta=Ys(),this.projectionDelta=Ys(),this.projectionDeltaWithTransform=Ys()}setAnimationOrigin(s,o=!1){const l=this.snapshot,c=l?l.latestValues:{},u={...this.latestValues},d=Ys();(!this.relativeParent||!this.relativeParent.options.layoutRoot)&&(this.relativeTarget=this.relativeTargetOrigin=void 0),this.attemptToResolveRelativeTarget=!o;const m=St(),p=l?l.source:void 0,x=this.layout?this.layout.source:void 0,g=p!==x,w=this.getStack(),v=!w||w.members.length<=1,$=!!(g&&!v&&this.options.crossfade===!0&&!this.path.some(wR));this.animationProgress=0;let _;this.mixTargetDelta=C=>{const k=C/1e3;px(d.x,s.x,k),px(d.y,s.y,k),this.setTargetDelta(d),this.relativeTarget&&this.relativeTargetOrigin&&this.layout&&this.relativeParent&&this.relativeParent.layout&&(Fl(m,this.layout.layoutBox,this.relativeParent.layout.layoutBox),_R(this.relativeTarget,this.relativeTargetOrigin,m,k),_&&aR(this.relativeTarget,_)&&(this.isProjectionDirty=!1),_||(_=St()),Jn(_,this.relativeTarget)),g&&(this.animationValues=u,JM(u,c,this.latestValues,k,$,v)),this.root.scheduleUpdateProjection(),this.scheduleRender(),this.animationProgress=k},this.mixTargetDelta(this.options.layoutRoot?1e3:0)}startAnimation(s){var o,l,c;this.notifyListeners("animationStart"),(o=this.currentAnimation)==null||o.stop(),(c=(l=this.resumingFrom)==null?void 0:l.currentAnimation)==null||c.stop(),this.pendingAnimation&&(Ca(this.pendingAnimation),this.pendingAnimation=void 0),this.pendingAnimation=xt.update(()=>{bd.hasAnimatedSinceResize=!0,this.motionValue||(this.motionValue=Po(0)),this.currentAnimation=KM(this.motionValue,[0,1e3],{...s,isSync:!0,onUpdate:u=>{this.mixTargetDelta(u),s.onUpdate&&s.onUpdate(u)},onStop:()=>{},onComplete:()=>{s.onComplete&&s.onComplete(),this.completeAnimation()}}),this.resumingFrom&&(this.resumingFrom.currentAnimation=this.currentAnimation),this.pendingAnimation=void 0})}completeAnimation(){this.resumingFrom&&(this.resumingFrom.currentAnimation=void 0,this.resumingFrom.preserveOpacity=void 0);const s=this.getStack();s&&s.exitAnimationComplete(),this.resumingFrom=this.currentAnimation=this.animationValues=void 0,this.notifyListeners("animationComplete")}finishAnimation(){this.currentAnimation&&(this.mixTargetDelta&&this.mixTargetDelta(cR),this.currentAnimation.stop()),this.completeAnimation()}applyTransformsToTarget(){const s=this.getLead();let{targetWithTransforms:o,target:l,layout:c,latestValues:u}=s;if(!(!o||!l||!c)){if(this!==s&&this.layout&&c&&h7(this.options.animationType,this.layout.layoutBox,c.layoutBox)){l=this.target||St();const d=hn(this.layout.layoutBox.x);l.x.min=s.target.x.min,l.x.max=l.x.min+d;const m=hn(this.layout.layoutBox.y);l.y.min=s.target.y.min,l.y.max=l.y.min+m}Jn(o,l),Qs(o,u),Bl(this.projectionDeltaWithTransform,this.layoutCorrected,o,u)}}registerSharedNode(s,o){this.sharedNodes.has(s)||this.sharedNodes.set(s,new sR),this.sharedNodes.get(s).add(o);const c=o.options.initialPromotionConfig;o.promote({transition:c?c.transition:void 0,preserveFollowOpacity:c&&c.shouldPreserveFollowOpacity?c.shouldPreserveFollowOpacity(o):void 0})}isLead(){const s=this.getStack();return s?s.lead===this:!0}getLead(){var o;const{layoutId:s}=this.options;return s?((o=this.getStack())==null?void 0:o.lead)||this:this}getPrevLead(){var o;const{layoutId:s}=this.options;return s?(o=this.getStack())==null?void 0:o.prevLead:void 0}getStack(){const{layoutId:s}=this.options;if(s)return this.root.sharedNodes.get(s)}promote({needsReset:s,transition:o,preserveFollowOpacity:l}={}){const c=this.getStack();c&&c.promote(this,l),s&&(this.projectionDelta=void 0,this.needsReset=!0),o&&this.setOptions({transition:o})}relegate(){const s=this.getStack();return s?s.relegate(this):!1}resetSkewAndRotation(){const{visualElement:s}=this.options;if(!s)return;let o=!1;const{latestValues:l}=s;if((l.z||l.rotate||l.rotateX||l.rotateY||l.rotateZ||l.skewX||l.skewY)&&(o=!0),!o)return;const c={};l.z&&$h("z",s,c,this.animationValues);for(let u=0;u<xh.length;u++)$h(`rotate${xh[u]}`,s,c,this.animationValues),$h(`skew${xh[u]}`,s,c,this.animationValues);s.render();for(const u in c)s.setStaticValue(u,c[u]),this.animationValues&&(this.animationValues[u]=c[u]);s.scheduleRender()}getProjectionStyles(s){if(!this.instance||this.isSVG)return;if(!this.isVisible)return lR;const o={visibility:""},l=this.getTransformTemplate();if(this.needsReset)return this.needsReset=!1,o.opacity="",o.pointerEvents=gd(s==null?void 0:s.pointerEvents)||"",o.transform=l?l(this.latestValues,""):"none",o;const c=this.getLead();if(!this.projectionDelta||!this.layout||!c.target){const p={};return this.options.layoutId&&(p.opacity=this.latestValues.opacity!==void 0?this.latestValues.opacity:1,p.pointerEvents=gd(s==null?void 0:s.pointerEvents)||""),this.hasProjected&&!Ka(this.latestValues)&&(p.transform=l?l({},""):"none",this.hasProjected=!1),p}const u=c.animationValues||c.latestValues;this.applyTransformsToTarget(),o.transform=oR(this.projectionDeltaWithTransform,this.treeScale,u),l&&(o.transform=l(u,o.transform));const{x:d,y:m}=this.projectionDelta;o.transformOrigin=`${d.origin*100}% ${m.origin*100}% 0`,c.animationValues?o.opacity=c===this?u.opacity??this.latestValues.opacity??1:this.preserveOpacity?this.latestValues.opacity:u.opacityExit:o.opacity=c===this?u.opacity!==void 0?u.opacity:"":u.opacityExit!==void 0?u.opacityExit:0;for(const p in Ec){if(u[p]===void 0)continue;const{correct:x,applyTo:g,isCSSVariable:w}=Ec[p],v=o.transform==="none"?u[p]:x(u[p],c);if(g){const $=g.length;for(let _=0;_<$;_++)o[g[_]]=v}else w?this.options.visualElement.renderState.vars[p]=v:o[p]=v}return this.options.layoutId&&(o.pointerEvents=c===this?gd(s==null?void 0:s.pointerEvents)||"":"none"),o}clearSnapshot(){this.resumeFrom=this.snapshot=void 0}resetTree(){this.root.nodes.forEach(s=>{var o;return(o=s.currentAnimation)==null?void 0:o.stop()}),this.root.nodes.forEach(mx),this.root.sharedNodes.clear()}}}function dR(e){e.updateLayout()}function mR(e){var n;const t=((n=e.resumeFrom)==null?void 0:n.snapshot)||e.snapshot;if(e.isLead()&&e.layout&&t&&e.hasListeners("didUpdate")){const{layoutBox:r,measuredBox:i}=e.layout,{animationType:a}=e.options,s=t.source!==e.layout.source;a==="size"?er(d=>{const m=s?t.measuredBox[d]:t.layoutBox[d],p=hn(m);m.min=r[d].min,m.max=m.min+p}):h7(a,t.layoutBox,r)&&er(d=>{const m=s?t.measuredBox[d]:t.layoutBox[d],p=hn(r[d]);m.max=m.min+p,e.relativeTarget&&!e.currentAnimation&&(e.isProjectionDirty=!0,e.relativeTarget[d].max=e.relativeTarget[d].min+p)});const o=Ys();Bl(o,r,t.layoutBox);const l=Ys();s?Bl(l,e.applyTransform(i,!0),t.measuredBox):Bl(l,r,t.layoutBox);const c=!c7(o);let u=!1;if(!e.resumeFrom){const d=e.getClosestProjectingParent();if(d&&!d.resumeFrom){const{snapshot:m,layout:p}=d;if(m&&p){const x=St();Fl(x,t.layoutBox,m.layoutBox);const g=St();Fl(g,r,p.layoutBox),u7(x,g)||(u=!0),d.options.layoutRoot&&(e.relativeTarget=g,e.relativeTargetOrigin=x,e.relativeParent=d)}}}e.notifyListeners("didUpdate",{layout:r,snapshot:t,delta:l,layoutDelta:o,hasLayoutChanged:c,hasRelativeLayoutChanged:u})}else if(e.isLead()){const{onExitComplete:r}=e.options;r&&r()}e.options.transition=void 0}function hR(e){e.parent&&(e.isProjecting()||(e.isProjectionDirty=e.parent.isProjectionDirty),e.isSharedProjectionDirty||(e.isSharedProjectionDirty=!!(e.isProjectionDirty||e.parent.isProjectionDirty||e.parent.isSharedProjectionDirty)),e.isTransformDirty||(e.isTransformDirty=e.parent.isTransformDirty))}function pR(e){e.isProjectionDirty=e.isSharedProjectionDirty=e.isTransformDirty=!1}function fR(e){e.clearSnapshot()}function mx(e){e.clearMeasurements()}function gR(e){e.isLayoutDirty=!1}function bR(e){const{visualElement:t}=e.options;t&&t.getProps().onBeforeLayoutMeasure&&t.notify("BeforeLayoutMeasure"),e.resetTransform()}function hx(e){e.finishAnimation(),e.targetDelta=e.relativeTarget=e.target=void 0,e.isProjectionDirty=!0}function vR(e){e.resolveTargetDelta()}function xR(e){e.calcProjection()}function $R(e){e.resetSkewAndRotation()}function yR(e){e.removeLeadSnapshot()}function px(e,t,n){e.translate=bt(t.translate,0,n),e.scale=bt(t.scale,1,n),e.origin=t.origin,e.originPoint=t.originPoint}function fx(e,t,n,r){e.min=bt(t.min,n.min,r),e.max=bt(t.max,n.max,r)}function _R(e,t,n,r){fx(e.x,t.x,n.x,r),fx(e.y,t.y,n.y,r)}function wR(e){return e.animationValues&&e.animationValues.opacityExit!==void 0}const TR={duration:.45,ease:[.4,0,.1,1]},gx=e=>typeof navigator<"u"&&navigator.userAgent&&navigator.userAgent.toLowerCase().includes(e),bx=gx("applewebkit/")&&!gx("chrome/")?Math.round:ur;function vx(e){e.min=bx(e.min),e.max=bx(e.max)}function ER(e){vx(e.x),vx(e.y)}function h7(e,t,n){return e==="position"||e==="preserve-aspect"&&!SM(ux(t),ux(n),.2)}function kR(e){var t;return e!==e.root&&((t=e.scroll)==null?void 0:t.wasRoot)}const SR=m7({attachResizeListener:(e,t)=>Sc(e,"resize",t),measureScroll:()=>({x:document.documentElement.scrollLeft||document.body.scrollLeft,y:document.documentElement.scrollTop||document.body.scrollTop}),checkIsScrollRoot:()=>!0}),yh={current:void 0},p7=m7({measureScroll:e=>({x:e.scrollLeft,y:e.scrollTop}),defaultParent:()=>{if(!yh.current){const e=new SR({});e.mount(window),e.setOptions({layoutScroll:!0}),yh.current=e}return yh.current},resetTransform:(e,t)=>{e.style.transform=t!==void 0?t:"none"},checkIsScrollRoot:e=>window.getComputedStyle(e).position==="fixed"}),NR={pan:{Feature:HM},drag:{Feature:VM,ProjectionNode:p7,MeasureLayout:s7}};function xx(e,t,n){const{props:r}=e;e.animationState&&r.whileHover&&e.animationState.setActive("whileHover",n==="Start");const i="onHover"+n,a=r[i];a&&xt.postRender(()=>a(t,Kc(t)))}class CR extends Ra{mount(){const{current:t}=this.node;t&&(this.unmount=tD(t,(n,r)=>(xx(this.node,r,"Start"),i=>xx(this.node,i,"End"))))}unmount(){}}class AR extends Ra{constructor(){super(...arguments),this.isActive=!1}onFocus(){let t=!1;try{t=this.node.current.matches(":focus-visible")}catch{t=!0}!t||!this.node.animationState||(this.node.animationState.setActive("whileFocus",!0),this.isActive=!0)}onBlur(){!this.isActive||!this.node.animationState||(this.node.animationState.setActive("whileFocus",!1),this.isActive=!1)}mount(){this.unmount=qc(Sc(this.node.current,"focus",()=>this.onFocus()),Sc(this.node.current,"blur",()=>this.onBlur()))}unmount(){}}function $x(e,t,n){const{props:r}=e;if(e.current instanceof HTMLButtonElement&&e.current.disabled)return;e.animationState&&r.whileTap&&e.animationState.setActive("whileTap",n==="Start");const i="onTap"+(n==="End"?"":n),a=r[i];a&&xt.postRender(()=>a(t,Kc(t)))}class LR extends Ra{mount(){const{current:t}=this.node;t&&(this.unmount=aD(t,(n,r)=>($x(this.node,r,"Start"),(i,{success:a})=>$x(this.node,i,a?"End":"Cancel")),{useGlobalTarget:this.node.props.globalTapTarget}))}unmount(){}}const If=new WeakMap,_h=new WeakMap,PR=e=>{const t=If.get(e.target);t&&t(e)},IR=e=>{e.forEach(PR)};function UR({root:e,...t}){const n=e||document;_h.has(n)||_h.set(n,{});const r=_h.get(n),i=JSON.stringify(t);return r[i]||(r[i]=new IntersectionObserver(IR,{root:e,...t})),r[i]}function DR(e,t,n){const r=UR(t);return If.set(e,n),r.observe(e),()=>{If.delete(e),r.unobserve(e)}}const MR={some:0,all:1};class RR extends Ra{constructor(){super(...arguments),this.hasEnteredView=!1,this.isInView=!1}startObserver(){this.unmount();const{viewport:t={}}=this.node.getProps(),{root:n,margin:r,amount:i="some",once:a}=t,s={root:n?n.current:void 0,rootMargin:r,threshold:typeof i=="number"?i:MR[i]},o=l=>{const{isIntersecting:c}=l;if(this.isInView===c||(this.isInView=c,a&&!c&&this.hasEnteredView))return;c&&(this.hasEnteredView=!0),this.node.animationState&&this.node.animationState.setActive("whileInView",c);const{onViewportEnter:u,onViewportLeave:d}=this.node.getProps(),m=c?u:d;m&&m(l)};return DR(this.node.current,s,o)}mount(){this.startObserver()}update(){if(typeof IntersectionObserver>"u")return;const{props:t,prevProps:n}=this.node;["amount","margin","root"].some(OR(t,n))&&this.startObserver()}unmount(){}}function OR({viewport:e={}},{viewport:t={}}={}){return n=>e[n]!==t[n]}const zR={inView:{Feature:RR},tap:{Feature:LR},focus:{Feature:AR},hover:{Feature:CR}},BR={layout:{ProjectionNode:p7,MeasureLayout:s7}},Uf={current:null},f7={current:!1};function FR(){if(f7.current=!0,!!P1)if(window.matchMedia){const e=window.matchMedia("(prefers-reduced-motion)"),t=()=>Uf.current=e.matches;e.addListener(t),t()}else Uf.current=!1}const jR=new WeakMap;function VR(e,t,n){for(const r in t){const i=t[r],a=n[r];if(rn(i))e.addValue(r,i);else if(rn(a))e.addValue(r,Po(i,{owner:e}));else if(a!==i)if(e.hasValue(r)){const s=e.getValue(r);s.liveStyle===!0?s.jump(i):s.hasAnimated||s.set(i)}else{const s=e.getStaticValue(r);e.addValue(r,Po(s!==void 0?s:i,{owner:e}))}}for(const r in n)t[r]===void 0&&e.removeValue(r);return t}const yx=["AnimationStart","AnimationComplete","Update","BeforeLayoutMeasure","LayoutMeasure","LayoutAnimationStart","LayoutAnimationComplete"];class HR{scrapeMotionValuesFromProps(t,n,r){return{}}constructor({parent:t,props:n,presenceContext:r,reducedMotionConfig:i,blockInitialAnimation:a,visualState:s},o={}){this.current=null,this.children=new Set,this.isVariantNode=!1,this.isControllingVariants=!1,this.shouldReduceMotion=null,this.values=new Map,this.KeyframeResolver=K1,this.features={},this.valueSubscriptions=new Map,this.prevMotionValues={},this.events={},this.propEventSubscriptions={},this.notifyUpdate=()=>this.notify("Update",this.latestValues),this.render=()=>{this.current&&(this.triggerBuild(),this.renderInstance(this.current,this.renderState,this.props.style,this.projection))},this.renderScheduledAt=0,this.scheduleRender=()=>{const m=Sn.now();this.renderScheduledAt<m&&(this.renderScheduledAt=m,xt.render(this.render,!1,!0))};const{latestValues:l,renderState:c}=s;this.latestValues=l,this.baseTarget={...l},this.initialValues=n.initial?{...l}:{},this.renderState=c,this.parent=t,this.props=n,this.presenceContext=r,this.depth=t?t.depth+1:0,this.reducedMotionConfig=i,this.options=o,this.blockInitialAnimation=!!a,this.isControllingVariants=im(n),this.isVariantNode=M9(n),this.isVariantNode&&(this.variantChildren=new Set),this.manuallyAnimateOnMount=!!(t&&t.current);const{willChange:u,...d}=this.scrapeMotionValuesFromProps(n,{},this);for(const m in d){const p=d[m];l[m]!==void 0&&rn(p)&&p.set(l[m],!1)}}mount(t){this.current=t,jR.set(t,this),this.projection&&!this.projection.instance&&this.projection.mount(t),this.parent&&this.isVariantNode&&!this.isControllingVariants&&(this.removeFromVariantTree=this.parent.addVariantChild(this)),this.values.forEach((n,r)=>this.bindToMotionValue(r,n)),f7.current||FR(),this.shouldReduceMotion=this.reducedMotionConfig==="never"?!1:this.reducedMotionConfig==="always"?!0:Uf.current,this.parent&&this.parent.children.add(this),this.update(this.props,this.presenceContext)}unmount(){this.projection&&this.projection.unmount(),Ca(this.notifyUpdate),Ca(this.render),this.valueSubscriptions.forEach(t=>t()),this.valueSubscriptions.clear(),this.removeFromVariantTree&&this.removeFromVariantTree(),this.parent&&this.parent.children.delete(this);for(const t in this.events)this.events[t].clear();for(const t in this.features){const n=this.features[t];n&&(n.unmount(),n.isMounted=!1)}this.current=null}bindToMotionValue(t,n){this.valueSubscriptions.has(t)&&this.valueSubscriptions.get(t)();const r=Go.has(t);r&&this.onBindTransform&&this.onBindTransform();const i=n.on("change",o=>{this.latestValues[t]=o,this.props.onUpdate&&xt.preRender(this.notifyUpdate),r&&this.projection&&(this.projection.isTransformDirty=!0)}),a=n.on("renderRequest",this.scheduleRender);let s;window.MotionCheckAppearSync&&(s=window.MotionCheckAppearSync(this,t,n)),this.valueSubscriptions.set(t,()=>{i(),a(),s&&s(),n.owner&&n.stop()})}sortNodePosition(t){return!this.current||!this.sortInstanceNodePosition||this.type!==t.type?0:this.sortInstanceNodePosition(this.current,t.current)}updateFeatures(){let t="animation";for(t in Io){const n=Io[t];if(!n)continue;const{isEnabled:r,Feature:i}=n;if(!this.features[t]&&i&&r(this.props)&&(this.features[t]=new i(this)),this.features[t]){const a=this.features[t];a.isMounted?a.update():(a.mount(),a.isMounted=!0)}}}triggerBuild(){this.build(this.renderState,this.latestValues,this.props)}measureViewportBox(){return this.current?this.measureInstanceViewportBox(this.current,this.props):St()}getStaticValue(t){return this.latestValues[t]}setStaticValue(t,n){this.latestValues[t]=n}update(t,n){(t.transformTemplate||this.props.transformTemplate)&&this.scheduleRender(),this.prevProps=this.props,this.props=t,this.prevPresenceContext=this.presenceContext,this.presenceContext=n;for(let r=0;r<yx.length;r++){const i=yx[r];this.propEventSubscriptions[i]&&(this.propEventSubscriptions[i](),delete this.propEventSubscriptions[i]);const a="on"+i,s=t[a];s&&(this.propEventSubscriptions[i]=this.on(i,s))}this.prevMotionValues=VR(this,this.scrapeMotionValuesFromProps(t,this.prevProps,this),this.prevMotionValues),this.handleChildMotionValue&&this.handleChildMotionValue()}getProps(){return this.props}getVariant(t){return this.props.variants?this.props.variants[t]:void 0}getDefaultTransition(){return this.props.transition}getTransformPagePoint(){return this.props.transformPagePoint}getClosestVariantNode(){return this.isVariantNode?this:this.parent?this.parent.getClosestVariantNode():void 0}addVariantChild(t){const n=this.getClosestVariantNode();if(n)return n.variantChildren&&n.variantChildren.add(t),()=>n.variantChildren.delete(t)}addValue(t,n){const r=this.values.get(t);n!==r&&(r&&this.removeValue(t),this.bindToMotionValue(t,n),this.values.set(t,n),this.latestValues[t]=n.get())}removeValue(t){this.values.delete(t);const n=this.valueSubscriptions.get(t);n&&(n(),this.valueSubscriptions.delete(t)),delete this.latestValues[t],this.removeValueFromRenderState(t,this.renderState)}hasValue(t){return this.values.has(t)}getValue(t,n){if(this.props.values&&this.props.values[t])return this.props.values[t];let r=this.values.get(t);return r===void 0&&n!==void 0&&(r=Po(n===null?void 0:n,{owner:this}),this.addValue(t,r)),r}readValue(t,n){let r=this.latestValues[t]!==void 0||!this.current?this.latestValues[t]:this.getBaseTargetFromProps(this.props,t)??this.readValueFromInstance(this.current,t,this.options);return r!=null&&(typeof r=="string"&&(H5(r)||G5(r))?r=parseFloat(r):!lD(r)&&Aa.test(n)&&(r=S9(t,n)),this.setBaseTarget(t,rn(r)?r.get():r)),rn(r)?r.get():r}setBaseTarget(t,n){this.baseTarget[t]=n}getBaseTarget(t){var a;const{initial:n}=this.props;let r;if(typeof n=="string"||typeof n=="object"){const s=o2(this.props,n,(a=this.presenceContext)==null?void 0:a.custom);s&&(r=s[t])}if(n&&r!==void 0)return r;const i=this.getBaseTargetFromProps(this.props,t);return i!==void 0&&!rn(i)?i:this.initialValues[t]!==void 0&&r===void 0?void 0:this.baseTarget[t]}on(t,n){return this.events[t]||(this.events[t]=new M1),this.events[t].add(n)}notify(t,...n){this.events[t]&&this.events[t].notify(...n)}}class g7 extends HR{constructor(){super(...arguments),this.KeyframeResolver=XU}sortInstanceNodePosition(t,n){return t.compareDocumentPosition(n)&2?1:-1}getBaseTargetFromProps(t,n){return t.style?t.style[n]:void 0}removeValueFromRenderState(t,{vars:n,style:r}){delete n[t],delete r[t]}handleChildMotionValue(){this.childSubscription&&(this.childSubscription(),delete this.childSubscription);const{children:t}=this.props;rn(t)&&(this.childSubscription=t.on("change",n=>{this.current&&(this.current.textContent=`${n}`)}))}}function b7(e,{style:t,vars:n},r,i){Object.assign(e.style,t,i&&i.getProjectionStyles(r));for(const a in n)e.style.setProperty(a,n[a])}function qR(e){return window.getComputedStyle(e)}class GR extends g7{constructor(){super(...arguments),this.type="html",this.renderInstance=b7}readValueFromInstance(t,n){var r;if(Go.has(n))return(r=this.projection)!=null&&r.isProjecting?yf(n):fU(t,n);{const i=qR(t),a=(z1(n)?i.getPropertyValue(n):i[n])||0;return typeof a=="string"?a.trim():a}}measureInstanceViewportBox(t,{transformPagePoint:n}){return n7(t,n)}build(t,n,r){i2(t,n,r.transformTemplate)}scrapeMotionValuesFromProps(t,n,r){return l2(t,n,r)}}const v7=new Set(["baseFrequency","diffuseConstant","kernelMatrix","kernelUnitLength","keySplines","keyTimes","limitingConeAngle","markerHeight","markerWidth","numOctaves","targetX","targetY","surfaceScale","specularConstant","specularExponent","stdDeviation","tableValues","viewBox","gradientTransform","pathLength","startOffset","textLength","lengthAdjust"]);function WR(e,t,n,r){b7(e,t,void 0,r);for(const i in t.attrs)e.setAttribute(v7.has(i)?i:r2(i),t.attrs[i])}class KR extends g7{constructor(){super(...arguments),this.type="svg",this.isSVGTag=!1,this.measureInstanceViewportBox=St}getBaseTargetFromProps(t,n){return t[n]}readValueFromInstance(t,n){if(Go.has(n)){const r=k9(n);return r&&r.default||0}return n=v7.has(n)?n:r2(n),t.getAttribute(n)}scrapeMotionValuesFromProps(t,n,r){return G9(t,n,r)}build(t,n,r){j9(t,n,this.isSVGTag,r.transformTemplate,r.style)}renderInstance(t,n,r,i){WR(t,n,r,i)}mount(t){this.isSVGTag=H9(t.tagName),super.mount(t)}}const YR=(e,t)=>s2(e)?new KR(t):new GR(t,{allowProjection:e!==T.Fragment}),XR=WD({...xM,...zR,...NR,...BR},YR),QR=vD(XR),Ei=({blogPageMode:e=!1})=>{const[t,n]=T.useState(!1),[r,i]=T.useState(()=>{if(typeof window<"u"){const d=localStorage.getItem("darkMode");return d?JSON.parse(d):!1}return!1}),[a,s]=T.useState(!1),o=em(),l=J0();T.useEffect(()=>{const d=()=>n(window.scrollY>50);return window.addEventListener("scroll",d),()=>window.removeEventListener("scroll",d)},[]),T.useEffect(()=>{document.documentElement.classList.toggle("dark",r),localStorage.setItem("darkMode",JSON.stringify(r))},[r]);const c=d=>{if(s(!1),d.startsWith("#")){if(l.pathname!=="/"){o("/"+d);return}const m=document.querySelector(d);m&&m.scrollIntoView({behavior:"smooth"})}else o(d)},u=[{href:"#about",label:"About"},{href:"#research",label:"Research"},{href:"#publications",label:"Publications"},{href:"#projects",label:"Projects"},{href:"/blog",label:"Blog"},{href:"/cv",label:"CV"},{href:"#contact",label:"Contact"}];return h.jsxs("nav",{className:Ue("fixed inset-x-0 top-0 z-50 transition-all duration-300",t?"py-2":"py-4"),children:[h.jsx("div",{className:"container mx-auto px-6",children:h.jsxs("div",{className:Ue("relative overflow-hidden rounded-2xl transition-all duration-300","bg-white/5 dark:bg-black/10 backdrop-blur-xl","border border-white/20 dark:border-white/10","shadow-2xl shadow-black/10",t?"bg-white/7 dark:bg-black/15 shadow-black/20":"bg-white/5 dark:bg-black/10 shadow-black/10"),children:[h.jsx("div",{className:"absolute inset-0 bg-gradient-to-br from-blue-500/2 via-purple-500/2 to-pink-500/2"}),h.jsxs("div",{className:"relative flex items-center justify-between px-6 py-4",children:[h.jsx("button",{onClick:()=>o("/"),className:"text-xl font-bold text-bg hover:text-blue-200 transition-colors duration-300",children:h.jsx(sC,{className:"w-6 h-6 hover:scale-110 transition-transform duration-300"})}),h.jsxs("div",{className:"hidden xl:flex items-center space-x-2",children:[u.map(({href:d,label:m})=>h.jsx("button",{onClick:()=>c(d),className:Ue("px-4 py-2 rounded-lg text-sm font-medium transition-all duration-300","text-foreground/70 hover:text-foreground hover:bg-white/10","backdrop-blur-sm border border-transparent hover:border-white/20"),children:m},d)),h.jsx("button",{onClick:()=>i(!r),className:Ue("p-2 rounded-lg transition-all duration-300","text-foreground/70 hover:text-foreground hover:bg-white/10","backdrop-blur-sm border border-transparent hover:border-white/20"),"aria-label":"Toggle dark mode",children:r?h.jsx(Uv,{className:"w-5 h-5"}):h.jsx(Iv,{className:"w-5 h-5"})})]}),h.jsxs("div",{className:"xl:hidden flex items-center space-x-3",children:[h.jsx("button",{onClick:()=>i(!r),className:Ue("p-2 rounded-lg transition-all duration-300","text-foreground/70 hover:text-foreground hover:bg-white/10","backdrop-blur-sm border border-transparent hover:border-white/20"),"aria-label":"Toggle dark mode",children:r?h.jsx(Uv,{className:"w-4 h-4"}):h.jsx(Iv,{className:"w-4 h-4"})}),h.jsx("button",{onClick:()=>s(!a),className:Ue("p-2 rounded-lg transition-all duration-300","text-foreground/70 hover:text-foreground hover:bg-white/10","backdrop-blur-sm border border-transparent hover:border-white/20"),"aria-label":"Toggle menu",children:a?h.jsx(O6,{className:"w-5 h-5"}):h.jsx("svg",{className:"w-5 h-5",fill:"none",stroke:"currentColor",viewBox:"0 0 24 24",children:h.jsx("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M4 6h16M4 12h16M4 18h16"})})})]})]})]})}),h.jsx(hD,{children:a&&h.jsx(QR.div,{initial:{opacity:0,y:-20},animate:{opacity:1,y:0},exit:{opacity:0,y:-20},transition:{duration:.3},className:"xl:hidden mt-4 px-6",children:h.jsxs("div",{className:Ue("relative overflow-hidden rounded-2xl","bg-white/15 dark:bg-black/25 backdrop-blur-xl","border border-white/20 dark:border-white/10","shadow-2xl shadow-black/20"),children:[h.jsx("div",{className:"absolute inset-0 bg-gradient-to-br from-blue-500/5 via-purple-500/5 to-pink-500/5"}),h.jsx("div",{className:"relative flex flex-col p-4 space-y-1",children:u.map(({href:d,label:m})=>h.jsx("button",{onClick:()=>c(d),className:Ue("text-left px-4 py-3 rounded-lg font-medium transition-all duration-300","text-foreground/70 hover:text-foreground hover:bg-white/10","backdrop-blur-sm border border-transparent hover:border-white/20"),children:m},d))})]})})})]})},_x="Junyoung Park",u2=[{title:"Source-Free Domain Adaptation for Remote-Sensing Object Detection Using Low-confidence Pseudo Labels",authors:"Jin Kim, Junyoung Park, Hyunsung Jang, Namkoo Ha, Kwanghoon Sohn",venue:"IEEE Geoscience and Remote Sensing Letters (GRSL)",year:"2025",type:"Journal",citations:2,link:"https://ieeexplore.ieee.org/abstract/document/10949131"},{title:"Enhancing Source-Free Domain Adaptive Object Detection with Low-confidence Pseudo Label Distillation",authors:"Ilhoon Yoon, Hyeongjun Kwon, Jin Kim, Junyoung Park, Kwanghoon Sohn",venue:"European Conference on Computer Vision (ECCV)",year:"2024",type:"Conference",citations:13,link:"https://arxiv.org/abs/2407.13524"},{title:"Layer-wise Auto-Weighting for Non-Stationary Test-Time Adaptation",authors:"Junyoung Park, Jin Kim, Hyeongjun Kwon, Ilhoon Yoon, Kwanghoon Sohn",venue:"IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)",year:"2024",type:"Conference",citations:17,link:"https://arxiv.org/pdf/2311.05858.pdf"},{title:"적외선 영상에서의 객체 검출을 위한 소스-프리 비지도 도메인 적응 연구",authors:"Ilhoon Yoon, Junyoung Park, Hyeongjun Kwon, Jin Kim, Hyunsung Jang, Jaemin Park, Kwanghoon Sohn",venue:"멀티미디어학회논문지",year:"2024",type:"Conference",citations:0,link:"https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE11860634"},{title:"코사인 유사도를 활용한 안면 영상의 모션 블러 측정과 이를 추정하는 회귀 네트워크",authors:"Junyoung Park, Keunhoon Choi, Kwanghoon Sohn",venue:"제32회 신호처리합동학술대회",year:"2022",type:"Conference",citations:0,link:""}],ZR=()=>u2.length,JR=()=>u2.reduce((e,t)=>e+t.citations,0),eO=()=>{const e=t=>t.replace(new RegExp(`\\b${_x}\\b`,"g"),`<span class="font-bold underline">${_x}</span>`);return h.jsx("section",{id:"publications",className:"py-20 section-gradient",children:h.jsxs("div",{className:"container mx-auto px-6",children:[h.jsxs("div",{className:"text-center mb-16",children:[h.jsx("h2",{className:"text-4xl md:text-5xl font-bold mb-6 text-foreground",children:"Publications"}),h.jsx("div",{className:"w-24 h-1 bg-gradient-to-r from-research-500 to-purple-500 mx-auto mb-8"}),h.jsx("p",{className:"text-lg text-foreground/80 max-w-3xl mx-auto font-medium",children:"Selected publications showcasing contributions to the field of deep learning and artificial intelligence research."})]}),h.jsx("div",{className:"max-w-5xl mx-auto space-y-8",children:u2.map((t,n)=>h.jsx("div",{className:"liquid-glass-card rounded-lg p-6 shadow-lg hover:shadow-xl transition-all duration-300 hover:transform hover:scale-[1.02] shimmer",children:h.jsx("div",{className:"flex flex-col md:flex-row md:items-start md:justify-between gap-4",children:h.jsxs("div",{className:"flex-1",children:[h.jsxs("div",{className:"flex items-center gap-3 mb-3",children:[h.jsx("span",{className:`px-3 py-1 rounded-full text-xs font-medium ${t.type==="Journal"?"bg-gradient-to-r from-blue-100 to-blue-200 text-blue-800 dark:from-blue-900/50 dark:to-blue-800/50 dark:text-blue-200":"bg-gradient-to-r from-green-100 to-green-200 text-green-800 dark:from-green-900/50 dark:to-green-800/50 dark:text-green-200"}`,children:t.type}),h.jsx("span",{className:"text-sm text-foreground/70 font-medium",children:t.year})]}),h.jsx("h3",{className:"text-xl font-semibold mb-2 text-foreground hover:text-transparent hover:bg-clip-text hover:bg-gradient-to-r hover:from-research-600 hover:to-purple-600 transition-all cursor-pointer",children:t.title}),h.jsx("p",{className:"text-foreground/80 mb-2 font-mono text-sm font-medium",dangerouslySetInnerHTML:{__html:e(t.authors)}}),h.jsx("p",{className:"gradient-text font-medium mb-3",children:t.venue}),h.jsxs("div",{className:"flex items-center gap-4 text-sm text-foreground/70",children:[h.jsxs("span",{className:"flex items-center gap-1 font-medium",children:[h.jsx("svg",{className:"w-4 h-4",fill:"currentColor",viewBox:"0 0 24 24",children:h.jsx("path",{d:"M16 4v4h4V4h-4zm-2-2h8v8h-8V2zM4 4v4h4V4H4zM2 2h8v8H2V2zm0 10v4h4v-4H2zm-2-2h8v8H2v-8zm10 2v4h4v-4h-4zm-2-2h8v8h-8v-8z"})}),t.citations," citations"]}),h.jsx("a",{href:t.link,className:"gradient-text hover:opacity-80 transition-opacity font-medium",children:"View Paper →"})]})]})})},n))}),h.jsx("div",{className:"text-center mt-12",children:h.jsxs("a",{href:"https://scholar.google.com/citations?user=QQVxhyUAAAAJ&hl",className:"liquid-glass-button inline-flex items-center px-6 py-3 rounded-lg font-semibold transition-all transform hover:scale-105 shadow-lg",children:["View All Publications",h.jsx(F0,{className:"w-4 h-4 ml-2"})]})})]})})},tO=()=>{const{ref:e,isVisible:t}=F5(.2),n=[{year:"2024.09 ~ Now",title:"AI Researcher",place:"SBS"},{year:"2022.09 ~ 2024.08",title:"M.S. in EEE",place:"Yonsei Univ. DIML Lab"},{year:"2022.01 ~ 2022.08",title:"Lab Intern",place:"Yonsei Univ. DIML Lab"},{year:"2016.03 ~ 2022.08",title:"B.S. in EEE",place:"Yonsei Univ."},{year:"2021.06 ~ 2021.08",title:"Lab Intern",place:"Yonsei Univ. DSP Lab"}];return h.jsx("section",{id:"about",className:"py-20 section-gradient",ref:e,children:h.jsx("div",{className:"container mx-auto px-6",children:h.jsxs("div",{className:"max-w-6xl mx-auto",children:[h.jsxs("div",{className:`text-center mb-16 transition-all duration-1000 ${t?"opacity-100 translate-y-0":"opacity-0 translate-y-10"}`,children:[h.jsx("h2",{className:"text-4xl md:text-5xl font-bold mb-6 text-foreground",children:"About Me"}),h.jsx("div",{className:"w-24 h-1 bg-gradient-to-r from-primary to-purple-500 mx-auto mb-8 transition-all duration-1000 delay-300 transform scale-x-0 origin-center",style:{transform:t?"scaleX(1)":"scaleX(0)"}})]}),h.jsxs("div",{className:"grid lg:grid-cols-2 gap-12 items-start",children:[h.jsxs("div",{className:`space-y-8 transition-all duration-1000 delay-200 ${t?"opacity-100 translate-x-0":"opacity-0 -translate-x-10"}`,children:[h.jsx("div",{className:"liquid-glass-card rounded-2xl p-8 shadow-lg hover:shadow-xl transition-all duration-500 shimmer",children:h.jsxs("div",{className:"space-y-6",children:[h.jsxs("p",{className:"text-lg text-foreground/90 leading-relaxed",children:["I received my M.S. in Electrical and Electronic Engineering from"," ",h.jsx("a",{href:"https://diml.yonsei.ac.kr/",target:"_blank",rel:"noopener noreferrer",className:"text-blue-600 dark:text-blue-400 hover:text-blue-700 dark:hover:text-blue-300 underline decoration-blue-600/30 dark:decoration-blue-400/30 hover:decoration-blue-700/50 dark:hover:decoration-blue-300/50 underline-offset-2 transition-all duration-300 font-medium",children:"Yonsei University's DIML Lab"}),", under the supervision of Professor Kwanghoon Sohn. Currently, as an AI Researcher at"," ",h.jsx("a",{href:"https://www.sbs.co.kr/",target:"_blank",rel:"noopener noreferrer",className:"text-blue-600 dark:text-blue-400 hover:text-blue-700 dark:hover:text-blue-300 underline decoration-blue-600/30 dark:decoration-blue-400/30 hover:decoration-blue-700/50 dark:hover:decoration-blue-300/50 underline-offset-2 transition-all duration-300 font-medium",children:"SBS"}),", I am dedicated to driving innovation and pushing the boundaries of artificial intelligence."]}),h.jsx("p",{className:"text-lg text-foreground/90 leading-relaxed",children:"Working within a content-creation environment provides me with unique satisfaction, as I witness firsthand how AI enhances storytelling and production. My primary research focus lies in advancing video-based AI technologies, continuously exploring novel approaches to analyze, generate, and enhance visual content."}),h.jsx("p",{className:"text-lg text-foreground/90 leading-relaxed",children:"With expertise spanning computer vision, natural language processing, and audio processing, I strive to create AI systems that are both powerful and executable in real-world scenarios."})]})}),h.jsxs("div",{className:"grid grid-cols-2 gap-6",children:[h.jsxs("div",{className:"liquid-glass-card rounded-xl p-6 text-center shadow-lg hover:shadow-xl transition-all duration-500 group shimmer hover:scale-105",children:[h.jsx("div",{className:"text-3xl font-bold gradient-text mb-2 group-hover:scale-110 transition-transform duration-300",children:ZR()}),h.jsx("div",{className:"text-sm text-foreground/70 font-medium",children:"Publications"})]}),h.jsxs("div",{className:"liquid-glass-card rounded-xl p-6 text-center shadow-lg hover:shadow-xl transition-all duration-500 group shimmer hover:scale-105",children:[h.jsx("div",{className:"text-3xl font-bold gradient-text mb-2 group-hover:scale-110 transition-transform duration-300",children:JR()}),h.jsx("div",{className:"text-sm text-foreground/70 font-medium",children:"Citations"})]})]})]}),h.jsx("div",{className:`transition-all duration-1000 delay-400 ${t?"opacity-100 translate-x-0":"opacity-0 translate-x-10"}`,children:h.jsx("div",{className:"relative",children:h.jsxs("div",{className:"space-y-6 relative",children:[h.jsx("div",{className:"absolute left-8 top-8 bottom-8 w-0.5 bg-gradient-to-b from-primary/60 via-purple-500/60 to-pink-500/60 rounded-full"}),n.map((r,i)=>h.jsx("div",{className:`relative transition-all duration-700 ${t?"opacity-100 translate-y-0":"opacity-0 translate-y-10"}`,style:{transitionDelay:t?`${(i+5)*150}ms`:"0ms"},children:h.jsxs("div",{className:"flex items-start gap-6",children:[h.jsx("div",{className:"flex-shrink-0 relative z-10",children:h.jsxs("div",{className:"w-16 h-16 bg-gradient-to-r from-primary via-purple-500 to-pink-500 rounded-full flex items-center justify-center shadow-lg shadow-primary/30 group hover:scale-110 transition-all duration-300 liquid-glass-card",children:[h.jsx(iC,{className:"w-7 h-7 text-primary drop-shadow-sm"}),h.jsx("div",{className:"absolute inset-0 w-16 h-16 rounded-full bg-gradient-to-r from-primary/30 to-purple-500/30 animate-ping opacity-30"})]})}),h.jsx("div",{className:"flex-1 min-w-0 pb-8",children:h.jsxs("div",{className:"liquid-glass-card rounded-xl p-6 shadow-lg hover:shadow-xl transition-all duration-500 border border-primary/10 hover:border-primary/20 group shimmer",children:[h.jsx("div",{className:"absolute top-0 left-0 right-0 h-1 bg-gradient-to-r from-primary to-purple-500 rounded-t-xl opacity-60 group-hover:opacity-100 transition-opacity duration-300"}),h.jsxs("div",{className:"relative z-10",children:[h.jsx("h4",{className:"text-sm font-semibold text-primary/80 mb-2 tracking-wide uppercase",children:r.year}),h.jsx("p",{className:"text-xl font-bold gradient-text mb-2 group-hover:scale-105 transition-transform duration-300 origin-left",children:r.title}),h.jsx("p",{className:"text-foreground/70 font-medium",children:r.place})]})]})})]})},r.year))]})})})]})]})})})},nO=()=>{const{ref:e,isVisible:t}=F5(.2),n=[{title:"Domain Adaptation",description:"Developing robust AI models capable of adapting effectively to diverse real-world environments, enhancing performance and accuracy across varying contexts and data distributions.",icon:"🌐",color:"from-blue-500 to-cyan-500"},{title:"Multimodal AI",description:"Integrating vision, language, and audio modalities to create intelligent systems capable of understanding context, reasoning deeply, and generating natural, human-like responses.",icon:"🧠",color:"from-purple-500 to-pink-500"},{title:"Test-Time Training / Post-Training",description:"Improving AI adaptability by aligning models at inference time, fine-tuning or managing their performance dynamically and ensuring continued accuracy in changing environments.",icon:"🎯",color:"from-green-500 to-teal-500"},{title:"Representation Alignment for Foundation Models",description:"Leveraging foundation models to build unified representations, aligning pre-trained knowledge effectively with specific tasks, thus improving model generalization and transferability.",icon:"🛠️",color:"from-orange-500 to-red-500"}];return h.jsx("section",{id:"research",className:"py-20 section-gradient",ref:e,children:h.jsxs("div",{className:"container mx-auto px-6",children:[h.jsxs("div",{className:`text-center mb-16 transition-all duration-1000 ${t?"opacity-100 translate-y-0":"opacity-0 translate-y-10"}`,children:[h.jsx("h2",{className:"text-4xl md:text-5xl font-bold mb-6 text-foreground",children:"Research Areas"}),h.jsx("div",{className:"w-24 h-1 bg-gradient-to-r from-primary to-purple-500 mx-auto mb-8 transition-all duration-1000 delay-300 transform scale-x-0 origin-center",style:{transform:t?"scaleX(1)":"scaleX(0)"}}),h.jsx("p",{className:"text-lg text-foreground/80 max-w-3xl mx-auto font-medium",children:"My research spans multiple domains of deep learning, each contributing to the advancement of artificial intelligence and its real-world applications."})]}),h.jsx("div",{className:"grid md:grid-cols-2 lg:grid-cols-4 gap-8",children:n.map((r,i)=>h.jsxs("div",{className:`group liquid-glass-card rounded-xl p-6 shadow-lg hover:shadow-xl transition-all duration-700 border border-border hover:border-primary/50 transform ${t?"opacity-100 translate-y-0 scale-100":"opacity-0 translate-y-10 scale-95"}`,style:{transitionDelay:t?`${i*150}ms`:"0ms"},children:[h.jsx("div",{className:`w-16 h-16 rounded-lg bg-gradient-to-r ${r.color} flex items-center justify-center text-2xl mb-4 group-hover:scale-110 group-hover:rotate-3 transition-transform duration-300`,children:r.icon}),h.jsx("h3",{className:"text-xl font-semibold mb-3 text-foreground group-hover:text-primary transition-colors duration-300",children:r.title}),h.jsx("p",{className:"text-foreground/80 leading-relaxed font-medium",children:r.description})]},i))})]})})},rO=()=>{const e=[{title:"[SBS] Multimodal AI Services for Media",description:"Developing an AI service for contents creation using multimodal AI.",tech:["Video Understanding","Natural Language","Audio Processing","Vector Database"],image:"https://github.com/user-attachments/assets/2cf64843-7bad-4136-afa8-f70bbfd7b667",link:"",github:""},{title:"[LIG Nex1] Source Free Domain Adaptation",description:"Light-weight domain adaptation using source free approaches.",tech:["Object Detection","Computer Vision","Autonomous Driving"],image:"https://github.com/user-attachments/assets/368f7d13-7320-4fb3-932d-9086c05e4679",link:"https://www.youtube.com/watch?v=Mn1YU1Uji-g",github:""},{title:"[LIG Nex1] Test Time Adaptation",description:"Light-weight domain adaptation using test-time adaptation approaches.",tech:["Classification","Computer Vision","Parameter Efficient Learning"],image:"https://github.com/user-attachments/assets/7db274be-6b0b-468c-bad5-caed9a654c21",link:"https://www.youtube.com/watch?v=x3ri6DUKyAY",github:"https://github.com/junia3/LayerwiseTTA"},{title:"[Alchera] Blur Face Detection",description:"Solving the data degradation issue in facial recognition framework algorithms.",tech:["Face Recognition","Computer Vision","Model Optimization"],image:"https://github.com/user-attachments/assets/f31d4c3c-a9c4-4f31-aaa8-5d4237018f8e",link:"",github:"https://github.com/minsu1206/BlurFaceDetection"}];return h.jsx("section",{id:"projects",className:"py-20 section-gradient",children:h.jsxs("div",{className:"container mx-auto px-6",children:[h.jsxs("div",{className:"text-center mb-16",children:[h.jsx("h2",{className:"text-4xl md:text-5xl font-bold mb-6 text-foreground",children:"Projects"}),h.jsx("div",{className:"w-24 h-1 bg-gradient-to-r from-research-500 to-purple-500 mx-auto mb-8"}),h.jsx("p",{className:"text-lg text-foreground/80 max-w-3xl mx-auto font-medium",children:"Innovative deep learning projects that demonstrate practical applications of cutting-edge research in real-world scenarios."})]}),h.jsx("div",{className:"grid md:grid-cols-2 gap-8 max-w-6xl mx-auto",children:e.map((t,n)=>{const r=t.link&&t.link.trim()!==""&&t.link!=="#",i=t.github&&t.github.trim()!==""&&t.github!=="#",a=t.image.startsWith("http")||t.image.includes("/");return h.jsxs("div",{className:"group liquid-glass-card rounded-xl overflow-hidden shadow-lg hover:shadow-xl transition-all duration-300 hover:scale-[1.05] shimmer",children:[h.jsxs("div",{className:`h-48 relative overflow-hidden ${a?"":t.image}`,children:[a&&h.jsx("img",{src:t.image,alt:t.title,className:"w-full h-full object-cover"}),h.jsx("div",{className:"absolute inset-0 bg-black/20 group-hover:bg-black/10 transition-colors"}),h.jsx("div",{className:"absolute inset-0 bg-gradient-to-br from-white/10 via-transparent to-black/20"}),h.jsx("div",{className:"absolute bottom-4 left-4 right-4",children:h.jsx("div",{className:"flex gap-2 flex-wrap",children:t.tech.slice(0,3).map((s,o)=>h.jsx("span",{className:"px-2 py-1 bg-black/30 backdrop-blur-sm rounded text-white text-xs font-medium border border-white/30",children:s},o))})})]}),h.jsxs("div",{className:"p-6",children:[h.jsx("h3",{className:"text-xl font-semibold mb-3 text-foreground group-hover:text-transparent group-hover:bg-clip-text group-hover:bg-gradient-to-r group-hover:from-research-600 group-hover:to-purple-600 transition-all",children:t.title}),h.jsx("p",{className:"text-foreground/80 mb-4 leading-relaxed font-medium",children:t.description}),(r||i)&&h.jsxs("div",{className:"flex items-center gap-2",children:[r&&h.jsx("a",{href:t.link,target:"_blank",rel:"noopener noreferrer",className:"liquid-glass-button px-4 py-2 rounded-lg text-sm font-medium transition-all transform hover:scale-105",children:"Live Demo"}),i&&h.jsx("a",{href:t.github,target:"_blank",rel:"noopener noreferrer",className:"liquid-glass-button px-4 py-2 rounded-lg text-sm font-medium transition-all transform hover:scale-105",children:"GitHub"})]})]})]},n)})}),h.jsx("div",{className:"text-center mt-12",children:h.jsxs("a",{href:"/cv",className:"liquid-glass-button inline-flex items-center px-6 py-3 rounded-lg font-semibold transition-all transform hover:scale-105 shadow-lg",children:["View Full CV",h.jsx(F0,{className:"w-4 h-4 ml-2"})]})})]})})},ma=[{id:"1",name:"ai theory",slug:"ai theory",description:"ML/DL research and theoretical insights",color:"bg-research-500"},{id:"2",name:"ai papers",slug:"ai papers",description:"Deep Learning Paper Summaries and Reviews",color:"bg-blue-500"},{id:"3",slug:"ai technology",name:"ai technology",description:"Latest trends and advancements in AI",color:"bg-green-500"},{id:"4",name:"personal insights",slug:"personal insights",description:"Industry perspectives and analysis",color:"bg-purple-500"}],iO=`---
title: "cs231n 내용 요약 (0) - What is deep learning?"
category: "ai theory"
publishedAt: "2022-11-01"
thumbnail: "https://user-images.githubusercontent.com/79881119/210956163-47545d18-c612-4669-a8ff-32b4dbb26dfd.png"
---

# Overview

인공지능이란 무엇일까? 원래도 최근에 유명해진 분야이긴 하지만 그림 그리는 AI 등등 성능이 많이 올라오면서 이쪽 분야를 공부하고자 하는 사람들이 많아진 것 같다. 흔히 듣는 인공지능에 대한 용어들 중 AI, 딥러닝, 머신러닝, 데이터 사이언스나 빅데이트 등등 혼용해서 사용되는 경우가 많다. 그러다보니 결국 deep learning이란 무엇이고, 어떤 걸 공부하는 분야인지 애매해지는 경우가 생긴다. 지금부터 작성할 글들은 스탠포드 강의인 [cs231n](https://cs231n.github.io/) 관련 블로그를 참고했으며, 사실상 거의 번역본이라고 보면 된다. 모든 저작권은 해당 홈페이지에 있으며, 본인은 이를 일종의 공부 목적/도움을 받고자 하는 불특정 다수에게 어느 정도 내가 이해한 바를 기준으로 전달하려고 작성하게 되었다.

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/210956163-47545d18-c612-4669-a8ff-32b4dbb26dfd.png" alt="cs231n01-1" width="500"/>
</div>

# What is AI?
이번에 작성하는 글은 일종의 <U>오리엔테이션</U>와 같다. 본격적으로는 다음 글부터 이론적인 내용이 나오는데, 우선 강의에서 다루고자 하는 큰 주제가 무엇인지 짚고 넘어가고 싶었다. AI는 <U>artificial intelligence</U>의 약자로, 인공적으로 생성해낸 지능이라고 말할 수 있다.   
본인은 무교라 종교적인 이야기를 하고자 하는 것은 아니지만 인간이 창조할 수 있는 불가능의 영역 중 하나가 바로 인류/혹은 그와 유사한 지성을 가진 객체라 생각한다. 왜냐하면 감정이나 이성의 영역이 우주만큼 무한하고 헤아릴 수 없는 영역이기 때문. 그럼에도 불구하고 본인은 공대생으로 이 진로를 택했고, 공부하면서 느낀 점은 마치 천문학과에서 이런저런 우주의 비밀을 풀어내는 것처럼 인간도 어쩌면 인공지능을 해결하는 것이 미지의 영역에 발을 들이는 것이라 느끼기 시작했다. 물론 지금은 그런 걸 신경쓰기보단 SOTA 논문 찾기에 바쁘지만.   
아무튼 어찌저찌 다시 결론을 내자면 AI는 인간의 지성/지능을 대표하는 하나의 객체라고 말할 수 있고, 이런 객체가 <U>로봇</U>으로 나타날수도(하드웨어), <U>프로그램</U>으로 나타날수도(소프트웨어) 있다. 인간이 여러 감각 정보들을 받아들인 후 뇌에서 이를 처리해서 특정 정보로 인식하는 프로세스를 컴퓨팅 환경으로 생각하면, 특정 modality를 sensing하는 인터페이스가 있고 이를 종합적으로 처리할 수 있는 프로세싱 모듈로 하여금 정보 해석 능력을 요구하게 된다.   
따라서 우리가 흔히 가장 큰 바운더리로 언급할 수 있는 것이 AI이며, 이는 인공적으로 만든 모든 지능의 객체가 표현되는 방식이라고 볼 수 있다. 이러한 AI를 구현하는데 필요한 것이 바로 인터페이스로 하여금 받아들이는 <U>정보(information)</U>, 정보 해석 능력을 요구당하는 <U>기계의 학습(machine learning)</U>으로 구성된다. 이를 최근 들어 조금 더 세분화하여 AI 최근 분야에서는 정보 처리와 관련된 기술을 <U>data processing/data science</U>로 분류했으며, 데이터 사이언스에서 주로 포커싱하는 것은 딥러닝/머신러닝과 같이 기계가 학습되는 부분에 대한 방법론을 제시하는 것보다 정보를 잘 정제해서 유의미한 인사이트를 알고리즘에 활용하고자 하는 것이다. 그리고 머신러닝의 다양한 방법론 중 하나인 neural network(신경망) 학습이 gradient descent based optimization과 빅데이터(방대한 데이터를 의미한다), GPU와 같은 하드웨어의 발전으로 현실적인 연구가 가능해지면서 발전한 것이 딥러닝(deep learning)이고, 아마도 대부분 최근에 AI와 관련되어 들어보았던 내용은 딥러닝 base인 연구들이 많았을 것이다. 

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/210958358-5a0abde7-49fc-47b1-a5aa-08dcd5084c8f.png" alt="cs231n01-2" width="700"/>
</div>

크게 데이터와 머신러닝의 축으로 진행되던 AI에서 <U>딥러닝</U> 분야가 발전된 이유는 feature engineering 때문이다. 컴퓨터에게 데이터로 하여금 잘 정제해서 유의미한 인사이트를 전달하는 것이 필요한데 전통적인 머신러닝 방식은 데이터를 가지고 와서 이를 분류하거나 서로 다른 이미지에서 같은 물체를 찾아내는 작업에 등등에 대해 여러 정형화된 알고리즘을 활용했고, 만약 데이터셋의 feature를 제대로 가공할 수 있는 알고리즘을 찾지 못하면 좋은 성능을 기대하기 힘든 경우가 많았다. 결국 인공지능인데 input에 대해 기대하는 output을 라벨링하는 것 뿐만 아니라, input으로 들어가는 modality에서 유의미한 feature를 가공하는 작업조차 인간이 하나의 알고리즘으로 만들어줘야하고, 사실상 이렇게 만들어진 알고리즘은 <U>데이터 수가 많아질수록 일반화 성능이 떨어진다</U>는 문제가 있었다. 결국 우리는 input에서 스스로 유의미한 feature를 찾아내고 이를 활용하여 결과를 낼 수 있는 시스템을 만들고 싶었고, 바로 deep learning은 deep neural network based learning을 활용하여 기존 머신러닝의 성능을 뛰어넘은 분야가 되었다.

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/210959561-07549eed-d4d4-4fc4-a4ec-27163c86dc04.png" alt="cs231n01-3" width="400"/>
</div>

간단하게 설명하면 다음과 같다. Input space를 $$X$$, feature space를 $$Y$$, 각 input $$x sim X$$에 대해 대응되는 output $$z sim Z$$가 있다고 가정해보자. 여기서 space는 하나의 <U>집합</U>이고, 물론 우리는 세상에 존재하는 모든 인과관계에 대한 supervision을 가질 수 없기 때문에 각 space는 열린계로 가정하되 우리가 관측 가능한 subspace에 대해서만 본다고 생각해보자. Subspace란 부분 집합이라고 생각하면 된다. 일반적인 머신러닝에서는 input을 통한 output 예측 과정이 다음과 같다. 고정된 알고리즘 $$F,~G$$에 대해서,

$$
    F(x) = y,~G(y) = \\hat{z} \\approx z
$$

위와 같이 합성 함수의 형태로 표현할 수 있다. 물론 실제로 함수가 존재하는 것은 아니지만 input $$X$$로 하여금 output $$Z$$를 잘 예측할 수 있는 $$F,~G$$를 모델링하고자 한다. 보통의 머신러닝에서는 feature extraction 알고리즘에 해당되는 $$F$$가 feature engineering을 통해 실현되고 update가 되지 않는다는 문제가 있다. 여기서 update가 되지 않는다는 것은 구현한 알고리즘이 추가 데이터나 학습을 통한 성능 향상을 이뤄낼 수 없다는 것이다. 이와 마찬가지로 함수 G는 feature engineering을 통해 추출한 feature extraction을 활용하여 input에 대한 추론을 시작한다. 앞서 feature engineering이 <U>성능 수렴의 문제</U>가 있었기 때문에 이 부분도 더이상 성능 향상을 기대할 수 없게 된다.

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/211228489-a36aba3f-89db-4378-b048-7aff011d6290.png" alt="cs231n01-4" width="400"/>
</div>

바로 이런 측면에서 흔히 머신러닝과 딥러닝의 차이를 보여줄 때 등장하는 그래프가 나오게 된다. 데이터 수가 많아질수록 학습할 수 있는 resource는 많아지는데, 전통적인 알고리즘 방식으로는 성능 향상을 기대하기 힘들었기 때문에 딥러닝을 사용하여 기존 방식보다 성능을 높여보겠다는 시도로 인공지능의 발전을 이끌어낼 수 있었다.

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/211228640-ce8a50f8-64b5-433b-a7c7-6dcf858502f7.png" alt="cs231n01-5" width="400"/>
</div>

최대한 전반적인 부분을 다루고자 장황하게 설명한 것 같지만 위의 다이어그램과 같이 포함된다고 생각하면 된다. **AI**라는 큰 concept에서 소프트웨어나 하드웨어의 퍼포먼스를 구현하기 위해 제시된 **머신러닝**이 있고, 이러한 머신러닝의 여러 알고리즘 중 NN(Neural Network)를 발전시킨게 **딥러닝**이다.

# 마무리하면서...
간단하게 인공지능에 대해서만 언급하고 이번 글은 마친다. cs231n은 인공지능 공부를 처음 시작하면서 수강했던 강의기도 하고, 사실 지금 와서도 굉장히 스탠다드한 입문 강의로 많이 추천하기는 하지만 정말로 인공지능 입문에 적합한지는 사실 아직은 잘 모르겠다. 항상 공부하면서 느끼는 건데 이 분야는 정답이 정해진 루트가 있는게 아니라 그냥 내가 열심히 해야만 무언갈 얻을 수 있는 것 같다. 갑자기 일기장이 된 것 같지만 암튼 마무리`,aO=`---
title: "cs231n 내용 요약 (1) - Image classification"
category: "ai theory"
publishedAt: "2022-11-02"
thumbnail: "https://user-images.githubusercontent.com/79881119/211229357-29c7e82a-eaa7-4b7e-81ba-81841ed2ec33.png"
---

# What is image classification?
<U>이미지 분류</U>(Image classification)는 컴퓨터비전(computer vision)으로 해결하고자 하는 여러 가지 task(challenge) 중 가장 기본이라고 볼 수 있다. 기존 머신러닝이 해결하기 힘들었던 데이터 수에 따른 성능 수렴을 해결했던 딥러닝 방식은 이미지 분류 대회에서 AlexNet이라는 네트워크가 우승하면서 시작되었다. 대부분의 사람들은 AI라고 하면 그 시작을 알파고로 기억해주는 사람도 많고, 물론 알파고도 RL 분야에서는 상당히 세상의 이목을 집중시켰던 중요한 이벤트긴 하지만 본인에게는 **AlexNet**이 조금 더 인상깊게 다가온다. 
<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211229357-29c7e82a-eaa7-4b7e-81ba-81841ed2ec33.png" width="500"/>
</div>
다시 image classification으로 돌아와서, 컴퓨터비전에서 해결하고자 하는 이미지 분류는 간단하게 input으로 특정 이미지를 주면 고정된 카테고리 분류들 중 하나에 매칭하는 것이다. 위의 그림은 CIFAR-10 dataset의 예시로, 총 10가지의 클래스로 구분되며 각 이미지를 잘 나타낼 수 있는 label(dog, cat 등등)로 지표화가 된 상태이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211229816-ec1c4c9a-3d7e-4ea7-9f1c-d9af75bf2c66.png" width="500"/>
</div>

이러한 이미지를 인간이 보았을 때는 누가 봐도 <U>고양이</U>지만 컴퓨터의 입장은 좀 다르다. 모든 이미지는 컴퓨팅 환경에서 양자화되고, 3차원 메모리의 형태로 각 픽셀 위치에 따른 RGB value로 매핑된다. 컴퓨터가 처음 고양이 이미지를 받아들였을 때 이 숫자들의 나열을 통해 곧바로 '고양이'라고 추측할 수 있는 가능성은 10% 뿐이다. 따라서 우리는 컴퓨터로 하여금 <U>최대한 다양한 고양이 이미지들의 숫자 배열 사이에서 규칙을 찾아내게 하고 싶은 것</U>이다. Image classification의 개요에 대해 간단하게 짚고 넘어왔다.

# What is challenging?
더 깊게 들어가기 전에 우선 텐서(Tensor)에 대한 개념을 언급하는게 좋을 것 같다. Matrix(행렬) 개념을 일반화한 형태인 Tensor(텐서)는 숫자 혹은 데이터의 배열이다. Matrix에서의 rank 개념은 행렬이 가지는 independent vector의 개수이며, 이와 유사하게 Tensor도 rank 개념은 해당 <U>텐서가 가지는 차원 수</U>를 의미한다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211230352-c1d20e34-770e-4854-9ee2-e2e4aac8ff5c.png" width="500"/>
</div>

만약 어떤 Tensor가 묶음으로 표현될 수 있는 dimension을 $N$개 가지고 있다면, 그 Tensor는 $N$차원의 rank를 가지는 Tensor가 된다. 예를 들어 이미지는 RGB의 채널을 가지는 $H \\times W$ 크기의 matrix 모음이기 때문에 다음과 같이 표현할 수 있다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211230627-b36d86ae-f5ae-471f-82c5-3d409ee87f25.png" width="500"/>
</div>

고양이 이미지의 각 픽셀은 특정 위치에서의 색을 의미하며(보다 구체적으로는 3차원 공간에서 2차원 카메라로 rendering된 색상) 다양한 색이 빛의 삼원색인 RGB의 조합으로 표현이 가능하다. 디지털 카메라 환경에서 RGB 값은 $0 \\sim 255$의 값으로 양자화되며(8-bit unsigned), 이는 <U>computing situation</U>에서도 동일하게 적용된다. 따라서 고양이 이미지를 $H \\times W$의 spatial dimension을 가지는 matrix를 R, G, B 세 묶음으로 가지고 있는 Tensor로 표현할 수 있다.   
앞으로의 모든 게시글에서 Image의 resolution은 이미지의 <U>spatial dimension</U>을 의미하고, Image의 channel은 이미지의 <U>RGB</U> 축을 의미한다고 생각하면 된다. 결론은 지금 다루고 있는 이미지 분류 task에서 input으로 사용되는 image의 데이터 형태는 3차원의 Tensor로 해석할 수 있다는 것이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211231096-d6f43001-9449-4c98-8602-511ac5ec87d2.png" width="800"/>
</div>

하지만 이렇게 데이터로 표현된 이미지에는 <U>큰 문제가</U> 있다. 만약 특정 피사체를 찍는 각도가 바뀐다면(Viewpoint variation), 분명 같은 이미지임에도 불구하고 데이터 상으로는 크게 상이한 결과가 출력될 것이다. 이미지 분류를 위해 사용되는 image의 데이터 형태를 컴퓨터가 받았을 때, 동일한 object임에도 <U>보는 각도에 따른 value 차이가 생겨서</U> 이를 일반화할 수 있는 예측 알고리즘을 찾기 힘들다는 것이다.   
마찬가지로 조도 환경(Illumination conditions)에 따른 데이터 차이도 문제가 된다. 단순히 밤에서 낮으로 바뀐다거나, 아니면 광원의 위치가 바뀌어서 우리가 보는 각도에서의 전경과 후경의 색감에 차이가 생기게 되면 위의 경우와 마찬가지로 같은 장면에 대해 잘못된 예측 알고리즘을 적용할 수 있다.   
이를 제외하고도 scale variation(동일한 사물이지만, 카메라가 가까워지고 멀어짐에 따라 물체의 크기가 다양해질 수 있고 이로 인해 잘못된 예측을 할 수 있음), Deformation(예를 들어 고양이는 <U>액체이므로</U> dynamic한 pose를 보여주는데, 이러한 사물 변형에 대해 robust한 모델링이 힘들다는 것), 물체와 배경 색이 유사해서(보호색) 분류에 차질이 생긴다던지 전경이 후경을 가려서 물체의 구분을 방해한다던지 그리고 같은 class의 물체에 대해서도 다양한 모습이 존재할 수 있다는 문제점 등등 <U>image를 통한 machine learning에 제약이 많다는 것</U>을 알 수 있다. 이런 모든 데이터의 가능성에 대해서 네트워크를 구성한다는 것은 불가능에 가까우며, 여기서 바로 딥러닝의 근간이 될 <U>data-driven algorithm</U>을 해결책으로 사용하게 된다.

# Data-driven algorithm
데이터 기반 알고리즘은 <U>귀납적 추리</U>에 가깝다. 최단 경로 찾기 문제나 이진 탐색과 같이 어느 정도 한정된 자원을 가정하고 시작하는 알고리즘과는 다르게 image classification과 같은 task에서 해결하고자 하는 문제는 위에서 설명한 다양한 <U>challenging situation</U>에 무관한 예측이 가능한 알고리즘이다. 단순히 '의자'라는 객체만 하더라도 세상에는 정말 다양한 종류의 의자가 있으며, 심지어 의자의 고정관념을 깨는 예술품들이 등장할 수도 있다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211232116-3bb0a20c-90ef-4d53-92b8-60d0d0f519aa.png" width="600"/>
</div>

솔직히 말하자면 사람도 오른쪽 그림을 보고 의자라고 말하긴 조금 힘들 것 같은데, 아무튼 여러 조도 환경, 카메라의 회전이나 위치 등등 모든 상황에 대처할 수 있는 알고리즘을 구성하고자 하는 것이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211232302-e4a7ba44-0aab-4294-abcb-886ea40e56d6.png" width="600"/>
</div>
적게는 수천장부터, 많게는 수백/억의 데이터를 통해 이를 구현하고자 한다. 최대한의 데이터를 통한 최적화 과정으로, 관측할 수 없는 세상의 모든 <U>객체 데이터</U>에 적용할 수 있는 알고리즘을 찾는 과정이다. 앞으로 언급할 대부분의 딥러닝 알고리즘은 바로 이러한 방법을 사용하게 된다.



# Image classification pipeline

이미지 분류를 앞서 언급한 개념들을 토대로 진행하는데, 총 <U>3개의 구성 요소</U> 혹은 <U>단계</U>로 구분할 수 있다. 그 중 첫번째는 **입력 데이터**이다. $K$개의 라벨(class의 개수)에 각각 매칭되어있는 $N$개의 이미지를 data-driven 최적화 과정에 사용한다. 이를 'Training data'라고 부른다. Supervision(지도) 학습에서는 training data가 이처럼 라벨링이 되어있어야한다.   
두번째로는 학습 단계인 **learning**이다. 이 단계에서는 최적화 과정에서 사용할 목적 함수 혹은 알고리즘을 정의하게 된다. 첫번째 구성 요소였던 training data를 활용하여 최대한 좋은 성능을 내고자 하는 것이 두번째 단계가 된다.   
마지막 세번째는 **evaluation**이다. 최적화 과정에서 실제로 학습 과정 중인 네트워크가 얼마나 좋은 성능을 내는지 확인하는 과정이 필요하고, 이 과정에서 사용될 수 있는 데이터가 필요하다. Training data와 마찬가지로 성능을 확인할 수 있어야하므로 데이터에는 지표화가 되어있어야 하며, 이를 validation/test data라고 부른다. 알고리즘이 예측한 결과에 대응되는 지표는 ground truth라고 부른다.

# Nearest Neighbor Classifier
지금 소개하고자 하는 classifier는 딥러닝에서 사용하는 neural network 구조는 아니다. 상당히 비효율적인 알고리즘을 소개할 것인데, 이를 언급하는 이유는 <U>data-driven algorithm</U>이랑 <U>gradient descent algorithm</U>이랑 혼동하지 않는 것이 중요하기 때문이다. 두 알고리즘은 머신러닝과 딥러닝의 관계와 비슷하게 말할 수 있다. data-driven algorithm을 활용한 함수 최적화 방식에 gradient descent algorithm이 있을 수 있지만 두 개념은 <U>서로 다른 개념</U>이기 때문에 꼭 NN형태의 딥러닝이 아니더라도 data-driven approach를 사용할 수 있다. 예를 들어 gradient descent 방식이나 WGAN-GP(Wessertein-GAN with gradient penalty) 방식 등등 현재 딥러닝에서 활용 및 언급되는 모든 최적화 알고리즘은 이전에도 이미 존재했었고, AI 분야가 아닌 통신, 수학 등등에서 주로 사용되던 것이었다.   
Nearest neighbor classifier는 추론을 위해 모든 데이터가 필요하다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211233597-11a527e0-72e3-49b4-a8cc-1093508f674f.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/211233625-49c6f4f6-22a9-4059-8028-0a25888c70d8.png" width="400"/>
</div>

왼쪽 그림은 CIFAR-10의 몇몇 샘플을 보여주는 그림이고, 오른쪽 그림은 test image와 비교했을때, training image와 가장 유사하다고 판단한 그림 10장을 보여준 모습이다. 가장 가까운 이웃을 찾는 알고리즘 이름에 맞게 유사함의 정도를 측정하는 메트릭을 정하기만 하면 training image와 test image 사이의 거리를 비교함으로써 nearest neighbor이 매핑된 클래스가 곧 test image에 대해 예측하고자 하는 클래스가 된다는 것. 흔히 거리를 계산하는 방법은 다음과 같다. Distance와 norm의 개념은 ground truth가 있는 텐서, 행렬 혹은 벡터 간 연산에서 주로 활용되는 편인데, order $p$에 따라 다르게 표현이 가능하다. Norm과 distance의 차이는 norm은 벡터의 크기를 표현하고, distance는 두 벡터의 차이에 대한 크기라고 생각하면 된다. Norm으로 나타낸 벡터의 크기는 <U>원점에서 벡터 좌표까지의 거리</U> 혹은 <U>magnitude</U>라 부른다.

$$
    L_p = \\left( \\sum_i^n \\vert x_i \\vert^p \\right)^{\\frac{1}{p}}    
$$

Order 값을 $p$로 가지는 norm을 $L_p$로 표현하며, 식은 위에서 보는 바와 같다. 위의 식은 $x$를 $n$차원의 벡터라고 가정하고 작성하였다. 주로 사용되는 norm은 $L_1$ norm 그리고 $L_2$ norm이다. 위의 식에 $p = 1$을 대입하면 다음과 같다.

$$
    \\begin{aligned}
        L_1 =& \\left( \\sum_i^n \\vert x_i \\vert \\right) \\newline
        =& \\vert x_1 \\vert + \\vert x_2 \\vert + \\vert x_3 \\vert + \\cdots + \\vert x_n \\vert 
    \\end{aligned}    
$$
L1 norm은 Taxicab norm 혹은 Manhattan norm이라고도 한다. 식을 보면 알 수 있듯이 각 요소의 절댓값에 대한 합으로 정의된다. 이번에는 $p = 2$를 대입해보도록 하자.

$$
    \\begin{aligned}
        L_2 =& \\sqrt{\\sum_i^n x_i^2 } \\newline
        =& \\sqrt{x_1^2 + x_2^2 + x_3^2 + \\cdots + x_n^2 }
    \\end{aligned}    
$$

아마 일반적인 사람들이 알고 있는 원점에서의 벡터 좌표까지 거리를 구하는 공식일 것이다. 유클리드 공간에서의 벡터 크기를 측정하는 공식이기 때문에 Euclidean norm이라 불린다. 그리고 벡터 연산을 통해 inner product 형태로 바꿔볼 수도 있다.

$$
    \\begin{aligned}
        L_2 =& \\sqrt{\\sum_i^n x_i^2 } \\newline
        =& \\sqrt{x \\cdot x} \\newline
        =& \\sqrt{x^\\top x} \\newline
        =& \\sqrt{x_1^2 + x_2^2 + x_3^2 + \\cdots + x_n^2 }
    \\end{aligned}    
$$

<U>Distance</U>는 벡터 차원을 확장해서 생각해보면, 서로 다른 두 텐서의 <U>차이에 대한 norm</U>을 구하는 것과 같다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211241290-4282638c-9493-4854-896a-da54eac738da.png" width="600"/>
</div>

다시 원래 설명하던 내용으로 돌아와서, nearest neighbor 방식은 위와 같은 거리 공식(metric)을 기반으로 test image와 training image 사이의 거리를 구한다.
위는 $L_1$ distance 예시이고, $L_2$ distance을 적용해서 구할 수도 있다. 이렇게 구한 distance 기준으로 각 training image와의 최소 거리를 구한 뒤, 가장 거리가 가까운 이미지의 라벨을 기준으로 test image의 라벨을 예측하게 된다.   
$L_1$, $L_2$ distance 어떤 걸 사용하는지에 따른 장단점이 다른데, 일반적으로 제곱으로 계산되는 $L_2$ distance가 $L_1$에 비해 vector 거리에 대해 더 unforgiving한 면이 있다. 거리 공식 자체가 제곱이다보니, 직선 형태로 그려지는 $L_1$ distance보다 1보다 큰 값의 오차에 대해 quadratic하기 때문이다. Unforgiving이라는 말은 같은 정도의 오차가 발생했을때 penalty를 더 많이 준다는 의미와 같다.   
Nearest neighbor 방법은 매우 단순하면서 간단하지만, 가장 가까운 거리의 image만을 판별 기준으로 삼는다면 정확도가 많이 떨어질 수도 있다. Image 벡터 사이의 거리만 비교한다면 그냥 <U>육안상 색감이 비슷하지만, 서로 다른 object에 속하는 두 이미지</U>를 같은 class로 결정할 수 있기 때문이다. 이를 방지하고자, 보다 많은 표본을 통해 예측하려고 한다. 바로 이 방법이 뒤이어 바로 설명할 KNN($K$-nearest neighbor)이고, 이 방법에서는 거리 비교 후 <U>가장 가까운 거리의 image를 포함한 $k$개의 sample</U>을 통해 예측하게 된다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211242160-3f49e1f0-aa21-4000-97bd-bd8706b15c01.png" width="800"/>
</div>

그림에서 볼 수 있듯이 만약 $k$가 적다면 가장 가까운 거리의 샘플을 기반으로 예측이 되므로 경계선이 끊어지거나 prediction map이 불연속인 형태를 가지게 된다. 이와는 다르게 근처 5개의 샘플을 기반으로 주를 이루는 class로 예측을 할 경우 이러한 문제가 어느 정도는 해소되는 것을 확인할 수 있다. 하지만 $K$ 값이 무작정 크면 성능이 오히려 악화되는 문제가 생기는데, 이는 사실 직관적으로 관찰 가능한 사실이지만 이후에 다시 언급하도록 하겠다.

# Hyperparameter 조정
흔히 네트워크를 학습하는 과정에서 우리가 manually 조정할 수 있는 파라미터를 hyperparameter라고 부른다. 네트워크의 weight와 같은 parameter 개념과는 다르게 하이퍼파라미터는 최적값을 찾기 위해 실험하는 입장에서 직접 grid search를 해야한다. 그렇다면 KNN에서의 $K$를 정하는 것과 $L_1,~L_2$ norm 중 어떤 것이 더 좋은 성능을 보이는지 알기 위해서는, 직접 해당 조건에 따라 실험을 진행한 후, 가장 좋은 성능을 보이는 값을 적용하는 것이다.   
여기서 중요한 점은 training set, test set 이외에 validation set이다. 성능을 평가하는 과정에서 파라미터 조정에는 training dataset이 쓰이지는 않지만 hyperparameter 조정에는 test dataset과 같은 metric의 기준이 필요하다. KNN의 경우에는 training dataset이 일종의 <U>이미지 사전</U> 역할을 하고 test dataset이 <U>찾고 싶은 단어</U>가 되는데, 여기서 test dataset으로 하이퍼파라미터 조정을 하게 되면 일반화된 데이터에 대한 hyperparameter value가 아닌 특정 test dataset에 overfitting된 value가 된다.   
딥러닝을 먼저 공부한 사람 중에서 헷갈리는 사람이 있을까 싶어 다시 언급하자면 지금 설명하는 방법은 neural network 알고리즘이 아닌 KNN 알고리즘이다. 따라서 일반적으로 DNN에서 언급하는 training set에 대한 overfitting과는 다르게 KNN에서는 overfitting이 test set에 대해서 정의되어야 맞다. 결론적으로 두 경우 모두 해결법으로 validation dataset을 통한 모니터링이지만, 두 알고리즘에서의 overfitting이 맥락으로는 비슷해도 서로 다른 것이라는 걸 짚고 넘어가고 싶었다.   
따라서 기존에 분류되던 training data/test data 이외에 validation data가 필요하고, 일반화를 위해 <U>training dataset</U>을 여러 fold로 나눈 뒤 이 중 하나의 fold를 <U>validation dataset</U>으로 사용한다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211243687-c6c31a8a-6528-462c-9a5b-384167f73e34.png" width="800"/>
</div>

이렇게 되면 학습 과정에서 성능을 평가할 때 여러 fold에 대해서 평균을 구해볼 수 있으므로, hyperparameter인 $K$를 조정하는 과정에서 공평한 비교가 가능해진다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211243813-41facc7a-ed22-48f3-993e-dacfdcbb35dc.png" width="600"/>
</div>

이렇게 validation과 training을 여러 fold들에 대해서 cross-validation을 함으로써 얻어진 그래프는 위와 같다. Fold가 5개이므로 각각 validation에 사용될 때의 accuracy가 총 5개나 나올 것이고, 이를 통해 평균 정확도를 구하면 변인에 대해 일반화된 성능을 확인할 수 있다. 이러한 방식은 training data가 현저히 작을 경우 사용할 수 있는 아주 좋은 방법이다.   
결과를 보게 되면 <U>거리 비교가 많아질 때</U>($k$가 커질 때) 그만큼 표본은 많아져서 좋지만, 우리가 학습시킨 데이터가 test data에 대해서 <U>얼만큼 대표적인지 알 수 없기 때문에</U> 성능이 무작정 좋아진다고 할 수 없다. 학습시킨 데이터 기준으로 test data와 <U>유사한데 다른 label을 가진 sample들</U>이 많으면 오히려 독이 될 수 있다는 것이다. 이는 필연적으로 $k$가 계속하여 증가하면 확률적으로 생길 수 있는 일이기 때문에(관측 가능한 데이터는 한정적이므로) 성능이 최대가 되는 지점은 중간인 $k = 7$에 존재한다.

# 한계점

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211244279-ccbaf27a-a9a5-41b4-ab07-3185fed018b2.png" width="800"/>
</div>

우리는 이 4개의 이미지를 보고 서로 다르다고 생각하지 않을 것이다. 분명 약간씩의 왜곡이나 다른 점은 있지만, <U>다 동일한 사람의 이미지를 기반으로 생성</U>된 결과라는 걸 알 수 있다. 그러나 KNN의 경우에는 불가능하다. 2차원 이미지의 경우 이미지 데이터가 옆으로 몇 픽셀만 이동하더라도 <U>같은 사진임에도 불구하고</U> distance가 매우 클 수 있다. 특히나 $L_2$ distance의 경우 모든 픽셀 값들의 차이가 제곱합으로 계산되기 때문에 오차가 더 심하게 나타난다. 그렇기 때문에 KNN은 이미지와 같은 고차원인 데이터에 적용하기가 힘들고, 그렇기 때문에 **neural network**를 활용하는 방법을 고안해낸다. 이 부분부터 다음에 다룰 내용이다.`,sO=`---
title: "cs231n 내용 요약 (2) - Linear classification"
category: "ai theory"
publishedAt: "2022-11-03"
thumbnail: "https://user-images.githubusercontent.com/79881119/211488989-dfb1c301-de53-4d3a-b143-e3f7bce0870a.png"
---

# 이전 글에서...

이전 포스팅 기준으로 컴퓨터비전에서 가장 대표적인 task인 image classification에 대해서 설명했다. 그리고 간단한 모델인 KNN(k -Nearest Neighbor) classifier에 대한 소개도 했었다. Image Classification에서 해결해야할 여러 문제들을 제시했고, 이러한 문제들을 해결하기 위해 data driven algorithm을 사용한다고 언급했었다. 그러나 단순히 각 샘플에 대한 거리 메트릭 비교를 통한 분류의 경우 다음과 같은 두 가지 문제점을 가지고 있다.


1. 이 classifier는 test data에 대한 대조군으로 모든 training data를 계속 기억해야한다. 그러므로 이 데이터가 계속 메모리를 차지하고 있기 때문에 메모리가 비효율적으로 사용된다.
2. 하나의 데이터를 분류해내기 위해 모든 training data와 비교해야하므로 계산 과정이 expensive하다.

​KNN 방식은 일반화 성능을 기대하기 힘들면서 동시에 메모리를 비효율적으로 사용한다는 점이 걸림돌이 된다. 이러한 문제로부터 앞으로 Image classification에 보다 효율적인 방법을 필요로 하였고, 그 다른 방법이 바로 Neural Network(신경망)을 이용한 학습이다. 따라서 이전까지 다뤘던 내용 전반은 사실 딥러닝에 대한 내용이 아니었고 computer vision과 같은 task를 어떠한 방식으로 정의하는지, 그리고 data-driven algorithm의 의미와 해당 방법론을 선택한 이유에 대해서였다.


# 신경망 회로

<U>신경망 회로 방법</U>은 인간의 신경망이 작동하는 원리를 모방한 방법이다. 

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/211441289-bef0f13e-b447-4d94-bcdf-3654f6264518.png"/>
</div>

뉴런이 정보를 전달하고 받아들이는 과정을 input $$X$$에 대해 weight $$W$$로 반응하고, activation function $$f$$(활성화 함수)를 통해 output을 내보내는 과정을 거친다. 이를 Perceptron(퍼셉트론)이라고 부르는데, 사실 퍼셉트론은 뉴런을 완전히 모방할 수 없기 때문에 dendrite, soma, axon 등등을 <U>직접 퍼셉트론의 구성 요소에 대입</U>해서 설명하는 것은 옳지 않다. 다만 정보가 전달되는 과정을 input $$X$$에 대한 affine transform $$X\\cdot W + b$$으로 정의한 후, <U>논리 복잡도를 높이기 위해</U> 비선형 함수 $$f$$를 적용한 구조라고 생각하면 된다. 조금 더 구체적으로 들어가게 되면 우리는 어떠한 input $$X$$이 연산에 들어왔을 때, 사전에 정의된 parameter $$W$$와 $$b$$를 통해 output을 내보내고 여기에 비선형 함수를 적용한 결과를 하나의 점수 혹은 결과에 대한 지표로 생각해볼 수 있다.

$$
    output = f(X \\cdot W + b)    
$$

만약 정의된 parameter $$W$$와 $$b$$가 입력 dataset에 대해 <U>의도대로 잘 동작하는 값</U>이라면, output과 label(ground truth)와의 차이를 구했을 때 차이가 $$0$$에 수렴할 것이다.

$$
    \\rho(output,~label)    
$$

바로 여기서 정의해야하는 것이 output과 label의 차이를 유의미하게 계산해줄 거리 메트릭인 $$\\rho$$이며, 이 거리 메트릭을 기준으로 신경망 회로의 파라미터를 조금씩 조정해갈 것이다. 

# Score function / Loss function / Cost function

위에서 언급한 거리 메트릭 $$\\rho$$를 적절히 설정하는 것은 중요하다. Ground truth로 작용하는 label이 어떤 task에 대한 label인지도 중요하게 적용된다. 만약 거리 메트릭 $$\\rho$$가 적절하지 않은 함수가 된다면 <U>ground truth와 output의 차이</U>를 잘 나타낼 수 없거나, 학습 과정에서 <U>수렴이 불가능한 경우</U>가 생길 수 있다. 바로 여기서 사용되는 메트릭 $$\\rho$$를 함수 관점에서 명명한 것이 loss function 혹은 cost function이다. 둘 다 단어의 뜻을 보면 어떤 기준으로부터 <U>'얼마나 모자라는지'</U>에 대한 의미가 내포되어있고, 여기서 미리 조금 스포하자면 unsupervised learning, semi-supervised learning 그리고 supervised learning 모두 결론적으로는 ground truth 역할을 대신할 수 있는 <U>기준점</U>이 필요하다. 다시 돌아와서 하고자 했던 말은 신경망 회로 방법에서는 loss function과 cost function이 최적화에 필요하다는 것이다.   
이 글을 linear classification에 대한 글이기 때문에 해당 task에 맞춰 조금 더 설명하도록 하겠다. Loss라는 개념은 어느 정도 알았는데, 여기서 <U>score function</U>이라는 개념도 추가로 언급하겠다. Score function이란 날것의 데이터(raw data)를 classification에 맞게 각 class별 점수(score)로 mapping하는 함수가 되고, loss function이 이 score function을 이용해 예측된 score와 label과의 차이를 수치화한다. 날것의 데이터란 앞서 쭉 설명했던 것과 같이 신경망의 입력으로 사용되는 input $$X$$와 같은 의미다.   
그렇다면 image를 score로 mapping한다는 것이 구체적으로 어떻게 수식화가 되는지 확인해보도록 하자. $$N$$개의 이미지 샘플이 있고, 각각의 이미지는 $$K$$개의 클래스 중 하나로 대응된다.

$$
    x_i~(i = 1,~2,~3,~\\cdots,~N) \\rightarrow y_j~(j = 1,~2,~\\cdots,~K)    
$$

각각을 행렬 차원에서 해석하게 되면 예를 들어 CIFAR-10 dataset의 input image가 $$32 \\times 32 \\times 3$$의 크기를 가지므로,

$$
    x_i \\in \\mathbb{R}^D,~D = 32 \\times 32 \\times 3 = 3072
$$

이미지 샘플 $$x_i$$를 하나의 차원을 가지는 벡터로 바꿀 수 있고, 이 벡터의 크기는 <U>이미지 텐서의 3개의 차원을 모두 곱한 값</U>이 된다. 예시로 사용한 CIFAR-10은 이름에서 알 수 있듯이 총 10개의 class로 구성된 dataset이므로, 앞서 언급한 식에서 $$K = 10$$인 경우에 해당된다.

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/211444992-b6f5a431-1518-4e5b-95a5-9f1bfeacc66f.png"/>
</div>

$$
    \\begin{aligned}
        W &\\in \\mathbb{R}^{D \\times K}, \\newline
        X &\\in \\mathbb{R}^{1 \\times D}, \\newline
        b &\\in \\mathbb{R}^{1 \\times K}
    \\end{aligned}    
$$

$$
    f(X \\cdot W + b) = f(Y),~Y \\in \\mathbb{R}^{1 \\times K}    
$$

Activation function $$f$$는 벡터의 element-wise로 연산되기 때문에 affine mapping된 $$Y$$의 크기 그대로 output이 결정된다. 따라서 신경망 회로에 의해 $$3072$$의 차원을 가지던 input 이미지가 $$10$$개의 클래스 score로 치환될 수 있다. 앞서 소개했던 것처럼 학습 가능한 parameter인 $$W$$와 $$b$$는 weight, bias로 부른다. 지금까지 길게 써온 내용을 4가지로 요약하면 다음과 같다.

1. 단일 matrix 곱인 $$X \\cdot W$$는 효율적 연산이 가능하다. 여기서 효율적이란 말은 병렬화가 가능하다는 뜻으로, $$X \\cdot W$$ 에서 각 label score가 계산되는 부분은 $$W$$의 each row vector이다. 따라서 class 수는 총 10개지만 연산이 병렬화가 가능하다.
​
2. $$(x_i, y_i)$$ 데이터는 모두 고정이다. 그러나 함수에서의 parameter인 $$W$$, $$b$$는 조정 가능하다.

3. Training data $$(x, y)$$를 신경망에 통과시키면서 데이터셋에 대한 output을 잘 예측하는 $$W$$, $$b$$를 찾는 것이 목표가 된다. 그렇기 때문에 이전처럼 training data를 계속 메모리에 가지고 있을 필요가 없다. 즉, 학습이 끝나고 나면 training data는 메모리에 유지될 필요가 없다.

4. KNN처럼 test image를 traing image들과 하나하나 비교해서 보는 것보다 훨씬 빠르다.

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/211449255-1a05c825-aaf9-438e-9ea3-a6fab2063b3c.png" width="600"/>
</div>

연산 과정을 그림으로 표현하면 위와 같이 나타낼 수 있다. 사실 여기서 그림으로 나타낸 내용은 앞서 언급했던 신경망에서의 activation function $$f$$와는 다르게 동작한다. 이 예제에서는 $$f$$가 그냥 <U>affine function</U> $$X \\cdot W + b$$를 나타내는 함수라고 생각해주면 된다. 가장 우측의 결과가 classification에 사용될 예측 score가 된다. 점수표를 기준으로 해당 인공지능은 dog score가 가장 높게 나왔기 때문에 아마도 강아지라고 예측할 것이다. 물론 이건 명백한 오답.

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/211449542-34fdc8ae-dece-4435-a04f-5444f3982fad.png" width="600"/>
</div>

이미지가 고차원의 벡터로 확장되었는데, 3072개의 axis가 있는 좌표계에서의 하나의 점으로 해석할 수 있다. 모든 image dataset $$X$$를 3072차원에 그대로 mapping하고, linear classification을 진행하는 것과 같다. 위의 그림은 3072차원을 2차원으로 줄여서 이해하기 쉽게 그려놓은 그림이라고 보면 된다.   
이전에 설명했듯이 $$W$$의 각 row vector가 각각의 label 분류에 사용되는데, 기하적으로 해석한다면 <U>row vector를 변화시키는 것</U>은 <U>classifier가 다른 방향으로 rotate</U>하는 것과 같다. 그리고 bias에 해당하는 $$b$$는 classifier을 <U>원점 기준으로 이동</U>시키는 역할이라 보면 된다.   
그리고 linear classifier를 KNN 알고리즘과 같은 <U>template matching</U>으로도 해석이 가능하다. 모든 $$X \\cdot W$$의 계산은 $$W$$의 row vector와 $$X$$(column vector)의 내적으로 해석 가능하다. 그래서 $$W$$는 <U>template, prototype</U>로 학습이 가능한 상태가 되어 벡터 상으로 가장 가까운 값을 찾아가게 된다. 결국 이 문제는 각 샘플을 prototype으로 사용하여 가장 가까운 $$K$$개의 샘플을 통해 예측을 진행하는 KNN과 동일하게 해석이 가능하다. 잘 안 와닿을 순 있지만, inner product 계산 자체가 $$L_1$$, $$L_2$$ distance를 계산한 것처럼 거리 메트릭에 해당되기 때문이다.   
이를테면 horse의 경우에도 데이터셋에 두마리의 말이 서로 마주하는 형태가 된다던지, car의 경우에 다양한 색상이나 종류의 차를 구분할 수 있어야하지만, 데이터셋 상으로 biasing된 붉은색 정보가 두드러지는 상황이 생길 수 있다. 아래 그림은 각 class에 대해 학습된 weight가 된다.

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/211450707-ed5bda42-180e-41f7-8184-be5f90ee3fef.png" width="1400"/>
</div>

따라서 단순히 training dataset을 통한 Linear classifier 계산을 한다면 위의 그림과 같이 각 class 별로 <U>weight prototype</U>을 만들어내고, 이 prototype에 새로운 sample을 projection 했을 때 유사할수록 그 값이 크게 나오는 메커니즘이 되기 때문에 결론적으로는 일반화 성능이 그리 좋다곤 말할 수 없는 상황이 된다. 이런 문제들을 해결하기 위해 이후에 hidden layer를 사용하여 prototype 형태의 학습에서 벗어나고자 하는 <U>심층 신경망 구조</U>를 고안하게 된다.

# Weight and bias

만약 bias와 weight를 따로 학습하고 연산하게 되면, 앞서 설명했던 row vector 연산의 병렬화는 bias에 대해서는 적용될 수 없다. 하지만 결국 bias가 더해지는 형태는 $$X$$와 $$W$$가 linear projection된 각각의 element에 element-wise한 합을 구하는 과정이기 때문에 굳이 따로 학습할 필요 없이 축을 추가하여 계산할 수 있다.

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/211451107-24831ca5-7421-4a22-a453-a01f393a9684.png" width="800"/>
</div>

Input에 $$1$$을 element로 추가해주고, bias를 row vector 우측으로 확장시킨다. 이렇게 되면 더이상 두 연산을 따로 처리할 필요없이 함께 최적화가 가능하다.

# Loss functions

사실상 계속 설명했던 내용은 신경망으로 하여금 <U>원하는 개수의 class 만큼 score를 예측</U>하는 과정이었다. 네트워크가 입력된 데이터에 대해 오답을 내거나 애매한 정답을 output으로 내보낼 경우 이를 수치화하고 최적화에 사용할 수 있게끔 기준을 정해야할 필요가 있다. 따라서 앞서 소개했던 식에서의 objective function인 $$\\rho$$의 기본적인 형태에 대해서 다룰 것이고, 그 중 support vector machine에 대해서 간략하게 소개해보도록 하겠다.

## Support vector machine(SVM)

SVM의 기본 원리는, 각 이미지에 대해 원하는 정답이 있을 것이며 그 정답에 해당되는 score를 다른 class에 대한 정답보다 특정 threshold 이상 margin($\\Delta$)을 주고 싶을 때 사용한다. Linear classification model에 대해 정답이 되는 class index에 대한 score는 최대화하되, 나머지 class index에 대한 score는 최소화하는 방향이다. 1부터 $$K$$까지의 class index를 나타내는 변수 $$j$$에 대해 각 $$i$$번째 샘플($$x_i$$)의 score는 다음과 같이 표현할 수 있다.

$$
    s_j = f(x_i,~W)_j
$$

SVM loss의 정의는 정답에 해당되는 class의 score를 다른 class의 score보다 특정 margin($$Delta$$) 이상으로 주고싶을 때 사용하기 때문에, $$i$$번째 샘플에 대한 SVM loss는 다음과 같다.

$$
    L_i = \\sum_{j \\neq y_i} \\max (0, s_j - s_{y_i} + \\Delta)    
$$

$$i$$번째 샘플의 정답은 $$y_i$$이고, 이는 $$K$$개의 클래스 중 하나의 값으로 매핑되어있다고 생각해보자. <U>$$s_{y_i}$$는 $$i$$번째 샘플이 정답인 $$y_i$$ 클래스일 점수</U>를 의미하고, 나머지 <U>$$s_j$$는 $$i$$번째 샘플이 정답이 아닌 $$j$$ 클래스일 점수</U>를 나타낸다. 만약 $$j$$번째 클래스일 점수가 정답인 $$y_i$$번째 클래스일 점수보다 margin 이상 작지 않다면 $\\max$ 함수 뒤에 있는 인자가 0보다 큰 값을 가지게 되므로 loss가 증가하게 된다. Loss의 정의는 task마다 다르지만 원하고자 하는 기준과 멀어질수록 큰 값을 가지게 하는 것이 일반적이고, 지금 상황에서는 정답인 클래스일 점수보다 나머지 클래스일 점수들이 margin 이상으로 낮아지지 않으면 loss 값이 증가하는 구조가 된다.

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/211475405-2a57768c-e9c3-4954-af7b-249ee97e14d7.png" width="800"/>
</div>

그림으로 표현한 것이 위의 그림이다. 결론적으로는 다른 클래스의 모든 점수를 margin 만큼 차이가 나게끔 줄일 때까지 학습이 진행된다. 이를 신경망에서의 행렬 연산으로 표현하면 다음과 같다.

$$
    L_i = \\sum_{j \\neq y_i} \\max (0, W_j^\\top x_i - W_{y_i}^\\top x_i + \\Delta)  
$$

Score 값을 구하고 싶은 class index를 $$k$$라고 했을 때, 해당 연산에 필요한 요소는 $$W$$ matrix에서의 $$k$$번째 row vector이므로 sample $$x_i$$와 내적 연산을 통해 inner production을 진행한다. 위와 같이 threshold를 기준으로 loss를 주는 방식을 hinge loss라고 부르며, 만약 네트워크의 예측에 따라 cost를 더 크게 주고 싶거나, loss를 미분 가능하게 만들고 싶다면 다음과 같은 함수를 사용할 수 있다.

$$
    \\max (0,~-)^2    
$$

하지만 SVM loss에는 큰 문제점이 있는데, 바로 학습이 완료된 $$W$$가 우리가 원하는 방향대로 특정 class의 score를 $$\\Delta$$만큼 크게 만들 수 있다고 한다면 그와 마찬가지로 $$W$$의 모든 scalar 배수들 또한 같은 역할을 할 수 있다. 예를 들어 $$\\lambda > 1$$인 모든 $$\\lambda$$에 대해, $$\\lambda W$$가 내보내는 score 또한 1보다 큰 값으로 scaling되므로 같은 조건을 만족할 수 있게 된다. Optimization 관점에서 <U>global minima</U>가 여러 곳 존재한다는 것은 <U>convex optimization의 조건에 부합하지 않는다</U>. 만약 우리가 해결하고자 하는 문제에서 이를 제한할 수 있는 조건이 없다면 학습 속도가 느려지거나 발산할 수 있는 문제가 생긴다. 따라서 이러한 문제를 없애주기 위해 regularization penalty를 주어, 보다 feasible(실제로 탐색했을때 유의미한 manifold를 의미한다)한 영역만 찾고자 한다.

$$
    R(W) = \\sum_k \\sum_l W_{k,~l}^2    
$$

2차원 matrix $$W$$의 $$L_2$$ norm은 정의에 입각하여 위와 같이 구할 수 있고, 수많은 $$W$$들 중에서 우리는 원점으로부터 거리가 가장 가까운($$n$$차원의 hypersphere) 곳이 최적화가 되었을 때 이상적인 weight가 되게끔 해준다. 결국 우리가 최소화해야할 loss는 앞서 언급했던 <U>hinge loss</U>와, 방금 위에서 소개한 <U>$$L_2$$ regularization loss</U>가 된다.

$$
    \\begin{aligned}
        L =& \\frac{1}{N} \\sum_i L_i + \\lambda R(W) \\newline
        L =& \\frac{1}{N} \\sum_i \\sum_{j \\neq y_i} \\max \\left( 0, W_j^\\top x_i - W_{y_i}^\\top x_i + \\Delta \\right) + \\lambda \\sum_k \\sum_l W_{k,~l}^2
    \\end{aligned}
$$

보통 최적화하고 싶은 loss function이 여러 개일 때, hyperparameter $$\\lambda$$를 사용자가 직접 정하고, 이를 cross-validation하면서 찾아간다. 최적화 관점에서는 앞서 설명했던 것처럼 보다 convex optimization에 가깝게 해주기 위해 정규화 term을 추가해주고, 학습이 끝났을 때의 성능을 기준으로 생각해보면 weight parameter가 클수록 input의 값 변화에 따라 변동성이 큰 네트워크가 생성될 수 있기 때문에 overfitting의 문제가 있다. 따라서 보통 regularization이라고 이름이 붙은 loss나 방법들의 경우 설명하는 내용들을 찾아보면 오버피팅을 방지하는 효과가 있기 때문에 사용한다라고 설명되어있다. 물론 정답이긴 하지만 regularization을 사용하는 이유의 전부가 아니라는 점만 말하고 넘어가고 싶었다.

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/211479174-82d490d5-3ddb-42da-a788-ed431b0cd512.png" width="800"/>
</div>

## Hyperparameters in SVM

앞서 조정할 수 있는 hyperparameter로 $$\\lambda$$를 설명했는데, 사실 hinge loss 부분의 $$Delta$$ 역시 사용자가 직접 정해줘야하는 문제가 있다. 하지만 앞서 말했던 것과 같이 $$\\lambda$$가 weight의 scale에 대해 고려해줄 수 있는 값이 되기 때문에, 만약 $$\\lambda$$ 값을 조정하게 되면 그에 맞게 $$Delta$$도 조정해야한다. 두 하이퍼파라미터가 성능에 있어 독립적으로 작동하는 것이 아닌 서로 연관된다는 점에서 우리는 <U>$$Delta$$를 굳이 변화시킬 필요 없이</U> weight의 크기를 조절해줄 수 있는 $$\\lambda$$ 값만 hyperparameter로 고려할 수 있다. 계속 설명했던 SVM classifier는 class의 개수에 무관하게 적용될 수 있는 식이었다. Class 개수가 2인 binary classification에 대해서 생각해보면 만약 margin을 $$1$$로 고정해서 사용할 경우에,

$$
    L_i = C \\max \\left(0,~1-y_iw^\\top x_i \\right) + R(W)    
$$

위와 같이 표현할 수 있고 식에서의 상수 $$C$$는 $$\\lambda$$의 역수에 비례하는 hyperparameter로 사용된다. Binary support vector machine은 $$y_i$$가 class의 인덱스를 나타내는 것이 아닌 $$-1,~1$$의 값으로 사용한다. 

## Softmax

앞서 소개했던 SVM은 classifier의 한 종류였고, 또다른 classifier로 대표적으로 사용되는 softmax classifier를 소개하도록 하겠다. 사실상 최근에 진행한 모든 프로젝트나 딥러닝 관련 과제들에서 SVM을 사용했던 적은 없었으며, 대부분의 energy based function을 적용하는 과정에서 softmax 함수 형태를 자주 볼 수 있기 때문에 어찌보면 SVM보다 조금 더 중요한 개념이라고 해도 될 것 같다. 우선 softmax는 <U>binary logistic regression</U>을 multiclass를 가지는 classifier에 사용했다고 요약할 수 있다. Binary logistic regression이란, 대상이 되는 데이터가 분류되는 카테고리가 $$0$$과 $$1$$, 두 개라고 생각하고 시작한다. 그리고 각각의 카테고리로 분류될 확률의 합은 1이다. Logistic regression은 선형 모델을 특수하게 사용하기 때문에 단순한 linear regression과는 차이가 있다. 하지만 logistic model의 경우 target이 되는 $$y$$의 범위가 $$0$$부터 $$1$$ 사이에 놓이게 된다. 다음과 같은 예시를 보자. 만약 독립변수 $$x$$에 대해 $$0$$과 $$1$$, 두 개의 값을 가지는 종속 변수 $$y$$를 예측하는 모델이 필요하고, 이를 일반적인 선형 모델로 구현한다고 생각해보면 선형 모델은 $$y = Wx+b$$로 나타낼 수 있다. 선형 회귀는 여러 데이터 점을 대표하는 1차 방정식의 기울기(weight)와 절편(bias)를 구하는 것인데, 종속 변수가 오직 두 개만 존재하는 classification에서는 선형 모델링이 크게 도움되지 않는다.

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/211484095-3c025ec6-cb8a-4343-b6c2-3ad650aafd82.png" width="800"/>
</div>

선형 함수의 치역은 우리가 parameter로 하여금 데이터를 어떠한 방식으로 주어도 $$0$$과 $$1$$ 사이의 값이 아닌 무한대의 영역에 뻗어있게 된다. 사실상 우리가 원하는 결과랑 무관하기 때문에, 실제로 우리가 얻고자 하는 종속 변수 $$y$$인 $$0$$과 $$1$$을 기준으로 mapping이 가능한 방법을 고안하였다. 이 방법이 바로 logistic 모형인 $$g(x) = \\frac{e^x}{1+e^x}$$과 검벨 모형 $$g(x) = e^{-e^x}$$가 된다. 그러나 검벨 모형의 경우 exponential 연산이 중첩되기 때문에 연산이 어려웠고, 이 중 계산상 간단한 logistic model을 사용하게 되었다. 

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/211484962-8bc6cedf-cbba-453c-9be5-fe2919b0f5b7.png" width="400"/>
</div>

Logistic function의 형태를 보면 알 수 있듯이 연속 독립 변수인 $$x$$에 대해 종속 변수의 결과가 항상 0과 1 사이의 값으로 나오게 된다. 연산에 사용되는 개념으로는 <U>Odds</U>와 <U>Logit</U>이 있다. Odds는 실패에 대해 성공할 확률의 비율을 의미한다. 만약 특정 독립 변수가 들어왔을 때 종속 변수가 $$1$$에 속할 확률을 성공할 확률이라고 생각한다면,

$$
    \\text{Odds} = \\frac{p(y = 1 \\vert x)}{1-p(y = 1 \\vert x)}    
$$

위와 같이 표현이 되고, 확률인 $$p$$는 0부터 1까지의 값을 가질 수 있기 때문에 여기에 log를 취한 logit은

$$
    \\text{Logit} = \\log(\\text{Odds}) = \\log \\left( \\frac{p}{1-p} \\right)    
$$

으로, 실수 전체의 범위를 갖게 된다. 앞서 말했던 것과 같이 종속($$p$$)에 대해서 독립($$x$$)의 관계를 정의할 수 있는 함수를 가져왔고, 이제는 linear regression을 적용해볼 수 있다.

$$
    \\log \\left( \\frac{p}{1-p} \\right) = Wx+b    
$$

하지만 이 식은 $$y$$에 대한 관계로 정의되지 않았기 때문에 이를 직접 종속 변수인 $$p$$를 통해 표현하게 되면 logistic regression 문제로 치환할 수 있다.

$$
    p = \\frac{e^{b+Wx}}{1+e^{b+Wx}}    
$$

즉, 신경망 조건에서도 softmax를 사용한 classification이 가능함을 증명할 수 있다. 지금까지는 binary에 대해서만 살펴보았고, 만약 multiclass인 경우에는 이를 보다 확장시킨 개념으로 바꿀 수 있다. Logistic regression을 사용할 때 사용되는 logistic 함수는 input에 대한 score를 class의 확률 추정으로 변환할 수 있다. 따라서 SVM의 classifier에서는 score를 그대로 사용했다면, 여기서는 logistic mapping을 토대로 확률을 사용할 수 있게 된다.

$$
    p_i = \\frac{e^{z_i}}{ \\sum_{j = 1}^k e^{z_j}},~\\text{for }i = 1,~2,~cdots,~k    
$$

이를 softmax function이라고 부른다. $$i$$가 의미하는 것은 클래스의 인덱스이고, $$z_i$$는 score라고 생각하면 된다. 결국 $$k$$개의 class를 확률로 반환하기 위해서는 $$k$$개의 score를 logistic 연산을 통해 $$0 ~ 1$$ 사이의 값을 가지는 벡터로 변환한다. 다르게 표현하면 <U>normalized probability</U>라고도 한다. 변환된 벡터에 대한 loss를 계산할 때, 기준이 되는 목표는 class에 맞는 index의 확률을 $$1$$에 가깝게 만드는 것이고, loss를 연산하는 과정에서는 softmax 결과에 negative log를 취한 값을 사용한다.

$$
    L_i = -\\log \\left( \\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}} \\right) = -f_{y_i} + \\log \\sum_j e^{f_j}
$$

결국 이를 최소화하는 것은 목표가 되는 class의 점수 $$f_{y_i}$$를 키우고, 나머지 점수들을 낮추는 방향이 되기 때문에 결론적으로는 SVM과 softmax의 결은 동일하다고 할 수 있다. 이렇게 해석되는 loss를 cross-entropy loss라고 부른다.

# Cross entropy loss and Information theory

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/211488989-dfb1c301-de53-4d3a-b143-e3f7bce0870a.png" width="400"/>
</div>

일반적으로 혼란스럽거나 불확실할 때 엔트로피가 높다는 표현을 사용한다. 확률적으로 생각하게 되면 특정 정보가 어떤 것인지 확신할 수 없을 때 엔트로피가 크다고 표현할 수 있다. 샤넌 엔트로피라고 부르는 식은 가능한 outcome(결과 혹은 정보) $$x_1,sim x_n$$에 대해 각각의 결과가 도출될 수 있는 probability인 $$p(x_1),sim p(x_n)$$가 있다고 했을 때 다음과 같이 표현할 수 있다.

$$
    H(X) = -\\sum_{i = 1}^n p(x_i) \\log p(x_i)    
$$

이 식은 확률 $$p(x)$$가 $$p(x)$$에 대해 보존할 수 있는 정보량으로 해석된다. 만약 각 결과가 나올 확률을 확신할 수 없다면 $$p(x_i)$$가 균등하게 분포될 것이고, 만약 그렇다면 모든 결과를 품기 위해 정보 보존량은 그만큼 증가해야하기 때문에 entropy 값이 증가한다. 그와 반대로 만약 각 결과가 나올 확률을 확신할 수 있을 정도로 $$p(x_i)$$가 균등하지 않다면 모든 결과를 품지 않아도 결과에 대한 정보 보존이 가능하기 때문에 entropy 값이 감소한다. 이번에는 만약 예측된 $$p(x)$$가 $$q(x)$$에 대한 정보량을 얼마나 보존할 수 있는지 확인해야하는 경우를 생각해보자. 우리는 예측해야할 분포에 대한 정보를 알고 있고, 이를 ground truth $$q(x)$$라고 생각해보자.

$$
    H(p,~q) = -\\sum_{i = 1}^n q(x_i) \\log p(x_i)  
$$

Classification의 관점에서 보면 $$x_i$$는 각각 특정 class index를 기준으로 정답인 class는 확률 $$1$$, 나머지 class는 확률 $$0$$이 되는 것이 이상적이다. 앞서 계속 본 softmax 식에 해당되는 $$\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}$$가 예측된 확률 $$p(x_i)$$이다. Cross-entropy 식을 다음과 같이 분리하면,

$$
    H(p,~q) = -\\sum_{i = 1}^n q(x_i) \\log \\left( \\frac{p(x_i)}{q(x_i)} \\right) + q(x_i) \\log q(x_i) 
$$

위의 식에서 좌측의 term은 KL divergence로, 두 분포 $$p(x)$$와 $$q(x)$$의 거리를 표현한다. 이 값은 거리에 해당되므로 0보다 크거나 같은 값을 가지기 때문에 다음과 같은 대소 관계를 가진다.

$$
    H(p,~q) = D_{KL}(q \\parallel p) + H(q) \\ge H(q)
$$

따라서 cross-entropy는 <U>entropy</U>보다 크거나 같을 수 밖에 없다. 원래의 확률인 $$H(q)$$는 상수로 취급하기 때문에 이에 대한 미분값은 $$0$$이 되고, 결론적으로는 학습에 영향을 미치지 않기 때문에 cross-entropy를 최적화하는 것은 KL divergence를 최적화하는 것과 같다.   
모델이 예측하는 score는 normalized되지 않은 값을 가지는데, 이를 exponential 함수가 포함된 softmax를 통해 normalized probability로 바꾼다. <U>특정 이미지가 어떤 class에 속할 확률</U>은 곧 likelihood를 최대화하는 것과 같으며, 다음의 Bayes에서 MLE와 같은 의미를 가질 수 있다.

$$
    p(x_i \\vert y_i) = \\frac{p(y_i \\vert x_i)p(x_i)}{p(y_i)}    
$$

하지만 이럴 경우 input의 prior를 고려할 수 없다는 문제가 있다. 하지만 신경망에서는 이를 matrix $$W$$를 통해 대체가 가능하다. 만약 $$y_i$$가 input $$x_i$$에 대해 parameter $$W$$를 통한 변환으로 해석된다면, 기존의 prior를 다음과 같이 바꿀 수 있다.

$$
    p(x_i \\vert W) = \\frac{p(y_i \\vert W)p(W)}{p(y_i)}    
$$

따라서 classification에 대한 likelihood를 최적화하는 과정에서 동시에 $$W$$가 prior 역할을 대신해줄 수 있기 때문에 MAP(Maximum a posterior)로 해석 가능하다는 관점이다.

# Normalization trick
위의 식대로 exponential을 계산하고, 이에 log likelihood를 적용하는 과정을 거치게 되면 denominator($$\\sum_j e^{f_j}$$)로 사용되는 exponential의 합이 매우 커지는 문제가 발생한다. 이는 score가 normalized되지 않았기 때문인데, 연산 과정에서 값이 너무 커지게 되면 오버플로우가 발생하거나 일부 값을 유실할 수 있기 때문에 값을 줄여서 연산이 가능하게끔 다음과 같은 trick을 사용한다.

$$
    \\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}} = \\frac{e^{f_{y_i} - \\delta}}{\\sum_j e^{f_j - \\delta}}    
$$

분모와 분자에 모두 같은 값인 $$e^{-\\delta}$$로 나눠주면 계산값은 동일하다. 따라서 원래의 score에서 가장 maximum value인 $$f_{j^*} = \\max (f_j)$$에 대해서,

$$
    \\frac{e^{f_{y_i} - f_{j^*}}}{\\sum_j e^{f_j - f_{j^*}}}
$$

이와 같이 최댓값을 기준으로 re-scaling해주게 된다면, 최댓값을 기준으로 모두 0보다 작거나 같은 값이 되기 때문에 exponential 값이 $$0 \\sim 1$$에 형성될 수 있다.

# SVM과 Softmax, 어떤 것이 더 좋을까?
지금까지 linear classification에서 사용될 수 있는 <U>두 가지 classifier</U>인 support vector machine(SVM)과 softmax에 대해서 살펴보았다.

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/211558562-1341ba8d-b681-40e9-be6d-cc804ee49fc4.png" width="800"/>
</div>

**SVM**은 신경망의 output으로 나오는 score function의 결과를 토대로 <U>hinge loss</U>를 적용하는 분류기이며, **Softmax**는 신경망의 output으로 나오는 score function을 softmax를 통해 <U>normalized probability</U>로 치환하여, MLE 혹은 MAP 최적화를 진행하는 <U>cross entropy loss</U>를 사용한다.   

## SVM에서의 regularization, softmax에서는?

앞서 SVM에서는 동일한 loss를 내보내는 $$W$$ 값이 존재할 수 있다는 것을 언급했다. 그럼에도 불구하고 softmax에서도 $$L_2$$ regularization을 사용하는 이유에 대해서 살펴보면 다음과 같다.

$$
    (1, -2, 0) \\rightarrow (e^1, e^{-2}, e^0) = (2.71, 0.14, 1) \\rightarrow (0.7, 0.04, 0.26)    
$$

어떤 $$W$$를 통한 neural network 연산을 통해 계산된 결과가 위와 같다고 하자. Softmax classifier를 사용하기 때문에 score인 $$(1, -2, 0)$$을 softmax 함수를 사용한 normalized probability $$(0.7, 0.04, 0.26)$$으로 바꿀 수 있다. 이 $$W$$에 regularization을 주어 이보다 0.5배만큼 element가 작아진 neural network parameter에 대해서 계산된 확률은 다음과 같다.

$$
    (0.5, -1, 0) \\rightarrow (e^{0.5}, e^{-1}, e^0) = (1.65, 0.37, 1) \\rightarrow (0.55, 0.12, 0.33)    
$$

결과가 더 diffuse된(보다 dense한 확률 분포라고 표현하며, 흔히 엔트로피가 낮은 상태로 표현함) 형태가 되었다. 즉, weight parameter가 작아지려 하면 할수록 output probability는 보다 uniform해진다는 것. 결국 SVM의 점수표와 비교하여 절대적인 값이나 차이를 반영한다기 보다는 각 확률의 대소는 그대로 가져가되, confidence에 차이가 생기게 된다.

## 결론은..

SVM과 Softmax의 퍼포먼스에 대한 차이가 그리 크지는 않다. 그래서 task마다도, 사람들 마다도 어떤 classifier가 더 적합하다고는 정답을 내릴 수 없다. Softmax classifier에 비해 SVM이 margin보다 큰 점수차에 대한 loss를 무시할 수 있기 때문에 더 <U>local objective</U>(집중할 수 있는 부분에만 신경을 쓰는 것)하다고 표현된다. 반면에 Softmax는 아무리 score가 차이가 나더라도 <U>loss가 0이 되지 않기 때문에</U> 이러한 조건을 만족하지 않는다. 이를 요약하자면 SVM은 한 번 조건을 만족하게 되면 학습을 멈추게 되고, softmax는 계속 학습을 진행하고, 성능을 높이려는 방향으로 parameter 학습을 진행한다.   
나름의 장단점이 있기에 어떤 classifier가 더 좋다고는 확신할 수 없지만, <U>개인적으로는</U>(그냥 제 의견입니다만) softmax classifier가 지속적으로 학습이 가능하다는 점, logistic 연산을 통해 계산된 class의 confidence를 활용할 수 있다는 점에서 더 좋아보인다.`,oO=`---
title: "cs231n 내용 요약 (3) - Optimization(최적화)"
category: "ai theory"
publishedAt: "2022-11-04"
thumbnail: "https://user-images.githubusercontent.com/79881119/211700575-e61dc169-0b31-427a-8dc7-6f1a74bfad84.png"
---

# 신경망 학습

바로 이전 글에서는 linear classification에서 사용할 수 있는 두 classifier인 SVM(support vector machine)과 softmax에 대해 소개하였다. 대표적인 특징으로 **SVM**은 각 class 별 점수를 affine function $f$를 통해 구한 뒤, <U>정답에 해당되는 class의 점수</U>를 다른 class의 점수보다 특정 <U>margin($\\Delta$) 이상</U> 차이나게끔 구별하게끔 동작하였고, **softmax**는 이런 점수를 <U>logistic 함수</U>를 활용, normalized probability로 바꾸어 <U>KL divergence를 최소화</U>하게끔 동작하였다. 그래서 결론에서도 언급했던 것처럼, regularization을 제외한 loss term에서는 SVM은 특정 조건만 만족하면 loss가 0이 되어서 학습을 중단하는 반면에, softmax는 지속적인 학습이 가능하다는 특징이 있었다.   
그렇다면, 구체적으로 loss function을 통해 parameter를 최적화하는 방법들에 대해서 알아보도록 하자. 해당 내용은 최적화 이론 관련된 수업을 듣게 되면 보다 이해하기 쉬운데, 딥러닝에서는 주로 gradient based approach만 다루기 때문에 가끔 몇몇 논문에서 **penalty term**이나 **projection**, 혹은 **Jacobian, Hessian** 등등 나오게 되면 당황하는 경우가 있다. 사실 generative model에서는 굉장히 유명한 논문 중 하나인 WGAN 또한 최적화 이론의 linear programming, duality form 등등 <U>일반적인 딥러닝 지식으로는 이해할 수 없는 개념들</U>이 많기 때문에 보다 엄밀하게 이해하고자 한다면 probability and random variables, optimization theory 관련 수업을 같이 수강하는 것을 추천한다.   
서론이 길었고, 신경망의 학습법에 다루기 위해 두 개의 주요 개념을 먼저 가지고 오면, 하나는 **score function**이고, 또다른 하나는 **loss function**이다.

1. Score function은 예제에서의 $f$와 같으며, parameter로 구성되어있고 raw image pixel(이미지 데이터)를 각 class별 점수로 치환하는/매칭하는 함수이다.
2. Loss function은 특정 parameter를 통해 계산된 score 결과가 우리가 원하는 기준과 얼마나 가까운지(부합하는지)에 대한 척도가 된다. 측정하는 방식으로는 SVM의 hinge loss, softmax의 cross entropy loss를 설명했었다.

$$
    \\begin{aligned}
        L_i =& \\sum_{j \\neq y_i} \\max \\left( 0, W_j^\\top x_i - W_{y_i}^\\top x_i + \\Delta \\right) \\newline
        L_i =& -\\log \\left( \\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}} \\right) = -f_{y_i} + \\log \\sum_j e^{f_j}
    \\end{aligned}    
$$

위의 식은 각각 SVM에서 사용되는 hinge loss와 softmax에서 사용되는 cross-entropy loss를 식으로 표현한 것이다. 결국 우리가 score function, loss function을 각 classifier에 대해서 정의한 이유는 현재 parameter($W,~b$)에서의 결과를 토대로 기준점에서 얼만큼 떨어져있는지 파악하고(loss function), 이를 활용하여 우리가 원하는 score function $f$를 찾아가고 싶은 것이다. Score function $f$를 구성하는 두 인자인 <U>weight</U>($W$)와 <U>bias</U>($b$)를 최적화하는 과정이므로, 이 부분을 해결할 수 있는 수학적 방법론이 곧 <U>optimization theory</U>고 지금부터 신경망 모델 및 딥러닝에서 사용되는 가장 기본적인 <U>최적화 방법</U>에 대해서 다룰 것이다.


# Optimization
Loss function이 최솟값을 가지도록 하고 싶다면, 가장 간단한 방법은 loss function의 개형을 아는 것이다. 고등학교 수학에서 배웠던 이차함수 개형에서 최솟값 혹은 최댓값을 찾는 문제를 기억할 것이다. 우리가 해당 함수의 극값을 구할 때 공식을 사용하여 간단하게 구할 수 있었던 이유는 이차함수가 대표적인 convex(볼록), concave(오목) function에 해당되며, convex 및 concave optimization에서는 optimal solution이 유일하게 존재하고, 그 <U>optimal solution은 gradient가 0인 꼭짓점에 존재했기 때문</U>이다. 우리가 정의한 loss function도 이러했다면 보다 쉬운 문제가 되었겠지만, 안타깝게도 고차원의 데이터(이미지, 텍스트 임베딩 등등)를 다루는 상황에서는 간단한 함수의 형태를 가정할 수 없다는 것이 문제가 된다. 일반적으로 이렇게 non-analytic 함수의 형태를 분석할 때 주로 사용하는 방법은 feasible direction에 대한 constraints를 주고 분석하는 것이다.   
예를 들어 우리가 개형을 알 수 없는 함수 $L(\\cdot)$이 있고, 현재의 weight $W$에 대해 loss 값을 $L(W)$로 측정했다고 생각해보자. 어느 방향이 global optimal solution을 찾을 수 있는 방향일지는 모르지만, $W$와 같은 dimension을 가지는 방향 벡터 $W_1$을 정의할 수 있고, 임의의 step size $\\alpha$에 대해 단계적으로 진행하면서 $L(W + \\alpha W_1)$ 직선 상의 loss function value를 구할 수 있다.

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/211700575-e61dc169-0b31-427a-8dc7-6f1a74bfad84.png" width="800"/>
</div>

그림에서 좌측에 보이는 것이 바로 위에서 언급했던 것과 같이 loss function의 현재 값($L(W)$)을 기준으로 특정 방향으로 진행했을 때의 함숫값을 분석한 경우의 예시가 된다. 이처럼 feasible direction을 가정하고, 해당 방향으로 진행하게 되면 함수 전체의 형태는 시각화할 수 없지만 특정 범위 내에서의 loss function을 분석할 수 있다. 마찬가지로 만약 feasible direction에 방향 벡터 하나를 더 추가한다면 $L(W + \\alpha W_1 + \\beta W_2)$가 될 수 있고, 이렇게 될 경우 관찰할 수 있는 축이 두 개가 생기기 때문에 우측이나 중앙과 같이 colormap으로 그 형태를 확인할 수 있는 것을 볼 수 있다. 같은 색을 가지는 부분이 그 높이(함숫값)이 같은 부분이 되고, 이를 contour line이라고 부른다. 축은 contour line이 변화하는 방향(contour line에 수직인 방향)으로 그려지게 된다. 실제로 SVM에서 사용되는 hinge loss를 예시로 들어보도록 하자.

$$
    L_i = \\sum_{j \\neq y_i} \\max \\left( 0, W_j^\\top x_i - W_{y_i}^\\top x_i + 1 \\right)
$$

만약 $\\Delta = 1$인 경우 hinge loss는 위와 같이 표현되며, 데이터셋으로는 <U>1차원의 점으로 구성된 3개의 샘플</U>을 사용한다고 생각해보자. 각 샘플에 대해서 loss를 구하면 다음과 같다.

$$
    \\begin{aligned}
        L_0 =& \\sum_{j \\neq y_0} \\max \\left( 0, W_j^\\top x_0 - W_{y_i}^\\top x_0 + 1 \\right) \\newline
        L_1 =& \\sum_{j \\neq y_1} \\max \\left( 0, W_j^\\top x_1 - W_{y_i}^\\top x_1 + 1 \\right) \\newline
        L_2 =& \\sum_{j \\neq y_2} \\max \\left( 0, W_j^\\top x_2 - W_{y_i}^\\top x_2 + 1 \\right)
    \\end{aligned}
$$

보다 간단히 확인해보기 위해 regularization term은 우선 무시하도록 하자. 아무튼 SVM에서 사용되는 hinge loss는 모든 샘플에 대해서 구한 값을 기준으로 평균을 사용한다.

$$
    L = \\frac{L_0 + L_1 + L_2}{3}    
$$

앞서 가정했던 상황은 1차원의 점으로 구성된 3개의 데이터 샘플이었기 때문에 $x_i, w_j$는 스칼라와 같다. 가로축을 weight라고 생각하고 세로축을 각 loss value($L_0,~L_1,~L_2$)라고 생각하면 다음과 같이 그래프를 그려볼 수 있다.

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/211702662-7272dcce-7541-4032-b981-34057e3acbbf.png" width="800"/>
</div>

세 loss function 모두 최소화 될 수 있는 지점이 바닥과 평평하게 맞닿아 있는 부분의 모든 $W$에 해당될 것이다. 여기서 생각해볼 수 있는 사실은 SVM에서 사용된 hinge loss function이 고차원으로 올라가게 되더라도 convex function임을 유지할 수 있지 않을까라는 사실이다. 그러나 실질적으로 convex optimization을 가정하고 문제 풀이를 하기에는 고차원의 데이터에 대한 loss의 전체 구조는 불명확하기 때문에 적절하지는 않다고 판단된다. 그리고 convex optimization을 진행하는 과정에서 함수의 gradient 및 hessian을 구해야할 경우가 생기는데, 각각 모두 loss function이 weight 전체에 대해 미분이 가능하다는 전제가 있어야하기 때문에 이 또한 하나의 constraint로 작용할 수 있다.   


# Optimize methods

## Random search

결국 특정 weight가 loss function을 최소화할 수 있게 하려면 <U>다른 방법이 필요</U>하다. 그 중 가장 먼저 생각해볼 수 있는 알고리즘은 일종의 brute-force인데, Weight를 무작위로 대입하고 이를 토대로 $L(W)$를 연산한 뒤에 기존에 연산했던 loss에 비해 작은 값이 나온다면 $W$를 업데이트하는 방식이다. 그러나 이 과정은 컨셉만 확인하더라도 알 수 있듯이, weight를 랜덤 값으로 할당하고 이 중에서 가장 좋은 성능을 보이는 weight를 찾는 방법이기 때문에 <U>최적화가 거의 불가능</U>하다고 말할 수 있다. 실수 차원에서의 weight parameter는 무한에 가까운 경우의 수를 가지기 때문에 단순히 랜덤한 예측으로 최적의 weight를 찾을 확률은 0에 가깝다.   
그렇기 때문에, 우리는 랜덤한 기준으로 weight를 잡고(이를 초기화라고 부른다) 그 weight를 <U>어떤 방향으로 변화시켰을 때</U> 성능 개선이 이루어지는 지에 대해 초점을 맞출 것이다. 한번에 global optimal solution을 가지는 weight를 찾는 것은 불가능하지만 지금보다 나은 weight를 찾는 것은 가능하다. 흔히 눈을 가린 hiker가 하산하는 상황으로 비유하곤 한다. 눈을 가린 hiker는 <U>산의 가장 밑부분</U>이 어딘지는 모르지만 지금 서 있는 위치에서 조금씩 움직였을 때, <U>어떤 위치로 가야만 내려가는 길인지</U> 알 수 있다.

## Random local search

위에서 언급한 것과 같이 지금 위치에서 모든 feasible direction으로 움직여본 뒤, 이 중 loss value를 가장 최소로 만드는 방향으로 이동하고 같은 과정을 반복한다. 하지만 움직인 모든 곳에서 모든 방향의 함숫값을 연산해야하기 때문에 이 또한 비효율적이라고 말할 수 있다. 이 정도면 눈을 가린 hiker라기 보다는 <U>눈도 가리고 기울기를 느끼지 못하는 hiker</U>라고 보는게 좀 더 그럴듯한 비유가 될 것 같다.

## Following the gradient

굳이 모든 방향을 탐색해보지 않더라도 현재 위치에서 함숫값을 최소로 만들 수 있는 방향에 대해 알 수 있는 방법이 있다. 바로 gradient를 계산하는 것이다. Gradient의 단순한 정의는 함수의 특정 위치에서 변화량 혹은 기울기인데, 이를 다르게 표현하면 <U>함수의 특정 위치에서 함숫값을 가장 크게 변화시킬 수 있는 방향</U>으로 정의된다.

$$
    \\frac{df(x)}{dx} = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h}   
$$

도함수의 정의는 위와 같고, gradient는 이를 다차원으로 확장시킨 개념과 같다.

$$
    \\nabla f(x) = \\begin{bmatrix}
        \\frac{\\partial f(x)}{\\partial x_1} \\newline
        \\frac{\\partial f(x)}{\\partial x_2} \\newline
        \\vdots \\newline
        \\frac{\\partial f(x)}{\\partial x_n}
    \\end{bmatrix}   
$$

Gradient를 구하는 방법으로는 미소 단위인 $\\delta$에 대해 도함수의 정의를 따라 계산하는 numerical gradient가 있고, 실제 function이 미분 가능한 형태로 주어질 경우 analytic하게 구하는 gradient가 있다.

$$
    \\begin{aligned}
        \\frac{df(x)}{dx} =& \\frac{f(x+\\delta) - f(x)}{\\delta} \\newline
        \\frac{df(x)}{dx} =& \\frac{f(x+\\delta/2) - f(x-\\delta/2)}{\\delta}
    \\end{aligned}   
$$

Numerical gradient를 구할 때 $\\delta$에 대해 대칭으로 구하는 방법이 있고, 위와 같이 원래 도함수 정의에 맞게 구하는 방법이 있다. 아래 방법이 조금 더 tangential line의 기울기와 가깝지 않을까 추측해본다. 이렇게 구한 gradient는 앞서 설명했던 것처럼 함수의 특정 지점에서 함숫값을 최대로 만드는 방향에 대한 정보라고 했기 때문에, 이 방향에 <U>negative value</U>만큼 weight를 주게 되면 함숫값을 최소로 만드는 방향이 된다.

$$
    W_{i+1} = W_i - \\alpha \\nabla_W L(W_i))    
$$

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/211714508-593931e0-4b0b-4c0e-bed6-1ead7548f0ad.png" width="800"/>
</div>

다만 gradient의 반대 방향으로 얼마나 parameter를 update하는가는 hyperparameter로 정의되며(step size $\\alpha$), 이 값이 너무 작을 경우 수렴 속도가 너무 느리거나 너무 클 경우 수렴하지 않고 발산하는 문제가 발생하게 된다. Gradient에 대한 step size를 너무 크게 주었을 경우 <U>overshoot</U> 되었다고 표현하며, 더이상 하강하지 못하고 고점에서 진동하는 경우나, 혹은 역으로 더 높은 위치로 이동하게 된다.

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/211714870-7a29541f-c6f8-4390-8ad9-4d6042e735bb.png" width="300"/>
</div>

그렇기 때문에 적절한 step size를 정해주는 것이 중요하다. 앞서 말했던 것과 같이 gradient를 구하는 방법은 numerical과 analytic이 있다고 했었다. 그러나 사실 우리가 앞서 정의했던 hinge loss나 cross entropy loss의 경우 $W$에 대해서 미분이 가능하며, 이는 <U>analytic한 형태로 표현이 가능</U>하다.

$$
    L_i = \\sum_{j \\neq y_i} \\max \\left( 0, W_j^\\top x_i - W_{y_i}^\\top x_i + 1 \\right)
$$

위의 식은 sample $i$에 대한 hinge loss를 표현한 식이다. 이 식을 $W_{y_i}$(weight parameter 중 정답인 class의 score을 구하기 위한 요쇼)에 대해서 편미분하게 되면, 다음과 같이 표현할 수 있다. 다만 $\\max$ 연산은 조건에 따라 미분이 달라지기 때문에 엄밀히 따지자면 gradient가 아닌 sub-gradient의 개념으로 접근하게 된다.

$$
    \\nabla_{w_{y_i}} = -\\sum_{j \\neq y_i} \\mathbb{1} \\left(W_j^\\top x_i - W_{y_i}^\\top x_i + 1 > 0 \\right) x_i
$$

결국 loss 계산은 정답인 class에 대해서 나머지 class들이 점수 상으로 <U>얼마나 거리가 떨어져있는지</U> 계산하는 과정이므로, 만약 margin인 $1$보다 그 값이 크다면 $\\max \\left( 0, W_j^\\top x_i - W_{y_i}^\\top x_i + 1 \\right) = W_j^\\top x_i - W_{y_i}^\\top x_i + 1$이 되기 때문에,

$$
    \\nabla_{w_{y_i}}^j = 
    \\begin{cases}
        \\frac{\\partial}{\\partial W_{y_i}} \\left( W_j^\\top x_i - W_{y_i}^\\top x_i + 1 \\right) = -x_i, & W_j^\\top x_i - W_{y_i}^\\top x_i + 1 > 0 \\newline
        0, & W_j^\\top x_i - W_{y_i}^\\top x_i + 1 \\leq 0
    \\end{cases}
$$

위와 같이 표현할 수 있다. 모든 $j$에 대해서 더한 것이 구하고자 하는 loss의 analytic form이 된다. 위의 식은 정답인 class에 대한 row를 기준으로 한 결과고, 만약 정답이 아닌 class에 대한 row를 기준으로 한다면

$$
    \\nabla_{w_j} = 
    \\begin{cases}
        \\frac{\\partial}{\\partial W_j} \\left( W_j^\\top x_i - W_{y_i}^\\top x_i + 1 \\right) = x_i, & W_j^\\top x_i - W_{y_i}^\\top x_i + 1 > 0 \\newline
        0, & W_j^\\top x_i - W_{y_i}^\\top x_i + 1 \\leq 0
    \\end{cases}
$$

와 같이 계산할 수 있다.

$$
    \\nabla_{w_{y_i}}^j = 
    \\begin{cases}
        \\frac{\\partial}{\\partial W_{y_i}} \\left( W_j^\\top x_i - W_{y_i}^\\top x_i + 1 \\right) = -x_i & W_j^\\top x_i - W_{y_i}^\\top x_i + 1 > 0 \\newline
        0 & W_j^\\top x_i - W_{y_i}^\\top x_i + 1 \\leq 0
    \\end{cases}
$$

# Stochastic gradient descent, mini-batch gradient descent의 차이

이렇게 각 샘플당 gradient를 계산하는 과정은 상당히 오래 걸린다. 만약 모델이 무겁거나 더 복잡한 형태라면 <U>샘플 하나하나의 gradient를 계산하는 과정은</U> 비효율적이다. 따라서 샘플을 여러 묶음으로 분리하여 gradient를 계산하는 **mini-batch gradient descent**를 사용하기도 한다. 이 방법은 <U>Stochastic Gradient Descent(SGD)</U>의 특수한 버전이라고 생각하면 된다.   
Stochastic이란 random한 프로세스를 의미하는데, 예를 들어 $N$개의 샘플에 대한 loss를 한 번에 연산하기 힘들기 때문에 학습 시에 <U>랜덤한 비복원 추출로 샘플 하나씩 추출</U>하고, 이 샘플 하나에 대한 gradient만 계산하면 된다고 생각해보자. 아래에서 보는 것과 같이 데이터셋 전체를 하나의 묶음(batch)로 보고 계산한다면, gradient가 향하는 방향이 절대적으로 전체 데이터셋에 대한 분포를 반영하기 때문에 가장 이상적인 방향으로 최적화가 가능하다(물론 convex optimization의 가정을 가지고 있어야 한다). 하지만 연산 환경에서 약 100만장에 달하는 대용량 데이터셋이 사용된다면 <U>한번에 loss를 계산하는 것은 거의 불가능</U>하기 때문에(메모리 문제, 속도 문제) 이를 분리해서 학습하게 된다. 하지만 이 경우에도 단순 stochastic gradient descent 방법은 noisy한 weight update가 진행되기 때문에 데이터셋 전체의 분포를 통한 최적화에 비해 비효율적으로 학습된다는 문제가 있다. 따라서 이를 적절히 타협을 본 <U>mini-batch optimization</U>을 통해 최적화하게 된다. 여기서 mini-batch란 랜덤하게 1개의 샘플을 비복원추출을 하는 것이 아니라, $K$개의 샘플을 추출함으로써 전체 데이터셋의 분포의 형태를 모집단을 통해 유추하면서 학습할 수 있게끔 한다는 개념이다.   
이러한 방법이 학습에 도움이 되는 이유는 training dataset이 서로 **correlation**을 가지고 있기 때문이다. 만약 특수한 상황을 가정하여, <U>120만개의 image set</U>이 모두 <U>1000개의 이미지 데이터</U>를 복사해서 만들어진 이미지라 한다면, 굳이 <U>1200개</U>의 같은 gradient를 똑같이 계산하는 것과 같다. 물론 실제 데이터셋을 이렇게 구성하지는 않지만, 각 데이터가 서로 어느 정도 연관성을 가진다는 전제 하에 <U>mini batch의 gradient</U>가 결국 데이터셋 전반의 틀을 모방하여 계산하는 것과 같기 때문에(앞서 말했던 모집단의 개념) 크게 문제될 것이 없다.

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/211718291-66494f31-f22a-4221-aaf8-b751c03ff011.png" width="700"/>
</div>


# 결론

마무리하자면, 신경망에서 <U>softmax</U> 및 <U>SVM</U> classifier에서 weight($W$)와 $x$를 연산한 score와 실제 label인 $y$와의 차이로 발생하는 loss가 있고, 이 data loss에 $W$의 정규화를 위한 loss가 더해져서 <U>objective</U>(학습 목표)가 되었다. 그리고 이 objective를 최소화시키는 방향으로 optimization을 진행할 수 있었으며, 최적화 방법은 analytic하게 계산된 <U>gradient</U>를 통해 <U>parameter update</U>를 하는 것이었다. 여기에 추가적으로 loss term의 gradient를 모든 샘플에 대해 연산하는 것은 복잡하기도 하고 overfitting의 위험이 있기 때문에 dataset 전체를 하나의 batch로 학습하는 것이 아닌 <U>mini-batch gradient descent</U>를 사용하는 것까지 배울 수 있었다.

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/211720139-4d46fd4a-c9d8-48f1-9c55-402b56ed9750.png" width="400"/>
</div>`,lO=`---
title: "cs231n 내용 요약 (4) - Backpropagation"
category: "ai theory"
publishedAt: "2022-11-05"
thumbnail: "https://user-images.githubusercontent.com/79881119/212449963-f797bdf8-6a43-46a6-993a-a57edb010647.png"
---


# 들어가며...
앞선 글에서 다루었던 내용들은 linear classification task에 대해 classifier로 사용될 수 있는 **support vector machine**(SVM)과 **softmax**에 대한 형태, 그리고 각각의 classifier의 <U>최적화 과정</U>에서 loss function으로 사용되는 **hinge loss**와 **cross-entropy loss**에 대해서 살펴볼 수 있었다. 또한 weight $W$와 bias $b$를 통해 parameterize되는 score function $f$ 및 loss function을 최적화할 수 있는 여러 방법들 중 <U>gradient descent</U>에 대해서 언급했었고, 모든 sample에 대한 analytic gradient를 구하는 연산 과정이 computationally cost했기 때문에 <U>stochastic gradient descent</U>(SGD) algorithm과 보다 noisy한 학습을 줄이고 빠른 최적화를 위한 방법인 mini-batch gradient descent에 대해서도 확인할 수 있었다. 지금부터 다룰 내용은 이전에 살펴본 단일 신경망(perceptron)을 포함한 다층 신경망 구조(Multilayer perceptron)에서 어떤 방식으로 parameter를 효율적으로 최적화할 수 있는지에 대한 방법론으로, 이 글에서 인공지능의 침체기와 그 발전 방향에 대한 역사 총체에 대해서 다룰 것이다.


# Perceptron

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/212449963-f797bdf8-6a43-46a6-993a-a57edb010647.png" width="700"/>
</div>

<U>인공 신경망</U>이라는 개념(Artificial neural network, ANN)은 1943년에 발표된 [A logical calculus of the ideas immanent in nervous activity](https://link.springer.com/article/10.1007/BF02478259)에서 처음 제안되었다. 해당 논문을 publish한 McCulloch와 Pitts는 위와 같은 인간의 신경 구조를 복잡한 스위치들이 연결된(일종의 논리 함수) 네트워크로 표현할 수 있다고 설명하였다. 하지만 해당 연구에서는 perceptron을 실질적으로 활용할 생각은 못했고, 이보다 응용에 가까운 알고리즘을 제시한 것이 바로 현재의 인공지능 시대를 처음으로 열고자 했던 [perceptron 논문](https://psycnet.apa.org/record/1959-09865-001) 이었고, 이는 1958년 Frank Rosenblatt에 의해 발표되었다. Resenblatt은 퍼셉트론이라는 선형 분류를 수행할 수 있는 feed forward neural network를 제안하였고, 이 구조는 우리가 흔히 알고 있는 input에 대해 weight를 곱하고, 여기에 activation function(활성화 함수)를 적용하여 그 값이 특정 threshold보다 크면 $1$, 작으면 $-1$을 출력하는 형태의 구조를 가지고 있었다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/212450295-57e6ec96-275f-4ef1-8e23-b8400958242a.png" width="700"/>
</div>

현재 사용하고 있는 딥러닝도 형태만 살짝 다를 뿐 이러한 함수 구조를 여러 개의 node와 여러 개의 layer로 구성했다는 점에서 perceptron과의 근본적인 형태는 동일하다. Frank Rosenblatt의 perceptron은 현재의 딥러닝과 같이 당시에는 학계와 여러 언론으로부터 기대와 주목을 받았으며, <U>곧 세상을 인공지능이 대체할 수 있을 것</U>이라는 보도가 나기 시작했다.   
그러나 이런 기대와 열기는 1969년 MIT의 Marvin Minsky와 Seymour Papert가 저자로 참여한 [Perceptrons](https://leon.bottou.org/publications/pdf/perceptrons-2017.pdf)라는 책을 통해 한계를 수학적으로 증명당하면서 급격히 줄어들게 되었다. 
<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/212450524-bf186934-a43e-4d03-958f-4467ceaf6b01.png" width="700"/>
</div>
Minsky와 Papert는 단순한 선형 분류를 할 수 있는 perceptron의 경우 <U>간단한 XOR 논리 연산을 추행할 수 없다</U>는 것을 지적하였고, 가장 간단한 논리 문제에서부터 제안된 방법론이 수행될 수 없다는 점은 인간의 일상생활에서 접할 수 있는 대부분의 문제를 해결할 수 없음을 의미하였다. Minsky는 이에 추가적으로 perceptron을 여러 층으로 구성한 multilayer perceptron(MLP)이 해당 논리 문제를 해결할 수 있을 것이라 제안했지만, 그와 동시에 MLP의 parameter를 학습할 수 있는 방법론을 제시하지 못하였다.


# Multilayer perceptron
인공 신경망에 대한 한계점을 찾아내자, 연구자들의 관심은 금방 사그라들었지만 이에 굴하지 않고 꾸준히 연구를 계속하는 사람들도 있었다. 1986년 [Parallel Distributed Processing](http://www.cs.toronto.edu/~fritz/absps/pdp2.pdf)라는 책을 통해 hidden layer를 가진 multi-layer perceptron과 backpropagation 알고리즘을 제시하였다. 기존의 perceptron은 단일 신경망 layer를 가지고 있었기 때문에 linear classification의 한계를 넘어설 수 없었지만, <U>multilayer perceptron</U>(MLP)는 hidden layer라는 추가 layer를 제안함으로써, XOR과 같은 문제에서 선형 분류선을 추가할 수 있는 가능성을 보여주었다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/212450702-29c12691-7823-47a3-9f32-9389888e5240.png" width="400"/>
</div>

그러나 MLP의 경우 parameter의 수가 단일 perceptron에 비해 많아지면서 적절한 weight와 bias를 찾는 과정이 어려웠는데, 여기서 backpropation 알고리즘이 제안되면서 이 문제를 해결할 수 있었다. Backpropagation algorithm이란 input을 다층 신경망에 정방향(feed-forward) 방향으로 통과시킨 후 output값(에측값)을 토대로 원하는 기준(ground truth)와 비교함으로써, 해당 차이(error)를 역방향(backward)으로 돌려주면서 <U>parameter를 update</U>하게 된다.


# Backpropagation
사실 concept만 이런 식으로 정의하게 되면 이해가 쉽지 않기 때문에 다음과 같은 예시를 들어보도록 하겠다. 이전에 다루었던 단일 신경망(perceptron)을 활용한 선형 분류기의 경우 score function $f$을 구성하는 요소가 weight 및 bias 였고, 이를 단순한 trick을 사용하여 단일 layer의 parameter를 weight로만 정의할 수 있었다. 이를 식으로 표현하게 되면 score function $f$와 loss function $\\rho$에 대하여,

$$
    \\begin{aligned}
        \\hat{y} =& f(W;~X) = X \\cdot W \\newline
        \\delta =& \\rho(\\hat{y},~y)
    \\end{aligned}    
$$

score function $f$를 통해 예측한 prediction $\\hat{y}$과 ground truth $y$를 loss metric $\\rho$를 통해 비교한 error 값인 $\\delta$를 정의할 수 있다. Error가 의미하는 바는 곧 output이 <U>이동해야할 방향</U>(direction)와 같다. 다르게 말하자면 input $W,~x$에 대한 prediction $\\hat{y}$의 변화량을 제시했기 때문에, 기준점에 대한 input의 변화량은 미분을 통해 구할 수 있다. 길게 돌고 돌아서 설명하긴 했지만 이전에 설명했던 gradient descent 방식을 풀어서 설명한 것이다. Parameter인 <U>$W$의 변화량에 대한 $\\delta$의 변화</U>는 다음과 같다.

$$
    \\frac{\\partial \\delta}{\\partial W}    
$$

여기서 만약 $\\delta$ 값을 구할 수 있다면, 이에 따라 $W$의 변화량도 계산할 수 있다. 물론 여기서 구한 변화량은 hyperplane에 대한(tangential plane) direction이 되기 때문에 실제 loss function의 global minima를 찾을 수는 없다. 이 역시 step size를 기반으로 조금씩 내려가는 형태의 <U>gradient descent</U> 알고리즘을 적용해야한다.

$$
    \\delta_W = \\delta \\times \\frac{\\partial W}{\\partial \\delta} = \\delta \\times \\frac{\\partial \\hat{y}}{\\partial \\delta} \\times \\frac{\\partial W}{\\partial \\hat{y}}
$$

방금은 layer가 하나인 경우에 대해서만 설명하였고, 만약 이러한 layer가 여러 층 있다고 생각해보자. 식에서 $\\sigma$는 <U>activation function</U>으로 생각해주면 된다.

$$
    \\begin{aligned}
        \\hat{y} = f(W_1,~W_2,~\\cdots,~W_n;~X) =& \\sigma( \\cdots \\sigma(\\sigma(X \\cdot W_1) \\cdot W_2) \\cdots W_n) \\newline
        \\delta =& \\rho(\\hat{y},~y)
    \\end{aligned}    
$$

수식을 단순하게 하기 위해 $y_k$는 $k$번째 레이어 이후의 결과값이라고 생각해보자. 따라서 위의 수식에서 $\\hat{y} = y_n$이다. 앞서 단일 perceptron의 경우와 동일하게 이번에도 output의 변화량에 대한 input의 변화량을 나타낼 수 있다.

$$
    \\delta_{W_n} = \\delta \\times \\frac{\\partial \\sigma}{\\partial \\delta} \\times \\frac{\\partial W_n}{\\partial \\sigma} = \\delta \\times \\frac{\\partial y_n}{\\partial \\delta} \\times \\frac{\\partial \\sigma}{\\partial y_n} \\times \\frac{\\partial W_n}{\\partial \\sigma}
$$

다만 이번에는 output $\\hat{y}$가 activation function $\\sigma$에 대한 추가 합성 함수로 구성되기 때문에 위와 같은 <U>chain rule</U>을 따르게 된다. 마찬가지로,

$$
    \\delta_{W_{n-1}} = \\delta \\times \\frac{\\partial \\sigma}{\\partial \\delta} \\times \\frac{\\partial y_{n-1}}{\\partial \\sigma} \\times \\frac{\\partial W_{n-1}}{\\partial y_{n-1}}
$$

위와 같이 forward 과정에서 이미 연산된 결과 $y_k$에 대해 local derivatives를 계속해서 계산된 미분값에 곱해가게 되고, 이를 backpropagation이라고 한다. 위는 multilayer perceptron에 대한 예시였고, 다음과 같은 간단한 논리 회로에 대해 예시를 보게 되면

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/212582088-59a9e8b1-aaab-4be0-9016-b13966f4aa50.png" width="400"/>
</div>

초록색으로 표시된 값들이 실제 input($x,~y,~z$ 그리고 논리 연산에 의한 값)이고, 빨간색으로 표시된 값들이 미분값에 해당된다. 위와 같이 input에 대해 어떠한 결과가 도출되는 복잡한 신경망 회로를 구성한 것이 multilayer perceptron이다. 이를 학습시키기 위해 backpropagation을 진행하는 과정을 <U>직관적인 그림</U>으로 표현한 것이 위의 그림이다. 첫번째 빨간색 미분값은 앞서 보았던 metric에 의한 오차 그대로를 미분한 것과 같다. 논리 회로를 하나의 함수 $f$라고 생각한다면 이를 $f$로 미분한 결과는 그대로 $1$이 되기 때문이다. 그 다음으로 볼 수 있는 두번째 빨간색 미분값($q$라고 적혀있는 부분에 해당)은 $-4$이다. 왜냐하면 $f = qz$를 $q$로 편미분한 것은 $z$값이기 때문이다. 세번째 빨간색 미분값($x$라고 적혀있는 부분에 해당)은 $-4$이다. 왜냐하면 $f = qz = (x+y)z$를 $x$로 편미분한 것은 $z$값이기 때문이다(by chain rule). 네번째 빨간색 미분값($y$라고 적혀있는 부분에 해당)은 $-4$이다. 왜냐하면 $f = (x+y)z$를 $y$로 편미분한 것은 $z$값이기 때문이다(by chain rule). 그리고 마지막 빨간색 미분값($z$라고 적혀있는 부분에 해당)은 $3$이다. 왜냐하면 $f$를 $z$로 편미분한 것은 $x+y$값이기 때문이다. 이 논리 회로의 연산 과정은 간단하고, 크게 어렵지 않지만 해당 예시에서 얻어갈 수 있는 insight는 다음과 같다.

- Forward process 과정에서 계산된 output이 backward process 과정에서의 각 input에 대한 gradient 연산에 사용된다.
- 가장 말단(신경망 전체 함수 $f$의 끝부분)에서 계산된 gradient를 기준으로 이전 과정의 gradient가 chain rule에 의해 곱해지는 구조가 된다.


# Backpropagation의 해석

Backpropagation은 <U>local process</U>(지역적 연산 결과)이다. 회로 다이어그램에서의 각 gate는 input을 받아 두 가지 계산 결과를 가진다. 첫번째는 말 그대로 gate의 논리에 따른 연산 결과(위의 그림 예시에서 볼 수 있었던 초록색 값), 그리고 두번째는 input에 대한 output의 local gradient(위의 그림 예시에서 볼 수 있었던 빨간색 값)를 계산한 값이다. 실제로 gate가 속해있는 전체 회로가 어떤 구조로 구성되어있어도 이와는 상관없이 <U>각 gate에서 수행되는 논리 계산</U>과 <U>gradient 계산</U>은 완전히 독립적으로 생각할 수 있다. 그렇기에 forward pass가 한번 끝나게 되면, backpropagation이 진행되는 동안에 전체 회로의 output value를 기준으로 신경망을 구성하는 모든 parameter에 대해서 gradient를 계산할 수 있는 것이다. 이를 실질적으로 수행할 수 있게 만드는 수학적 메커니즘인 chain rule로 하여금 gate의 <U>모든 input에 대해 gradient를 곱하는</U> 형태로 계산이 진행되어야하는 것이다. 여기서 gradient의 곱은 반대로 가는 경로(backward process)에 놓이는 모든 output에 대해서 생각해주면 된다. 즉, chain rule에 따른 multiplication은 곧 복잡한 신경망 회로 내에서 서로 무관하게 놓인 perceptron들을 <U>유기적으로 학습</U>할 수 있는 방법론이 된 것이다. 앞의 예시를 다시 가져와서 생각해보면,

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/212582088-59a9e8b1-aaab-4be0-9016-b13966f4aa50.png" width="400"/>
</div>

덧셈 연산을 수행하는 gate는 input으로 $-2$, $5$를 받고 output인 $3$을 내보냈다. Gate는 단순히 input에 대해서 덧셈을 수행하기 때문에, 모든 input에 대한 gradient는 $+1$이다. 덧셈 연산을 수행하는 gate를 포함한 전체 논리회로는 최종 결과인 $-12$를 출력한다. Backward pass(backpropagation) 과정에서 chain rule이 적용되고, backpropagation 연산 결과, 덧셈 gate는 해당 output에 대한 gradient로 $-4$를 가지게 된다. 여기서 만약 회로가 만약 <U>'output으로 더 큰 값을 뽑아내길 원한다'</U>고 생각해보자. 회로에서 덧셈 gate의 gradient가 마이너스값($-4$)이었으니, 실제로 덧셈 gate의 output으로는 <U>더 작은 값을 원하겠거니</U> 생각할 수 있다. 빨간색으로 계산된 gradient 결과는 결국 output에 대한 각 gate output의 변화량 및 방향을 표현한 것이기 때문이다. Backpropation을 진행하는 과정에서, 덧셈 gate는 자기가 받는 모든 input(그림에서는 $x$, $y$)에 대한 local gradient에 덧셈 gate의 output $q$에 대한 gradient를 모두 곱하게 된다. 따라서, input $x$에 대해서나 $y$에 대해서 모두 $1 \\times (-4) = -4$가 되는 것이다. 이를 토대로 우리가 input과 회로 전체에 대해 이해할 수 있는 바는 $x$, $y$가 모두 감소하게 되면 덧셈 gate의 output도 감소하게 되고, gate의 output은 증가하게 된다는 것이다. 이를 요약하자면 회로의 output으로 하여금 <U>gate에 적용되는 gradient</U>에 해당 gate의 input 변수들의 local gradient를 곱하게 되면 각 input들의 적용되는 gradient를 계산할 수 있고, 이를 통해 <U>각 input이 어떤 방향으로 얼만큼 변화해야</U> **원하는 output을 얻을 수 있는지**에 대한 정보를 알아낼 수 있다. 회로 안의 모든 gate는 backpropagation이라는 과정을 통해 서로 소통할 수 있게 되고, 소통 수단은 chain rule을 기반으로 계산된 값이 될 것이다.


# Example of two dimensional neuron

$$
    f(w,~x) = \\frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}    
$$

위의 식은 입력 차원이 $2$인 단일 신경망에 대해서 sigmoid activation function을 적용하는 간단한 회로이다. 위의 식에서 사용되는 각 논리 회로에 대한 미분은 다음과 같다.

$$
    \\begin{aligned}
        f(x) =& \\frac{1}{x} \\rightarrow \\frac{df}{dx} = -\\frac{1}{x^2} \\newline
        f(x) =& x + c \\rightarrow \\frac{df}{dx} = 1 \\newline
        f(x) =& e^x \\rightarrow \\frac{df}{dx} = e^x \\newline
        f(x) =& ax \\rightarrow \\frac{df}{dx} = a
    \\end{aligned}    
$$

식을 기반으로 회로를 구성하면 다음과 같은 그림이 된다. 

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/212587574-66d72e68-6c54-49a4-84f5-9cb9ebcff619.png" width="900"/>
</div>

그림을 보면 각 연산 순서에 맞게 value(초록색)와 gradient(빨간색)를 계산해놓은 것을 알 수 있다. 앞서 예제에서 했던 것과 같이 backpropagation을 진행하면 다음과 같다. 가장 output인 $1.00$을 기준으로 시작해보자. $\\frac{1}{x}$ gate에 대한 local gradient 식은 $\\frac{df}{dx} = -\\frac{1}{x^2}$가 된다. 식에서 $x$에 유일한 input인 $1.37$을 대입하고, 역방향을 기준으로 gate의 gradient를 모두 곱하면 $1 \\times (-1/(1.37)^2) = -0.53$이다. 그 다음에 있는 1을 더하는 operation은 gradient에 영향을 주지 않기 때문에(input에 상관없이 local gradient가 1이기 때문에) $-0.53$이 그대로 전달된다.   
그 다음으로 exponential operation에 대한 local gradient($\\frac{df}{dx} = e^x$)는 input에 대한 output과 local gradient가 서로 같은 값을 가지기 때문에, 앞서 연산된 gradient에 output을 곱하게 되면 $-0.53 \\times 0.37 = -0.20$이 된다. $-1$을 곱하는 연산은 gradient 부호를 바꾸는 연산이 되므로 전달되는 값은 $(-0.20) \\times (-1) = 0.20$이 된다.   
이후로는 input이 두 갈래로 나뉘게 되는데, 우선 $w2$와 이어진 부분을 보게 되면 덧셈 연산으로 구성되기 때문에 gradient가 변수에 대해서 $1$이 고정값으로 쓰인다. 따라서 $0.20$이 그대로 유지된다. 위쪽으로도 또다른 덧셈 연산이 이어지고, 여기서도 동일하게 $0.20$이 유지되는 것을 알 수 있다. 다시 위쪽 덧셈 연산 gate를 기준으로 두 갈래로 나뉘게 되는데, 여기서도 마찬가지로 덧셈 연산이기 때문에 $0.20$의 gradient가 유지된다.   
남은 부분은 곱셈 연산 gate인데, gate로 전달된 gradient가 $0.20$이기 때문에 여기서 서로 다른 input을 곱한 것이 해당 input에 대한 gradient가 된다. 예를 들어 $f(x,~y) = xy$인 gate가 있다면, 각 input에 대한 local gradient는 $\\frac{\\partial f}{\\partial x} = y,~\\frac{\\partial f}{\\partial y} = x$가 되는 것이다. 이를 토대로 계산하게 되면 $w0$에 대한 gradient는 $0.20 \\times x0 = -0.20$, $x0$에 대한 gradient는 $0.20 \\times w0 \\simeq 0.39$(여기서 연산이 살짝 안 맞는데, 이 부분은 소숫점 아래 두번째 자리까지 반올림하면서 생긴 오차인 듯)이 된다. $w1,~x1$에 대한 gradient도 같은 방법을 통해 계산하게 되면 각각 $-0.39,~-0.59$가 되는 것을 쉽게 알 수 있다.   


# Sigmoid function gradient
위에서 사용된 회로는 sigmoid function을 여러 기본 논리 회로를 통해 표현한 형태가 된다. 그러나 굳이 이렇게 세세히 분리할 필요 없이 sigmoid function의 analytic한 도함수를 구할 수 있다.

$$
    \\begin{aligned}
        \\sigma (x) =& \\frac{1}{1+e^{-x}} \\newline
        \\frac{d \\sigma(x)}{dx} =& \\frac{e^{-x}}{(1+e^{-x})^2}
    \\end{aligned}    
$$

Sigmoid function의 도함수는 다음과 같이 표현할 수 있다.

$$
    \\frac{d \\sigma(x)}{dx} = \\left( \\frac{1+e^{-x}-1}{1+e^{-x}} \\right)\\left( \\frac{1}{1+e^{-x}} \\right) = (1-\\sigma(x))\\sigma(x)  
$$

그렇다면 위에서 계산된 결과를 토대로 보게 되면, $w0x0 + w1x1 + w2$가 곧 sigmoid function의 input인 $x$가 되기 때문에 $\\sigma(x) = 0.73$, $\\sigma (x)(1-\\sigma (x)) = 0.73 \\times (1-0.73) = 0.1971$임을 바로 구할 수 있다.


# 결론

기존 linear classification만 가능했던 퍼셉트론의 한계를 극복하기 위해 다층 신경망 구조를 역사와 함께 간단하게 소개했고, 그러한 다층 신경망을 학습시키기 위한 방법으로 제시된 backpropagation이란 개념을 알아볼 수 있었다. Backpropagation은 각 레이어마다 local gradient 계산을 통해 모든 신경망 구조의 요소들을 유기적으로 엮어주는 역할을 했으며, perceptron이 해결하지 못한 실생활의 task에 적용될 수 있는 딥러닝의 발전이 시작된 지점이었다.


# Appendix

##### Question : Draw a model diagram of following equation and calculate gradient with python simulation. 
$$
    f(x,~y) = \\frac{x+\\sigma (y)}{\\sigma (x) + (x+y)^2}    
$$

##### Answer

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/212597740-e67d424b-86a6-40aa-90a5-2e5ec6036749.png" width="700"/>
</div>

\`\`\`python
x = 3 # example values
y = -4

# forward pass
sigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator   #(1)
num = x + sigy # numerator                               #(2)
sigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)
xpy = x + y                                              #(4)
xpysqr = xpy**2                                          #(5)
den = sigx + xpysqr # denominator                        #(6)
invden = 1.0 / den                                       #(7)
f = num * invden # done!                                 #(8)

# backprop f = num * invden
dnum = invden # gradient on numerator                             #(8)
dinvden = num                                                     #(8)
# backprop invden = 1.0 / den 
dden = (-1.0 / (den**2)) * dinvden                                #(7)
# backprop den = sigx + xpysqr
dsigx = (1) * dden                                                #(6)
dxpysqr = (1) * dden                                              #(6)
# backprop xpysqr = xpy**2
dxpy = (2 * xpy) * dxpysqr                                        #(5)
# backprop xpy = x + y
dx = (1) * dxpy                                                   #(4)
dy = (1) * dxpy                                                   #(4)
# backprop sigx = 1.0 / (1 + math.exp(-x))
dx += ((1 - sigx) * sigx) * dsigx                                 #(3)
# backprop num = x + sigy
dx += (1) * dnum                                                  #(2)
dsigy = (1) * dnum                                                #(2)
# backprop sigy = 1.0 / (1 + math.exp(-y))
dy += ((1 - sigy) * sigy) * dsigy                                 #(1)
\`\`\`

##### Answer solution
코드에 맞게끔 다이어그램에서의 value와 gradient를 매칭시키면 다음과 같다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/212598822-4e6e5958-2a9f-48dc-b96b-2b9bc5ff9f85.png" width="500"/>
    <img src="https://user-images.githubusercontent.com/79881119/212598817-aa15d8c5-02d8-474d-aff1-e822bb2973fd.png" width="500"/>
</div>

빨간색으로 표시된 부분이 forward process에서 계산된 value, 초록색으로 표시된 부분이 backward 연산 시 계산된 gradient를 변수명에 맞게 위치시킨 것이다. 가장 먼저, $f(x,~y)$를 기준으로 곱셈 연산 gate가 있기 때문에 앞선 예제에서와 같이 input을 서로 교차해서 곱해주면된다. Output에 대한 gradient는 $1$이기 때문에, $1$에 각각의 input을 곱해주면 된다. 따라서 코드는 다음과 같다.

\`\`\`python
# backprop f = num * invden
dnum = invden # gradient on numerator                             #(8)
dinvden = num                                                     #(8)
\`\`\`
다음으로는 $1/x$ 연산에 대한 gradient를 곱해주기 위해 input인 \`\`\`den\`\`\`에 대한 $-1/x^2$을 곱해준다. 코드는 다음과 같다.

\`\`\`python
# backprop invden = 1.0 / den 
dden = (-1.0 / (den**2)) * dinvden                                #(7)
\`\`\`

그리고 \`\`\`xpysqr\`\`\`과 \`\`\`sigx\`\`\`가 더해지게 되므로, \`\`\`dsigx\`\`\`와 \`\`\`dxpysqr\`\`\`은 각각 \`\`\`dden\`\`\`의 gradient가 그대로 유지된다.

\`\`\`python
# backprop den = sigx + xpysqr
dsigx = (1) * dden                                                #(6)
dxpysqr = (1) * dden                                              #(6)
\`\`\`
제곱에 대한 gradient는 쉽게 구할 수 있으므로 \`\`\`dxpy\`\`\`는 넘어가도록 하겠다.

\`\`\`python
# backprop xpysqr = xpy**2
dxpy = (2 * xpy) * dxpysqr                                        #(5)
\`\`\`
마찬가지로 \`\`\`dxpy\`\`\`의 gradient는 덧셈 게이트의 input인 \`\`\`x\`\`\`와 \`\`\`y\`\`\`에 대해 그대로 유지된다. 따라서 \`\`\`dx\`\`\`와 \`\`\`dy\`\`\`는 다음과 같다.

\`\`\`python
# backprop xpy = x + y
dx = (1) * dxpy                                                   #(4)
dy = (1) * dxpy                                                   #(4)
\`\`\`

앞선 예제에서 $\\sigma$ 함수의 gradient를 표현하는 방법에 대해서 알아보았다. 해당 공식을 그대로 사용하게 되면 output인 \`\`\`sigx\`\`\`에 대해서

\`\`\`python
# backprop sigx = 1.0 / (1 + math.exp(-x))
dx += ((1 - sigx) * sigx) * dsigx                                 #(3)
\`\`\`

위와 같으며, 기존에 이미 연산된 \`\`\`dx\`\`\`가 있기 때문에 여기에 추가로 더해주게 된다. \`\`\`dnum = x + sigy\`\`\`에 대해 덧셈 gate는 gradient가 유지되므로 다음과 같이 계산할 수 있다.

\`\`\`python
# backprop num = x + sigy
dx += (1) * dnum                                                  #(2)
dsigy = (1) * dnum                                                #(2)
\`\`\`

여기에 마지막으로 $\\sigma$ 함수의 gradient를 계산해주면 마무리된다.

\`\`\`python
# backprop sigy = 1.0 / (1 + math.exp(-y))
dy += ((1 - sigy) * sigy) * dsigy                                 #(1)
\`\`\`
`,cO=`---
title: "cs231n 내용 요약 (5) - Neural Network"
category: "ai theory"
publishedAt: "2022-11-06"
thumbnail: "https://user-images.githubusercontent.com/79881119/212606281-bb0f97b8-8f89-4378-8f6b-6fde347130a7.png"
---


# 들어가며...

딥러닝의 침체기에 걸쳐있던 perceptron과 이를 극복하기 위한 multilayer perceptron 구조, 그리고 해당 구조를 학습시킬 수 있는 방법으로 backpropagation에 대해 알아볼 수 있었다. 이번 글도 똑같은 구조인 neural network에 대해서 살펴볼텐데, 이전까지는 방법론에 대한 내용이 주를 이루었다면 이번에는 그보다는 조금 더 <U>architecture, structure</U>에 대해서 살펴볼 예정이다.


# Neural network
뇌과학이나 해부학적인 지식이 없이도 neural network를 이해할 수 있다. 이는 perceptron의 설계가 직관적이고 수학적이기 때문이다. Linear classification에서는 주어진 이미지 데이터에 대해 <U>class별 점수를 계산</U>하고(score function $f$), 여기서 단일 perceptron의 score function을 구성하는 weight parameter $W$를 정의할 수 있었다. 예를 들어 CIFAR-10 dataset에서는 input으로 사용되는 벡터의 크기가 $32 \\times 32 \\times 3$의 column vector였으며, score function이 동작하게끔 weight는 $10 \\times 3072$의 matrix로 구성하여 10개의 class에 대한 점수를 계산했었다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/212606281-bb0f97b8-8f89-4378-8f6b-6fde347130a7.png" width="700"/>
</div>

이는 단일 parameter에 대한 경우였고, 조금 더 복잡한 구조를 가진 neural network를 생각해보자. 가령 두 개의 perceptron이 서로 연결되어있는 형태의 score function이 다음과 같다고 생각해보면,

$$
    s = W_2 \\max(0,~W_1x)    
$$

더이상 이 연산 결과는 affine하지 않게 된다. 여기서 affine하지 않다는 것은 **'선형적이지 않다'**는 뜻이다. 만약 $W_1$이 $100 \\times 3072$의 matrix라면, $W1 \\cdot x$ 연산을 통해 $100$ 차원의 hidden feature(vector)를 계산할 수 있다. $W_2$와의 연산 이전에 계산되는 $\\max$ 함수의 경우 생성된 $100$차원의 vector에 대해 non-linear한 연산을 진행한다. Non-linear 연산을 진행하는 이러한 함수를 <U>activation function</U>이라고 부르며, 흔히 여러 개의 perceptron이 연결된 신경망 구조를 포함한 다양한 neural network 구조에서 연산의 노드를 분리하기 위한 장치로 사용된다. Activation function의 종류는 다양하며, 그 중 기본적인 형태에 대해서는 뒤에서 소개하도록 하겠다. 위에서 볼 수 있는 $\\max$ 함수는 $0$보다 작은 값은 $0$으로, 그보다 큰 값은 그대로 내보내는 함수로 <U>Rectified Linear Unit</U>(ReLU) 함수라고 부른다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213332542-a5eb034c-356b-4eb0-8847-4500c1b3dcdc.png" width="700"/>
</div>

단순히 여러 층을 쌓는 neural network 구조를 생각해보자. 만약 linear operation을 $n$개의 layer에 대해 수행하게 되면,

$$
    y = x \\bigodot_{i=1}^n W_n = x \\odot W
$$

위와 같이 $\\bigodot_{i=1}^n W_n = W$를 연산한 것과 동치가 되기 때문에 linear operation $1$개를 적용한 것과 차이가 없다. 즉 단순히 weight 개수만 늘리게 되면 <U>신경망 네트워크의 연산 복잡도</U>를 높일 수 없다는 것이다. Deep neural network를 구성하기 위해서는 activation function이 필수적임을 알 수 있다. 그리고 이렇게 연산되는 전체 네트워크를 학습하는 과정은 backpropagation임을 이전 글에서 확인하였다.


# Modeling neuron

신경망 네트워크의 시작은 <U>생체의 neural system</U>을 컴퓨터 환경에서 구현하는 것을 목표로 시작되었다. 결국 engineering 환경에서 성능을 좋게 만드는 것이 궁극적인 목표가 되겠지만, 신체의 neural system을 간단하게나마 컴퓨팅 환경이나 논리 환경에서 구현하는 것으로 연구가 시작되었다고 볼 수 있다. 우리 뇌가 계산을 하는 과정의 가장 기본이 되는 단위는 neuron(뉴런)이다. 대략 $860$억 개의 뉴런이 실제 인간의 뇌 신경 환경에서 쓰이고, 각각은 대략적으로 $10^{14}$ ~ $10^{15}$개의 시냅스로 서로 연결되어있다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213333618-16ab5dc1-3297-455a-a21d-6551a720bcf5.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/213333676-5d89f078-3a26-403d-a40b-bd347794c813.png" width="300"/>
</div>

생물 시간은 아니지만 인체 신경망 구조가 작동하는 원리를 간단하게 보면 다음과 같다. 우선 각 뉴런 단위들은 input 신호를 **dendrites**를 통해 받아들이고, **axon**을 통해 output signal을 출력하게 된다. 각 **branch**를 따라서 axon들은 시냅스를 통해 다시 다른 뉴런들의 **dendrites**로 연결된다. 마찬가지로 이를 컴퓨팅 환경에서 구현한 perceptron에서는 신호들이 axon을 통해 전달되는 형태는 $x$라는 input으로 들어오는 형태로 구현이 되었으며, 그림에서 볼 수 있듯이 $n$-dimensional data의 각 dimension에 대해 서로 다른 시냅스를 통해 dendrites와 연결되게끔 한다. 이때 여기서 곱해지는 weight $W$는 각 시냅스에서 들어온 신호와 뉴런 각 dendrites에서의 <U>상호작용하는 정도</U>(strength)를 나타낸다. 일반화된 위의 그림과 같은 네트워크에서는 이렇게 각 dendrites로 받아들이는 신호들을 weighted summation하게 된다. Input에 대한 output이 단일 스칼라 value로 나오는 경우를 생각해볼 수 있다($n$ to $1$ function).

$$
    \\sum_{i} w_i x_i + b
$$

만약 최종적으로 받아들인 모든 $W,~x$의 상호작용이 합해진 결과가 일정 threshold를 넘어서게 되면, 뉴런이 fire(activate)하여 활성화되고, 다시 다른 axon을 통해 신호를 전달하게 된다. 인간의 경우 시냅스에서 신호가 전달되고, 각 뉴런이 activate하는 타이밍이 전체 생체 활동에 중요한 부분을 차지하지만 computational 환경에서는 이를 가장 기본적인 형태로 <U>동기화시키기 때문에</U> spike 타이밍은 중요하지 않다. 그렇다면 생체 뉴런이 아닌 perceptron에서 고려해야할 것은 neuron이 얼만큼 들어온 신호에 대해서 **'fire(activation)'**하는지가 되는데 이를 'firing rate of the neuron'이라고 표현하며 이를 함수로 구현한 것이 곧 function $f$, <U>activation function</U>(활성화 함수)라고 부른다.

$$
    y = f \\left( \\sum_{i} w_i x_i + b \\right)
$$

따라서 인간의 <U>생체 뉴런</U>에서의 **firing, activating** 과정이 <U>computer perceptron</U>에서 구현된 형태가 앞서 언급했던 non-linearity 요소를 줄 수 있었던 **activation function**이다. 지금까지 설명한 퍼셉트론을 실제 파이썬 환경에서 하나의 class로 선언하면 다음과 같다. \`\`\`forward\`\`\` 메소드가 곧 뉴런으로 하여금 \`\`\`inputs\`\`\`($x$)에 연산을 진행하여 원하는 firing rate($0 \\sim 1$)를 내보내게 된다. 여기서 firing rate이 <U>normalize</U>되는 효과가 생기는데, 이는 뒤에서 추가로 설명하도록 하겠다.

\`\`\`python
class Neuron(object):
    def forward(self, inputs):
        """ assume inputs and weights are 1-D numpy arrays and bias is a number """
        cell_body_sum = np.sum(inputs * self.weights) + self.bias
        firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function
        return firing_rate
\`\`\`


# Single neuron as a linear classifier

단일 뉴런이 표현하는 식은 앞선 예제에서 $\\sigma \\left( \\sum_i w_i x_i + b \\right)$으로 정의할 수 있었고, 이는 결국 주어진 weight $W$와 input $x$에 대한 class probability와 같다. 만약 이 문제에 대해서 두 개의 class가 있는 경우라고 생각해보자($y$는 $0$ 또는 $1$). 즉 $0 \\sim 1$로 normalize된 하나의 값이 해당 class일 확률로 해석 가능하다는 것이다. 따라서 input $x$에 대하여 class가 $1$이 될 확률이 $p(y_i = 1 \\vert x_i;~w)$이며, 반대로 class $0$이 될 확률이 $1 - p(y_i = 1 \\vert x_i;~w) = p(y_i = 0 \\vert x_i;~w)$가 된다.   
직관적으로 보면 위와 같았고 실제로 예시를 통해 확인해보면 다음과 같다. 결국 말하고자 하는 것은 <U>단일 perceptron</U>이 classification에서 사용되는 것을 통해 앞서 미리 살펴본 <U>softmax</U>를 활용한 <U>linear classification</U>과 연결짓는 것이다. 만약 특정 input $x_i ~ (i = 1,~2,~\\cdots,~n)$에 대해서,

$$
    \\sigma \\left( \\sum_i w_i x_i + b \\right) = 0.2
$$

라고 가정해보자. Sigmoid 함수인 $\\sigma$는 설명했던 것과 같이 input이 어떤 값이 되던 상관없이 $0 \\sim 1$로 정규화가 가능하다고 말했었다. 따라서 위의 결과값을 토대로 $0$ 또는 $1$일 확률을 나타낸다면, 단순하게 다음과 같이 표현이 가능하다.

$$
    \\sigma \\left( \\sum_i w_i x_i + b \\right) = p(y_i = 1 \\vert x_i;~w) = 0.2
$$

출력값이 $0$에 가까워지면 $1$일 확률이 줄어들고, $1$에 가까워지면 반대로 $1$일 확률이 늘어나는 구조로 해석 가능하다. 반대로 $0$일 확률은,

$$
    1 - \\sigma \\left( \\sum_i w_i x_i + b \\right) = 1 - p(y_i = 1 \\vert x_i;~w) = 0.8
$$

이처럼 표현된다. 위의 상황은 softmax를 사용한 linear classification이었기 때문에 만약 SVM(Support Vector Machine)을 사용하게 된다면 activation function의 형태나 뉴런이 달라지게된다. 

$$
    R(W) = \\sum_k \\sum_l W_{k,~l}^2    
$$

Linear classification 글에서 소개했던 내용 중에서 data loss 외에 위의 식과 같은 <U>regularization loss</U>가 있다. 이런 regularization loss를 생체 뉴런으로 해석한 것이 'gradual forgetting'이다. 퍼셉트론을 구성하는 parameter가 최적화되는 과정에서 시냅스 가중치에 해당하는 weight를 $0$으로 유도하는 효과가 있기 때문이다.

​---

# Activation functions

인체 신경망 구조를 묘사한 퍼셉트론에서 firing rate을 구현하기 위한 구조적 장치로 activation function, non-linear function을 소개하였다. Neural network가 발전하면서 소개된 다양한 activation function를 확인해보자.

## Sigmoid($\\sigma$) function
<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213339599-ab505b03-2644-4c3b-af92-c74a3c5a7aa2.png" width="400"/>
</div>

우선 가장 먼저 소개할 함수는 sigmoid 함수이다. 이 함수가 가장 역사적으로 유명해진 이유는 다음과 같은 특성이 고려되었기 때문이다. 일단 input으로 모든 실수를 input으로 받을 수 있으며(정의역이 실수 전체), neuron의 firing rate를 반영하듯 아주 작은 negative number에서는 $0$, 큰 positive number에서는 $1$이 mapping될 수 있기 때문이다. 하지만 sigmoid 함수는 다음과 같은 단점들을 가지고 있다.   
첫번째 문제는 Sigmoid 함수는 $x$가 너무 작거나 클 경우 <U>수렴하는 형태의 그래프</U>를 가지기 때문에, <U>gradient vanishing</U>이 발생한다(gradient가 0에 수렴하는 것)는 것이다. Backpropagation 과정을 recall하면 각 <U>local gradient</U>를 gate의 output으로 전달된 <U>이전의 gradient</U>에 곱하면서 네트워크 전체를 학습하게 되는데, 만약 <U>sigmoid gate</U>의 local gradient가 $0$에 수렴하면, **해당 gate 이전에 놓인 weight** 전체의 학습이 어려워지게 되고, weight initialization이 잘못된 채로 학습을 시작하면 처음부터 학습이 거의 안되는 불상사가 발생할 수도 있다.   
두번째 문제로는 sigmoid 함수는 중간값이 $0$이 아님을 들 수 있다. 만약 activation function의 value가 음수를 가질 수 없다면, parameter update 과정에서 모든 weight가 같은 방향으로 update가 된다는 점 때문에 학습 속도가 매우 저하될 수 있다. 예시를 통해 보면 다음과 같다. 만약 어떤 input($x$)이 neuron으로 들어올 때, input($x$)을 구성하는 모든 값이 positive하다고 가정해보자.

$$
    f = \\sigma \\left( W^\\top x + b \\right)  = \\sigma \\left( y \\right)  
$$

그리고 sigmoid function에 대한 local gradient는 다음과 같다.

$$
    \\frac{d \\sigma(y)}{dy} = \\left( \\frac{1+e^{-y}-1}{1+e^{-y}} \\right)\\left( \\frac{1}{1+e^{-y}} \\right) = (1-\\sigma(y))\\sigma(y)  
$$

여기서 알 수 있는 사실은 sigmoid function의 output이 $0 \\sim 1$ 사이의 값을 가지기 때문에, sigmoid gate를 통해 전달되는 gradient는 <U>모든 input에 대해 양숫값</U>을 가지게 된다. 따라서 chain rule을 통해 weight $W$에 대해서,

$$
    \\begin{aligned}
        \\frac{\\partial f}{\\partial W} =& \\frac{\\partial f}{\\partial \\sigma} \\cdot \\frac{\\partial \\sigma}{\\partial y} \\cdot \\frac{\\partial y}{\\partial W} \\newline
        =& (1-\\sigma)\\sigma x > 0
    \\end{aligned}
$$

위와 같이 정리된다. 따라서 <U>모든 gradient가 동일한 방향</U>을 가지게 되므로 학습이 어렵다는 문제가 생긴다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213344264-7bf34af9-fd9b-4180-92eb-e7e5ebe6fdf9.png" width="300"/>
    <img src="https://user-images.githubusercontent.com/79881119/213344283-67231bd0-8f44-4329-b261-175a4ccffd55.png" width="300"/>
</div>

위의 그림이 weight column을 axis로 표현한 좌표계에서의 parameter update 과정을 나타낸 것이다. 만약 weight가 같은 방향으로만 update가 되어야한다면, 좌측과 같이 $W_2$에 대해서는 감소하면서 $W_1$에 대해서는 증가하는 형태로 학습이 불가능하므로, 우측과 같은 형태로 학습이 진행된다. 

## Hyperbolic tangent($\\tanh$) function

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213344841-bc551100-7760-4ee9-b226-a50cbb320776.png" width="400"/>
</div>

위의 함수는 $\\tanh(x) = 2\\sigma(2x) - 1$로 표현되는 하이퍼볼릭 탄젠트 함수이다. 이 함수의 구조를 보게 되면 local gradient가
$$
    \\frac{d}{dx} \\tanh(x) = (1 - \\tanh(x))(1 + \\tanh(x))
$$

위와 같이 나와서 우선적으로 앞서 소개했던 sigmoid에 비해 gradient가 크다는 장점이 있다. 또한 sigmoid value($\\sigma$)가 양수여서 <U>weight parameter update</U>가 지그재그로 진행되었던 기존 단점을 $\\tanh$는 해결할 수 있다. 하지만 이 경우에 대해서도 너무 크거나 작은 input value에 대해 gradient saturation 문제가 발생, 신경망 학습 시 activation function 이전의 node들에 대해 <U>학습이 죽어버리는 현상</U>이 여전히 발생한다.

## Rectified Linear Unit(ReLU) function

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213348547-512ac25f-04e3-4ec0-9f2a-ce356d4f3e08.png" width="400"/>
</div>

아마도 대부분 딥러닝을 접한 사람들이 많이 들어봤을 함수이다. 형태는 정말 간단하게 threshold가 $0$인 hinge loss가 된다. 해당 activation function은 [AlexNet 논문](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)에서 확인해볼 수 있으며, 아래 그래프와 같이 sigmoid function을 사용한 학습보다 ReLU를 사용한 학습 과정이 수렴 속도가 더 빠른 것을 확인할 수 있다(그래프 상에서 실선이 ReLU를 사용한 neural network, 점선이 sigmoid를 사용한 neural network의 수렴 과정을 보여준다).

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213348888-063ecd09-a49d-4896-a950-e4c812d75937.png" width="400"/>
</div>

​ReLU 함수의 장단점은 다음과 같다.   
우선 gradient vanishing 문제가 앞선 두 경우(시그모이드, 하이퍼볼릭 탄젠트) 둘 다 존재했었는데, 이 함수의 경우 $x$가 아무리 커지더라도 gradient가 $1$로 유지된다. 또한 시그모이드, 하이퍼볼릭 탄젠트 둘 다 exponential값을 연산해야하기 때문에 computational한 점이 문제가 되었는데, ReLU function의 경우 단순히 0보다 큰 값만 내보내는 과정을 통해 연산을 단순화할 수 있다.
하지만 ReLU는 <U>'die'에 취약</U>하다. ReLU gate를 통해 <U>큰 gradient가 흘러서</U> weight가 업데이트될 때, 다시는 activate될 수 없는 부분으로 학습이 될 수도 있다. 물론 ReLU 함수의 특성 상 weight 상에서 불필요한 부분들에 대한 연산을 배제하는 효과가 있기 때문에 어느 정도 deactivation하는 과정이 필요하지만, 만약 weight parameter가 잘못된 방향으로 학습되었을 경우, 해당 위치의 parameter에 대해서 network의 모든 노드들을 <U>최적화할 수 없다는 문제</U>가 발생한다. 따라서 learning rate, weight parameter 초기화 등등이 학습에 중요하게 작용하게 되므로 robust한 학습이 어렵다는 단점이 있다.

## Leaky ReLU, Parametric ReLU

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213350026-f11acb39-a9ef-465a-b579-30e4219ac8a8.png" width="400"/>
</div>

따라서 0보다 작은 value에 대해 작은 기울기를 가지는 선형 함수를 매핑함 LeakyReLU나, 네트워크 학습 과정에서 적절한 기울기 $a$를 학습할 수 있게끔 하는($y = ax$) parameteric ReLU 등등 기존 ReLU를 대체할 수 있는 activation function들이 제시되었다. 이후 여러 연구들에서 periodic activation function, GeLU, SiLU 등등 task에 따른 적절한 activation function이 제시되고 있다.


# Neural network architectures

## Graph structure and the number of layers

신경망 구조는 흔히 각 뉴런들에 대해 신호가 한쪽 방향으로 흘러가는 단일 방향의 그래프로 모델링된다. 이를 간단하게 표현하자면 특정 뉴런의 output이 다른 뉴런의 input이 되고, 이 <U>순서는 서로 바뀔 수 없다</U>(not commutable). 이런 형태의 그래프에서 cyclic 구조(어떤 노드의 output이 돌고 돌아 다시 자신의 input이 되는 형태의 그래프)는 허용하지 않는데, 이는 IIR(infinite impulse response) system이기 때문이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213358377-e65cd2e1-18c0-48f2-a65f-d72420066dee.png" width="600"/>
</div>

위와 같은 그림을 가지는 neural network structure를 표현할 때 '$3$개의 layer를 가지는 network'라고 표현한다. 보통 $N$개의 layer를 가지는 neural network를 정의하는 과정에서 <U>input layer</U>는 따로 counting하지 않는 것을 볼 수 있는데, 이는 input layer는 특정 modality를 가지는 입력 신호(이미지, 음성, 텍스트) 자체를 의미하기 때문이다.   
따라서 single layer neural network의 경우에는 hidden layer가 하나도 없는 구조를 지칭하며, logistic regression이나 SVM 등등을 single layer neural network라 부른다. 물론 이전에 언급했던 linear classification도 single neural network에 해당된다. 이러한 네트워크 구조들을 ANN(Artificial Neural Networks) 혹은 MLP(Multi-Layer Perceptrons)라 지칭한다. 하나의 레이어를 담당하는 perceptron이 연결되어 깊이를 가지는 신경망 층을 생성하게 된다. 다만 output layer에는 <U>activation function이 없는 경우</U>가 있는데, 보통 <U>classification의 기준</U>이 되는 score로 사용되거나, <U>정규화 없이 regression을 진행</U>하는 task인 경우가 이에 해당된다.

## Neural network size
Neural network의 크기는 흔히 parameter의 수로 정의하기도 한다. 신경망의 layer 갯수와 차원 갯수에 따라 학습 가능한 parameter의 수가 결정되는데, 다음과 같다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213358377-e65cd2e1-18c0-48f2-a65f-d72420066dee.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/213359552-f778f00f-1e94-4629-9d39-a1dcd92bc846.png" width="300"/>
</div>

그림을 보게 되면 layer가 노드(원)으로 표시가 되어있지만, 사실 실질적으로 layer 연산이 적용되는 부분은 <U>노드를 연결하는 엣지</U>(화살표)에 해당된다. 예를 들어 좌측의 구조를 가지는 네트워크에 대해 parameter 개수를 구하게 되면 bias를 포함한다고 가정했을 때,

$$
    \\begin{cases}
        \\text{input layer} \\rightarrow \\text{hidden layer 1}, & W \\in \\mathbb{R}^{3 \\times 4},~b \\in \\mathbb{R}^{1 \\times 4} \\newline
        \\text{hidden layer 1} \\rightarrow \\text{hidden layer 2}, & W \\in \\mathbb{R}^{4 \\times 4},~b \\in \\mathbb{R}^{1 \\times 4} \\newline
        \\text{hidden layer 2} \\rightarrow \\text{output layer}, & W \\in \\mathbb{R}^{4 \\times 1},~b \\in \\mathbb{R}^{1 \\times 1}
    \\end{cases}    
$$

위와 같으므로 총 학습 가능한 parameter의 개수는 각 parameter의 차원 수를 모두 더한 $41$개가 된다. 마찬가지로 우측과 같은 구조에 대해서도 같은 방법으로 계산해보면,

$$
    \\begin{cases}
        \\text{input layer} \\rightarrow \\text{hidden layer}, & W \\in \\mathbb{R}^{3 \\times 4},~b \\in \\mathbb{R}^{1 \\times 4} \\newline
        \\text{hidden layer} \\rightarrow \\text{output layer}, & W \\in \\mathbb{R}^{4 \\times 2},~b \\in \\mathbb{R}^{1 \\times 2}
    \\end{cases}    
$$

이므로 총 학습 가능한 parameter의 개수는 각 parameter의 차원 수를 모두 더한 $26$이 된다.


# Representation power
Neural Networks를 <U>fully connected layers</U>로 보는 방법은 네트워크를 구성하는 weight들을 parameter로 생각하여 함수를 정의하는 것이다. 이런 함수 형태의 묶음에 대해서 <U>representational power</U>를 정의할 수 있으며, Neural Network와 같이 모델링하는 방법에 대해 알아보도록 하자. 적어도 하나 이상의 hidden layer가 있는 neural network를 MLP로 정의하였고, 이때 neural network가 가지는 structural scale은 <U>적어도 2개 이상의 layer 수</U>를 가지게 된다. 이러한 neural network는 universal approximator임을 알 수 있다([참고 링크](http://neuralnetworksanddeeplearning.com/chap4.html)). Universal approximator에 대해 간략하게 설명하자면, 예를 들어 다음과 같이 real world에서 묘사하고 싶은 복잡한 함수 형태에 대해서,

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213361934-e6fce9aa-2536-4cca-aca6-50053a7d013f.png" width="400"/>
</div>

Neural network로 하여금 특정 input $x$에 대한 함숫값 $f(x)$를 예측하고자 한다. 지금은 단순히 차원 수가 $1$인(scalar인) 경우에 대한 function estimation을 가정했지만, $n$차원의 modality에 대한 multi-dimension function estimation 또한 가능하다. 즉 우리는 아무런 continous 함수 $f$를 가정할 수 있고 이러한 함수가 존재한다고 한다면 neural network function $g$를 이 함수에 근사시킬 수 있다는 것이다. 이렇게 된다면 neural network를 continous function으로 가정할 수 있게 된다.

$$
    \\begin{aligned}
        \\text{if any }f(x) \\text{ is continous and some }\\epsilon > 0, \\newline
        \\exists~g(x)~\\text{s.t. }\\forall x, \\vert f(x) - g(x) \\vert < \\epsilon 
    \\end{aligned}
$$

위의 참고 링크를 보게 되면 hidden layer 수가 하나인 neural network가 Universal approximator로 사용되는 예시들을 보여준다. 하지만 실제로 $2$개의 레이어를 가지는 neural network가 universal approximator라는 사실이 수학적으로는 가능하지만 실생활에서는(다차원의 복잡한 함수 형태가 존재하는 상황) 쓰이기 힘들다.

$$
    g(x) = \\sum_i c_i 1(a_i < x < b_i)    
$$

예를 들면 위와 같은 함수도 universal approximator로 사용될 수 있지만 머신러닝에서 이런 함수꼴을 전혀 사용하지는 않는 것을 볼 수 있다. Neural Network가 실생활에서 잘 활용되기 위해서는 <U>데이터의 통계적 성질</U>이나 실생활에서 마주하는 형태를 <U>잘 반영할 수 있는 함수 형태</U>를 찾고자 하는 것이고, 이러한 함수 형태를 찾는 과정으로 gradient descent 알고리즘을 활용한 optimization이 필요하다. 따라서 깊은 네트워크(여러 개의 레이어가 있는 모델)이 실생활의 데이터 접근에 조금 더 잘 활용이 될 수 있는 것은 여러 레이어를 가지는 네트워크의 <U>representation power</U>(표현력)이 커지게 되기 때문이고, 결국 **representation power**란 <U>Universal approximator가 표현 가능한 함수의 가짓수, 복잡도</U>를 포괄하는 의미로 작용한다.


# Why deep learning?
일반적으로 3개의 layer를 갖는 Neural Network는 2개의 layer를 가지는 Neural Network에 비해 representation power가 크기 때문에 실생활의 task에 대해 더 좋은 성능을 보이지만, 그렇다고 해서 무작정 layer를 키우는 방식이 절대적으로 성능 지표를 올릴 수 있는 <U>key point</U>가 되지 않는다. 이러한 특성은 <U>Convolutional Neural Network</U>와 차이가 있는데, CNN은 깊이가 깊어질수록 recognition 성능이 좋아지는 경향성을 어느 정도 보이기 때문이다(사실 CNN에서도 네트워크 깊이가 무작정 깊어지는 것이 좋은 것은 아니다). Image는 계층적 특징 구조를 가지고(물체의 윤곽부터 시작해서 물체의 디테일한 texture까지), 여러 층의 layer가 계층 구조를 분리하여 특징을 추출하거나 인식하는 효과를 줄 수 있다는 것이다. 갑자기 다른 주제로 이야기가 빠졌는데 다시 돌아오게 되면, neural network의 layer 수를 결정하는데 있어서 적절한 타협이 필요하다는 것이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213365482-8584de43-5b15-4a21-92ae-74e9f527996b.png" width="700"/>
</div>

위와 같은 그림을 보면, 붉은색으로 표시된 데이터와 녹색으로 표시된 데이터를 이진 분류하는 task에 대해 hidden layer의 개수가 decision area(구분선)에 미치는 영향을 시각화한 것이다. 큰 neural network로 갈수록 representation power가 커지기 때문에 보다 복잡한 함수 형태를 가질 수 있지만 이는 training dataset에 overfitting되는 문제를 일으킨다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213365915-860b745d-fa49-4201-acb1-f67cae79684c.png" width="700"/>
</div>

Overfitting이라는 것은 네트워크로 하여금 <U>training data</U>의 noise까지 불필요하게 학습해버린다는 문제인데, 앞서 본 그림에서 $20$ layer classifier는 모든 dataset에 대해 구분 가능한 representation learning을 진행한 탓에 <U>red, green 영역이 분리된 영역</U>도 생겨버리게 된다. 이럴 경우 실제 test dataset에 대해서는 정확도가 오히려 떨어지는 결과를 불러온다. 이러한 문제를 줄이기 위해 레이어 수를 줄이는 방법만 있는 것은 아니다. 앞서 linear classification에서 언급했던 $L_2$ regularization 그리고 학습 과정에서 특정 node(weight)를 무시하는 dropout 등등 여러 방법들이 있다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213366471-78948448-5394-4835-b7b9-2a804dc7ac4f.png" width="700"/>
</div>

Classification에 사용되는 loss 뿐만 아니라 정규화 작업에 사용되는 <U>regularization loss</U>의 최적화 과정에서의 중요도를 $\\lambda$라고 했을 때, 정규화에 대한 loss가 커질 수록 네트워크가 학습한 continous function이 점차 단순화되는 것을 알 수 있다. Regularization과 관련된 기존 글에서의 내용이 기억이 잘 안나겠지만 당시에 했던 말을 다시 해보자면, regularization 과정은 classification task에 있어서 <U>weight parameter가 고르게 분포하게끔</U> 해준다.   
보통 layer 수가 적은 모델일수록 local minima가 더 적다. Representation power가 증가할수록 표현 가능한 함수 복잡도가 커지지만 그만큼 weight에 따른 loss graph가 복잡하다. 그러나 layer 수가 적은 네트워크의 minima의 경우 학습하기는 쉽지만, 실제 global minima와 오차 범위가 크기 때문에(weight를 학습 가능한 축의 개수로 인식하면, feasible area가 줄어들기 때문) 학습 결과 성능이 좋지 않은 경우가 있다. 그러나 큰 모델일수록 상당히 많은 local minima가 존재하지만, 실제로 학습했을 때 더 좋은 결과를 보여줄 수 있다.   
작은 네트워크는 학습할 때 local minima의 깊이, 갯수가 현저히 적기 때문에 최적화는 훨씬 쉽지만 그만큼 성능 변동이 커서(혹은 최적화 성능이 최대 성능과 많이 차이가 나는 경우) 문제가 될 수 있고, 큰 네트워크는 local minima가 많아서 학습 과정에서 global minima를 찾기 힘들지만 이러한 variance를 감안하고서라도 학습 결과를 보았을 때 성능이 좋은 경우가 많다는 것이다. 이는 weight의 초기화 과정 자체가 결과에 미치는 영향이 줄어들어 일반화가 쉽다는 것이다.   
길게 설명했던 내용은 결국 overfitting을 방지, representation 학습을 하는 과정에서 <U>도대체 왜 딥러닝이 좋은 평가를 받을 수 있는지</U>에 대한 해석이었다. 단순히 레이어 수를 줄여서 학습하는 것보다, 깊은 레이어를 학습하되 regularization 과정을 통해 overfitting을 방지하는 것이 초기화 및 학습 단계에서 안정적인 학습을 보장한다는 것이다.


# 결론

얕은 레이어를 가진 모델은 <U>간단한 함수를 학습</U>하고 그만큼 수렴성이 높다는 장점이 있다. <U>Local minima</U>가 적으며 <U>global minima</U>에 보다 수렴하기 쉽지만, 수렴한 minima가 실제 네트워크의 <U>최대 성능에 비해 떨어질 수 있다</U>. 그러다보니 얕은 레이어의 모델은 weight 초기화에 변동이 매우 심하고, 그렇기 때문에 <U>loss function의 전체 구조</U>를 알 수 없는 실생활의 여러 데이터에 대해 <U>일반화될 수 없다는 문제</U>가 발생한다. 그렇기 때문에 레이어가 더 깊은 모델을 사용하는데, 앞서 설명했던 것과 같이 <U>representation power</U>가 커지면 그만큼 <U>overfitting</U> 위험성이 생긴다. 따라서 이를 해결하기 위해 레이어 수를 줄이는 방향이 아닌, <U>regularization을 사용</U>하게 된다. Deep neural network의 representation learning 과정에서 <U>local minima가 많고</U> 학습 시 수렴한 point가 <U>전체 함수의 global minima가 아닐 수 있지만</U>, 오히려 이런 loss들의 성능이 나쁘지 않아 실생활에서 다양하게 일반화하기 좋다.
`,uO=`---
title: "cs231n 내용 요약 (6) - Data preprocessing, Weight initialization, Batch Normalization"
category: "ai theory"
publishedAt: "2022-11-07"
thumbnail: "https://user-images.githubusercontent.com/79881119/213600588-8f6cfa86-243f-4354-882c-40df505ec05a.png"
---


# 들어가며...

이전까지 했던 내용에 대한 **전반적인 요약**은 다음과 같다. Linear classification을 예시로 들면서 softmax, SVM(support vector machine)의 classifier를 소개했었고, 이러한 classifier가 학습되는 과정에서 사용되는 score function의 한 형태인 <U>neural network</U>를 언급했었다. 생체 뉴런을 유사한 형태로 표현한 <U>perceptron</U>의 구조와 각 연산이 가지는 의미에 대해서 살펴봤었고, 단순한 논리 구조를 벗어나 <U>non-linearity</U>가 적용된 여러 층의 레이어를 가지는 <U>deep neural network</U>를 chain-rule에 기반하여 최적화하는 과정을 통해 보다 복잡한 형태의 함수를 가지는 real-world task들에 대한 universal approximator로 사용될 수 있는 것을 확인하였다. 다만 레이어가 깊어지면 깊어질수록 <U>representation power</U>(복잡한 함수를 표현할 수 있는 정도)는 증가하지만 그에 따른 부작용으로 overfitting(weight값의 표준편차가 커지는 것, training dataset에 네트워크가 과적합되는 것)이 생겼고, 이를 해결하기 위한 수단으로 여러 regularization 방법들(dropout, $L_2$ regularization 등등)을 소개하였다.   
이번에는 지금까지 살펴본 딥러닝 네트워크의 구조나 의의가 아닌, 실질적으로 학습 과정에서 필요한 데이터 전처리, Weight의 초기화에 대해 알아보고 학습 시 batch normalization과 같은 regularization이 가지는 장점에 대해 알아보는 글이 될 것이다.


# Data preprocessing(전처리)

사용할 데이터 $X$에 여러 조작을 가해보도록 하자. 여기서 사용되는 data $X$는 $X \\in \\mathbb{R}^{N \\times D}$의 batch 단위의 matrix를 가정하도록 하자. $N$은 data sample의 갯수를 의미하고, $D$는 각 샘플의 dimensionality(차원)을 의미한다. 예를 들어 만약 $3 \\times 32 \\times 32$의 RGB 채널을 가지는 이미지가 총 10장이 있다면 $N = 10,~D = 3072$가 될 것이다.

## Original data
가장 먼저 사용할 데이터 자체를 의미하는 original data에 대해서 살펴보자. 원본은 아무런 preprocessing이 되지 않은 그대로를 의미하고, 다음과 같이 데이터가 분포한다고 가정해보자.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213594618-5db41741-b847-4fae-a6e3-d7dbce8e8f31.png" width="300"/>
</div>

간단하게 $2$차원의 데이터를 가정했으며, 좌표평면의 각 점은 sample을 의미한다.

## Mean substraction

데이터 전처리에서 사용되는 방법 중 하나는 data 분포에 존재하는 bias를 없애주기 위한 '<U>mean substraction</U>' 작업이다. Mean substraction은 각각의 dimension에 대한 평균값을 빼주게 된다.

\`\`\`python
original_data = X #(assume that X is numpy array with shape N*D)
zero_centered_data = X - np.mean(X, axis = 0)
\`\`\`

\`\`\`numpy\`\`\` 모듈을 잘 모르는 사람이 있을 수도 있기 때문에 언급하자면 \`\`\`np.mean(_, axis=0)\`\`\` 메소드의 경우에는 인자로 들어가게 되는 array의 <U>$0$번째 축</U>에 대한 평균을 구하고자 하는 것이다. $0$번째 축을 따라서 평균을 내는 것은 다시 말하자면 $2$차원의 데이터에 대해서 첫번째 column vector와 두번째 column vector($\\mathbb{R}^{N \\times 1}$)의 평균을 빼주는 과정이다. $2$차원 데이터에 대해서 각 column vecor의 평균으로 bias를 없애게 되면, original data가 가지던 분포의 중심점이 원점인 $(0, 0)$로 이동하게 된다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213595506-e8b3356f-67c8-4742-943e-7d1dac31aef1.png" width="300"/>
</div>

## Normalization

각 차원 축은 하나의 feature로 이해할 수 있고, 만약 위의 그림과 같이 <U>각 차원 축에 대해 분산값이 상이할 경우</U> 학습 과정에서 최적화 시 가지는 중요도나 learning rate 비율이 달라질 수 있기 때문에 이를 어느 정도 유사하게 맞춰주는 작업이 필요하다.

\`\`\`python
original_data = X #(assume that X is numpy array with shape N*D)
zero_centered_data = X - np.mean(X, axis = 0)
normalized_data = zero_centered_data / np.std(X, axis = 0)
\`\`\`

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213595891-6d7ac658-9be3-48d8-8193-78870d3a2492.png" width="300"/>
</div>


# PCA and Whitening

위에서 본 내용은 dataset sample을 정규화하는 preprocessing이었다. 일반적으로 feature vector는 위에서 보는 바와 같이 각 column마다 correlation이 어느 정도 존재하는 형태가 된다($y = ax$). 이러한 correlation을 풀어주는 작업이 <U>PCA and Whitening</U>이라고 생각하면 된다.

\`\`\`python
original_data = X #(assume that X is numpy array with shape N*D)
zero_centered_data = X - np.mean(X, axis = 0)
covariance_matrix = zero_centered_data.T.dot(zero_centered_data)/X.shape[0]
\`\`\`

연산 과정은 다음과 같다. 가지고 있는 data $X \\in \\mathbb{R}^{N \\times D}$에 대해 zero-centered matrix를 구한 뒤, 이렇게 구해진 zero-centered matrix를 서로 inner product한 뒤 샘플의 개수 $N$으로 나눠주면 covariance matrix를 구할 수 있게 된다.

$$
    Cov(X) = \\frac{\\left(X-\\mu(X) \\right)^\\top \\left(X-\\mu(X) \\right)}{N}    
$$

구한 covariance matrix의 각 요소가 의미하는 것은 $i$번째 feature와 $j$번째 feature의 관련성을 의미한다. 위의 예시에서는 $2 \\times 2$ matrix가 나오게 된다. 따라서 covariance matrix는 자동으로 symmetric matrix가 되는데, 이 행렬의 diagonal component는 feature 각각에 대한 autocorrelation이고, autocorrelation은 수학적으로 보면 variance가 된다. Covariance matrix에 singular value decomposition(SVD)를 수행하게 되면 covariance matrix로부터 eigenvector $U$, eigenvalues $V$, singular values $S$를 추출할 수 있게 된다.

\`\`\`python
U, S, V = np.linalg.svd(covariance_matrix)
\`\`\`

SVD를 수행하는 이유는 covariance matrix의 요소가 각 feature에 대한 correlation을 표현한다고 하였는데, 이 식에서 orthonormal matrix $U$를 projection에 대한 basis로 사용하여 원본 데이터(평균에 대한 bias가 제거된)의 correlation을 없애기 위한 용도로 작용한다.

\`\`\`python
X_dr = zero_centered_data.dot(U)
\`\`\`

Dot production이 의미하는 것은 orthonormal basis $U$에 따른 축 rotation을 의미한다. 앞서 살펴봤던 바와 같이 기존 데이터가 각 feature vector에 대한 correlation을 가지고 있었는데, 축을 회전함으로써 이를 제거해줄 수 있다(decorrelation 과정).

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213595506-e8b3356f-67c8-4742-943e-7d1dac31aef1.png" width="300"/>
    <img src="https://user-images.githubusercontent.com/79881119/213597407-40ac00c8-6802-4342-b096-054e8b35bd40.png" width="290"/>
</div>

여기서 PCA는 <U>Principal Component Analysis</U>인데, 이는 SVD로 하여금 추출된 feature basis를 사용하는 것이 아니라 '중요한' feature만 사용하겠다는 의미가 된다. 만약 eigenbasis에서 eigenvalue가 큰 값을 기준으로 정렬했다고 생각한다면 $D$개의 feature 중에서 정말 중요한 $100$개를 사용한다고 생각해볼 수 있다. 물론 위의 경우와 같이 $2$차원 데이터일 경우엔 굳이 PCA를 적용할 필요가 없지만, 만약 feature의 차원 수가 늘어나게 되면 유의미한 feature만 사용하는 것이 중요하다. <U>Curse of dimension</U>이라 표현되는 해당 문제는 data가 구성하는 manifold는 실제로 데이터가 놓인 공간이 아닌, 공간의 일부를 이루는 hyperplane 등등 특정 표면을 구성한다는 점에서 등장한다.

\`\`\`python
Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced becomes [N x 100]
\`\`\`

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213598817-8a7b082a-abcf-44e4-9ca7-ae6c7e3c947e.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/213598863-fcf469d6-24f5-4614-8b83-d21acd81eaa7.png" width="400"/>
</div>

대표적인 데이터셋 중 하나인 Swiss roll의 경우 3차원으로 구성되어있지만 데이터셋 분포는 말려있는 하나의 plane(평면)을 구성하며, 이를 그대로 3차원의 공간에 대해서 활용하는 것보다 <U>데이터 분포를 가장 잘 나타낼 수 있는 평면</U>을 기준으로 projection해서 사용하는 것이 PCA의 한 예시가 된다.   
앞서 살펴본 예시에서 차원 수를 줄여서 사용한다면, 우리가 사용할 데이터셋은 $N \\times D$에서 $N \\times 100$으로 적은 feature를 가지게 되면서 그와 동시에 decorrelation이 진행된 데이터셋이 될 것이다. 여기에 추가로 data를 eigenbasis에 대해 projection 시킨 다음에 각각의 dimension을 eigenvalue로 나눠주게 되면, 각 basis에 data sample들이 가지는 variance로 normalization이 가능하다. 이를 whitening이라고 한다. $X$는 애초에 basis에 대해서 회전된 상태(projection)이고, 우리가 가지는 각 feature에 대한 분산 정보는 eigenvalue가 가지고 있다. 왜냐하면 해당 행렬의 diagonal에는 각 basis에 대한 $\\sigma$가 있고, 해당 value가 covariance matrix에서 diagonal element를 결정하기 때문이다.

\`\`\`python
# whiten the data:
# divide by the eigenvalues (which are square roots of the singular values)
Xwhite = Xrot / np.sqrt(S + 1e-5)
\`\`\`

Whitening이라고 불리는 작업은 평균이 $0$이고 identity covariance matrix를 가지는 white noise 형태의 분포를 만든다는 관점에서 나온 이름이다.
<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213600588-8f6cfa86-243f-4354-882c-40df505ec05a.png" width="800"/>
</div>


# Weight initialization
KNN(K-neareat neighbors)과 같은 <U>instance-based learning</U>(training dataset이 instance로 사용되어 test data의 예측에 관여하는 것)이 아닌 <U>model-based learning</U>은 모델 구조에 따라 weight parameter가 존재하고, 이를 update하는 과정으로 학습을 진행한다. 그렇기 때문에 학습을 시작하게 될 <U>weight의 초기화</U>에서도 성능의 차이가 발생하는데, 어떤 식으로 하는 것이 가장 효율적일까?

## Zero initialization
가장 먼저 생각해볼 수 있는 것은 모든 weight를 $0$으로 초기화하는 것이다. Weight의 초기화 방법은 배제하고 최적화된 네트워크의 parameter가 어떤 값을 가지는지 모두 예측할 수 없지만, 수없이 많은 parameter를 가지는 deep neural network 구조에서 weight의 절반은 <U>positive</U>로, 나머지는 <U>negative</U>로 학습이 될 것으로 예상할 수 있다(큰 수의 법칙). 그렇기에 처음부터 평균이 $0$인 상태로 시작하면 어떤 weight는 positive로, 어떤 weight는 negative로 가면서 자연스럽게 우리가 생각했던 이상적인 weight parameter의 구조를 가질 수 있지 않을까 싶지만 이는 잘못된 관점이다. 만약 network의 모든 뉴런이 같은 output을 내보내면, backpropagation 과정에서도 같은 값으로 update가 될 것이다. 즉 모든 weight가 같은 값으로 초기화되면 weight matrix가 assymetric하지 않게 된다. 간단한 예시를 위해 다음과 같은 perceptron code를 짜보았다.

\`\`\`python
import numpy as np

# sigmoid as an activation function
def sigmoid(x):
    return 1 / (1 +np.exp(-x))

# input : 2 * 3 [N * D]
input = np.array([[-1, 0, 1], [-1, 1, 0]])

# target : 2 * 2 [N * out]
target = np.array([[0, 1], [1, 0]])

# Weight 1 [3 * 4]
W1 = np.zeros((3, 4))

# Weight 2 [4 * 2]
W2 = np.zeros((4, 2))

# feed forward
hidden = sigmoid(input.dot(W1))
out = hidden.dot(W2)

# calculate difference
diff = target - out

# backpropagation
W2 -= hidden.T.dot(diff)
W1 -= input.T.dot((hidden*(1-hidden)))

# print results
print(W1, W2)
\`\`\`

코드는 가독성이 떨어지기 때문에 수식으로 표현하면 다음과 같다.

$$
    \\begin{aligned}
        \\text{input} =& \\begin{bmatrix}
            -1 & 0 & 1 \\newline
            -1 & 1 & 0
        \\end{bmatrix},~\\text{target} = \\begin{bmatrix}
            0 & 1 \\newline
            1 & 0
        \\end{bmatrix} \\newline
        \\text{output} =& \\sigma \\left( input \\odot W1 \\right) \\odot W2 \\newline
        \\text{diff} =& \\text{target} - \\text{output}
    \\end{aligned}    
$$

위의 코드를 돌렸을 때의 weight parameter를 직접 확인해보면,

$$
    W_1 = \\begin{bmatrix}
        0.5 & 0.5 & 0.5 & 0.5 \\newline
        -0.25 & -0.25 & -0.25 & -0.25 \\newline
        -0.25 & -0.25 & -0.25 & -0.25
    \\end{bmatrix},~W_2 = \\begin{bmatrix}
        -0.5 & -0.5 \\newline
        -0.5 & -0.5 \\newline
        -0.5 & -0.5 \\newline
        -0.5 & -0.5
    \\end{bmatrix}
$$

위와 같이 각 row의 모든 값이 동일하게 update되는 것을 알 수 있다. 사실상 weight parameter의 개수가 의미하는 것이 neural network에서 가용할 수 있는 node의 갯수가 되는데, 이렇게 update가 되면 굳이 큰 parameter 개수를 가지는 weight를 사용할 필요성이 없어지는 것이다. 다르게 말하면, parameter의 수가 많아도 <U>representation power가 떨어지게</U> 된다. 사실 이러한 문제는 모든 parameter를 $0$으로 초기화하는 상황 뿐만 아니라 같은 값으로 초기화할 때 발생하는 문제다.


\`\`\`python
import numpy as np

# sigmoid as an activation function
def sigmoid(x):
    return 1 / (1 +np.exp(-x))

input = np.array([[-1, 0, 1], [-1, 1, 0]])
target = np.array([[0, 1], [1, 0]])

value1 = 0.01
value2 = 0.09

W1 = value1 * np.ones((3, 4))
W2 = value2 * np.ones((4, 2))

# feed forward
hidden = sigmoid(input.dot(W1))
out = hidden.dot(W2)

# calculate difference
diff = target - out

# backpropagation
W2 -= hidden.T.dot(diff)
W1 -= input.T.dot((hidden*(1-hidden)))

# print results
print(W1, W2)
\`\`\`

W1, W2 각각을 동일한 값으로 초기화한 후 학습하였다. 결과는 다음과 같다.

$$
    W_1 = \\begin{bmatrix}
        0.51 & 0.51 & 0.51 & 0.51 \\newline
        -0.24 & -0.24 & -0.24 & -0.24 \\newline
        -0.24 & -0.24 & -0.24 & -0.24
    \\end{bmatrix},~W_2 = \\begin{bmatrix}
        -0.23 & -0.23 \\newline
        -0.23 & -0.23 \\newline
        -0.23 & -0.23 \\newline
        -0.23 & -0.23
    \\end{bmatrix}
$$

## Small random numbers
Overfitting 및 위에서 언급한 문제를 해결하기 위해서는 weight를 <U>같은 값으로 초기화</U>하는 방법을 사용할 수 없다. 다음으로 생각해볼 수 있는 상황은 모두 random하게 생성하는 것이다.

\`\`\`python
W = 0.01* np.random.randn(D, H)
\`\`\`
여기서 $H$는 hidden layer의 output dimension을 의미한다. \`\`\`randn\`\`\`은 $D \\times H$의 정규 분포 랜덤 난수를 생성하므로, 코드에 따라 $0.01$의 표준편차를 가지는 가우시안 분포를 따르는 weight matrix $W \\in \\mathbb{R}^{D \\times H}$가 생성된다.

## Calibrating the variances with $\\frac{1}{\\sqrt{N}}$

위의 제시된 방법은 output의 분포가 가지는 variance가 input 개수에 따라 증가할 수 있다는 것이다. 위에서와 같이 임의의 상수 \`\`\`0.01\`\`\`의 표준편차를 가지는 가우시안 분포를 생성하는 것이 아닌, 각 neuron의 output 분포를 $1$의 variance(혹은 표준편차)를 갖게끔 해준다. 특정 layer의 input 개수가 $n$이라면 해당 layer의 연산을 담당하는 neuron의 weight을

\`\`\`python
w = np.random.randn(n) / sqrt(n)
\`\`\`

로 초기화하게 되면, 네트워크의 모든 neuron들이 같은 output distribution을 갖게되고, 이러한 방법이 convergence 속도를 증가시킬 수 있다.

$$
    \\begin{aligned}
        \\text{Var}(s) =& \\text{Var} \\left( \\sum_i^n w_i x_i \\right) \\newline
        =& \\sum_i^n \\text{Var} (w_i x_i) \\newline
        =& \\sum_i^n \\left( E(w_i) \\right)^2 \\text{Var} (x_i) + \\left( E(x_i) \\right)^2 \\text{Var}(w_i) + \\text{Var} (x_i) \\text{Var}(w_i) \\newline
        =& \\sum_i^n \\text{Var} (x_i) \\text{Var}(w_i) \\newline
        =& (n \\text{Var}(w)) \\text{Var} (x)
    \\end{aligned}    
$$
해당 concept에 대한 수식 증명은 위와 같다.

## Sparse initialization
위에서 언급했던 calibration 관련 문제(neuron의 output마다 분포가 달라지는 현상)를 해결하기 위해서는 모든 weight matrix를 0으로 초기화를 해야하지만, 앞서 말했던 것처럼 이대로 학습을 진행하면 모든 weight가 동일하게 학습되는 문제가 발생하기 때문에 고정된 갯수의 neuron을 정해두고 랜덤한 gaussian noise(이 때는 calibration이 진행되지 않은, 앞서 봤던 small random number에 해당된다)에서 샘플링한 weight만 연산하게 된다. 즉, 연산량을 고정시켜서 output distribution이 항상 동일하게 유지되게 하는 것이다.

## Initializing bias
Bias는 모든 값을 $0$으로 초기화해도 괜찮다. 이는 만약 weight의 assymetry가 보장되면 bias가 동일한 값을 가지더라도 node에 따른 assymetry가 유지되기 때문이다. ReLU 특성상 $0$보다 작은 값에 대해서 gradient를 주지 않기 때문에 해당 non-linearity를 가진 neuron에 대해서는 $0.01$과 같이 작은 양의 값으로 초기화하는 걸 선호하는 case도 있지만, 이러한 과정이 실질적으로 performance에 긍정적인 영향을 준다는 근거는 없으며 더 안좋은 결과를 보여주기도 한다. 따라서 bias는 $0$으로 초기화하는 것이 가장 일반적이다. 

## He initialization
He et al. 저자들이 밝힌 바로는([참고링크](https://arxiv.org/abs/1502.01852)) ReLU를 사용하는 network에 기반하여 다음과 같은 초기화 과정이 더 낫다고 하였다. 논문에 weight initialization 말고도 실험한 부분들이 유의미한 내용을 담고 있기 때문에 한번쯤 보는 것을 추천한다.

\`\`\`python
W = np.random.randn(n)*sqrt(2.0/n)
\`\`\`


# Batch normalization

[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)(이른바 ResNet)를 시작으로 convolutional neural network 구조에 <U>batch normalization</U>을 추가하는 것이 trend가 되었다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213616285-70c21d20-a4e9-4cc9-a291-87c8b041e133.png" width="800"/>
</div>

[Batch normalization](https://arxiv.org/abs/1502.03167)은 말 그대로 <U>'batch' 단위로 정규화를 진행</U>하는 작업이며, batch normalization을 포함하여 layer normalization, instance normalization 그리고 group normalization이 있다. 각각 그림을 보게 되면 정규화를 진행하는 단위가 다른 것을 알 수 있는데, 예를 들어 layer normalization의 경우에는 단일 batch 내에서 채널 전체의 평균 및 표준편차를 통해 정규화를 진행하게 되고, Instance normalization은 단일 batch, 그리고 channel에 대해서 normalization을 진행한다. 마지막으로 group normalization은 단일 배치 내에서 채널을 묶어서 하나의 그룹을 생성하고, 이 그룹 내에서 normalization을 진행한다. 이 중에서 지금 소개할 것은 가장 흔하게 사용되는 batch normalization이다.   
보통 gradient descent 알고리즘에서 단일 샘플을 사용하지 않고 mini-batch(혹은 batch라고도 부른다)를 사용하여 parameter update를 진행한다. 이유는 앞서 작성한 글에서도 확인할 수 있지만 다시 한번 언급하자면 단일 샘플로 업데이트하게 될 경우 noisy한 학습이 진행되고, batch 단위로 진행할 때와는 다르게 병렬 연산이 불가능하기 때문에 비효율적이다.

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/211718291-66494f31-f22a-4221-aaf8-b751c03ff011.png" width="700"/>
</div>

만약 여러 레이어를 가진 neural network에 batch 단위로 샘플을 통과시키게 되는 상황을 생각해보자. Batch는 랜덤하게 dataset으로부터 추출하기 때문에 각 batch 마다의 데이터 분포는 차이가 있으며, 마찬가지로 layer를 통과하면서 생기는 feature map의 분포 또한 균등하지 않다는 문제가 발생한다. 이를 '<U>internal covariance shift</U>'라고 부른다. 사실 이 부분이 중요한 이유는 앞서 설명해왔던 weight initialization 과정과도 직결되기 때문인데, 만약 학습되는 batch 단위와 중간 layer에서의 feature map 분포 및 형태가 상이하다면 최적화 과정에서 <U>initialization metric</U>을 확정적으로 사용할 수 없게 된다. 따라서 이러한 batch에 따른 분포 차이를 줄여주기 위해, batch 단위로 정규화를 진행하여 <U>변동성을 줄이고 학습을 안정화</U>시키고자 한 것이 batch normalization의 concept이다.   
각 batch에 대한 mean(평균) 및 variance(분산)은 정의에 따라 다음과 같이 구할 수 있다.

$$
    \\begin{aligned}
        \\mu_\\text{batch} =& \\frac{1}{m} \\sum_{i=1}^m x_i   \\newline
        \\sigma^2_\\text{batch} =& \\frac{1}{m} \\left(x_i - \\mu_{\\text{batch}} \\right)^2
    \\end{aligned}
$$

여기서 $x_i$가 의미하는 바는 $m$ 만큼의 batch가 하나의 데이터 묶음을 차지할 때, batch 내에서의 $i$번째 샘플이다. 이렇게 구한 각 batch mean, variance를 활용하여 batch dataset을 normalize 및 scale and shift를 진행한다.

$$
    \\hat{x_i} = \\frac{x_i - \\mu_\\text{batch}}{\\sqrt{\\sigma_\\text{batch}^2 + \\epsilon}}    
$$

하지만 단순히 위와 같은 방법으로 정규화를 진행한다면, non-linearity를 가지는 neural network로 하여금 linear regime에 머무르게끔 강제하는 것이 될 수 있고(representation power가 줄어드는 것을 의미), 각 레이어에서 activation function 이전의 layer의 output에 대해 적용되는 batch normalization layer이 모두 다른 형태의 representation에 대해 적용될 수 있게끔 learnable parameter인 $\\gamma$, $\\beta$를 두어 scale and shift를 진행한다.

$$
    y_i = \\gamma \\hat{x_i} + \\beta
$$


# Appendix for batch normalization
Batch normalization은 사실상 딥러닝 네트워크를 건드려본 사람이라면 필수적으로 알아야 하는 개념이지만, 말만 들어보고 적용만 했을 뿐 생각보다 <U>실제로 구현하는 방식</U>이나 <U>동작하는 원리</U>를 잘 모르고 쓰는 일이 많다. 그렇기 때문에 직접 공식을 기반으로 forward, backward가 어떻게 코드로 동작하는지 구현해보도록 하겠다. 사실 코드랑 내용 전부 다른 페이지 내용을 기반으로 작성하는 것이라 굳이 이 글을 보기 귀찮다면 참고 링크를 통해 직접 내용을 확인해봐도 좋을 것 같다.([참고링크1](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html), [참고링크2](http://proceedings.mlr.press/v37/ioffe15.html))

## Batch normalization를 사용하는 이유와 그 과정
배치 단위로 정규화를 진행하는 말 그대로 'Batch normalization'은 input $$x가 있으면 배치 단위로 정규화를 진행하는 과정이다. 이는 흔히 activation function이 뒤따르는 convolution layer 사이사이 들어가게 되며, covariance shift(feature map 및 batch 단위로 분포가 왔다 갔다 하는 현상)을 방지하여, 보다 큰 learning rate에 대해서도 안정적인 학습을 보장한다던지, 빠른 optimization을 가능하게 하는 등 다양한 장점이 있다. 하지만 Batch normalization은 무분별하게 사용되면 안되는데, 이를테면 GAN based 구조를 가진 모델에서 batch normalization이 encoder나 decoder 말단에 들어가서 <U>학습을 방해하는 경우</U>도 있기 때문이다. 사실 간단하게 설명하자면 batch normalization이 등장한 것은 안정적인 학습을 위함이었지만, 그렇다고 해서 모든 task에 만병통치약과 같은 존재가 될 순 없다는 것이다. 논문에 제시된 batch normalization 과정은 위에서 소개했던 수식 전개 과정과 동일하다.

$$
    \\begin{aligned}
        \\mu_\\text{batch} =& \\frac{1}{m} \\sum_{i=1}^m x_i   \\newline
        \\sigma^2_\\text{batch} =& \\frac{1}{m} \\left(x_i - \\mu_{\\text{batch}} \\right)^2 \\newline
        \\hat{x_i} =& \\frac{x_i - \\mu_\\text{batch}}{\\sqrt{\\sigma_\\text{batch}^2 + \\epsilon}} \\newline
        y_i =& \\gamma \\hat{x_i} + \\beta
    \\end{aligned}
$$

가장 먼저, batch 단위로 평균($\\mu$)과 분산($\\sigma^2$)을 구해주고, 구한 평균과 분산을 기반으로 샘플을 정규화한다. 그런 뒤 학습 가능한 parameter인 $\\gamma$와 $\\beta$를 통해 scaling and shifting이 진행된다. 학습 가능한 parameter인 $\\gamma$와 $\\beta$는 다음과 같이 업데이트된다.

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/213619378-f30a8913-43fd-4c29-9278-d4fa04b2f273.png" width="1000"/>
</div>

흔히 backpropagation을 계산할 때 위와 같이 diagram을 그린 뒤, chain rule에 맞춰서 local gradient를 구하고 이 값에 backward된 gradient value를 곱하게 되면, output부터 차례대로 input까지 거쳐온 <U>모든 parameter에 대한 gradient</U>를 구할 수 있게 된다.

## Forward propagation

\`\`\`python
N, D = x.shape

#step1: calculate mean
mu = 1./N * np.sum(x, axis = 0)

#step2: subtract mean vector of every trainings example
xmu = x - mu

#step3: following the lower branch - calculation denominator
sq = xmu ** 2
\`\`\`

천천히 3단계 정도씩 끊어서 볼 예정이다. 가장 먼저 샘플에 대한 평균을 구한다. sample \`\`\`x\`\`\`의 모양은 \`\`\`batch * dimension\`\`\`이므로 \`\`\`axis=0\`\`\`을 기준으로 평균을 구해야 batch에 대한 평균을 구할 수 있다. 참고로 \`\`\`np.sum()\`\`\` 메소드가 아닌 \`\`\`np.mean()\`\`\` 메소드를 통해 평균값을 바로 구할 수 있다.

\`\`\`python
mu = np.mean(x, axis=0)
\`\`\`

평균을 구했으니 이를 sample에서 빼준다. 여기서 중요한 점은 우리가 \`\`\`x - mu\`\`\`를 두 가지 계산에 동시에 활용할 것인데, 바로 첫 번째는 아래와 같이 normalize된 $x$를 구할 때의 \`\`\`x - mu\`\`\` term에 활용할 것이고 동시에 var를 구하기 위한 식에도 적용할 것이다.

$$
    \\begin{aligned}
        \\hat{x_i} =& \\frac{x_i - \\mu_\\text{batch}}{\\sqrt{\\sigma_\\text{batch}^2 + \\epsilon}} \\newline
        \\sigma^2_\\text{batch} =& \\frac{1}{m} \\left(x_i - \\mu_{\\text{batch}} \\right)^2
    \\end{aligned}
$$

위의 두 식에 공통적으로 들어있는 $x_i - \\mu_\\text{batch}$를 생각해주면 된다.

\`\`\`python
#step4: calculate variance
var = 1./N * np.sum(sq, axis = 0)

#step5: add eps for numerical stability, then sqrt
sqrtvar = np.sqrt(var + eps)

#step6: invert sqrtvar
ivar = 1./sqrtvar
\`\`\`

\`\`\`sq\`\`\`는 위에서 구했던 \`\`\`x - mu\`\`\`의 제곱이기 때문에 이를 평균낸 것이 분산 공식이다. 위에서 언급했던 것과 마찬가지로 이 식도 \`\`\`np.mean()\`\`\` 메소드로 대체 가능하다.

\`\`\`python
var = np.mean(sq, axis = 0)
\`\`\`

그리고 step 5에서 \`\`\`eps\`\`\`를 더하는 형태의 방법은 흔히 컴퓨팅 환경에서 \`\`\`0\`\`\`으로 division되는 error를 방지하기 위함이다. 나머지 계산을 통해 구하고자 하는 output을 나타내면 다음과 같다.

\`\`\`python
#step7: execute normalization
xhat = xmu * ivar

#step8: Nor the two transformation steps
gammax = gamma * xhat

#step9
out = gammax + beta
\`\`\`
\`\`\`xhat\`\`\`(normalized된 \`\`\`x\`\`\`), 학습 가능한 parameter인 \`\`\`gamma\`\`\`, \`\`\`beta\`\`\`를 통한 scaling과 shifting 과정이다.

## Backward propagation

$$
    \\begin{aligned}
        \\frac{\\partial l}{\\partial \\hat{x_i}} =& \\frac{\\partial l}{\\partial y_i} \\cdot \\gamma \\newline
        \\frac{\\partial l}{\\partial \\sigma_B^2} =& \\sum_{i=1}^m \\frac{\\partial l}{\\partial \\hat{x_i}} \\cdot (x_i - \\mu_B) \\cdot -\\frac{1}{2}(\\sigma_B^2 + \\epsilon)^{-3/2} \\newline
        \\frac{\\partial l}{\\partial \\mu_B} =& \\sum_{i=1}^m \\frac{\\partial l}{\\partial \\hat{x_i}} \\cdot \\frac{-1}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\newline
        \\frac{\\partial l}{\\partial x_i} =& \\frac{\\partial l}{\\partial \\hat{x_i}} \\cdot \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}} + \\frac{\\partial l}{\\partial \\sigma_B^2} \\cdot \\frac{2(x_i - \\mu_B)}{m} + \\frac{\\partial l}{\\partial \\mu_B} \\cdot \\frac{1}{m} \\newline
        \\frac{\\partial l}{\\partial \\gamma} =& \\sum_{i=1}^m \\frac{\\partial l}{\\partial y_i} \\cdot \\hat{x_i} \\newline
        \\frac{\\partial l}{\\partial \\beta} =& \\sum_{i=1}^m \\frac{\\partial l}{\\partial y_i}
    \\end{aligned}
$$

사실 backward propation 과정은 위와 같이 논문에 수식으로 정리되어 있다. 하지만 실제로 이를 코드로 구현하는 작업이 생각보다는 복잡하다. 각각의 식이 유도된 부분을 각 gate마다 천천히 살펴보면 다음과 같다. 우선 가장 말단에 있는 gate부터 backpropagation을 진행하게 되면,

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/213634136-116b0839-b86b-4e1f-b3f5-f89fa21661b5.png" width="150"/>
</div>

$$
    \\text{out} = \\gamma \\hat{x} + \\beta    
$$
위와 같은 연산을 하게 되므로 덧셈 연산 gate의 두 input인 $\\gamma \\hat{x},~\\beta$에 대한 local gradient는 각각 다음과 같이 구할 수 있다.

\`\`\`python
dgammax = dout
dbeta = np.sum(dout, axis=0)
\`\`\`

\`\`\`dbeta\`\`\`를 구하는 과정에서 \`\`\`np.sum()\`\`\` 메소드가 사용되는 부분이 이해가 잘 가지 않아서 살펴보니, numpy broadcasting은 사실 $\\beta$의 크기 그대로를 더하는 것이 아니라, 이 값을 broadcasting하여 \`\`\`gammax\`\`\`의 dimension에 맞게끔 확장시켜 더하게 된다. 따라서 \`\`\`dbeta\`\`\`를 구하는 것은 broadcasting을 고려해야하므로 이처럼 표현된 것으로 이해할 수 있었다.

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/213634981-7b834205-7d0d-4289-ab0b-b60643a58d7d.png" width="150"/>
</div>

그 다음은 곱셈 연산 게이트에 대해서 backpropagation을 진행한다. 이 operation에 대한 backpropagation을 계산하기 전, \`\`\`dgammax\`\`\`가 이전의 backpropagation value이기 때문에 chain rule에 따라, 해당 value에 local gradient를 곱하여 해당 gate의 각 input에 대해 흘러가는 gradient를 구하게 된다. 사실 \`\`\`*\`\`\` operation에 대한 gate local gradient는 이전 게시글에서 다룬 것과 같이, gate로 들어오는 backpropagation value에 각자 다른 길로 들어오는 input value를 곱해주면 된다. 이를 테면 위쪽 input은 \`\`\`xhat\`\`\`이고 아래쪽 input은 \`\`\`gamma\`\`\`이므로, 위쪽 gate에 대한 backpropagation value는 \`\`\`dgammax\`\`\`에 \`\`\`gamma\`\`\`를 곱한 결과가 \`\`\`dxhat\`\`\`이 되고 반대로 아래쪽 gate에 대한 backpropagation value는 \`\`\`dgammax\`\`\`에 \`\`\`xhat\`\`\`을 곱한 결과가 된다.

\`\`\`python
dxhat = dgammax * gamma
dgamma = np.sum(dgammax * xhat, axis=0)
\`\`\`

그리고 앞서 살펴본 바와 같이 \`\`\`gamma\`\`\` 또한 \`\`\`beta\`\`\`와 같이 broadcasting되는 값이기 때문에 \`\`\`np.sum()\`\`\` 메소드를 통해 dimension을 맞춰주게 된다. 이제 학습 가능한 parameter인 \`\`\`gamma\`\`\`와 \`\`\`beta\`\`\`에 대한 backpropagation이 끝났고, normalization에서의 backpropagation을 쭉 계산해보면 다음과 같다.

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/213672252-1fa03a1b-e8e6-442b-937a-213a59b452de.png" width="600"/>
</div>

가장 오른쪽의 곱셈 연산 gate에 대해서는 앞서 했던 연산과 동일하므로 주어진 코드에서 연산은 다음과 같이 진행된다.

\`\`\`python
divar = np.sum(dxhat*xmu, axis=0)
dxmu1 = dxhat * ivar
\`\`\`
sample variance \`\`\`divar\`\`\`(inverse variance) 또한 broadcasting 되었으므로 \`\`\`np.sum\`\`\` 해주고, \`\`\`dxmu1\`\`\`은 이전의 backpropagation value였던 \`\`\`dxhat\`\`\`에 \`\`\`ivar\`\`\`를 곱해준 것과 같다. 여기서 중요한 것은 \`\`\`dxmu\`\`\`에 대한 계산이 두 개가 필요하다는 것인데, 이는 아래와 같이 \`\`\`x - mu\`\`\`가 두 갈래로 나뉘어 계산되었기 때문이다.

<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/213672765-3330f613-8271-4d53-b4ac-c122a53afda7.png" width="150"/>
</div>

따라서 이 gate에 대한 backprop을 계산하기 위해서는 분리된 두 input에 대한 gradient를 따로 계산한 뒤에 더해줘야 한다. 위로 가는 \`\`\`dxmu1\`\`\`은 계산했고 나머지 아래 부분에 대한 공식을 차례로 보면, inverse에 대한 local gradient는 $\\frac{d}{dx} \\left( \\frac{1}{x} \\right) = -\\frac{1}{x^2}$이기 때문에,

\`\`\`python
dsqrtvar = -1. /(sqrtvar**2) * divar
\`\`\`

코드가 위와 같으며 마찬가지로 square root에 대한 local gradient는 $\\frac{d}{dx} \\left( x + \\epsilon \\right) = \\frac{1}{2 \\sqrt{x + \\epsilon}}$이 되기 때문에,

\`\`\`python
dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar
\`\`\`

이처럼 표현 가능하다. $\\frac{1}{N} \\sum_i x_i$에 대한 부분이 조금 복잡한데, 현재 미분의 대상이 되는 것이 행렬이기 때문에 다음과 같이 정의할 수 있다.

$$
    \\frac{d}{dx} \\left( \\frac{1}{N} \\sum_i x_i \\right) = \\frac{1}{N} \\begin{pmatrix}
        1 & \\cdots & 1 \\newline
        \\vdots & \\ddots & \\vdots \\newline
        1 & \\cdots & 1
    \\end{pmatrix}    
$$

\`\`\`python
dsq = 1. /N * np.ones((N,D)) * dvar
\`\`\`

마지막 부분은 $\\frac{d}{dx} \\left( x^2 \\right) = 2x$에서,

\`\`\`python
dxmu2 = 2 * xmu * dsq
\`\`\`

이제 드디어 \`\`\`dxmu2\`\`\`를 구했으므로 뺄셈 연산 gate에 대한 local gradient를 구하게 되면 \`\`\`mu\`\`\`는 빼주고 \`\`\`x\`\`\`는 더해주는 process이므로,

\`\`\`python
dx1 = (dxmu1 + dxmu2)
dmu = -1 * np.sum(dxmu1 + dxmu2, axis=0)
\`\`\`

위에서 보는 바와 같이 \`\`\`dx1\`\`\`은 positive(+), \`\`\`dmu\`\`\`는 negative(-) 방향이 된다. \`\`\`dx\`\`\` 또한 \`\`\`dxmu\`\`\` 계산과 동일하게 \`\`\`dx2\`\`\` 연산이 추가로 필요한데, 이는 앞서 구했던 matrix의 미분 공식과 같은 공식이 적용된다.

\`\`\`python
dx2 = 1. /N * np.ones((N,D)) * dmu
dx = dx1 + dx2
\`\`\`

이를 모두 합친 과정이 다음과 같은 backpropagation 코드가 된다.

\`\`\`python
#unfold the variables stored in cache
xhat, gamma, xmu, ivar, sqrtvar, var, eps = cache

#get the dimensions of the input/output
N,D = dout.shape

#step9
dbeta = np.sum(dout, axis=0)
dgammax = dout #not necessary, but more understandable

#step8
dgamma = np.sum(dgammax*xhat, axis=0)
dxhat = dgammax * gamma

#step7
divar = np.sum(dxhat*xmu, axis=0)
dxmu1 = dxhat * ivar

#step6
dsqrtvar = -1. /(sqrtvar**2) * divar

#step5
dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar

#step4
dsq = 1. /N * np.ones((N,D)) * dvar

#step3
dxmu2 = 2 * xmu * dsq

#step2
dx1 = (dxmu1 + dxmu2)
dmu = -1 * np.sum(dxmu1+dxmu2, axis=0)

#step1
dx2 = 1. /N * np.ones((N,D)) * dmu

#step0
dx = dx1 + dx2
\`\`\`
`,dO=`---
title: "cs231n 내용 요약 (7) - Regularization, Loss function"
category: "ai theory"
publishedAt: "2022-11-08"
thumbnail: "https://user-images.githubusercontent.com/79881119/216201990-806c4d39-6157-41e9-abef-9dd42455b559.png"
---


# 들어가며...
바로 이전 게시글에서 batch normalization에 대한 개념과 해당 요소를 실제 딥러닝에서 어떻게 연산하는지 코드를 통해 확인해보았다. 이번 게시글에서는 batch normalization과 같이 정규화 역할을 하지만, layer 사이의 covariance shift를 줄이기 위한 목적보다는 <U>overfitting</U>을 방지하기 위한 목적으로 사용되는 여러 regularization term에 대해 알아보고, 딥러닝에서 다루는 supervised learning task 중 가장 대표적인 **classification**과 **regression**에 대한 loss function에 대해서 알아보도록 하자.   
사실 대부분의 내용은 이전 게시글을 잘 살펴보면 이미 언급한 내용이긴 하지만, 대부분 perceptron이나 linear classifier 등등 설명하면서 보조적으로 곁들인 경우가 많아서 이렇게 따로 다루고자 한다.


# Regularization
<U>Regularization</U>이라는 단어가 가지는 딥러닝에서의 의미를 보기 전에 일반적 의미에 집중하면 다음과 같다. **수학**에서나, **통계학, 경제학** 그리고 **컴퓨터 과학**에서의 정규화는 얻고자 하는 정답이 '<U>단순</U>'하고자 할 때 사용한다. 

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/216201990-806c4d39-6157-41e9-abef-9dd42455b559.png" width="400"/>
</div>

Regularization이 사용되는 방법은 크게 두가지로 구분할 수 있는데, 각각을 살펴보면 다음과 같다.

- **Explicit Regularization**이란, optimization 과정에서 explicit term을 더해주는 것이다. Prior, penalty 혹은 constraints가 될 수 있다. 흔히 regularization term이나 penalty term은 optimization function에 cost를 주어 optimal solution을 unique하게 만들어준다.

- **Implicit Regularization**이란 explicit regularization을 제외한 모든 형태의 regularization을 의미한다. Early stopping(학습 도중 적당히 fitting되었다면 멈추는 것)이나 robust loss function을 사용하는 등의 방법이 될 수 있다.

이러한 여러 방법들 중 오늘 살펴볼 $L_1$, $L_2$ regularization은 <U>explicit한 constraints</U>로 적용되는 경우가 많으며(weight decay라는 property로 조절된다), 네트워크 구조상 regularization으로 사용되는 dropout과 같은 방법은 implicit regularization에 해당된다.   
앞서 Linear classifier에 대한 게시글에서도 살펴보았듯, 특정 $W$가 SVM loss를 최적화할 수 있다면 $1$보다 큰 모든 $\\alpha$에 대해 $\\alpha W$ 또한 같은 조건을 만족하기 때문에 <U>non-unique solution</U> 문제가 발생하고 이로 인해 학습 속도가 저하되거나 수렴하지 못하는 문제가 발생한다고 했었다. SVM loss와는 다르게 softmax와 관련된 task에서 해석했던 내용은 $W$가 커지면 커질수록 각 노드별 output value의 차이가 벌어지게 되고, 이로 인해 복잡한 함수에 수렴하는 neural network가 overfitting될 수 있다고 했다. 그렇기 때문에 결국 weight parameter $W$를 너무 커지지 않도록 조절하는 것이 <U>explicit regularization</U>의 한 방법이 될 수 있고, 이러한 방법들 중 일부를 소개하면 다음과 같다.

## L2 regularization
가장 일반적인 형태의 regularization이다. 모든 parameter value의 squared magnitude를 penalize함으로써, 최소화하는 loss term에 더해주어 weight parameter 또한 줄일 수 있게 해주는 방법이다. 딥러닝 네트워크는 여러 layer와 각 layer를 구성하는 parameter로 연결되어있는데, 이에 대한 regularization term은 loss weight $\\lambda$에 대해 다음과 같이 정의할 수 있다.

$$
    \\frac{1}{2} \\vert \\lambda W \\vert^2    
$$

Loss term의 앞부분에 $1/2$이 곱해진 이유는 gradient를 구했을 때 weight에 2가 곱해져서 실제로는 $2 \\times \\lambda$ 만큼의 weight가 최적에 관여하기 때문이다. 해당 term이 포함된 loss function은 다음과 같이 표현할 수 있다.

$$
    \\text{Cost} = \\frac{1}{n} \\sum_{i=1}^n L(y_i, \\hat{y_i}) + \\frac{\\lambda}{2} \\vert W \\vert^2  
$$

L2 regularization을 사용하는 regression model을 <U>ridge regression model</U>이라고 부른다. L2 penalize는 직관적으로 보게 되면 peaky한 값들에 더 많은 페널티를 부여한다. 그래서 뒤에 추가로 설명하게 될 L1 regularization보다 이상치에 대한 용인성이 낮다는 특징이 있다. Weight decay, regularization은 training dataset에 지나치게 적응된 weight가 학습되는 것을 방지하는 효과가 생긴다. 예컨데 weight의 <U>특정 node의 값이 지나치게 커져야만</U> training dataset에 대한 성능을 높일 수 있다면, regularization term 없이는 결국 해당 parameter가 지나치게 커지는 양상을 보이게 된다. 이런 문제는 모든 위치의 parameter에 동일한 양상을 보이며, 결국 학습이 완료된 후 parameter는 앞서 본 것과 같이 complexity가 높은 polynomial을 그리게 된다. 일반화의 성능을 높이기 위해서는 <U>training dataset에 대한 overfitting</U>이 아닌, training dataset이 <U>포함된 전체 분포에 대한 정보를 학습</U>해야하기 때문에 정규화를 진행하는 것이 중요하다.

## L1 regularization
위에서는 L2 norm을 사용한 L2 regularization이었고, 이번엔 L1 norm을 사용한 L1 regularization에 대해서 살펴보도록 하자.

$$
    \\text{Cost} = \\frac{1}{n} \\sum_{i=1}^n L(y_i, \\hat{y_i}) + \\lambda \\vert W \\vert
$$

L1 regularization을 사용하는 regression model을 <U>Lasso regression model</U>이라고 부른다. L1 penalize는 앞서 보았던 L2 regularization보다는 이상치에 대해서 더 큰 페널티를 부여하지 않기 때문에, 용인성이 보다 크다고 할 수 있다. 만약 특정 modality를 학습하는 과정에서 이상치에 대한 정보를 유지하면서 학습하고 싶다면 L1 regularization 방법이 좋을 것이고, 그게 아니라 평균적인 weight를 만들고 싶다면 L2 regularization 방법이 더 좋을 것이다.

두 개의 vector가 있다고 생각해보고, 각각의 L1 norm과 L2 norm에 대해 구하면 다음과 같다.

$$
    \\begin{aligned}
        v_1 = (0.5,~-0.5,~0) \\newline
        v_2 = (0.3,~0.3,~-0.4) \\newline
        \\vert\\vert v_1 \\vert\\vert_1 = \\vert\\vert v_2\\vert\\vert_1 = 1 \\newline
        \\vert\\vert v_1 \\vert\\vert_2 = \\sqrt{0.5} \\newline
        \\vert\\vert v_2 \\vert\\vert_2 = \\sqrt{0.34}
    \\end{aligned}    
$$

여기서 알 수 있는 내용은 L1 regularization의 경우 서로 다른 weight parameter에 대해서도 동일한 값을 가질 수 있지만, L2 regularization의 경우 weight parameter가 달라지게 되면 무조건 다른 값을 가지게 되고, L2 norm의 기준은 <U>sparse한 parameter일수록 더 큰 값을 가진다는 것</U>이다. 따라서 **weight parameter**를 기준으로 생각했을때 모델을 sparse하게 구성하고 싶다면 **L1 regularization**을 사용하고, dense하게 구성하고 싶다면 **L2 regularization**을 사용하는 것이 일반적이다.

## Elastic network regularization

물론 L2와 L1 loss를 동시에 사용하는 경우도 있는데, 이를 <U>Elastic network regularization</U>이라고 부르고 다음과 같이 사용한다.

$$
    \\text{Cost} = \\frac{1}{n} \\sum_{i=1}^n L(y_i, \\hat{y_i}) + \\lambda_1 \\vert W \\vert^2 + \\lambda_2 \\vert W \\vert
$$

## Max norm constraints
Infinity norm을 정의하는 방식도 있다. 다만 <U>infinity norm</U>의 경우 norm의 정의에 따라 <U>weight parameter의 최댓값</U>으로 정의되는데, 이를 통해 weight parameter가 가질 수 있는 최댓값을 제한하는 형태의 loss가 된다.

$$
    \\text{Cost} = \\frac{1}{n} \\sum_{i=1}^n L(y_i, \\hat{y_i}) + \\lambda \\max(W)
$$

## Dropout
지금까지 언급했던 내용은 <U>object/cost function</U>에 추가로 줄 수 있는 <U>penalty term, regularization term</U>에 대한 내용이었으며 모두 explicit한 loss term을 더해주는 방식을 사용했다. 그러나 정규화 방법에는 직접 최적화에 사용될 object function을 정의하는 방식이 아닌 학습법에 대한 내용인 implicit regularization이 있고, 여러 방법들 중 고전적이고 가장 유명한 방법인 <U>dropout</U>에 대해서 살펴보도록 하자.   
기존 neural network가 overfitting되는 문제는 deep neural network의 representation power가 크고, 이에 따라 신경망 구조가 묘사할 수 있는 functional complexity가 높아진다는 점이었다. 실생활의 여러 task를 해결하기 위해서 neural network의 representational power가 증가하는 것은 어쩌면 당연하지만, 오히려 증가한 표현력 때문에 training data에 대해 과적합이 발생할 수 있다는 문제는 이전 게시글에서도 다뤘던 내용이었다.   
그렇기 때문에 neural network의 모든 노드를 학습 과정에서 update하지 않고, 일부 노드만 update함으로써 간접적으로 network의 구조를 단순화시키는 방법이 제시되었다. 바로 이 방법이 학습 과정에서 특정 확률($p$)에 따라 일부 노드를 turn off하는 drop-out 방식이며, 이름을 보면 알 수 있듯이 네트워크 학습 시 특정 노드를 버리는 듯한 효과를 보여준다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/216229922-6002394f-2ee7-4e5a-a4c4-3a5e8059df38.png" width="800"/>
</div>

좌측의 그림을 보게 되면 일반적인 neural network(multilayer perceptron)의 경우 모든 노드가 엣지로 연결되어, output 연산을 할 때 네트워크 전체가 관여하는 한편 우측의 그림은 일부 노드를 turn off 함으로써 <U>학습 과정</U>에서 네트워크 전체가 관여하지 않고 <U>일부 노드만 학습되게끔</U> 한다.   
여기서 중요한 점은 '학습 과정에서만' 해당 regularization이 적용된다는 것인데, 예를 들어 batch normalization과 같은 regularization에서도 training, inference 시에 해당 layer가 작동하는 방식이 달랐던 것을 기억해보면 이해하기 쉽다. 결국 overfitting을 방지하기 위한 layer modification 작업이기 때문에 training 단계에서는 랜덤한 node 추출(stochastic)을 통해 네트워크 구조를 단순화시킬 필요가 있지만, 학습이 완료된 후 <U>parameter가 고정된 후</U>에는 굳이 네트워크 구조를 단순화시킬 필요가 없기 때문이다.


# Loss function
위에서 다룬 내용은 **regularization** 방법이었다. Model의 complexity를 직접 조절하거나 loss term을 추가해주는 방식을 줌으로써 training dataset에 과적합되지 않고 generalization 효과를 높여주는 수단으로 해석할 수 있었다.   
특히 explicit regularization 방법으로 제시된 $L_1,~L_2$ loss 혹은 Max norm 방식은 기존 task에 맞는 data loss에 더해지는 형태로 보여지는데, 여기서 <U>regression</U>이나 <U>classification</U>과 같이 task에 맞는 <U>loss function</U>의 구체적인 형태를 정의하지 않고 단순히 data loss를 $\\frac{1}{n} \\sum_{i=1}^n L(y_i, \\hat{y_i})$로 표현했었다. 여기서 $n$이 의미하는 것은 training data의 갯수를 의미하고, 만약 batch 단위로 학습이 된다면 각 batch를 구성하는 sample 수를 의미한다.   
따라서 이번에는 실제로 위에서 표현한 loss term이 어떻게 표현될 수 있는지 간단하게 소개하도록 하겠다. 수식 전개를 위해 layer weights $W_k$에 의한 Neural network $f$를 각 input sample $x_i$에 대해 합성함수 꼴인

$$
    \\begin{aligned}
        f(x_i; W_k),~(i =& 1,~2,~\\cdots,~n\\text{  and  }k = 1,~2,~\\cdots,~l) \\newline
        \\text{Let }&k^{th}\\text{ perceptron's function as } f_k(\\cdot,~W_k), \\newline \\newline
        f(x_i; W_k) =& f_l \\circ f_{l-1} \\circ \\cdots \\circ f_1(x_i; W_1)
    \\end{aligned}   
$$

처럼 표현 가능하다. 위의 notation을 기억한 채로 다음 파트의 수식을 이해하면 된다.

## Classification task
Classification은 앞서 linear classifier 글에서도 주로 소개했던 대표적인 task이며 computer vision이나 NLP를 통틀어 deep learning의 가장 기본이 되는 task이기 때문에 따로 설명을 하지는 않겠다. 어떠한 modality를 지닌 dataset이 있고, dataset sample 각각은 사전에 정의된 distribution에 따라 매칭되는 label이 있다. Classification loss는 SVM(Support Vector Machine), Softmax가 있는데 이 중에서 SVM이라는 cost function에 대해 먼저 살펴보면,

$$
    L_i = \\sum_{j \\neq y_i} \\max \\left( 0, f_j - f_{y_i} + 1 \\right)
$$

score margin($\\Delta$)가 $1$인 SVM에 대해 위와 같이 표현 가능하다. Hinge loss는 구조상 margin이 되는 기준점에서 미분이 불가능하기 때문에 squared hinge loss를 사용하는 경우도 있다.

$$
    L_i = \\sum_{j \\neq y_i} \\max \\left( 0, f_j - f_{y_i} + 1 \\right)^2        
$$

물론 위와 같이 사용하게 되면 제곱으로 penalty가 들어가기 때문에 SVM loss를 사용했을 때와 결과는 달라지지만, 특정 task에서는 <U>squared hinge loss</U>를 사용하는 것이 성능 향상에 더 효과적이었다는 경우도 존재한다.   
Score function $f$의 output을 토대로 hinge loss를 사용하는 SVM 방식 대신, softmax probability를 사용(<U>normalized probability</U>)하여 확률 분포 개념으로 접근한 softmax classifier도 있다.

$$
    L_i = -\\log \\left( \\frac{e^{f_{y_i}}}{\\sum_{j} e^{f_j}} \\right)    
$$

Classification 문제는 보통 class의 개수가 합리적일 때 사용하는 것이 좋다. 그러나 NLP task와 같이 단어 수가 절대적으로 많이 필요한 경우(<U>English dictionary</U>)에는 전체 softmax를 계산하는 과정에서 정답이 되는 class가 제외하고 나머지 class가 distracting factor(방해 요소)로 작용하며, 단순히 성능을 제외하고 보아도 연산 과정이 <U>많은 cost를 차지</U>하게 된다.   
이러한 문제들을 직면한 몇 가지의 task에서 조금 다르게 변형한 classification loss도 존재한다. NLP task에서 제안된 hierarchical softmax는 단어를 하나의 tree로 구성하게 된다([참고 링크](https://arxiv.org/pdf/1310.4546.pdf)).
그렇게 되면 각 label은 tree를 따라가는 하나의 path로 대표될 수 있고, softmax classifier는 모든 label에 대해 동일한 softmax로 학습되는 형태가 아니라 left/right branch를 구분하는 식으로 학습되게 된다. 만약 전체 label에 대해 연산을 진행한다면 word 갯수 $W$ 만큼의 softmax 연산을 진행해야하지만, 이와 같은 메커니즘으로는 좌/우 중 하나의 branch만 선택하면 되므로 $\\log_2(W)$의 연산만 진행할 수 있다.   
Hierarchical softmax에 대해 조금 더 자세히 살펴보면 다음과 같다. 만약 $n(w,~j)$가 root로부터 $w$까지의 path를 구성하는 node 중에서 $j$번째 노드를 의미하며, $L(w)$는 이 path의 길이를 의미한다고 생각해보자. 따라서 $n(w,~1) = \\text{root}$이며 $n(w,~L(w)) = w$이다. 내부의 임의의 노드 $n$에 대해서 $\\text{ch}(n)$은 노드 $n$에 대한 fixed child를 의미하며 $\\left< x \\right>$는 $x$가 참이라면 $1$, 거짓이라면 $-1$의 값을 내보낸다고 생각하자. Hierarchical softmax $p(w_O \\vert w_I)$은 다음과 같이 정의된다.

$$
    p(w \\vert w_I) = \\prod_{j = 1}^{L(w) - 1} \\sigma \\left(~\\left< n(w,j+1) = \\text{ch} (n(w, j)) \\right>~ \\cdot {v_{n(w, j)}^\\prime}^\\top v_{w_I} \\right)
$$

Loss term을 보면 알 수 있듯이, 각 word는 path를 따라서 child node에 대한 softmax($\\sigma$)만 연산하는 것을 알 수 있다.

## Attribute classification
위에서 언급한 SVM, Softmax loss 모두 각 dataset $x_i$에 대해 하나의 correct answer $y_i$가 존재한다고 가정한다. 그러나 만약 class가 아니라 $y_i$가 각 attribute의 유/무를 표현하는 binary vector이고, 각 attribute가 exclusive(one-hot encoding처럼 오직 하나만 1이고 나머지는 0)인 경우가 아니라면 어떻게 해야할까? 예를 들어 인스타그램에 있는 이미지는 방대한 해시태그 중 몇몇의 hashtag로 라벨링될 수 있다. 그렇다면 각 이미지는 여러 해시태그로 mapping될 수 있다. 간단한 접근법으로는 각 attribute에 대해 binary classifier를 설계하는 것이다.

$$
    L_i = \\sum_j \\max (0, 1-y_{ij} f_j)    
$$

모든 attribute 카테고리 $j$에 대한 binary classifier loss를 더한 것과 같다. $y_{ij}$는 $i$번째 샘플이 $j$번째 attribute를 가지고 있다면 $+1$, 그렇지 않다면 $-1$의 값이 된다. $f_j$는 $j$번째 attribute에 대한 score가 될 것이고, 만약 잘못 예측된다면(실제 y_{ij}와 다른 부호를 가지게 되는 것) loss가 축적되는 형태가 된다.   
위에서 언급한 loss는 hinge loss였고, 다르게 풀어볼 수 있는 것은 binary classifier로 logistic regression classifier를 사용하는 것이다. Binary logistic regression classifier는 binary class 값으로 $0$과 $1$을 가지게 되고, class $1$에 대한 probability는 다음과 같이 구할 수 있다.

$$
    P(y = 1 \\vert x;~w,b) = \\frac{1}{1 + e^{-(w^\\top x + b)}} = \\sigma (w^\\top x + b)    
$$

Class $0$인 확률은 반대로 $1$에서 뺀 것과 같다. 따라서 해당 구조에서의 class $1$로 분류될 threshold는 $0.5$라고 할 수 있으며, 이는 $\\sigma(w^\\top x + b) > 0.5$이며 $w^\\top x +b > 0$라고 해석할 수도 있다.

$$
    L_i = -\\sum_j y_{ij} \\log (\\sigma (f_j)) + (1 - y_{ij}) \\log (1 - \\sigma (f_j))    
$$

결국 $y_ij$는 $0$ 혹은 $1$ 중 하나의 값을 가지게 되고 sigmoid function은 $\\sigma$로 하여금 <U>probability를 최대화하는 방향</U>으로 학습하게 된다. 이를 negative log likelihood 관점에서의 <U>binary cross entropy</U>로 풀어쓴 식이 위와 같으며 attribute output $f_j$에 대한 gradient는 매우 간단하게 표현할 수 있다.

$$
    \\partial L_i / \\partial f_j = \\sigma (f_j) - y_{ij}
$$

## Regression task
위에서 살펴본 classification과는 다르게 regression은 <U>real-value 값을 추정하는 문제</U>이다. 예를 들어 부동산에 올라온 매물의 특징(평수, 위치, 부대시설 등등)을 활용하여 시세(매매가)를 예측하는 문제거나, image 상에서 특정 object의 길이를 예측하는 문제가 이에 해당된다. Regression은 예시를 통해 확인할 수 있듯이 미리 정해진 class가 있거나 discrete한 attribute가 존재하지 않고 feature를 통해 연속적인 값을 예측하게 된다.   
따라서 이 task에서는 네트워크가 예측한 값과 실제 값을 비교하는 metric을 사용하게 되고, 흔히 L2 squared norm이나 L1 norm 기반으로 차이를 연산한다.

$$
    L_i = \\vert\\vert f-y_i \\vert\\vert_2^2    
$$

L2 norm은 위의 식대로 계산되며, 제곱 term은 input에 대해 monotonic operation이므로($f - y_i$가 커질수록 증가함) optimal parameter를 변화시키지 않고 간단하게 연산이 가능하다는 장점이 있다. L1 norm의 경우에는 output 차이에 대한 연산식을 다음과 같이 적용한다.

$$
    L_i = \\vert\\vert f - y_i \\vert\\vert_1 = \\sum_j \\vert f_j - (y_i)j \\vert    
$$

위와 같이 풀어쓴 이유는 gradient 연산이 L2와는 다르게 $f-y_i$의 값에 의존하지 않고 부호에 의존하기 때문이다. $j$는 norm 연산을 하는 dimension이라고 생각해보면 된다. $j$번째 dimension에 대해 sample $i$에 대한 prediction difference는 다음과 같이 표현할 수 있다.

$$
    f - y_i = (\\delta_{i1},~\\delta_{i2},~\\cdots,~\\delta_{ij})    
$$

각 input에 대한 local gradient는 앞서 설명했던 것과 같이 difference에 대한 부호와 관련된다.

$$
    \\partial L_i / \\partial f_j = sign(\\delta_{ij})
$$

## Regression 수렴이 어려운 이유
L2 loss나 L1 loss는 식에서 볼 수 있듯이 network의 output이 <U>정확한 실수값을 예측</U>해야하는 것을 알 수 있다. 그러나 softmax에서는 굳이 정확한 output을 내보내지 않더라도 probability도 normalize하는 과정을 통해 조정될 수 있기 때문에 최적화 관점에서는 <U>optimal solution에 수렴할 수 있는 확률</U>이 높다.   
또한 L2 loss(MSE loss)의 경우에는 오차에 대해 민감하다는 단점이 있는데, 앞서 언급했던 L2 regularization의 특징과 같이 이상치에 대한 robustness가 떨어진다. 이런저런 문제들 때문에 regression task에서는 <U>특정 범위 이내로 output을 안정화하기</U> 어렵다.   
만약 특정 영화 작품에 대해 대중들의 평가를 예측하는(별 1개부터 5개까지 예측하는) regression task를 생각해보자. 단순히 별 갯수를 regression으로 예측하는 것보다 5개의 class를 가진 classification task로 치환하여 풀게 되면, <U>single output에 의한 supervision</U> 뿐만 아니라 네트워크가 추출한 regression distribution을 함께 활용할 수 있다. 이는 각 class에 대한 confidence 지표로 사용될 수 있다는 장점이 있다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/216246901-ccf3d75d-37ce-4c7a-bd70-4a5aa2185b5d.png" width="1000"/>
</div>


# 마무리하며...

위에서 언급된 loss term 이외에도 graph나 tree와 같이 복잡한 구조를 예측하는 task들도 존재한다. 디테일한 부분까지 언급하게 되면 딥러닝 기본 개념에서 scope가 많이 벗어나게 되므로 최대한 관련된 부분까지만 언급하고 마무리하도록 하겠다. 이번에 알게된 것들은 regularization 방법이 있고, 해당 방법들을 활용하여 overfitting을 방지할 수 있다는 점과 동일한 task에도 다양한 형태의 loss function이 적용될 수 있다는 점이다.
`,mO=`---
title: "cs231n 내용 요약 (8) - Learning and evaluation"
category: "ai theory"
publishedAt: "2022-11-09"
thumbnail: "https://user-images.githubusercontent.com/79881119/216607435-654a064b-5582-48aa-8324-2419eac0e4e7.png"
---


# 들어가며...
이전 게시글에서 다뤘던 내용은 neural network에서 고정으로 사용될 수 있는 내용이었다. 여기서 고정으로 사용된다는 것은 학습 시에 <U>변하지 않는 것</U>을 의미한다. 예를 들어 정규화 방법으로 L1 regularization을 선택할 수도, L2 regularization을 선택할 수도 있지만 task에 따라 선택한 objective function은 불변이라는 점이다. 정규화를 제외하고도 네트워크 구조나 데이터 전처리 등등에 대해서 살펴볼 수 있었다.   
이번 글에서 다룰 내용은 학습 시 변할 수 있는 부분, 대표적으로 <U>weight parameter</U>를 학습시키는 방법이나 직접 조정해가며 찾는 <U>hyperparameter searching</U>에 대한 개념이 될 것이다.


# Learning
**Gradient check**란 <U>analytic gradient</U>(실제 미분 가능한 함수 형태에서 도함수를 정의하여 계산하는 것)과 <U>numerical gradient</U>(미분이 불가능한 함수 형태에서 input의 미소 변화 $h$에 대한 output의 변화 $f(x+h) - f(x)$의 비율을 도함수로 사용하는 것)를 서로 비교하는 것이다.

#### Use the centered formula
$$
    \\begin{aligned}
        \\frac{df(x)}{dx} =& \\frac{f(x+h) - f(x)}{h}~\\text{(bad)} \\newline
        \\frac{df(x)}{dx} =& \\frac{f(x+h) - f(x-h)}{2h}~\\text{(use instead)}
    \\end{aligned}    
$$
$h$를 아주 작은 수로 가정했을때, 일반적인 도함수 정의를 따라가는 numerical gradient는 위와 같이 계산할 수 있다. 고등학교 수학에서 배울 수 있듯이 도함수의 정의는 다음과 같이 정의되는 것을 알 수 있다.
$$
    f^\\prime (x) = \\frac{df(x)}{dx} = \\lim_{h \\rightarrow 0} \\frac{f(x+h)-f(x)}{h}    
$$
그러나 만약 $f^\\prime(x)$를 analytic하게 구하기 어려운 함수라면(너무 복잡해서 미분이 어려운 함수), 실제로 함수 형태를 미분해서 analytic한 연산 결과를 내는 것보다 함숫값의 차이를 통해 <U>도함수에 근사하는 연산값</U>을 도출할 수 있다.   
따라서 앞서 소개한 식과 같이 $h$를 아주 작은 수로 가정하여 numerical gradient를 구하게 된다. Centered formula를 사용하는 이유는 아래 그림을 참고해보도록 하자.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/216496862-ed3655a7-0910-49c5-9ed9-e900b26458d2.png" width="1200"/>
</div>
검은색으로 표시된 $f^\\prime(x)$가 실제로 구해야하는 analytic gradient 값이고, 이를 근사하기 위해 미소 단위 $h$에 대해 함숫값의 차이에 대한 비율을 통해 기울기를 계산하고자 한다. 만약 붉은색 선과 같이 $h$만큼 이동한 함숫값과의 차이를 계산하게 되면 함수의 곡률이 큰 경우(tangential line에서 급격하게 벗어나는 경우) <U>기울기 오차가 발생</U>하게 된다. 따라서 녹색 선과 같이 $f(x)$를 중앙에 두는 형태의 계산을 통해 보다 <U>실제 도함수 값</U>(접선의 기울기)을 따라가고자 하는 방식이 centered formula가 되겠다.   

#### Use relative error for comparison
위에서 설명한 내용은 보다 유사한 numerical gradient를 구하기 위한 방법이었고, 지금부터 설명할 내용은 <U>numerical gradient</U>와 <U>analytic gradient</U>를 어떻게 하면 공정하게 비교할 수 있을지에 대한 부분이다.   
만약 difference에 대한 절댓값이나, square를 측정한 뒤 이를 특정 threshold보다 크면 오차가 크다고 판단하는 경우를 생각해보자. Difference가 $10^{-4}$가 나왔더라도 두 gradient value가 $1.0$ 정도라면 이 정도의 오차는 충분히 작은 값으로 인식될 수 있지만, 만약 두 gradient value가 $10^{-5}$ 근처의 숫자이거나 더 작은 단위로 표현된다면 해당 오차는 상대적으로 매우 큰 값에 해당된다. 따라서 절대적인 값으로 threshold를 정하는 것보다는 상대적인 오차값, <U>relative error</U>를 정의하는 방식이 바람직하다.
$$
    \\frac{\\vert f_a^\\prime - f_n^\\prime \\vert}{\\max (\\vert f_a^\\prime \\vert,~\\vert f_n^\\prime \\vert)}    
$$

Gradient의 차이를 difference의 절댓값으로 정의한다면, 이 값을 실제 gradient 값의 최댓값으로 나눠주게 되면 gradient 차이로 나온 값이 <U>실제로 큰 오차에 해당되는지</U> 여부를 측정할 수 있다. 이외에도 네트워크의 깊이에 따른 차이도 존재하는데, 예를 들어 딥러닝 네트워크의 깊이가 깊어질수록 error도 점점 커지게 된다. 따라서 같은 error를 나타내더라도 layer의 갯수가 $10$개인 상황과 $1$개인 상황은 다르다.


# Single precition and double precision

이 부분에서 다룰 내용은 사실상 딥러닝과는 큰 연관이 없을 수도 있다. 그냥 단순히 오차에 대한 개념을 정리하려다 보니, 오차를 계산하는 과정에서 부동 소수점과 관련된 내용을 짚고 넘어가는게 좋을 것 같아서 가져오게 되었다.   
Single precision과 double precition은 C언어 프로그래밍에서 배우는 부분인데, 흔히 변수의 자료형을 정해줄 때 사용되는 단위인 \`\`\`float\`\`\`이나 \`\`\`double\`\`\`의 차이에 대한 내용이다.   
컴퓨터에서는 소수점 단위로 무한히 존재하는 실수를 표현하기 위해 소수점의 위치를 고정하지 않고 그 위치를 나타내는 수를 따로 적음으로써 표현하는 방식인 <U>부동소수점</U> 방법을 사용하게 된다. 고정 소수점 방식보다 더 넓은 범위를 커버할 수는 있지만 근삿값으로 표현되며, 연산 속도가 느리다는 특징이 있다. 또한 고정 소수점과 다르게 정수와 소수 부분이 명확하게 구분되진 않으나 유효 숫자의 갯수는 한정되어있다.   
초창기 컴퓨터에서는 각각 서로 다른 방식을 사용했었지만 지금은 거의 모든 컴퓨터들이 호환성을 보장하기 위해 IEEE에서 표준화된 754 형식을 사용중이다.

|부호|지수(E)|가수(mantissa)|
|:---:|:---:|:---:|
|1 bit |8 bit |23 bit(52 bit)|

사용되는 binary 자릿수인 32bit와 64bit에 따라 가수의 범위가 달라지게 된다. 32 bit를 사용한 표현이 <U>single precision</U>, 64 bit를 사용한 표현을 <U>double precision</U>이라 부른다. Error를 줄이기 위해서는 물론 더 많은 비트를 통해 표현 가능한 방식인 <U>double precision</U>을 사용하는 것이 좋다.

# Kinks in the objective
gradient에 대한 오차를 계산하는 과정에서 미분이 가능한 함수와 미분이 불가능한 함수를 구분하는 것은 중요하다. 그런 의미에서 'Kinks'란 <U>non-differentiable part</U>(미분 불가능한 부분)을 의미하며, 마치 ReLU function이나 SVM loss(hinge loss) 등등 여러 레이어 혹은 objective function에서 미분 불가능한 part가 생길 수 있다.   
예를 들어 ReLU function에서 $x = -10^{-6}$에서의 기울기를 계산한다고 생각해보자. ReLU(Rectified Linear Unit) function은 다음과 같이 정의된다.

$$
    \\text{ReLU}(x) = \\begin{cases}
        0, & \\text{if}~~x < 0 \\newline
        x, & \\text{otherwise}
    \\end{cases}    
$$

물론 $x = 0$에서 미분이 불가능하기 때문에 전체 함수의 도함수를 구하는 것은 불가능하지만, <U>미분이 가능한 각 부분</U>에 대해서 sub-gradient를 계산하면 다음과 같다.

$$
    \\frac{d}{dx} \\left( \\text{ReLU(x)} \\right) = \\begin{cases}
        0, & \\text{if}~~x < 0 \\newline
        1, & \\text{otherwise}
    \\end{cases}       
$$

그렇기 때문에 ReLU function에서 $x = -10^{-6}$에서의 기울기는 원칙적으로는(analytic) $0$이 되어야한다. 그런데 만약 analytic한 기울기를 구하지 않고 작은 $h$에 대해서 numerical gradient를 게산하는 상황을 생각해보자.

$$
    \\left. \\frac{d}{dx} \\left( \\text{ReLU(x)} \\right) \\right\\rvert_{x = -10^{-6}} \\approx \\frac{\\text{ReLU}(-10^{-6}+h) - \\text{ReLU}(-10^{-6})}{h}    
$$

만약 $h$가 $10^{-6}$보다 작다면 analytic gradient와의 오차가 0이지만, 이보다 커진다면 오차가 $h - 10^{-6}$ 만큼 발생하게 된다. Loss function을 evaluation하는 과정에서 input이 네트워크를 통과하면서 kink를 거쳤는지 거치지 않았는지 판단하는 방법은 $f(x+h)$와 $f(x-h)$를 동시에 보는 방법이다. 앞서 예시로 들은 ReLU function에서 확인하게 되면, $\\max(x, y)$와 같은 형태에서 winner(더 큰 값이 어떤 값인지)를 체크하는 과정을 확인해보면 해당 value가 kinks region을 통과했는지 확인할 수 있다는 것이다. $f(x+h)$와 $f(x-h)$를 비교했을 때 값이 달라진다면(만약 $h$가 10^{-6}보다 크다면 ReLU$(\\cdot)$의 연산이 $0$에서 바뀌게 된다), kink point를 지나쳤고, 이로 인해 numerical gradient가 정확하지 않을 것이라는 사실을 확인할 수 있다.

## Other tips for gradient checking method

그리고 데이터셋을 많이 사용하지 않는 것이 중요하다. 만약 datapoint 수가 많아지게 되면, kinks를 포함하는 loss function이 확률적으로 늘어날 수 밖에 없기 때문이다. 또한 연산 과정에서 많은 샘플에 대한 gradient checking을 하는 과정이 효율적이지 않고 느리다.   
그리고 step size인 $h$를 적절히 조절하는 것도 중요하다. 만약 step size가 너무 작다면 <U>computational limitation</U> 때문에 정확도가 많이 떨어질 수 있고, 반대로 너무 크다면 도함수의 정의와 많이 벗어나기 때문에 오차 계산의 정확도가 떨어질 수 있다. 또한 Kinks가 포함된 objective의 경우에는 kinks를 포함하는 영역이 많이 생기지 않게끔 $h$를 잘 조절하는 것이 중요하다.   
그리고 또 중요한 것은 gradient check는 <U>일반적으로 parameter의 특정 지점</U>에서(single point)에서 진행된다. 그렇기 때문에 특정 point에서는 gradient check를 했을 때 오차 범위 내에서 계산되더라도, 실제로 신경망 내부의 모든 부분에서 연산이 제대로 진행되었다는 보장은 할 수 없다(필요 조건이지, 충분 조건이 될 수 없다). 또한 random initialization을 하게되면 characteristic point(gradient 오차를 계산해야할 필수적인 부분이라고 생각하면 된다)가 아닌 지점에서 연산이 진행될 수 있고, 결국 gradient가 잘못 계산되었지만 이를 모르고 넘어갈 수도 있다. 예를 들어 support vector machine의 weight를 아주 작은 값으로 초기화한다면, 거의 모든 datapoint에 대해 zero-score를 내보낼 것이고 그렇다면 대부분의 data point에 대해서 gradient는 비슷한 양상(pattern)을 가질 것이다. 이는 gradient가 잘못 구현되었더라도 마찬가지며, 만약 어떤 score가 다른 값들보다 특정 수준 이상으로 커지게 되면 일반화되기 힘들다. 그러므로 <U>안정적인 계산을 위해서</U>는 짧은 **burn-in** time을 줌으로써 loss가 학습되기 시작한 이후에 gradient check를 하는 것이 바람직하다. 따라서 first iteration(학습 시작)부터 gradient check를 하는 것은 학습되는 parameter의 pathodology 관점에서 edge(가장자리)에 해당되기 때문에 gradient 연산이 부정확할 수도 있다.   
앞선 게시글에서 소개했던 것과 마찬가지로 data loss term에 regularization loss term을 더함으로써(<U>explicit regularization</U>) 네트워크의 overfitting과 parameter의 분산을 막았었다. 이때, data loss에 regularization loss를 $\\lambda$만큼 weight하여 더하는 형태로 loss를 구성하게 된다. 그러나 중요한 점은 regularization loss가 data loss를 넘어서게 되면, gradient 계산 시에 data에 의한 gradient보다 regularization에 대한 gradient가 더 커지게 된다. 이는 실제로 data loss gradient가 제대로 구현이 되었는지 확인하는 <U>절차상의 방해요소로 작용</U>할 수 있다. 따라서 data loss를 제대로 구현하였는지 확인하는 단계에서는 regularization loss를 빼고 체크하는 것이 좋다. Regularization term에 대한 gradient를 체크할 때에는 data loss contribution을 제외하고 체크하거나, $\\lambda$ term을 증가시켜 regularization term을 무시할 수 없을 정도로 크게 만든 상태에서 진행한다.   
또한 dropout이나 augmentation과 같은 implicit regularization도 gradient check 단계에서는 모두 배제해야한다. 말하다보니 지금까지 소개한 모든 정규화 방법들을 언급한 것 같은데, 아무튼 deterministic하지 않고 stochastic한 모든 형태의 작업은 연산의 정확도를 저하시키기 때문에 문제가 될 수 있다. 따라서 random seed를 고정한 채로 $f(x+h)$, $f(x-h)$를 계산하여 변동 가능성을 아예 없애거나 해당 정규화 방법들을 모두 제거한 상태에서 연산을 진행하는 것이 바람직하다.   
보통 네트워크를 구성하는 parameter의 수는 수백만의 size를 가지게 된다. 모든 dimension에 대해 gradient check를 진행하게 되면 연산이 오래 걸리게 되므로 실용적이지 못하다. 따라서 보통 대용량의 모델에 대한 gradient check를 진행하는 과정은 일부 parameter를 기준으로 삼으며, 나머지 parameter에 대한 gradient는 모두 정확하다고 가정한다. 여기서 조심해야할 부분은 gradient check를 각각의 parameter에 대해 진행해야한다는 점이다. 일부 parameter에 대해 gradient를 체크하는 것이 효율적이지만, <U>random하게 파라미터를 뽑아서</U> gradient check를 진행하는 것이 아니라 전체 네트워크에 대한 test를 진행하는 것과 같이 <U>고려해서 결정할 사항</U>이라는 것이다.


# Sanity check
일반적으로 optimization 과정을 시작하기 전, <U>네트워크가 제대로 구성되었는지</U> 확인할 수 있는 방법 중 하나가 바로 <U>sanity check</U>이다. 예를 들어 CIFAR-10 dataset에 softmax classifier를 달아놓은 상태라면 cross-entropy loss를 사용하는데, 만약 임의로 모든 weight를 초기화한 상태라면 각 class로 예측될 확률이 $0.1(10\\%)$에 수렴하게 된다. 그렇기 때문에 학습 시작 시 loss가 대략 $-\\log(0.1) = 2.302$이 되어야 한다. Softmax classfier가 아닌 Weston Watkins SVM(margin이 $1$인 SVM을 생각하면 됨)에서는 초기에 모든 margin들이 violated된 상태(모든 score가 거의 대부분 $0$에 가까운 값을 내보내는 상황)이므로, 초기 상태에서의 loss는 $9$가 나올 것이고, 만약 이 값이 나오지 않다면 초기화가 잘못되었을 가능성이 있다. 그리고 data loss 말고 regularization loss에 대해서도, regularization strength($\\lambda$)를 증가하면 전체 loss가 증가하는 식으로 sanity check가 가능하다.   
그리고 sanity check과 더불어 딥러닝 연구를 하면서 필요한 팁 중 하나인데, 바로 전체 데이터셋에 대해 바로 학습을 진행하는 것이 아니라 작은 dataset에 대해서 학습을 진행해보고, 제대로 loss가 감소하는지 확인해보는 것이다. 만약 작은 dataset에 대해서도 network가 overfitting되지 않고 학습에 문제가 생긴다면, 코드 상이나 구현 단계에서 문제가 있을 수 있음을 의미한다.


# Learning process
신경망 구조를 설계하고 위와 같은 sanity check가 모두 완료된 상태에서, 학습 과정에서의 monitoring이 필요하다. Setting이 완료된 후에도 학습 과정을 모니터링해야하는 이유는 learning rate나 optimizer 설정, 네트워크 레이어 세부 디테일 조정 등 <U>hyperparameter 최적화가 필요</U>하기 때문이다.

## Loss function
가장 먼저 확인할 것은 loss function(objective function)의 결과값이다. 모든 loss는 감소하는 방향으로 최적화를 진행하는데, 이 과정에서 각각의 indivisual batch가 네트워크를 통과하는(forward process) 과정에서 측정이 된다. 만약 <U>learning rate를 적절하게 조절</U>하지 못했다면 학습 속도가 너무 느리거나 loss가 발산하는 문제가 발생하게 된다(아래 그래프 참고).

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/216607435-654a064b-5582-48aa-8324-2419eac0e4e7.png" width="360"/>
    <img src="https://user-images.githubusercontent.com/79881119/216608230-502c31a9-9998-4c38-9acd-18bdf5717806.png" width="400"/>
</div>

Learning rate란 gradient descent 과정에서 loss에 대해 연산된 각 layer에서의 local gradient를 parameter에 어느 비율로 적용할 지에 대한 척도가 된다. Learning rate가 너무 크다면 loss가 발산하거나 일정 수준의 loss에서 수렴하게 되고, 그렇다고 해서 너무 작게 설정하면 학습 속도가 크게 저하되며 local minima에 빠질 위험이 있다. 모델이나 학습하고자 하는 데이터셋 및 task에 맞게 <U>learning rate를 적절하게 설정</U>하게 되면, loss가 잘 줄어드는 것을 확인할 수 있다. 우측 그래프는 각 epoch에 대해 모든 batch에 대한 loss를 그린 그래프와 같은데, 각 epoch에 대해 모든 batch에 따른 loss를 보게 되면 감소하는 경향성을 확인할 수 있다. 흔히 wiggle(loss가 지나치게 진동하는 형태)되는 그래프는 <U>batch size를 키우면</U> noise를 줄일 수 있다.

## Train/Validation accuracy
그러나 loss가 줄어든다고 무조건 좋은 것은 아니다. 만약 <U>training dataset</U>에 계속 overfitting되고 있는 상황이라면 <U>training loss가 감소하는 것</U>만으로는 이를 확인할 수 없기 때문이다. 따라서 dataset을 단순히 training dataset으로 모두 사용하는 것보다, 실제로 학습된 네트워크가 inference(테스트) 시에도 generalization이 잘될 수 있는지 확인하기 위해 validation set을 따로 구성하게 된다. 따라서 딥러닝에서 사용되는 데이터셋은 크게 train/validation 그리고 test dataset으로 구분할 수 있다.

1. Training dataset : 학습 과정에서 쓰이는 학습 데이터로, weight parameter에 gradient descent를 적용한다.
2. Validation dataset : 학습 과정에서 쓰이는 테스트용 데이터로, weight parameter에 gradient descent를 적용하지 않는다.
3. Test dataset : 학습이 끝난 후 모델에 들어가는 실생활 데이터(test data), inference라고 불리는 부분이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/216610889-c7ee79a2-a465-4eed-940f-cf040efbd735.png" width="400"/>
</div>

위와 같이 그래프 상에서 training dataset에 대한 training accuracy와 validation dataset에 대한 validation accuracy로 구분 가능하다. 실제로 네트워크를 학습한 뒤에 inference 단계에서 사용하는 test dataset에는 ground truth가 없는 경우가 많기 때문에 정확도를 직접적으로 측정이 불가능하다. 따라서 training dataset과 validation dataset의 정확도를 서로 비교함으로써, 실제로 training data를 통한 학습에 의해 training dataset을 예측하는 performance를 올린 만큼, validation dataset에도 비슷한 성능이 나오는 것을 확인할 수 있고, 이를 통해 <U>학습된 네트워크의 일반화 성능을 측정</U>할 수 있다.


# Parameter updates
지금까지 공부했던 loss function 혹은 objective function, 그리고 backpropagation에 대한 정의와 방법에 대해 배우고 gradient 계산법은 모두 <U>weight parameter를 학습하기 위함</U>이었다. 그렇다면 실제로 parameter를 업데이트하는 여러 방법에 대해서 알아보고자 한다. 지금부터 살펴볼 내용은 pytorch에서 \`\`\`optimizer\`\`\`의 개념에 해당되고, 딥러닝 및 신경망 학습의 근간이 되는 optimization 방법인 gradient descent의 여러 변형에 대해서 살펴볼 것이다.

## Vanila update
우선 가장 쉽게 생각할 수 있는 방법은 단순히 각 data point에서의 gradient를 계산한 뒤, gradient의 반대 방향으로 일정 learning rate만큼 parameter를 업데이트하는 것이다. 어떤 함수에서의 gradient는 특정 point에서 <U>해당 함수의 함숫값을 가장 크게 증가시킬 수 있는 방향</U>을 의미하므로, 이에 반대되는 방향은 곧 <U>함숫값을 가장 크게 감소시킬 수 있는 방향</U>을 의미한다. 

\`\`\`python
# Vanilla update
x -= learning_rate * dx
\`\`\`

## Momentum update
위의 optimization 방법은 convex optimization이라면 항상 global optima에 도달할 수 있지만, 만약 convex optimization이 보장되지 않는다면 문제가 발생한다. 다음과 같은 그림을 보자. 

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/216743832-5341e9fc-4aad-4479-b84d-e811fe9925eb.png" width="600"/>
</div>

Convex 함수의 정의를 나타낸 그림이다. Convex function을 이해하기 위해서는 convex set 그리고 convex hull에 대한 개념도 필요하지만 간단하게 풀어서 설명하면 convex function이란 <U>정의역 전체에서 아래로 볼록한 구조</U>를 가지는 function이라고 할 수 있다. 따라서 모든 실수 $0 \\leq \\theta \\leq 1$에 대해서,

$$
    f(\\theta x_1 + (1-\\theta)x_2) \\leq \\theta f(x_1) + (1-\\theta) f(x_2)    
$$

를 만족하는 것이 convex function이다. 다차원 함수에서는 함수의 <U>hessian이 positive definite</U>인 경우를 의미한다. Convex 함수가 최적화에 있어 유리하게 가져갈 수 있는 장점 중 하나는 **오직 하나의**(유일한) global optima만 존재한다는 것이다.   
만약 어떤 점 $x^\\ast$에서 함숫값이 최소라고 생각해보자. Inner point에서의 SOSC(second-order sufficient condition)은 $\\left. \\nabla_x f(x) \\right \\rvert_{x = x^\\ast} = 0$ 이며  $\\left. \\nabla_x^2 f(x) \\right \\rvert_{x = x^\\ast}>0$이다. 만약 $x^\\ast$ 이외에 함숫값이 최소가 되는 점 $\\hat{x}$이 있다고 생각해보자. 'Global optima'가 유일하다는 가정을 뒤집은 것이다. 이러한 증명법을 귀류법이라고 한다. 그럴 경우 convex function의 정의에 의해 다음과 같이 풀어쓸 수 있다.

$$
    \\begin{aligned}
        \\text{For }\\forall x& \\in \\left(0,~1 \\right), \\newline
        f(\\theta x^\\ast + (1-\\theta)\\hat{x}) &\\leq \\theta f(x^\\ast) + (1-\\theta) f(\\hat{x})
    \\end{aligned}
$$

$f(x^\\ast)$ 그리고 $f(\\hat{x})$는 모두 global optima기 때문에 함수의 최솟값인 $m$이라는 값을 가진다고 생각하면, 위의 식은 다음과 같이 정리된다.

$$
    f(\\theta x^\\ast + (1-\\theta)\\hat{x}) \\leq m
$$

따라서 $x^\\ast,~\\hat{x}$ 경로상의 모든 함숫값들은 $m$보다 작다는 결론이 나오게 되고, 이는 $x^\\ast$와 $\\hat{x}$가 <U>global optima point라는 조건에 모순</U>된다.   
길게 convex function일 때의 최저점이 유일하다는 증명을 한 이유는 바로 일반적인 loss function은 <U>단순한 convex function으로 표현할 수 없기 때문</U>이다. 그렇다면 loss function으로 최적화를 하는 과정에서 gradient가 $0$인 local optima도 무수히 많이 존재하게 되고, 결국 단순히 gradient descent method로는 수렴이 어려워지게 된다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/216745607-2f913146-3c1c-4612-a6aa-1a3d24fe6da5.png" width="600"/>
</div>

그렇기 때문에 gradient 정보만 사용하는 방식보다는, local minimum 근처에서 원래의 관성대로 계속 최적화가 가능하게끔 해주는 방식인 momentum update를 생각해볼 수 있다. 이는<U>중력에 의해 계속 굴러가는 쇠구슬</U>을 생각해보면 이해하기 쉽다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/216745776-2c2338ad-de47-4488-9734-31e23768ec15.png" width="600"/>
</div>

Gradient의 역방향으로 내려오는 최적화 방식은, 산 꼭대기에서부터 굴러 내려오는 쇠구슬에 비유할 수 있다. 다만 중력이 존재하는 현실 세계와는 다르게 단순히 gradient descent 방식의 최적화는 다음과 같이 gradient가 역전되는 부분에서 산등성이를 넘어가지 못하는 문제가 발생한다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/216745932-a16c47be-a637-49f7-a356-e392f584f8ea.png" width="600"/>
</div>
그러나 만약 내려가는 방향으로의 속도가 유지된다는 가정 하에, local optima point의 깊이가 너무 깊지 않다면 momentum을 주는 형태로 산등성이를 넘어가게끔 해줄 수 있다.
<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/216746022-f49753a4-3b3e-48c1-8ba2-fd88b63f6983.png" width="600"/>
</div>

이를 코드로 표현하면 다음과 같다.

\`\`\`python
v = mu * v - learning_rate * dx
x += v
\`\`\`

$v$(velocity)는 현재 속도를 기준으로 gradient 역방향으로 추가된 힘을 받는다. 이미 내려가고 있는 방향으로 또다시 gradient가 붙게 되면 속도가 증가하며, 반대의 경우 속도가 감소하는 형태가 된다. $\\mu$는 <U>이전 batch</U>에서의 velocity가 <U>다음 batch</U>의 gradient descent에 끼치는 영향력을 의미한다. 일반적으로 안정적인 학습을 위해 $\\mu$로는 큰 값을 사용하며, $0.9$를 주로 사용한다.

## Nesterov momentum
앞선 momentum update 방식은 현재 위치를 기준으로 계산된 gradient를 momentum에 더해주고, 이렇게 구한 velocity로 <U>weight parameter를 업데이트</U>하게 된다.
<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/216746239-e39657ca-316c-4b0f-8958-ec692e0b56e9.png" width="900"/>
</div>

이와는 다르게 momentum에 대한 이동을 먼저 진행한 후에 gradient step을 진행하는 방식인 <U>nesterov momentum update</U>가 있다. Momentum term에 대해 먼저 이동한 뒤의 gradient를 연산한다는 관점에서 <U>look ahead</U> 방식이라 부른다.

\`\`\`python
x_ahead = x + mu * v
v = mu * v - learning_rate * dx_ahead
x += v
\`\`\`

하지만 이 방식은 기존 velocity에 따라 이동된 위치에서의 gradient를 새롭게 계산해야한다는 점에서causality 문제가 발생한다. 이전 방식과 같이 현재 위치에서의 gradient를 기준으로 하려면 다음과 같이 식을 수정할 수 있다.

\`\`\`python
v_prev = v # back this up
v = mu * v - learning_rate * dx # velocity update stays the same
x += -mu * v_prev + (1 + mu) * v # position update changes form
\`\`\`


# Annealing learning rate
최적화가 진행될수록 동일한 learning rate를 적용하는 것은 바람직하지 않을 수도 있다. 초반에는 최저점으로부터 거리가 멀기 때문에 빠른 최적화를 위해 어느 정도 step size를 키워도 상관없지만, optimal solution 근처에서는 loss가 진동하면서 최적화가 안될 수도 있기 때문이다. 따라서 learning rate를 annealing(줄여주는) 방법을 사용하게 된다. 이 내용은 pytorch 코드에서 \`\`\`scheduler\`\`\`에 해당되는 내용이다.

## Step decay
Training set 전체에 대해 최적화 과정을 거치는 것을 '1 epoch'이라 부르는데, 몇몇 epoch마다 learning rate를 줄여주는 방식을 step decay라 부른다. 예를 들어 step size가 $5$이고 factor가 $0.1$이라면 5 epoch마다 learning rate가 $1/10$로 감소하게 된다.

## Exponential decay
$$
    \\alpha = \\alpha_0 e^{-kt}    
$$
위와 같은 식으로 scehduling되는 방식이다. 학습이 시작될 당시의 learning rate를 $\\alpha_0$라고 했을 때 hyperparameter value $k$에 따라 $t$번째 epoch에서의 learning rate가 exponentially decaying하는 형태가 된다. 일반적으로 $k$를 직접 hyperparameter로 정하지 않고 감소 비율인 $e^{-k}$를 정하게 된다. 앞선 예시처럼 만약 factor가 $0.1$이라면 1 epoch마다 learning rate가 $1/10$로 감소하게 된다.

## $1/t$ decay
$$
    \\alpha = \\frac{\\alpha_0}{1+kt}
$$
exponential 그래프와 더불어 대표적인 반비례 그래프(점근선이 $y = 0$인)인 $y = \\frac{1}{x}$ 형태를 사용한 scheduling 방식이다.

위에서 소개한 여러 learning rate scheduling 방식 이외에도 overfitting을 방지하기 위한 loss term이나 다양한 스케쥴러가 각 task에 맞춰 사용된다.


# Second-order methods
최적화 이론을 공부하게 되면 optimization 방법에 단순히 gradient based method만 있는 것은 아니다. 사실상 gradient의 역방향으로 일정 step 만큼 이동하는 것은 first-order taylor series에 근사하는 방법 중 하나이다. Input에 $x$에 대해 continuous하면서 $n$차 미분이 가능한 함수를 $f \\in \\mathcal{C}^n$  이라 표현 가능하다. 이때 임의의 $x$에 대해 $f(x)$는 이미 알고있는 함숫값 $f(a)$를 통해 다음과 같이 근사할 수 있다.

$$
    f(x) = f(a) + \\frac{(x-a)}{1!}f^{(1)}(a) + \\frac{(x-a)^2}{2!}f^{(2)}(a) + \\cdots + \\frac{(x-a)^n}{n!} f^{(n)}(a) + o((x-a)^n)    
$$

여기서 $o(g(x))$라는 notation은 다음과 같은 의미를 가진다고 생각하면 된다.

$$
    \\lim_{x \\rightarrow 0,~x \\in \\Omega} \\frac{\\vert\\vert f(x) \\vert\\vert}{\\vert g(x) \\vert} = 0
$$
이 상황에서 만약 $f(x)$를 1차 미분에 근사하면 다음과 같이 정리할 수 있다.

$$
    f(x) \\approx = f(a) + (x-a)f^\\prime (x)    
$$
그러나 선형으로 근사하는 경우, 해당 함숫값을 중심으로 최저점을 찾을 수 없다는 문제가 있기 때문에 step size($\\alpha$)를 설정하는 기준을 명확하게 나타낼 수 없다. 다음과 같은 그림을 보면,

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/216804235-9c0223e0-c692-4ccc-ae26-71ea763c1bfc.png" width="400"/>
</div>

특정 지점에서 gradient를 계산하면 감소하는 방향을 찾을 수는 있으나 linear function에는 최솟값이 정의되지 않기 때문에 <U>step size를 찾을 수 없다</U>. 만약 gradient 대신 hessian(2차 미분)의 값을 안다면 $f(x)$를 quadratic function에 근사시킬 수 있기 때문에 보다 효율적인 최적화가 가능할 수 있다. 이러한 방법을 Newton's method라고 부르며, 다음과 같은 식을 통해 최적화가 가능하다.

$$
    x^{(k+1)} = x^{k} - {\\Delta f(x)}^{-1} \\nabla f(x)
$$

식에서 $\\Delta$는 2차 gradient($\\nabla^2$)을 의미한다. 해당 식이 구해질 수 있는 이유는 테일러 2차 급수까지 approximation된 $f(x)$를 미분했을 때 gradient가 $0$인 값을 찾은 것이다. 하지만 이 방법엔 크게 두 가지의 문제점이 존재한다.   
첫번째 이유는 <U>연산이 어렵다는 것</U>이다. 다차원 함수 $f(x),~\\mathbb{R^n} \\rightarrow \\mathbb{R}$에 대한 hessian은 다음과 같다.

$$
    \\nabla f(x) = \\begin{bmatrix}
        \\frac{\\partial f}{\\partial x_1}(x) \\newline
        \\frac{\\partial f}{\\partial x_2}(x) \\newline
        \\vdots \\newline
        \\frac{\\partial f}{\\partial x_n}(x)
    \\end{bmatrix}
$$

$$
    \\Delta f(x) = \\nabla \\left( \\nabla f(x) \\right) = \\begin{bmatrix}
        \\frac{\\partial^2 f}{\\partial x_1^2}(x) & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2}(x) & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n}(x) \\newline
        \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1}(x) & \\frac{\\partial^2 f}{\\partial x_2^2}(x) & \\ddots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n}(x) \\newline
        \\vdots & \\vdots & \\ddots & \\vdots \\newline
        \\frac{\\partial^2 f}{\\partial x_n \\partial x_1}(x) & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2}(x) & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}(x)
    \\end{bmatrix}    
$$

일반적으로 딥러닝에서 근사하고자 하는 $f(x)$의 경우, input 차원과 output 차원이 이보다 훨씬 커질 수 있기 때문에 문제가 발생한다. 예를 들어 $1024 \\times 1024$의 이미지를 처리하는 과정이라 생각하면 input dimension이 이미 million 단위가 되기 때문에 hessian 연산에 필요한 matrix의 크기는 $10^{12}$로 급증하게 된다.
두번째 문제는 hessian으로 근사한 quadratic function의 global optimal point가 원래 함수의 global optimal point와 다른 방향이 될 수 있다는 것이다. 앞서 본 예시에서는 1차 미분에 대해 approximated된 함수에 대해서 단순히 역방향으로 일정 step size만큼 이동하면 optimal point에 가까워지는 것을 보장할 수 있었다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/216805282-a44eef63-fba3-4622-94f2-260fc4951ad3.png" width="400"/>
</div>

그러나 2차 미분에 대한 approximation된 함수의 optimal value는 극솟값이 아닌 극댓값이 될 수도 있으며, 만약 초기화가 제대로 진행되지 않은 data point에서 학습이 진행되면 <U>오히려 loss가 발산</U>하는 문제가 발생할 수 있다. 이러한 문제점들 때문에 딥러닝에서는 $n$차원 이상의 taylor approximation에 대한 optimization 방법이 아닌 gradient descent method를 사용하게 되었다.


# Various methods of optimization

단순히 mini-batch를 사용한 stochastic gradient descent 방식을 사용하는 것은 일반적인 모든 네트워크의 parameter 학습에서 활용될 수 있는 정통법이지만 비효율적인 경우가 많다. 보통 간단한 task인 image classification이나 regression의 경우 loss function의 구조가 크게 복잡하지 않아 최적화 과정이 복잡하지 않지만 modality가 특수하거나 generative network 학습과 같이 parameter/hyperparameter 최적화가 어려운 경우, 혹은 여러 modality가 함께 최적화가 되는 경우(multimodality) 단순히 SGD 방식을 사용하게 되면 빠른 최적화가 불가능할 뿐만 아니라, loss에 여러 regularization term을 더하는 등 ablation을 활발하게 진행할 수 없게 된다. 길게 설명했지만 정리해서 한 마디로 풀어쓰자면 <U>robust한 최적화가 힘들다는 것</U>이다.


# Per-parameter adaptive learning rate methods
따라서 딥러닝 연구의 한 갈래로서 optimizer 연구도 함께 진행되기 시작했다. 기존 방식에서 parameter가 업데이트되는 형태를 보면 output에 대한 gradient 계산 후 해당 방향의 반대쪽으로 learning rate만큼 이동하는 식으로 최적화하게 된다. 그나마 local minimal point에 수렴하는 것을 방지하기 위해 momentum을 도입하지만, 이는 근본적으로 최적화 방법을 바꾸는 것은 아니다. Learning rate을 모든 파라미터에 대해 따로 적용하는 것은 최적의 hyperparameter를 찾아야하기 때문에 어렵기도 하고 <U>효율적인 방법이라고 볼 수 없다</U>. 그렇기에 각 parameter에 대해 어떤 식으로 gradient를 적용하면 좋을지에 대한 방법으로 다음과 같은 optimizer들이 제안되었다. 

## Adagrad

\`\`\`python
# Assume the gradient dx and parameter vector x
cache += dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)
\`\`\`

Input data point인 $x$에 대해 연산된 gradient를 $dx$라고 하자. $x$라고 표현된 data point는 사실상 어떠한 parameter value에 대해 연산된 결과로, parameter vector라고 표현하도록 하겠다. Adagrad는 이름에서 알 수 있듯이 adaptive한 gradient를 적용하자는 의미로, backpropagation 연산 과정에서 \`\`\`cache\`\`\`에 계산된 gradient인 $dx$를 지속해서 더해간다. 이렇게 저장된 \`\`\`cache\`\`\`를 gradient update 단계에서 normalize하게 되고, zero-division을 방지하기 위한 $\\epsilon$ 값과 함께 square root value가 더해져서 gradient인 $dx$를 나누게 된다.   
이렇게 정규화를 진행하게 되면 학습 초기 단계에 해당된다고 볼 수 있는 higher gradient(높은 gradient)에서는 effective learning rate이 줄어들 수 있고, small gradient에서는 상대적으로 effective learning rate이 증가하게 된다. 하지만 이러한 방식의 gradient를 적용한다면 어느 정도 이후에는 학습이 멈춰버리는 문제가 발생할 수 있다.

## RMSprop

\`\`\`python
cache = decay_rate * cache + (1 - decay_rate) * dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)
\`\`\`

RMSprop는 \`\`\`decay rate\`\`\`을 통해 이전 gradient의 영향력을 줄여가며 normalize를 하게 된다. 앞선 방법에 비해 learning rate을 monotonic하게 감소시키지 않기 때문에 학습이 초기에 종료되는 문제를 해결할 수 있다.

## Adam
\`\`\`python
m = beta1*m + (1-beta1)*dx
v = beta2*v + (1-beta2)*(dx**2)
x += - learning_rate * m / (np.sqrt(v) + eps)
\`\`\`

RMSprop 개념에 momentum을 추가한 것이 Adam에 해당된다. 위의 코드와 같이 동작하는데, \`\`\`m\`\`\`은 momentum과 관련된 지표이고 \`\`\`v\`\`\`는 위에서 본 cache 역할이라고 할 수 있다. Optimizer 중에서 가장 일반적으로 많이 사용되는 Adam의 경우 \`\`\`beta1 = 0.9\`\`\` 그리고 \`\`\`beta2 = 0.999\`\`\`를 사용하는 것이 default이다. 그런데 Adam을 사용하는 것보다 일부 네트워크 학습 시에는 SGD에 nesterov momentum을 적용하는 것이 효과적인 경우도 있다.   
Adam update 과정에는 <U>bias correction mechanism</U>도 있는데, bias 문제란 처음 몇 step동안 vector \`\`\`m\`\`\` 그리고 \`\`\`v\`\`\`가 초기화된 상태에서 $0$에 biasing되는 문제를 말한다. 이를 해결하기 위해 bias correction이 적용된 코드는 다음과 같다.

\`\`\`python
# t is your iteration counter going from 1 to infinity
m = beta1*m + (1-beta1)*dx
mt = m / (1-beta1**t)
v = beta2*v + (1-beta2)*(dx**2)
vt = v / (1-beta2**t)
x += - learning_rate * mt / (np.sqrt(vt) + eps)
\`\`\`

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/216900961-de163bb9-0be9-4b20-bab8-8bdec3a5fe28.gif" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/216900968-5761f3a0-6acd-47d1-9ca9-57d2a1e5a247.gif" width="400"/>
</div>


# Hyperparameter optimization

Parameter의 경우 학습 과정에서 loss fuction을 최적화하면서 chain rule에 따라 업데이트된다. 그리고 parameter를 효율적으로 최적화하는 것을 연구한 여러 optimizer 중 <U>Adam</U>이 가장 보편적으로 사용된다는 사실을 언급하고 넘어왔다. 그러나 hyper-parameter의 경우 학습하는 과정에서 최적화될 수 없기 때문에 manually 조절해야된다. 딥러닝에서 주로 사용되는, 그리고 정의되는 hyperparameter에는 다음과 같은 종류가 있다.

1. Initial learning rate
2. Learning rate policy
3. Regularization strength(loss penalty strength)

그러나 task마다 hyperparameter의 sensitivity가 모두 다르고, 하나하나 모두 searching하며 계산하기에는 무리가 있기 때문에 효율적인 방법을 통해 찾아가는 것이 중요하다.

## Implementation
큰 neural network는 training에 오랜 시간이 걸리기 때문에 hyperparameter 조정을 하는 것 자체가 이미 <U>며칠 혹은 몇 주</U>가 걸릴 수 있는 고된 작업이다. 지금부터 소개할 방법은 코드를 디자인하는 방법이나 network에 따라 모두 달라지기 때문에 모든 경우에 대해 <U>일반적으로 적용될 수 있는 방법은 아니라는 점</U>을 짚고 넘어가고 싶다.   
가장 대표적인 방법으로는 \`\`\`worker\`\`\`라는 training method를 가지고, 지속적으로 hyperparameter를 샘플링하고 optimization을 거치는 과정을 사용한다. 이런 방식에서는 worker는 각 epoch마다 validation performance를 체크하게 되며, 가장 최근(last epoch에서의)의 model checkpoint를 저장하거나 가장 좋은 성능을 보인 네트워크(loss, accuracy 등등)을 저장한다. 보통 pytorch를 사용하는 사람들은 \`\`\`torch.save()\`\`\` 메소드를 사용하여 \`\`\`.pth\`\`\` 혹은 \`\`\`.pt\`\`\` 파일로 저장하게 된다.

\`\`\`python
best_loss = 1e9
for epoch in epochs:
    for iteration in training_dataloader:
    # Training dataset을 통한 네트워크 파라미터 최적화
    # 1 epoch training이 끝나면 validation을 진행한다
    avg_loss = 0.0
    with torch.no_grad():
        for iteration in validation_dataloader:
        # Validation dataset을 통한 학습된 network의 평균 loss 계산
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save(model.state_dict(), "best_model.pt")
\`\`\`

예시를 들게 되면 위의 코드와 같다. Pytorch를 기준으로 작성하였고, 네트워크를 저장할 때의 기준은 Validation dataset에 대해 <U>loss function의 값</U>이 가장 작을 때가 된다. Task 마다의 차이는 있지만 일반적으로 가장 좋은 성능 metric을 보이는 네트워크 파라미터를 저장하는 경우가 대부분이고, 경우에 따라 가장 최근의 네트워크 파라미터(last epoch)을 저장하는 경우도 있다. $K$-nearest neighborhood 방식을 기억할지 모르겠지만, KNN에서의 hyperparameter에 해당하는 $k$를 결정하기 위해 training dataset과 validation dataset을 바꿔가며 평균값을 측정하였다. 사실 딥러닝에서는 이러한 방식이 불가능하다. 왜냐하면 instance(학습 데이터 자체)를 inference의 근거로 두는 instance based learning과는 다르게 한번 training sample이 네트워크 학습에 관여하게 되면 validation sample(학습에 사용되지 않고, 일반화 성능을 측정하기 위한 데이터셋)의 의미를 잃기 때문이다. 따라서 그 수만 충분하다면 deep learning algorithm에서는 single validation dataset만을 성능 평가에 사용하게 된다.

## Hyperparameter ranges
Hyperparameter를 찾는 과정은 parameter 최적화와는 다르게 objective function이 없기 때문에 <U>목적이 되는 value가 없다</U>. 결국 hyperparameter를 찾기 위해서는 임의의 feasible set(최적의 해를 찾기 위한 임의의 집합이라고 보면 된다)를 가정해야하는데, 모든 실수값을 랜덤하게 대입해볼 수 없기 때문에 다음과 같은 방법을 사용한다.

\`\`\`python
learning rate = 10 ** uniform(-6, 1)
\`\`\`

우선 learning rate의 경우, network optimization 과정에서 gradient의 역방향으로 얼마나 각 parameter를 업데이트할 지 결정하는 값으로, loss function을 효율적으로 감소시키기 위한 값을 찾는 것이 주된 목적이 된다.   
그럴 때 사용하는 방법이 바로 위와 같이 log scale을 사용한 searching 과정이다. 대부분 최적의 learning rate는 위에서 보는 바와 같이 $10^{-6} \\sim 10^1$ 사이에서 찾을 수 있다. 실제로 아무 dataset에 대해서 deep learning code를 돌려보게 되면 log scale로 조절해야 <U>유의미한 차이를 볼 수 있는 것</U>을 확인할 수 있다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/216958179-cb1cafd5-f9ca-401e-8b08-01c7d5d32069.png" width="400"/>
</div>

Grid search(정해진 간격만큼 떨어진 value를 테스트해보는 것)의 한 종류라고 볼 수 있는 log search와는 다르게 중요한 hyperparameter는 random search를 하는 것이 더 효과적이라는 연구도 있다([참고 링크](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)). 보통 hyperparameter를 건드릴 때, 특정 hyperparameter가 다른 것보다 훨씬 중요한 경우가 있는데, 주요 hyperparameter를 찾는 과정에서 랜덤으로 찾는 과정이 최적의 값을 찾는 상황에서 더 효율적이라는 것이다.   
그런데 무작정 랜덤하게 찾는게 좋다고 해서 정말로 임의의 값을 넣으면 안되고, 본인이 실험하면서 <U>feasibility를 볼 수 있는 영역 내에서 서칭</U>하는 것이 가장 효율적이라고 할 수 있다. 바로 아래에서 바로 설명할 내용이다.

## Careful with best values on border
Searching하고 있는 range가 별로일 수 있다. 예를 들어,

\`\`\`python
learning_rate = 10 ** uniform(-6, 1)
\`\`\`

이처럼 특정 hyperparameter를 찾는 범위를 설정했을 때, 각 value에 대해 loss value 그래프가 다음과 같다고 생각해보면

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/216960536-05148833-1de5-4c49-b14d-85aee3bd0c84.png" width="400"/>
</div>

단순히 실험 결과를 토대로 $10^{-6}$이 가장 좋은 performance(가장 작은 loss 값)을 보여주기 때문에 최적의 값을 찾았다고 결론을 내릴 수도 있지만, 실제로는 optimal point가 아닌 경우이다. 경향성이 위와 같이 단순한 monotonic function으로 나오지는 않지만, 본인이 설정한 searching area가 정말로 hyperparameter의 searching space 전반을 커버할 수 있는지 확인해야한다.

## Stage search from coarse to fine
<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/216961512-d9dec35b-d4e2-4338-b7f7-1dca4f49649f.png" width="400"/>
</div>
만약 당신이 친구와 up-down 게임을 진행한다고 생각해보자. 상대방은 $0$보다 크고 $100$보다 작거나 같은 임의의 자연수를 생각해낸다. <U>독심술을 가진 사람이 아니라면</U>, 처음부터 $2$나 $99$와 같은 boundary 숫자를 부르는 사람은 없을 것이다(물론 친구가 $1$을 선택할 수도 있긴하다).    
이처럼 우리는 최적의 값을 모를 경우, 해당 value가 포함될 수 있는 영역을 찾기 위해 coarse(듬성듬성)한 영역부터 시작해서 fine(빽빽한)한 영역을 찾기 시작한다. 그렇기 때문에 $50$을 먼저 부르고 상대방이 'up'을 외쳤다면 $75$를 외치는 과정을 통해 searching area를 $(0,~100)$에서 $(51, 100)$으로 줄이는 과정을 거치게 된다. 결국 hyperparameter를 찾는 과정도 이와 똑같다. 적당한 영역에서 최적의 값을 보이는 value를 찾았다면, <U>해당 영역 내에서 조금씩 더 좁혀나가면서</U> searching하는 것이다.
<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/216963369-46887907-90c8-4f6b-aca5-b7b3e3d8e1a1.png" width="400"/>
</div>
그리고 hyperparameter를 튜닝하는 과정은 굳이 network를 dataset 전체에 대해 학습하지 않아도 되므로 epoch를 작게 주거나 training dataset의 일부만 활용해서 <U>feasibility만 확인하는 것</U>이 좋다.

## Bayesian hyperparameter optimization
랜덤하게 찾는 위의 과정과는 다르게 hyperparameter를 제대로 찾는 알고리즘이 지속적으로 연구되었다. 주된 아이디어는 exploration(하이퍼파라미터의 개수, 혹은 찾는 범위에 비례하는 searching space)와 생기는 trade-off에 균형을 맞추는 것이다. Spearmint, SMAC 그리고 Hyperopt와 같이 라이브러리가 많이 생겼지만, 여전히 random search를 정교하게 진행하여 찾는 것보다는 최종 성능이 좋지는 않다고 한다.


# Ensemble을 통한 evaluation
신경망 네트워크의 performance를 높이고, 일반화 성능을 높이는 방법으로는 multiple network를 학습시키고, test 상황에서 모든 prediction의 평균을 사용하는 것이다(이를 <U>앙상블 기법</U>이라고 한다). 앙상블 기법에서 사용되는 네트워크의 수가 증가하면 증가할수록 performance는 증가하는 추세를 보인다(물론 계속 증가하는 것은 아닐 수도 있음). 보통 앙상블을 사용할 때는 네트워크가 다 비슷비슷한 상황보다는 서로 variation이 클수록 효과가 더 좋다. 다음은 앙상블을 구성할 때 사용할 수 있는 여러 approach를 소개해보도록 하겠다.

##### Same model, different initialization
서로 다른 네트워크를 학습할 때 사용할 수 있는 방법은 가장 최적의 hyperparameter(동일 hyperparameter)로 학습하되, parameter initialization만 다르게 하는 것이다. 이 approach의 단점은 initialization에만 네트워크들의 variance를 의존해야한다는 것인데, 사실 초기화 단계가 어찌되었든 최종 network parameter가 모두 유사하게 학습된다면 큰 효과가 없을 수 있다.

##### Top models discovered during cross-validation
가장 좋은 성능을 보이는 hyperparmeter 집합을 기준으로 $n$개의 모델을 뽑는다. 앞선 방법에 비해 network의 다양성은 보장될 수 있지만, optimal한 모델이 아닌 애매하게 optimal(sub-optimal)한 모델을 사용한다는 문제가 있다. 구현은 훨씬 간단하지만 만약 model이 제대로 된 성능을 보이지 못한다면 distracting factor로 작용할 수 있다는 문제가 있다.

##### Different checkpoints of a single model
만약 학습이 너무 오래 걸린다면, 단순히 가장 좋은 성능을 보이는 네트워크의 서로 다른 학습 단계에서의 checkpoint를 선택하는 방법을 생각해볼 수 있다. 하지만 이 역시 같은 네트워크가 기준이라는 점에서 다양성을 보장할 수 없기도 하고, 최적화가 제대로 진행되지 않았을 수도 있다.

##### Running average of parameters during training
학습 과정에서 exponentially decaying하는 네트워크의 parameter를 저장해놓고, 이를 통해 여러 iteration의 네트워크 parameter를 smoothing하는 과정이다. 만약 네트워크의 objective가 bowl형태의 convex이고, 네트워크가 최종 학습 단계에서 최적화 point를 제대로 찾지 못하고 ping-pong(왔다갔다)하고 있을 때 효과적으로 사용될 수 있는 방법이다.

Ensemble은 물론 좋은 머신러닝 기법 중 하나이지만, <U>test data에 대한 inference가 오래 걸린다</U>는 치명적인 문제를 가지게 된다. 이를 해결하기 위한 다른 inspring 중에는 ensemble knowledge를 단일 네트워크에 distillation하는 "Dark knowledge"라는 내용도 있다.


# 마무리하며..
오늘은 딥러닝 학습 시에 <U>변화해야하는 것</U>들(loss, parameter 그리고 hyperparameter)에 대해서 살펴볼 수 있었다. 사실상 딥러닝 기초 지식 그리고 연구에 필요한 내용은 각 task마다의 디테일한 수학 개념이나 domain knowledge를 제외하고는 거의 전부 설명했다고 볼 수 있다.   
지금 글은 네이버 블로그에 과거에 올렸던 cs231n 강좌 복습 글과 cs231n course note를 다시 참고하면서 작성하는 중이다. 사실 당시에는 본인이 굉장히 잘 이해하고 작성했다고 생각했지만 지금 다시 보니까 아는 척하고 쓴 내용도 정말 많은 것 같다. 이 글도 나중에 보게 되면 부끄러울 정도로 무지한 상태로 보일 수도 있을지도 모른다...
`,hO=`---
title: "cs231n 내용 요약 (9) - CNN(Convolutional neural network)"
category: "ai theory"
publishedAt: "2022-11-10"
thumbnail: "https://user-images.githubusercontent.com/79881119/218700569-99295503-eedf-45e6-8cd4-af6a4cf4a0f0.png"
---


# 들어가며...
이전까지의 글을 통해 길고 긴 여정을 거쳐 신경망에 대한 내용을 정리할 수 있었다. 딥러닝은 머신러닝의 한 방법론이라고 볼 수 있는 neural network를 확장시켜 보다 깊은 레이어층을 학습하고자 했던 연구였고, 그 연구가 많은 발전을 이루어 현재 AI가 <U>사업의 대부분을 구성하는</U> 세상이 되었다. 사실 딥러닝을 처음 공부했을 때는 공부해도 되는 분야인지 막막하기도 했었고, 공부하다보면 최근 논문이나 연구로 올수록 앞으로 내가 이 분야에서 대학원 생활을 통해 얻을 수 있는게 과연 얼마나 있을까라는 생각을 많이 하게 된다. 그럼에도 불구하고 더 늦지 않게 이 분야를 공부하기 시작했다는 점이 다행스럽다고 생각될 때도 많고 진로를 택한 이후로 '괜히 했다' 같은 후회는 해보지 않았던 것 같다. 아무튼 이번 게시글에서 다룰 내용은 2010년 초중반 이후 활발하게 발전할 수 있었던 딥러닝의 기본인 CNN에 대해서 알아보도록 하고, 도대체 왜 해당 아키텍쳐가 컴퓨터 비전에서 큰 각광을 받을 수 있었는지 정리해보도록 하겠다.


# Convolutional neural network
이른바 CNN이라고 불리는 네트워크 구조는 딥러닝에서 자주 활용되는 구조로 알려져있다. 그렇다면 대체 <U>convolutional neural network</U>가 어떻게 딥러닝에서 자주 활용될 수 있었을까? 우선 CNN에 대해서 보기 전에 수학적으로 정의된 convolution 연산에 대해 살펴보면 다음과 같다.


# Convolution in $1D$ signal
<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218650535-79cd0b77-cca7-4340-82d8-714a262b76c2.png" width="500"/>
</div>
두 개의 함수 $f$와 $g$가 있을 때, 두 함수의 합성곱을 표현하는 용어가 convolution이며 수학 기호로는 $f \\ast g$로 표시한다. 합성곱 연산은 두 함수 $f$와 $g$ 가운데 하나의 함수를 $y$축 대칭 및 shift시킨 후, 다른 함수에 곱한 결과를 적분하는 것을 의미한다. 말로는 표현이 어렵기 때문에 식으로 살펴보면,

$$
    f \\ast g(t) = \\int_{-\\infty}^\\infty f(\\tau) g(t - \\tau) d \\tau    
$$
위와 같이 표현 가능하다. Convolution된 함수를 $t$에 대한 새로운 함수 $h(t)$라고 생각하면, $t$는 $x$축으로 평행이동한 거리를 의미한다. Convolution 연산은 commutable(교환 법칙이 성립)하므로 다음 공식이 성립한다.

$$
    f \\ast g(t) = \\int_{-\\infty}^\\infty f(t - \\tau) g(\\tau) d \\tau  
$$
그림을 보게 되면 추가적으로 autocorrelation, cross-correlation이라는 연산도 정의되어있는데, 각각은 함수 사이의 연관성을 측정하는 지표로, cross-correlation은 서로 다른 두 함수, autocorrelation은 동일한 함수에 대한 지표로 작용한다. Autocorrelation은 cross-correlation 식과 동일하며 <U>correlation을 구하는 두 함수가 서로 같은</U> 특수한 경우로 생각하면 된다. 연속이며 실수 범위의 신호 $f$와 $g$에 대해서,

$$
    f \\star g(t) = f \\ast g(-t) = \\int^\\infty_{-\\infty} f(\\tau) g(t+\\tau) d \\tau
$$

위와 같으며,

$$
    g \\star f(t) = g \\ast f(-t) = \\int^\\infty_{-\\infty} g(\\tau) f(t+\\tau) d \\tau = \\int^\\infty_{-\\infty} g(t - \\tau) f(t) d \\tau    
$$

convolution과는 다르게 commutable하지 않은 것을 볼 수 있다. 물론 autocorrelation의 경우에는 commutable 특징이 그대로 유지된다. 1차원에서 convolution 연산이 가지는 수학적 의미를 그대로 이해한 채로 CNN을 보게 되면 어떤 점에서 차이가 있고, <U>어떠한 맥락에서</U> convolutional neural network로 정의되었는지 확인할 수 있다.


# 다시 돌아와서, CNN
Convolutional neural network는 기존에 다뤘던 모든 neural network 구조와 연산 방법만 다를 뿐 맥락은 서로 유사하다. 이전에 본 perceptron 기반 MLP는 다음과 같은 구조를 가지고 있었다.
<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218655242-ccc0348d-a375-4c3c-a0f4-d371bbb1cee1.png" width="600"/>
</div>
딥러닝 네트워크는 위의 perceptron을 하나의 단위로 여러 node로 구성된 각 layer와 다수의 노드를 포함한 여러 layer로 구성된 구조였다. 즉 학습 가능한 weight와 bias를 가지고 있다고 요약할 수 있다(붉은색으로 표시된 부분). 각 뉴런은 input을 받아들인 뒤 parameter인 weight, bias와 연산을 진행한 후 non-linearity 연산 gate로 표현된 <U>activation function</U>을 거치게 된다. 전체 네트워크는 score라는 output을 내보내는 미분 가능한 함수로서, loss function(SVM/Softmax 등등)을 최소화하는 방향으로 학습된다.
Convolutional neural network도 convolution layer를 일종의 weight, bias가 포함된 neuron으로 생각하면 같은 메커니즘으로 작동한다. 하지만 <U>convolutional neural network(CNN)</U>는 input이 image라는 <U>explicit assumption이 추가되었다는 점</U>이 차이가 되겠다. 이러한 explicit assumption을 통해 architecture가 가질 수 있는 property가 생기게 되었고, 더 적은 parameter 수를 가지고도 높은 성능을 기대할 수 있는 네트워크를 만들게 되었다.


# 신경망 아키텍쳐
일반적인 neural network 구조를 보면 앞서 말했던 것과 동일하게 input을 single vector로 받아들인 뒤, 여러 hidden layer를 통해 dimension을 늘이고 줄이는 형태로 변형을 가한다. 각 hidden layer는 neuron 여러 개로 구성되며, 각 뉴런은 이전 layer의 모든 neuron과 fully connected된 구조를 가진다. 그리고 각 layer에서의 node는 서로 연산을 공유하지 않고 독립적인 연산을 진행한다. 마지막 layer는 output layer로써 task에 맞게끔 score를 output으로 도출하는 역할을 한다.   
이러한 MLP(regular neural network) 구조는 차원 수가 급증하게 되면 <U>scalability</U>가 떨어진다. 예를 들어 CIFAR-10과 같은 dataset은 비교적 resolution이 작은 편에 속하는데($32 \\times 32 \\times 3$), 그마저도 첫번째 hidden layer가 input으로 받아들여야하는 차원 수가 $3072$가 된다. 물론 이 숫자만 보게 되면 그렇게 많지 않은 연산량이라고 생각될 수 있지만 ImageNet 데이터셋과 같이 resolution이 $224 \\times 224 \\times 3$이 된다면 $150,528$의 weight를 계산해야한다. 그리고 기억해야할 것이, input dimension만 이정도가 되고 output dimension까지 고려하면 weight와 bias에 필요한 parameter의 수가 급증하게 될 것이다. 딥러닝을 구현하기 위해서는 여러 레이어가 필요하고, 충분한 representation을 학습하기 위해서는 각 layer에 충분히 많은 수의 node를 할당해야할 것이다. 따라서 fully-connected layer로 구성한 기존 neural network 구조는 연산 복잡도도 높으며 fully-connected된 특징 때문에 overfitting의 위험성도 높다.
이런 측면에서 3차원의 구조를 가진 neuron, 즉 CNN은 image를 input으로 받아들인다는 explicit한 assumption과 아키텍쳐를 특정 구조로 제한한다는 점에서 얻는 이점이 있다. 기존 네트워크와는 다르게 각 layer는 input에 무관한 3개의 차원 정보인 <U>width, height, depth</U>를 가진다. 예를 들어 input image가 CIFAR-10이라면 input image에 대한 width, height, depth는 각각 $32$, $32$ 그리고 $3$임을 알고있는데, 이렇게 image modality가 가질 수 있는 차원에 대한 정보를 CNN의 각 layer도 가질 수 있다는 컨셉이다.
따라서 연산 구조를 보게 되면 이전의 neural network 연산이 진행되었던 것과 같이 input의 모든 부분을 연산에 포함시키지 않는 것을 확인할 수 있고, fully-connected 구조에서 벗어나면서 image에 대한 연산 overfitting을 방지할 수 있었다. 그리고 연산이 fully-connected manner에서 벗어나 input dimension에 무관하게 진행될 수 있으므로 parameter 수도 감소하게 되었다. 물론 마지막 layer의 <U>output</U>은 task에 따라 기존 <U>neural network의 output size와 동일하게 생성</U>되어야 한다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218660940-dc5beb8a-f507-4480-80bb-5d6d3c49b79c.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/218660841-e25dd445-4a6e-442c-ad50-41413bb0740c.png" width="550"/>
</div>


# How CNN calculate output feature for each hidden layer?

2D convolution 연산 과정을 나타내면 다음과 같다. 예를 들어 고양이 이미지를 분류하는 task에서, $H \\times W \\times 3$의 고양이 RGB 이미지가 있다고 생각해보자.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218661293-ff18070b-0bff-4728-b450-38b54dac5019.png" width="400"/>
</div>

기존 방식은 위와 같은 RGB image data를 <U>$1$차원 벡터로 확장시킨 후</U> 여러 층의 hidden layer를 통과시키는 과정이었다(좌측 이미지 참고).
<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218661737-1cf74236-6662-4b63-adc6-5e0430005849.png" width="240"/>
    <img src="https://user-images.githubusercontent.com/79881119/218662106-71b39dfc-e798-4532-9142-5bf06e291a10.png" width="640"/>
</div>

그러나 convolution layer에서는 이러한 flatten 과정 없이 원래의 이미지 dimension에 바로 연산이 가능한 convolutional kernel(filter)를 적용하게 된다. RGB 픽셀을 확대해서 나타내보면 우측 그림과 같다. Convolution 연산에서 사용되는 filter의 크기는 input과는 무관하기 때문에, <U>3차원 텐서 형태를 가지는 이미지</U>에 그대로 적용될 수 있다. 각 픽셀 별로 연산에 사용되는 kernel을 표현한 그림이 위와 같다. 각 kernel은 해당 크기에 상응하는 학습 가능한 parameter 수를 가진다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218662688-257853fd-1c41-42e1-9ee3-48d2ea25eadf.png" width="400"/>
</div>

CIFAR-10 dataset에 대한 예시를 입체적으로 표현한 것이 위의 그림과 같다. 옅은 분홍색으로 표현된 $32 \\times 32 \\times 3$ 크기의 텐서가 input image이고, 그 안에 들어있는 짙은 분홍색으로 표현된 작은 3차원 텐서가 convolution kernel에 해당한다. Convolution kernel은 filter가 커버하는 input 범위에 대한 cross-correlation 연산을 진행하고(자세한 연산은 뒤에서 언급하도록 하겠다) 모든 kernel에 대한 연산 결과를 output dimension에 맞게 추출한 것이 우측에 보이는 파란색 output tensor에 해당한다. Convolution 연산이 진행되는 방식과 convolution 연산 대신 cross-correlation을 언급한 이유에 대해 서술하면 다음과 같다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218663633-9b78b2de-97aa-4421-a45e-e90c562deec1.gif" width="400"/>
</div>

Convolution 연산은 kernel size, stide 그리고 padding이라는 <U>기본 hyperparameter</U>를 가진다. 위의 그림은 padding $= 1$, stride $= 1$ 그리고 kernel size $= 3$인 경우에 해당된다. 청록색으로 표현된 격자가 convolution 연산을 진행한 후 출력된 output을 의미하고, 아래에 있는 파란색의 격자가 input이다. 파란색의 격자 바깥쪽에 점선으로 된 부분이 바로 padding이다.

### Kernel
실질적으로 convolution filter가 <U>input에 적용될 범위</U>를 나타내며, kernel은 필터 자체와 같은 의미를 가진다. 따라서 위의 그림에서는 kernel이란 움직이는 회색 영역에 해당되며, 이때의 kernel size는 $3$임을 알 수 있다. 물론 filter의 kernel size가 무조건 spatial하게 동일해야하는 것은 아니다(가로, 세로가 같은 길이를 가질 필요는 없다).

### Padding
Input에 대해 외곽 부분(점선으로 그려진 부분)이 padding이라고 앞서 이미 설명했었다. Padding은 input 기준으로 constant value를 붙이는 zero-padding과 같은 방법이 있기도 하며, extrapolated padding 등 다양한 방식이 존재한다. Padding $= 1$이라는 의미는 input에 대해 상하좌우 모두 $1$칸씩 spatial dimension을 늘림을 의미하고, 이를 실제로 그림 상에서 확인해볼 수 있다. Padding도 kernel size와 마찬가지로 상하좌우 모두 동일할 필요는 없다.

### Stride
Input에 padding이 추가된 영역을 포함하여 kernel은 stride만큼 움직이며 연산을 진행한다. 위의 예시에서는 $3 \\times 3$의 kernel size를 가진 필터가 stride $= 1$씩 옮겨가며 연산을 진행하는 것을 볼 수 있다.

하지만 여기서 짚고 넘어가야할 점은 convolution 연산은 앞서 본 1D convolution 계산과는 다르게 input과 filter가 서로 역방향으로 연산되는 것이 아닌, 같은 방향에서 내적이 이루어지는 것을 확인할 수 있다. 그리고 위의 예시에서는 평면에 대해서 연산이 진행되는 것처럼 보이지만 사실 단일 channel에 대한 연산이 아닌, 모든 channel을 커버하는 하나의 filter가 존재하는 것이다.
<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218683663-ced88b17-4281-49c6-ad45-a6b4fc545117.png" width="1000"/>
</div>

따라서 실제 연산 과정은 위와 같다. 검은색으로 표현한 것이 $3 \\times 3$ kernel size를 가지는 filter를 의미하고, stride $= 1$만큼 이동하면서 연산이 진행되는데, 각 채널마다 $3 \\times 3$ 크기의 kernel이 적용되기 때문에 filter의 실제 크기는 RGB 채널을 모두 커버하는 $3 \\times 3 \\times 3$의 3D tensor가 된다. 앞서 convolutional neural network의 각 뉴런은 width, height 그리고 depth를 가진다고 했었는데 지금 보이는 필터의 kernel size가 곧 width와 height를 대표하는 값이며 input image 혹은 feature map의 channel size가 필터의 depth라고 할 수 있다. 연산이 진행되는 과정을 예시로 들면 다음과 같다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218684609-540d8251-15a7-4fbe-a0ee-4c05288e7c3d.png" width="600"/>
</div>

RGB value는 임의로 채운 값이고 필터도 마찬가지로 임의로 채운 값이다. 각 채널별로 필터가 곱해진 뒤 모두 더하는 형태로 inner projection이 진행되며, 이렇게 각 채널별로 더해진 값들이 다시 모두 더해져서 해당 영역에서의 filter가 적용된 output value는 $7 + 15 + 4 = 26$이 된다. 추가로 만약 bias가 존재한다면, output value는 $26 + \\text{bias}$ 꼴이 된다. 실제로 필터가 어떠한 역할을 하는지 시각화해보기 위해 toy code를 colab에서 작성해보았다. 우선 본인이 업로드하고 싶은 이미지를 업로드하면 된다. 나는 구글에서 쉽게 얻을 수 있는 아래와 같은 고양이 이미지를 업로드했다. 
<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218698278-4ec7a495-f2c5-4ae4-8eba-5e056e25f357.png" width="400"/>
</div>

이미지를 \`\`\`numpy array\`\`\`로 가져오기 위해 필요한 모듈 그리고 시각화에 필요한 모듈을 정의하였다. 그리고 $0$부터 $255$까지의 값을 가지는 image를 normalize 하였다.

\`\`\`python
import cv2
from google.colab.patches import cv2_imshow
import numpy as np

image = cv2.imread("cat.jpg")
image = image/255.0 # Normalize image to 0 ~ 1
\`\`\`

만약 google colab이 아닌 로컬에서 돌리고 싶거나 jupyter notebook을 이용한다면 \`\`\`google.colab.patches\`\`\` 대신 단순히 \`\`\`cv2.imshow\`\`\` 메소드를 사용하는 것을 추천한다. 그런 뒤 numpy array에 대한 convolution 함수를 다음과 같이 구성하였다.

\`\`\`python
def conv2d(image, out_channels, kernel, padding=0, strides=1):
    # Build kernel according to input image size
    image_height, image_width, image_channel = image.shape
    if type(kernel) == int:
        kernel_channel, kernel_height, kernel_width = image_channel, kernel, kernel
    elif type(kernel) == tuple or type(kernel) == list:
        kernel_channel, kernel_height, kernel_width = image_channel, kernel[0], kernel[1]

    kernel = np.random.randn(kernel_channel, kernel_height, kernel_width)

    # Calculate output shape according to input image size and filter size
    output_height = int(((image_height - kernel_height + 2 * padding) / strides) + 1)
    output_width= int(((image_width - kernel_width + 2 * padding) / strides) + 1)
    output_channel = out_channels
    output = np.zeros((output_height, output_width, output_channel))

    # padding(zero-padding) on input
    if padding != 0:
        image = np.pad(image, ((padding, padding), (padding, padding), (0, 0)), 'constant', constant_values=0)
    
    # calculate 2d convolution
    for c in range(output_channel):
        output_per_channel = np.zeros((output_height, output_width))
        for h in range(output_height):
            if (h * strides + kernel_height) <= image.shape[0]:
                for w in range(output_width):
                    if (w * strides + kernel_width) <= image.shape[1]:
                        output_per_channel[h][w] = np.sum(
                            image[h*strides : h*strides + kernel_height, w*strides : w*strides + kernel_height, :] * kernel
                        ).astype(np.float32)

            output[: ,:, c] = output_per_channel

    
    return output
\`\`\`

예외처리 없이 간단하게 구성하였다. 모든 convolution layer는 정규 분포로 초기화되며, 본인은 총 3개의 convolutional hidden layer가 있는 네트워크를 구상하였다. 각 convolution 사이에는 \`\`\`leakyrelu\`\`\` 메소드를 다음과 같이 정의하여 activation function으로 사용하였다. LeakyReLU는 $0$보다 작은 value에 $0.1$의 scaling을 주도록 구성하였다.

\`\`\`python
def np_leakyrelu(image):
    return np.where(image<0, 0.1*image, image)
\`\`\`

각각의 hidden layer에서의 output을 구하고, 이를 normalize하여 시각화가 가능하게끔 해주었다.

\`\`\`python
hidden1 = conv2d(image, 3, 3, 1, 1)
hidden2 = conv2d(np_leakyrelu(hidden1), 3, 3, 1, 1)
hidden3 = conv2d(np_leakyrelu(hidden2), 3, 3, 1, 1)

hidden1 = (hidden1 - np.min(hidden1))/(np.max(hidden1) - np.min(hidden1))*255.0
hidden2 = (hidden2 - np.min(hidden2))/(np.max(hidden2) - np.min(hidden2))*255.0
hidden3 = (hidden3 - np.min(hidden3))/(np.max(hidden3) - np.min(hidden3))*255.0
\`\`\`

\`\`\`hidden1\`\`\`, \`\`\`hidden2\`\`\`, \`\`\`hidden3\`\`\`를 모두 시각화하면 다음과 같다.
\`\`\`python
cv2_imshow(hidden1)
cv2_imshow(hidden2)
cv2_imshow(hidden3)
\`\`\`

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218700569-99295503-eedf-45e6-8cd4-af6a4cf4a0f0.png" width="600"/>
</div>
필터를 전혀 학습하지 않고 random하게 초기화한 상태로도 어느 정도 물체의 윤곽을 잘 추출해내는 것을 확인할 수 있었다. 만약 task에 따라서 filter가 최적화가 되면, 위에 보이는 이미지보다 image에 대한 feature map을 잘 추출할 수 있을 것이라고 생각되었다.


# Why is it convolution, not correlation?
Discrete signal $x$와 $h$가 있다고 생각해보자. 앞서 봤었던 one-dimensional convolution과는 다르게 discrete signal로 가정한 이유는 convolutional neural network가 <U>digitized image dataset</U>에 적용되기 때문이다. Height에 대한 dimension은 $i$라는 index를 따라가고, Width에 대한 dimension은 $j$라는 index를 따라간다고 생각해보자.

$$
    y(m, n) = x(m, n) \\ast h(m, n) = \\sum_{j = -\\infty}^\\infty \\sum_{i = -\\infty}^\\infty x(i, j) \\cdot h(m-i, n-j)
$$

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218709500-83a67c2b-75c5-4ff1-bbcc-ea26a01b4413.png" width="220"/>
    <img src="https://user-images.githubusercontent.com/79881119/218709353-99cd9c18-405b-4535-879a-ad36e7f4e978.png" width="300"/>
</div>
첫번째 이미지가 input에 해당되고 두번째 이미지가 kernel value라고 생각해보자. Kernel의 중앙 부분을 $0$이라고 생각한다면 예를 들어 $y(-1, -1)$을 구하는 과정은,

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218710641-0e6e7d8d-750e-4c1d-b92c-c6b21099a073.png" width="300"/>
</div>

이와 같이 뒤집어진 kernel filter가 적용되어야하는 것이다. 그러나 실제로 연산이 진행되는 과정을 보면 뒤집힌 필터에 대한 inner projection  연산이 아닌, 단순히 곱해져서 더하는 형태로 구성된다. 물론 본인이 위에서 구현한 \`\`\`numpy\`\`\`를 활용한 \`\`\`conv2d\`\`\` 메소드 예시도 단순히 필터에 곱하는 방식을 사용하였다. 만약 정의대로 한다면, 해당 연산은 $x(m, n) \\ast h(m, n)$이 아닌 $x(m, n) \\ast h(-m, -n)$이 적절하다. 그리고 이 연산은 앞서 one-dimension의 경우에도 설명했었지만 commutable하지 않은 <U>cross-correlation 식에 더 부합</U>한다. 사실 이 내용은 그렇게 중요하지는 않지만, 그대로 convolution 연산에 대해 근본적인 의문을 가질 수 있기 때문에 짚고 넘어가고 싶었다.


# Calculate output tensor shape
Convolution 연산을 활용한 network를 구성하기 위해서는 각 레이어에서 특정 convolution이 적용되었을 때의 output dimension을 알아야할 필요성이 있다. 일반화된 식을 pytorch 공식 홈페이지에서 소개하지만([참고 링크](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)), 이 글에 보다 자세히 옮기면 다음과 같다. 만약 input tensor의 shape이 $H_\\text{in} \\times W_\\text{in} \\times C_\\text{in}$이라고 하고, output tensor shape이 $H_\\text{out} \\times W_\\text{out} \\times C_\\text{out}$이라고 해보자.   
Convolution filter의 크기(kernel size)와는 상관없이 output channel dimension이 $C_\\text{out}$가 되어야하기 때문에 filter의 개수는 $C_\\text{out}$이어야 한다. 그리고 각 필터는 마찬가지로 크기와는 무관하게 input tensor의 모든 채널을 커버해야하므로 필터의 depth는 $C_\\text{in}$이 된다. 따라서 정해지지 않은 필터의 크기를 제외하고 필터의 개수과 depth를 고려한 convolutional filter의 tensor shape($H \\times W \\times C$)과 parameter 수는 다음과 같다.

$$
    (\\text{kernel size} \\times \\text{kernel size} \\times C_\\text{in}) \\times C_\\text{out}    
$$

소괄호 내부에 있는 것이 filter의 shape가 되고, 소괄호 전체에 곱해진 값이 필터 전체 갯수가 된다. 따라서 bias를 포함하지 않는 convolutional layer의 parameter 수는 위와 같이 계산할 수 있다. 만약 bias가 포함된다면 각 channel 마다의 bias가 포함되므로 parameter 수는 다음과 같이 증가한다.

$$
    (\\text{kernel size} \\times \\text{kernel size} \\times C_\\text{in} + 1) \\times C_\\text{out}   
$$

지금까지는 kernel size를 정해놓지 않고 단순히 convolutional layer 하나에서의 parameter 수에 대해서 언급했었고, 실질적으로 <U>필터가 적용되었을 때의 output tensor shape</U>에 대해 계산하면 다음과 같다.

$$
    \\begin{aligned}
        H_\\text{out} =& \\left( \\frac{H_\\text{in} + 2p - k}{s} + 1 \\right) \\newline
        W_\\text{out} =& \\left( \\frac{W_\\text{in} + 2p - k}{s} + 1 \\right)
    \\end{aligned}    
$$

위의 식에서는 dilation같은 특별한 convolution에 대한 output shape은 무시하였다. Notation에서의 $s$는 stride, $p$는 padding 그리고 $k$는 kernel size를 의미한다.


# Another modules in CNN
물론 convolutional neural network에도 convolution 연산이 선형 연산이 되기 때문에 non-linearity gate나 pooling layer 등등 다른 형태의 module이 보조적으로 사용된다. 그 중 대표적으로 자주 사용되는 일부 레이어들에 대해 소개하도록 하겠다.

### Non-linearity
AlexNet이 ImageNet 대회에서 CNN 구조로 우승했을 적에 <U>sigmoid</U> 대신 <U>ReLU</U>를 적용함으로써 빠른 성능 수렴을 얻을 수 있었다고 언급했다. 

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218717045-c8c28dfc-02bf-4f0c-bc8b-15305df00efb.png" width="500"/>
</div>

이렇듯 ReLU와 같은 non-linearity를 사용함으로써 연산 과정에서 더 많은 노드를 통한 feature extraction이 가능하다.

### Pooling layer
Pooling layer는 <U>spatial dimension을 줄여주는 역할</U>을 한다. 보통 convolution 연산들 뒤에 붙어 $H$, $W$의 차원 수를 줄이는 역할을 수행한다. Understanding based task(image classification)는 이미지 전체의 정보를 low resolution에 semantic 정보로 수용하는 것이 보다 연산 효율적이기도 하고, 실제로 성능 향상에 더 도움이 되기 때문이다. 그리고 task specific한 부분을 제외하고도 연산 효율성에 대해 언급하자면, convolutional neural network과 fully connected layer에 비해 input node에 대해 가지는 참조 범위가 작은 만큼 filter의 개수를 증가시켜 representation power를 늘리는 경향이 있는데 만약 spatial dimension이 그대로 유지되면 연산량이 레이어가 지날수록 급증하는 문제가 발생하고, 결론적으로는 MLP에 비해 가지는 장점인 <U>parameter efficiency</U>가 사라지게 된다. 따라서 high level feature를 사용하는 classification이 아닌 segmentation task에서도 이와 같은 연산 효율성을 보장하기 위해 pooling 연산을 통해 downsampling을 진행한 후, upsampling을 하는 방식을 채택하였다. 물론 이와 같은 구조에서는 coarse feature가 복구하지 못하는 image signal들이 있기 때문에(<U>일종의 aliasing</U>) 이를 보조하고자 제안된 방법들이 있다. 해당 내용들은 본문의 주제에서 벗어나기 때문에 넘어가도록 하겠다.   

### Fully connected layer head
Convolutional neural network에서도 score function에 대한 loss function은 동일하게 적용하기 위해 마지막 feature map을 flatten한 뒤 fully connected layer를 적용하거나 global average pooling 이후 fully connected layer를 적용하는 방법을 사용한다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218720917-493f4c2d-96e5-48c0-a6d1-408af53d1b64.png" width="600"/>
</div>

앞서 소개한 여러 module들을 포함시킨 대략적인 CNN의 구조는 위와 같다. Convolutional neural network에서 convolution layer와 fully connected layer는 학습 가능한 parameter를 가지지만 ReLU나 Pooling layer는 학습 가능한 parameter를 가지지 않는다.


# Max pooling vs Average pooling
Pooling은 아래 그림과 같이 output feature map의 spatial dimension을 줄여주는 역할을 한다. Pooling은 2D dimensional signal 특성상 <U>low filtering을 진행하는 것</U>과 같으며, 이미지에서의 frequency는 해상도와 관련이 있기 때문에 downsampling을 진행할수록 fine-feature보다 coarse-feature가 생성된다.
<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218721963-81a56e4c-25cb-4274-80a4-954d6670b8a3.png" width="400"/>
</div>

그런데 pooling을 진행하는 과정은 앞서 설명했던 것과 같이 parameter를 필요로 하지 않기 때문에 사용할 수 있는 방법은 max pooling이 진행되는 영역에서 최댓값을 뽑아내는 것과 평균을 취하는 방법 두 방법을 생각해볼 수 있다. 아마 low-level image 신호처리와 관련된 수업을 들어본 사람이라면 pooling 방식이 <U>이미지의 resolution을 줄이는 방법과 관련이 있다는 것</U>을 알 것이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218721760-5effd6cc-b7b3-4006-82e9-2772f95b10cc.png" width="600"/>
</div>

일반적으로 convolution layer 사이에는 max-pooling을 사용하는데, 이는 평균을 취하는 것보다는 주된 feature(하이라이트되는)를 feature를 취하기 때문이라고 분석할 수 있다. 물론 task마다의 차이는 있겠지만 학습된 filter가 적용되면 feature map은 오브젝트의 큰 윤곽부터 시작해서 디테일한 outline을 추출하게 되고, 사실상 classification이나 object detection 그리고 segmentation 등 다양한 downstream task를 해결하는데 보다 도움이 되는 signal value는 <U>최대치 근처의 다른 noise</U>가 아닌 <U>부각되는 부분들</U>이기 때문이다. 


# Useful baseline models
CNN을 기반으로 딥러닝이 발전되던 당시 최고 성능을 보인 <U>ResNet</U>는 그 구조를 확장시켜 ResNext, ConvNext를 포함한 다양한 task에서 backbone 구조로 채택될 정도로 인기가 많은 편이다. 이외에도 CNN 기반 컴퓨터 비전 딥러닝의 시작을 열 수 있었던 AlexNet, VGGNet 그리고 GoogleNet 등등 여러 중요한 <U>preliminary는 논문을 읽어보는 것을 추천</U>한다. 물론 AlexNet 이전에도 더 근본 논문인 LeNet도 있다.   
아키텍쳐 논문은 CNN baseline들과 Transformer baseline들을 모두 읽다보면 거의 비슷한 구조를 대부분 공유한다는 점을 알 수 있게 된다. 이외에도 network 구조를 효율적으로 찾고자 했던 efficientNet 논문이나 정해진 receptive field 모양을 가진 convolution의 한계를 해결하고자 한 deformable convolution, astrous pyramid pooling network 등등 여러 모듈과 관련된 논문들도 읽어보면 좋다.
`,pO=`---
title: "cs231n 내용 요약 (10) - Visualizing, Transfer learning"
category: "ai theory"
publishedAt: "2022-11-11"
thumbnail: "https://user-images.githubusercontent.com/79881119/219242274-f580c8cc-495e-46cb-8306-e4b0e7359975.png"
---


# 들어가며...

이 글이 아마도 cs231n과 관련된 <U>마지막 포스팅</U>이 될 것이다. 사실 깃허브 블로그를 오픈하고 기존 네이버 블로그에 작성했던 내용들을 다시 원래 강의 노트와 비교하면서 옮기고 있었는데, 예전에 작성했던 내용들을 보니 애매하게 적어둔 내용도 많고 잘 모르고 작성한 부분들도 은근 많았던 것 같다. 1년 전이기 때문에 지금도 그때와 비교해서 더 많이 아는 건 아니지만 정리하면서 최대한 예전에 공부했던 내용들을 다시 살펴보는 것이 <U>기초를 다지는 과정</U>에 효과적인 것 같다.   
지금부터 다룰 내용은 이전에 신경망의 구조나 parameter, functionality에 대해 살펴본 것보다 수식적으로 증명하거나 풀어낼 부분이 많지는 않다.


# Visualizing what convolutional neural networks learn
CNN이 처음으로 <U>ImageNet 대회에서 우승</U>한 이후로 deep learning, 그것도 neural network 기반의 알고리즘이 주목을 받기 시작했다. 

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219004015-6a3aba22-1fc1-4853-80a9-c30f9d39e28a.png" width="800">
</div>

표에서 보이는 XRCE라는 알고리즘은 딥러닝 기반이 아니었으며, 2011년 이전의 error가 표시되어있지는 않으나 사실상 2011년의 error rate가 딥러닝을 사용하지 않은 방식을 통해서는 얻을 수 있는 <U>최소의 수렴값으로 인식</U>되기 시작했다. 바로 이러한 인식과 알고리즘의 판도를 바꾼 game changer로 등장한 것이 AlexNet이었고, 무려 $10\\%$의 성능 향상을 보이며 이후 대회에서는 모두 <U>deep learning network가 우승</U>하기 시작했다.   
하지만 딥러닝 기반의 CNN이 ImageNet에서 우승한 이후에도 지속적인 의문점과 비판이 올라오기 시작했다. 대부분 예전 low-level vision task를 기계 학습과 관련된 여러 컴퓨터 알고리즘으로 해결하고자 했던 사람들이었고, 비판하는 내용은 대부분 deep learning 알고리즘은 연구의 판도를 바꿀 정도로 성능 향상에 큰 기여를 했으나 <U>작동 원리에 대한 엄밀한 설명이 불가능</U>하다는 주장이었다.
이전 포스팅까지 소개했었던 <U>MLP(Multi-Layer Perceptron)</U>과 같은 구조를 계속 설명했었고 네트워크가 학습되는 과정을 forward propagation, objective function 그리고 back-propagation의 순서대로 개념을 설명했었다. 결국 loss function을 최적화하는 방향으로 학습하는 과정을 이해하기 위해서는 gradient, linear algebra 등등 수학적인 지식이 필요하지만, 딥러닝이라는 분야가 <U>하나의 연구 분야로 인정받기 위해서</U>는 구체적으로 네트워크가 왜 이런 방식으로 학습을 하는 것이 <U>기적적인 성능 향상</U>을 이끌어냈는지 수학적 설명이 뒷받침될 필요가 있었다.   
Computer science에서는 이러한 의문이 중요한 문제로 자리잡았다. Data-driven 알고리즘은 deterministic 알고리즘과는 다르게 원리를 파악하고 개선시키고자 하는 방향을 잡을 수 없기 때문이다. 단순히 인간의 neuron 구조를 모방한 perceptron이 MLP로, 더 나아가 computer vision task를 위한 CNN으로 발전되었고 좋은 성능을 보인다는 점에서 <U>설명력이 부족하다는</U> 비판이 올라오게 되었다.

이렇듯 Neural network 구조의 input과 output에 대한 수학적 설명은 가능하지만, 내부 parameter가 prediction에 generalized될 수 있는 근거를 설명할 수 없다는 사실을 <U>'black box'</U>라는 용어를 통해 표현하게 되었다. 단순히 neural network에서는 이러한 black box(explicit한 input, output을 제외하고는 implicit하게 학습되는 내부를 직접 관측할 수 없어 원리를 설명할 수 없음)의 문제점을 해결하지 못했다. 처음 ImageNet 대회에서 우승했던 AlexNet도 논문화 과정에서 <U>해당 내용들을 자세히 서술하지 못해</U> 문제가 되었다.

그리고 단순히 학계에 있는 사람들을 설득해야할 필요성 뿐만 아니라 네트워크의 성능에 대한 설명이 필요했던 이유는 neural network 설계 과정에서 성능을 높이거나 error가 발생한 sample에 대해 explainablilty가 있어야 추후 딥러닝 연구가 가능했기 때문이다. 결국 deep learning에서 explainablilty는 기존 학계에 있던 <U>사람들을 설득하는 과정</U>에서도, 본인들의 <U>연구를 발전시키기 위한 기반</U>으로도 필요했던 부분인 것이다.

그렇기 때문에 deep learning network 내부에서 학습하는 형태를 간접적으로 확인하고자 weight visualization 혹은 특정 input $x$에 대한 layer activation $(f_l \\circ f_{l-1} \\circ \\cdots \\circ f_1(x))$ visualization과 같이 성능에 대한 <U>설명력을 뒷받침할 연구</U>들이 진행되었다.


# Layer activations
Visualizing 기술 중 하나는 forward pass에서 input에 대한 layer의 activation 결과를 보여주는 것이다. ReLU가 달려있는 네트워크에서는 activation을 관찰하게 되면 초반에는 blobby하고 dense한 모습을 보여주지만 training이 진행되면 될수록 sparse하고 localized된 모습을 보여준다.
실제로 결과를 확인해보기 위해 다음과 같이 임시 코드를 작성해보았다.

\`\`\`python
import torch
import torchvision
import torchvision.transforms as transforms

# Define CIFAR10 dataset
batch_size = 16

transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True, num_workers=2)
testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                         shuffle=False, num_workers=2)
\`\`\`

학습은 CIFAR-10에 대해 진행할 예정이고, 네트워크는 주어진 조건대로 ReLU를 activation function으로 사용하되 batchnormalization도 추가해주었다.
\`\`\`python
# Let's define simple CNN model
import torch
import torch.nn as nn
import torch.nn.functional as F

class SMPCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.feature1 = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True)
        )
        self.feature2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )
        self.feature3 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )
        self.feature4 = nn.Sequential(
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True)
        )
        self.feature5 = nn.Sequential(
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True)
        )

        self.classifier = nn.Sequential(
            nn.Linear(256*2*2, 64),
            nn.Dropout(0.5),
            nn.Linear(64, 10)
        )

    def forward(self, x):
        f1 = self.feature1(x)
        f2 = self.feature2(F.max_pool2d(f1, kernel_size=2, stride=2))
        f3 = self.feature3(F.max_pool2d(f2, kernel_size=2, stride=2))
        f4 = self.feature4(F.max_pool2d(f3, kernel_size=2, stride=2))
        f5 = self.feature5(F.max_pool2d(f4, kernel_size=2, stride=2))
        f5 = f5.view(f5.size(0), -1)
        score = self.classifier(f5)

        return score, (f1, f2, f3, f4, f5)
\`\`\`

그리고 각 layer의 output을 시각화하기 위해 \`\`\`score\`\`\` 말고 추가로 \`\`\`return\`\`\`해주었다. Training configuration에 따라 학습하는 코드는 함수를 통해 구현하였다.

\`\`\`python
# Let's define trainer function
from collections import defaultdict

def trainer(model, epochs, train_loader, val_loader, optimizer, criterion, device):
    features = defaultdict(list)
    model.to(device)
    for epoch in range(epochs):
        model.train()
        training_loss = 0.0
        training_acc = 0.0
        for i, (image, label) in enumerate(train_loader):
            optimizer.zero_grad()
            image, label = image.to(device), label.to(device)
            score, activations = model(image)
            loss = criterion(score, label)
            loss.backward()
            optimizer.step()
            _, prediction = torch.max(score, axis=1)
            accuracy = float(torch.sum(torch.eq(prediction, label)))/len(prediction)
            training_loss += loss.item()
            training_acc += accuracy

            if i%200 == 0:
                print(f"Epoch [{epoch+1}/{epochs}] (Iter [{i+1}/{len(train_loader)}]) ===> Training loss : {training_loss/(i+1):.6f}, Training accuracy : {100*training_acc/(i+1):.2f}%")
            
            if i%1000 == 0:
                features[epoch] += [activations]
      
        model.eval()
        with torch.no_grad():
            validation_loss = 0.0
            validation_acc = 0.0
            for i, (image, label) in enumerate(val_loader):
                image, label = image.to(device), label.to(device)
                score, _ = model(image)
                loss = criterion(score, label)
                _, prediction = torch.max(score, axis=1)
                accuracy = float(torch.sum(torch.eq(prediction, label)))/len(prediction)
                validation_loss += loss.item()
                validation_acc += accuracy                    

            print(f"Epoch [{epoch+1}/{epochs}] ===> Validation loss : {validation_loss/len(val_loader):.6f}, Validation accuracy : {100*validation_acc/len(val_loader):.2f}%")
    return features
\`\`\`

함수를 보게 되면 training dataset에 대해 일정 iteration 마다 출력된 feature map을 저장하고, 저장된 \`\`\`feature\`\`\`를 리스트가 임베딩된 딕셔너리 형식으로 리턴하게 된다. 코드와 함수를 보면 확인할 수 있듯이 학습 과정에는 \`\`\`batch size\`\`\`를 $16$으로 사용하였고 \`\`\`epoch\`\`\`는 $20$을 사용하였다. 학습 시 learning rate는 스케쥴링 없이 $10^{-3}$을 고정값으로 학습하였다.

\`\`\`python
# Train configurations
model = SMPCNN()
epochs = 20
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)
criterion = nn.CrossEntropyLoss()
device = "cuda" if torch.cuda.is_available() else "cpu"
\`\`\`

Classification task에 대한 학습 코드이기 때문에 \`\`\`criterion\`\`\`은 cross entropy loss를 사용한다. 학습 결과 test set에 대한 accuracy가 $79.49\\%$, training set에 대한 accuracy는 거의 $90\\%$를 보였다. 사실상 약간 overfitting이 발생했지만, data augmentation과 같은 regularization을 추가해주면 충분히 개선될 사항이고, 확인하고자 하는 것은 네트워크의 성능이 아니라 <U>학습 단계에서의 feature map activation</U>이기 때문에 넘어가도록 하겠다. Visualize는 pytorch 모듈의 \`\`\`make_grid\`\`\` 메소드를 사용하였고, 자세한 코드는 따로 첨부하지 않고 결과를 분석해보도록 하겠다. 우선 각 layer에 대해 $16$개의 batch image에 대한 평균 feature map을 나타내면 다음과 같다. 다음 세 이미지는 학습이 진행된 직후(epoch $0$)에서의 feature map activation이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219229561-e9a194cd-5f5a-471c-905c-4dbbbf48bbe7.png" width="300">
    <img src="https://user-images.githubusercontent.com/79881119/219229596-028a2912-de14-4b79-9ac1-f3bf3a65ad99.png" width="300">
    <img src="https://user-images.githubusercontent.com/79881119/219229644-aab29ce5-1458-4680-b186-90f270417704.png" width="300">
</div>

학습 초기의 이미지를 보게 되면 위와 같이 나오게 된다. 앞서 말했던 바와 같이 학습 초기에는 dense하고 blobby한 형태를 보여준다고 했는데, 보는 것과 같이 전반적으로 feature map의 형태가 매끄럽지는 않은 것을 확인할 수 있었다. 그와는 다르게 학습이 진행되면 진행될수록 sparse하고 localized된 모습을 보여주는 것을 확인할 수 있다(아래 그림 참고).

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219229190-c83f7d87-b398-44ff-b403-55a7ca3c8877.png" width="300">
    <img src="https://user-images.githubusercontent.com/79881119/219229257-569fed97-db8b-4b30-9f1c-f2a8ef37c097.png" width="300">
    <img src="https://user-images.githubusercontent.com/79881119/219229309-0cd0963d-f5b5-4f2f-8495-81b574ea38f8.png" width="300">
</div>
평균 activation map으로는 <U>차이가 명확하게 보이지 않았기 때문에</U> feature map을 각각 분리하여 각 채널 별로 어떤 특징을 잡아내는지 시각화했다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219230090-5a4c3353-03ec-4abc-a344-0cb2125d522b.png" width="300">
    <img src="https://user-images.githubusercontent.com/79881119/219230103-3d5a0478-8e6b-466d-b74a-ac952d27c469.png" width="300">
    <img src="https://user-images.githubusercontent.com/79881119/219230117-e197fcbc-dde5-40e8-8f31-7a4856477ae6.png" width="300">
</div>

위의 세 그림은 각 레이어의 activation map을 채널 별로 분리하여 나타낸 모습이고, 해당 activation map은 학습 초기 단계의 네트워크의 출력에서 나온 것이다. 결과를 보면 알 수 있듯이 각 채널 별로 유의미한 feature의 차이를 잡아내지 못하고 있으며(서로 유사한 형태를 가짐), prediction 시에 여전히 <U>training set에 의존하는 형태</U>의 feature를 출력하는 것을 확인할 수 있다. 사실상 이미지의 윤곽 형태를 그대로 필터링하고 있는 것을 볼 수 있고, 이러한 특징들은 딥러닝이 아닌 일반적인 알고리즘으로도 충분히 가능한 feature extraction임을 알 수 있다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219230642-5bef1840-fa64-47b9-88b4-5209fe0b803e.png" width="300">
    <img src="https://user-images.githubusercontent.com/79881119/219230651-c500da44-02cf-4800-b2ae-4e0b3be92ac6.png" width="300">
    <img src="https://user-images.githubusercontent.com/79881119/219230660-e75731c8-3715-4d36-9ed9-f78b716390b3.png" width="300">
</div>

그와는 다르게 학습이 어느 정도 진행된 후의 activation map 모습은 위와 같다. 앞서 확인했던 activation과는 다르게 sparse(일부 channel에만 유의미한 feature가 잡힘)한 특징이 그대로 드러나고, 이는 ReLU가 학습되는 과정에서 무의미하다고 간주되는 signal value를 출력하지 않는 형태로 학습이 진행되는 것과 어느 정도 부합하게 된다. 이러한 visualization을 통해 분석할 수 있는 사항은 ReLU activation의 단점이라고도 할 수 있는 dead activation making이며, 설계한 모든 filter channel를 유의미하게 활용할 수 없다는 단점이 있다.


# Visualize convolutional, fully-connected filters
Input에 대한 activation을 확인하는 방법도 있고, 다른 방법은 학습된 weight를 확인해보는 방법이다. 보통 초반 convolution layer를 확인하게 되면 input이 raw pixel에 해당되는 image이므로 interpret하기는 좋지만, network layer 상 깊은 곳의 fiter weight도 visualize가 가능하다. 잘 학습된 네트워크에서는 noisy한 패턴 없이 smooth한 형태를 보여주는 것이 일반적이다. 앞서 확인했던 네트워크의 구조를 일부 수정해서 학습시킨 뒤 <U>최종 학습된 네트워크</U>의 첫번째 convolutional layer와 두번째 convolutional layer의 weight를 보면 다음과 같다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219239665-2e9d0148-40b7-401c-933e-a7a32d97e429.png" width="550">
    <img src="https://user-images.githubusercontent.com/79881119/219239693-fcc82cd6-7b32-429d-82d2-4b3ba5dfde66.png" width="300">
</div>

사실 weight visualization을 위해서는 <U>kernel size가 큰 convolutional filter</U>가 필요한데, 요즘은 deeper layer를 구성하면서 receptive field를 키우는 형태의 네트워크가 많기 때문에 weight를 직접 visualize하는 것으로는 유의미한 관찰이 이루어질 것 같지는 않다. 이렇듯 visualization에 대해서 가장 처음으로 나왔던 연구라고 할 수 있는 [ZFNet](https://arxiv.org/pdf/1311.2901.pdf)이 있는데, AlexNet을 기반으로 visualization을 통해 성능 향상을 이루어내고 바로 다음 해의 ImageNet 대회에서 우승했다.


# Retrieving images that maximally activate a neuron
다른 visualization 기술에는 대량의 dataset image를 가지고, 이들을 network에 통과시키며 각 neuron의 activation을 확인해보는 것이다. 통과시키다 보면 각 neuron이 감당하는 receptive field 내에서 <U>이미지의 어떤 부분에 집중하는지</U> 확인할 수 있게 되고 이러한 방법은 딥러닝 기반 object detection 논문의 조상격인 [R-CNN](https://arxiv.org/abs/1311.2524)에서 소개되었다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219242274-f580c8cc-495e-46cb-8306-e4b0e7359975.png" width="950">
</div>

이런 방법의 문제점은 ReLU 뉴런이 사용되었을 경우 해당 뉴런은 semantic meaning을 가지지 않는다는 점이다. 오히려 <U>여러 층의 ReLU 뉴런을 특정 space의 basis vector로 간주</U>하여(신호를 끄고 키는 switch 역할을 축으로 생각해볼 수 있음) 이미지를 나타내는 좌표계로 생각해볼 수 있다. 다르게 표현하자면 visualization은 representation의 집합이라고 볼 수 있는 계에서 edge(boundary)에 속하는 요소들이 되고, 필터 weight에 해당되는 방향으로만 탐색하게 된다. Input에 대해 convolutional neural network는 선형 함수이며, trajectory를 탐색할 수 없다는 점이 representation space에서 <U>다양한 방향을 searching할 수 없다는 한계점으로</U> 이어진다([참고 링크](https://arxiv.org/abs/1312.6199)).


# Embedding code with t-SNE
Convolutional Neural Network는 image를 linear classifier에 의해서 <U>여러 class로 분리될 수 있는</U> score map을 추출한다. 여기서 생각해볼 수 있는 것은 <U>다차원의 이미지</U>를 저차원으로 embedding하여 분류하기 때문에, 저차원에서의 representation과 고차원에서의 위상이 대략 비슷할 것이다. 위에서 학습시킨 network를 그대로 사용하여 t-SNE의 결과를 비교해보도록 하자.
우선 딥러닝 네트워크가 적용되지 않은 상태에서 단순히 CIFAR-10 dataset을 t-SNE 방식으로 2차원의 manifold로 mapping하는 코드는 다음과 같다.

\`\`\`python
cls = []
embedding = []
for data in testloader:
    images, labels = data[0].to(device), data[1].to(device)
    embedding += images.view(images.shape[0], -1).cpu().numpy().tolist()
    cls += labels.cpu().numpy().tolist()

tsne = TSNE(n_components=2, random_state=0)
points = np.array(tsne.fit_transform(np.array(embedding)))
classes = np.array(cls)

plt.figure(figsize=(10, 10))
cifar = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
for i, label in zip(range(10), cifar):
    idx = np.where(classes == i)
    plt.scatter(points[idx, 0], points[idx, 1], marker='.', label=label)

plt.legend()
plt.show()
\`\`\`

위의 결과로 나온 그림을 보면 알 수 있듯이, 각 class에 대한 고차원 이미지가 단순히 2차원으로 임베딩될 경우 구분이 잘 안되는 모습을 확인할 수 있다.
<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219253195-4bdba5b5-d70e-4d98-9510-a09db50eb388.png" width="600">
</div>
이번에는 classification에 대해 학습된 네트워크를 통과한 feature map의 가장 마지막 부분($256$ dimension)을 t-SNE 시각화하는 코드를 작성해보았다.

\`\`\`python
cls = []
deep_features = []
model.eval() # resnet18
with torch.no_grad():
    for data in testloader:
        images, labels = data[0].to(device), data[1].to(device)
        _, features = model(images)
        deep_features += features[-1].cpu().numpy().tolist()
        cls += labels.cpu().numpy().tolist()

tsne = TSNE(n_components=2, random_state=0)
points = np.array(tsne.fit_transform(np.array(deep_features)))
classes = np.array(cls)

plt.figure(figsize=(10, 10))
cifar = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
for i, label in zip(range(10), cifar):
    idx = np.where(classes == i)
    plt.scatter(points[idx, 0], points[idx, 1], marker='.', label=label)

plt.legend()
plt.show()
\`\`\`
앞서 시각화했던 것과는 다르게 어느 정도 잘 분류하고 있는 것을 확인할 수 있다. 임의로 설계한 네트워크에 대해서도 classification 성능을 직접 시각화할 수 있는 것을 확인할 수 있다.
<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219253773-50ac9a70-c671-46d4-8f27-04be861e011b.png" width="600">
</div>

# Occluding parts of the image
또다른 방법으로는 input 이미지를 일부 가린 후 정답 class에 대한 confidence(확률)을 측정하는 것이다. 실험을 해보기 위해 이번엔 ImageNet에 대해 학습된 ResNet18을 사용해보았다. 코드는 다음과 같다.

\`\`\`python
import torch
import cv2
import matplotlib.pyplot as plt
import torchvision.transforms.functional as TF
from torchvision.models import ResNet18_Weights
from tqdm import tqdm
import numpy as np

model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', weights=ResNet18_Weights.DEFAULT)
model.to("cuda")
model.eval()
\`\`\`

사전 학습된 ResNet18은 \`\`\`torchvision\`\`\` 모듈에서 쉽게 가져올 수 있다. 가져온 model을 gpu에 올려주고, 학습이 아닌 evaluation을 진행할 것이기 때문에 network layer를 validation 용도로 바꿔준다. Evaluation에 사용한 이미지는 다음과 같다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219275258-243c6074-49eb-4cfe-b291-27542fe743cc.jpg" width="600">
</div>
ImageNet의 경우 CIFAR와는 다르게 class가 $1000$개로 구분된다. 그러다보니 단순히 고양이 사진이라도 <U>같은 클래스로 구분하는 것</U>이 아닌, 각 <U>품종에 따라 구분이 가능한 것</U>을 확인해볼 수 있다. 직접 사전 학습된 ResNet18에 위의 이미지를 넣기 위해 데이터 전처리를 다음과 같이 수행한다.

\`\`\`python
test_img = cv2.cvtColor(cv2.imread("test_img.jpg"), cv2.COLOR_BGR2RGB)
resized = cv2.resize(test_img, (224, 224))
input_image = TF.to_tensor(resized)
input_image = TF.normalize(input_image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
input_image = input_image.unsqueeze(0)
\`\`\`

ImageNet의 경우 이미지 resolution이 $224 \\times 224$로 학습되었기 때문에 classifier의 dimension을 맞춰주기 위해서는 이를 고려해서 resize를 진행해야한다. 또한 numpy 배열의 이미지를 tensor로 변환해주면서 normalize를 함께 진행해준다. 

예측을 하더라도 해당 클래스의 index가 어떤 class인지 알 수 없으면 파악할 수 없기 때문에 직접 index를 class로 매핑하는 파일을 가져와서 사용했다([참고 링크](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)).

\`\`\`python
with open("idx2label.txt") as f:
    idx2label = eval(f.read())

with torch.no_grad():
    output = model(input_image.to("cuda"))
    _, prediction = torch.max(output, axis=1)
    print(idx2label[prediction.item()])
\`\`\`

모델이 예측한 score를 softmax 확률값으로 치환했을때 최댓값을 가지는 index를 해당 이미지에 대한 class로 예측하게 되며, prediction 결과로 매칭된 class의 이름은 <U>'Siamese cat, Siamese'</U>으로 제대로 나온 것을 확인할 수 있다. 이제는 원본 이미지에 perturbation을 가한 뒤, prediction에 어떤 변화가 있는지 확인해보았다.

\`\`\`python
masked_img = []
mask_size = 32
for i in range(224):
    for j in range(224):
        left = int(max(0, j-mask_size//2))
        right = int(min(224, j+mask_size//2))
        top = int(max(0, i-mask_size//2))
        bottom = int(min(224, i+mask_size//2))
        
        masked = resized.copy()
        masked[top:bottom, left:right, :]=0
        masked_img.append(masked)

confidences = np.zeros(224*224)
correct = 0
total = 0

loading = tqdm(enumerate(masked_img))
for i, img in loading:
    input_image = TF.to_tensor(img)
    input_image = TF.normalize(input_image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    input_image = input_image.unsqueeze(0)
    with torch.no_grad():
        output = model(input_image.to("cuda"))
        output = torch.softmax(output, dim=1)
        _, estimate =  torch.max(output, axis=1)
        total += 1
        if estimate.item() == 284:
            correct += 1
        confidences[i] = output[:, 284].item()
        loading.set_description(f"Accuracy : [{correct}/{total}]")
\`\`\`

Mask의 크기는 $32 \\times 32$로 설정하였다. 검증 과정에서 confidence의 기준은 mask의 중심 좌표로 하였으며 validation 결과 $224 \\times 224$의 각 위치에 적용된 mask 중에 $43,348$개 만큼이 원래 class대로 예측이 된 것을 확인할 수 있었다. Confidence를 heatmap으로 확인하면 다음과 같다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219280382-0f746188-49fe-4f8b-bc3a-de6eaf35287e.png" width="800">
</div>

실제로 샴 고양이가 위치한 중앙부가 masking이 되는 상황에서 prediction이 떨어지는 것을 확인할 수 있다. 네트워크가 잘못 예측한 부분만 따로 binary로 표시하면 다음과 같다. 잘못 예측된 부분이란 masking되어 해당 class의 confidence가 떨어지는 상황에서 다른 class에 대한 confidence가 정답 label보다 커지는 상황을 의미한다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219282807-089d08ae-a2d9-424e-b3c6-eec74308e3fd.png" width="800">
</div>

좌측에 검은색으로 표시된 부분에 mask의 중앙부가 위치할 경우 prediction이 잘못되는 것을 확인하였고, 이를 원본 이미지에 적용하게 되면 <U>네트워크가 고양이의 얼굴 부분을 참고하여 prediction을 한다</U>는 형태로 explainability가 충족된다.


# Transfer learning and Fine-tuning

인공지능을 공부하게 된다면 학문 특성상 논문을 많이 읽을 수 밖에 없다. 사실상 cs231n과 같이 리뷰한 내용은 인공지능의 개요에 해당되고, deep learning이라는 분야에는 정말 다양한 specific task들이 존재하고 각 <U>task마다</U> 사용되는 방법론이나 연구 방향이 다르기 때문에 이런 <U>트렌드를 읽어내는 것</U>은 쉬운 일이 아니다. 그러나 다양한 task를 막론하고 연구를 시작하는 과정에서, 공통적으로 transfer learning과 fine tuning이라는 중요한 개념을 알아야하며 <U>사실상 항상 사용되는 개념</U>이기 때문에 알아두는 것이 중요하다.

ImageNet에서 우승한 이력이 있는 AlexNet, VGGNet 그리고 ResNet과 같은 network는 매우 큰 dataset에 대해 빠르면 며칠 혹은 몇 주 동안 training한 네트워크이다. ImageNet의 데이터셋은 120만개의 이미지를 가지고 있으며, 이보다 큰 대용량 데이터셋 ImageNet-22k의 경우에는 $14,197,122$개의 데이터셋으로 구성된다. 원하는 task에 맞게 사용하고 싶은데, 굳이 네트워크를 <U>처음부터 학습시킬 필요 없이</U>(이를 training from scratch라 부른다), 대용량 dataset에 대해 <U>유의미한 representation을 학습한 네트워크를 사용한다면</U> 오랜 학습 기간을 들이지 않고도 새로운 task에 적용하기 쉬울 수 있다는 것이다. 이렇게 이미 training된 모델의 representation을 새로운 task에 전이시키는 작업을 <U>transfer learning</U>, 그리고 transfer learning의 한 방법론 중 <U>파라미터를 미세 조정하는 방식</U>을 fine-tuning이라고 부른다. Fine tuning이란 이미 training된 모델을 그대로 사용하거나 backbone에 task specific한 아키텍쳐를 추가한 후에 freezing(파라미터를 고정), training(파라미터를 미세 조정)되는 레이어를 분리하여 원하는 목적에 맞게끔 수정하는 작업을 의미한다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219285463-f22eefa5-97ad-410c-baac-da1e89e35053.png" width="800">
</div>

### Convolutional Neural Network를 고정된 feature extractor로 생각하는 경우

ConvNet이 ImageNet에서 pretrained된 AlexNet이라고 가정하자, <U>가장 말단의 Fully connected layer</U>는 classifier고, 이 부분은 $1000$개의 class에 대한 score를 추출하게 되므로 만약 1000개의 class를 가지는 또다른 classificatioin task가 아니라면 굳이 사용될 아키텍쳐가 아니다. 따라서 이 부분을 제거하게 되고, 결국 남는 부분은 feature extraction만 진행하게 된다. <U>이 부분은 파라미터를 고정</U>시킨(freeze) 채로, 새로운 데이터셋에 대해서 새로운 classifier를 학습시키면 된다. Classifier는 앞서 배웠던 내용을 다시 생각해보면 linear SVM(Support Vector Machine)이 될 수도 있고, Softmax classifier가 될 수도 있다. 위의 그림에서 세번째 그림에 해당된다고 보면 된다. 실제로 R-CNN에서는 <U>ImageNet에서의 모델</U>을 가져온 뒤, 해당 representation을 object detection task에 transfer learining하는 방법론으로 SVM training 등 여러 fine-tuning을 사용하였다.

### Convolutional Neural Network를 fine tuning 하는 경우

물론 convolutional 네트워크(feature extraction)를 고정하지 않고, backpropagation으로 weight 조정을 하는 경우도 있다. 새로운 dataset에 대한 Overfitting을 피하기 위해서 몇몇 레이어는 고정된 상태에서 일부 레이어만 weight 조정을 할 수도 있다. 위의 그림에서 첫번째와 두번째 그림이 이를 나타낸다고 할 수 있다.


# How to fine-tune?
결국 fine-tuning할 때 미세 조정을 할 범위를 설정하는 과정에서 전략을 세워야 하는데, 수행하고 싶은 task가 있을 때 어떤 기준으로 fine-tuning하는 레이어를 선택해야할 지 정해진 가이드라인은 없다. 하지만 사람들이 일반적으로 fine-tuning하는 과정에서는 현재 가지고 있는 데이터셋의 규모, 그리고 fine-tuning을 진행할 source network가 학습한 데이터와의 distribution 차이를 고려한다. 다시 언급하지만 아래에 있는 내용들은 일반적인 경우에는 적용되지만 <U>항상 옳은 것은 아니라는 것</U>을 짚고 넘어가도록 하자.

### 새로운 Dataset 수가 적은데 기존 Dataset과 유사한 경우
데이터 수가 작기 때문에 convolutional network를 fine tuning하는 것은 overfitting의 우려가 있다. 하지만 새로운 데이터와 기존 데이터가 유사하기 때문에, high level feature(coarse feature map)이 비슷할 확률이 높다. 따라서 feature extractor는 그대로 사용(freeze)한 채로 linear classifier 부분만 training 하는 방법이 좋다.

### 새로운 Dataset 수가 많은데 기존 Dataset과 유사한 경우

가장 이상적인 상황이기 때문에 모델 전체를 학습해도 상관없고, 만약 기존에 training된 weight를 활용하고 싶다면 레이어의 일부를 fine-tuning 하여도 상관없다. 데이터 수가 충분하기 때문에 이런 경우에는 over-fitting에 대한 걱정이 적다. 따라서 충분히 성능을 높일 수 있기 때문에 네트워크 대부분을 fine-tuning하여 task-specific한 performance를 얻는 방법이 좋다.

### 새로운 Dataset 수가 적은데 기존 Dataset과 매우 다른 경우

가장 어려운 경우에 해당된다. Dataset이 매우 작기 때문에 첫번째 케이스와 같이 classifier에 대해서만 fine-tuning을 진행해야 하는 것은 맞는데, 해당 케이스와는 다르게 dataset이 상이하다는 문제가 있어 feature map이 유사하다는 assumption을 사용할 수 없게 된다. 그렇기 때문에 high level feature로부터 나온 feature map에 대해 classifier를 따로 학습하여 network의 head를 채우는 것 보다는, network 초반부의 feature acitvation에 대한 SVM을 학습하는 것이 도움이 될 수 있다고 한다. 사실 이런 경우에는 어떠한 방식을 쓰는지에 따라 성능 변화가 크기 때문에 정답이라고는 말을 못하겠다.

### 새로운 Dataset 수가 많은데 기존 Dataset과 매우 다른 경우

Dataset이 기존과 아예 다르다면 그냥 scratch(처음부터) training하는 것이 나아보이지만 그래도 실제로 성능 비교를 했을 때나 성능 수렴을 확인해보면 pre-training된 모델에 대해서 가중치 조정을 하는 것이 더 효과적인 것을 볼 수 있다. 이 경우에는 데이터셋 수가 많기 때문에 네트워크 전체를 fine-tuning하는 것이 도움이 될만한 high level feature를 얻을 수 있는 방법이다. 

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219288407-2fc71b5f-f906-4cb7-a433-f8936591ff9a.png" width="600">
</div>

Fine tuning 과정에서 고려해야할 사항은 다음과 같다. 우선 pre-training된 모델을 사용하면 구조에 제한이 걸리기 때문에 다양한 metric이나 task에 최적화된 네트워크를 구성할 수 없다는 단점이 있다. 그리고 학습 과정에서 네트워크를 처음부터 학습하는 것이 아닌 <U>optimization point를 옮기는 과정</U>이기 때문에 <U>learning rate을 작게 설정</U>하여 미세 조정을 거치게 된다. 보통 처음부터 학습하는 네트워크를 기준으로 그보다 $1/100$이나 $1/1000$ 만큼 더 적은 learning rate를 적용하여 task에 최적화하게 된다.
`,fO=`---
title: "DDPM 수식 증명만 죄다 조져버리기"
category: "ai theory"
publishedAt: "2023-04-02"
thumbnail: "https://user-images.githubusercontent.com/79881119/232325472-ed7e91ec-cd58-4826-8641-e3e110aee05d.png"
---


# 들어가며...

정말 수식 증명만 조지는 내용이라 DDPM 자체에 대한 내용은 이전 글을 참고하면 좋다. 근데 지금보니까 저 글을 쓸때도 완벽하게 이해하고 쓴 건 아닌 것 같다는 생각.. DDPM 논문 링크는 [여기](https://arxiv.org/abs/2006.11239). 이 어려운 벌써 논문이 3년이나 됐나 싶다.

# Forward and Reverse process

이미지 $x_0$에 작은 variance($\\beta$)의 **가우시안을 아주 조금씩** 계속($T$만큼) 더해가다보면 최종 output $x_T$은 $x_0$와 같은 spatial dimension을 가지는 가우시안 분포가 된다. 논문에서는 각 process에서의 variance를 스케줄링하여 **고정값으로 사용**.

### Forward process $q(x_t \\vert x_{t-1})$

$$
q(x_t \\vert x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I)
$$

그런데 알다시피 실제로 딥러닝을 학습시키려면 stochastic한 샘플링은 미분이 불가능하기 때문에 reparameterization trick을 사용한다. 각 time step에서 노이즈를 더할 때 이전 step의 output을 $\\sqrt{1-\\beta_t}$ 만큼 scaling 해주는 걸 알 수 있는데, 이렇게 하면 변수의 variance를 $1$로 유지할 수 있게됨.

$$
\\left(\\sqrt{1-\\beta_t}\\right)^2+\\left(\\sqrt{\\beta_t}\\right)^2 = 1
$$

수학적 귀납법을 통해 살펴보게 되면, $\\alpha_t = 1-\\beta_t$에 대해서

$$
\\text{For}~\\epsilon \\sim \\mathcal{N}(0, I), x_{t} = \\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon
$$

일반적인 $t$ 번째 step에서의 식이 위처럼 표현되니까

$$
x_t = \\sqrt{\\alpha_t}\\sqrt{\\alpha_{t-1}}x_{t-2} + \\sqrt{\\alpha_t}\\sqrt{1-\\alpha_{t-1}}\\epsilon_{t-2} + \\sqrt{1-\\alpha_t}\\epsilon_{t-1}
$$

위와 같이 $t>1$인 상황에서 한단계 더 확장해서 표현 가능하고, 뒤의 epsilon들을 잘 합치게 되면,

$$
\\sqrt{\\alpha_t}\\sqrt{1-\\alpha_{t-1}}\\epsilon_{t-2} + \\sqrt{1-\\alpha_t}\\epsilon_{t-1} \\rightarrow \\left(\\left(\\sqrt{\\alpha_t}\\sqrt{1-\\alpha_{t-1}} \\right)^2 + \\left(\\sqrt{1-\\alpha_t} \\right)^2\\right) \\epsilon
$$

그래서 이게 잘 정리가 된 것이 다음과 같은 식.

$$
q(x_t \\vert x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t}x_0, (1-\\bar{\\alpha}_t)I)
$$

이걸 또다시 **reparameterization**으로 표현하면 loss에서 써먹을 수 있게된다. 논문 전개과정에 주구장창 나오는 식.

$$
x_t := \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon,~\\epsilon \\sim \\mathcal{N}(0, I)
$$

### Reverse process $p_\\theta(x_{t-1} \\vert x_t)$

Forward process의 posterior를 모방하도록 학습될 녀석. 분포를 예측하는 과정임. 정방향에 유사하게 역방향을 학습하고 싶다? 어디서 본 것 같은 워딩인데 자세히 노려보면 VAE에서의 접근과 같기 때문에 결국 얘도 lower bound를 최적화해야함.

$$
p_\\theta(x_{0 : T}) := p(x_T) \\prod_{t=1}^T p_\\theta(x_{t-1} \\vert x_t)
$$

$x_T$부터 샘플링하는 과정을 위의 joint distribution probability로 표현이 가능하고,

$$
p_\\theta(x_{t-1} \\vert x_t) := \\mathcal{N}(x_{t-1}; \\mu_{\\theta}(x_t,~t), \\Sigma_\\theta (x_t,~t))
$$

Diffusion process의 가정인 ‘아주 작은 가우시안을 더해가는 과정’의 역과정은 ‘아주 작은 가우시안을 빼가는 과정’과 동일하기 때문에 각 포인트에서 **노이즈를 예측한 다음**에 빼주면서 샘플링이 가능하다.

위에서 식으로 전개한 forward process와 지금 언급하고 있는 reverse process를 ‘같게’ 만드는 것이 loss function이 될 것이다.


# Loss function

### Evidence lower bound(ELBO) in DDPM

$$
\\mathbb{E}(-\\log p_\\theta (x_0)) \\leq \\mathbb{E}_q \\left( -\\log p_\\theta(x_0 \\vert z) -\\log \\frac{p_\\theta(z)}{q_\\phi(z \\vert x_0)} \\right)
$$

위의 식이 VAE에서 온건데, ELBO의 앞쪽 term은 $z$로부터 $x$를 reconstruction하는 부분에 대한 decoder 학습을 담당하고, 뒤쪽 term은 gaussian과 같은 pre-defined 분포로 가정한 $p_\\theta(z)$와 encoder의 output $q_\\phi(z \\vert x)$의 분포가 가까워지게끔 하는 KL divergence에 해당된다. 위의 term에서 reconstruction과 KL divergence에서의 decoder 부분을 갈아끼우게 되면,

$$
\\mathbb{E}(-\\log p_\\theta (x_0)) \\leq \\mathbb{E}_q \\left( -\\log p_\\theta(z) -\\log \\frac{p_\\theta(x_0 \\vert z)}{q_\\phi(z \\vert x_0)} \\right)
$$

이처럼 되고, latent $z$를 time step에 대한 변수로 치환하면서 encoder의 변수를 제거하게 되면(DDPM에서는 parameteric 학습을 encoder에 대해 진행하지 않기 때문) 우리가 얻고자 하는 DDPM의 ELBO를 표현할 수 있다.

$$
\\mathbb{E}(-\\log p_\\theta (x_0)) \\leq \\mathbb{E}_q \\left( -\\log p_\\theta(x_T) -\\log \\frac{p_\\theta(x_{0:T-1} \\vert x_T)}{q(x_{1:T} \\vert x_0)} \\right)
$$

그리고 각 process의 joint는 Markov process이기 때문에 summation으로 풀어쓸 수 있음.

$$
\\mathbb{E}(-\\log p_\\theta (x_0)) \\leq \\mathbb{E}_q \\left( -\\log p_\\theta(x_T) -\\log \\prod_{t \\ge 1} \\frac{p_\\theta(x_{t-1} \\vert x_t)}{q(x_t \\vert x_{t-1})} \\right)
$$

뒤쪽 $\\log$랑 production 위치를 바꾸면 summation이 됨.

$$
\\mathbb{E}(-\\log p_\\theta (x_0)) \\leq \\mathbb{E}q \\left( -\\log p_\\theta(x_T) - \\sum_{t \\ge 1}\\log \\frac{p_\\theta(x_{t-1} \\vert x_t)}{q(x_t \\vert x_{t-1})} \\right)
$$

그렇다면 굳이 왜 기존의 **VAE랑 다르게 위치를 바꾸었냐**라고 본다면 직관적으로는 forward process와 reverse process를 같게끔 학습하기 위함이라고 말할 수 있지만 보다 엄밀하게 표현하자면 VAE에서 학습시키는 decoder는 **implicit conditional probability**인 $p_\\theta(x_0 \\vert z)$를 학습하는 것이 주된 목적이었다면 Diffusion에서 학습시키는 decoder는 **diffusion process**가 나타내는 **미분 방정식을 학습**하는 것이 주된 목적이 되기 때문이다. 

헌데 여기서 문제가 발생하는 것은 encoder 역할을 수행했던 diffusion process $q$는 이전 step의 sample에 대해 노이즈를 더하는 과정 $q(x_t \\vert x_{t-1})$을 수행했기 때문에 실제로 학습해야하는 그 역과정 $q(x_{t-1} \\vert x_t)$의 분포는 알 수 없다는(intractable) 것.

$$
q(x_{t-1} \\vert x_t) = \\frac{q(x_t \\vert x_{t-1})q(x_{t-1})}{q(x_{t})}
$$

본인은 이걸 한동안 이해 못했던거 같은데 쉽게 이해하는 법은 다음과 같음.

$x_{t-1}$에다가 더하는 노이즈를 알고 있기 때문에 $x_t$에 대한 확률 분포는 normal distribution으로 명확하게 알 수가 있음$(q(x_t \\vert x_{t-1}))$ 근데 실제로 각 time step에서 노이즈가 더해진 애들의 분포 자체는 알 수 없으니까 $(q(x_t),~q(x_{t-1}))$ Bayes 식에서 intractable한 term이 두 개나 생겨서 분포에 접근하기가 힘듦.

그렇기 때문에 위의 식에 모든 term에 대해 $x_0$을 조건부에 추가하면 항상 분포를 알 수 있게 되므로(normal distribution의 누적이니까) tractable하게 바꿀 수 있다.

$$
\\begin{aligned}
q(x_{t-1} \\vert x_t,x_0) =& \\frac{q(x_t \\vert x_{t-1},x_0)q(x_{t-1},x_0)}{q(x_{t},x_0)} \\newline
=& \\frac{q(x_t \\vert x_{t-1},x_0)q(x_{t-1}\\vert x_0) q(x_0)}{q(x_{t} \\vert x_0) q(x_0)} \\newline
=& q(x_t \\vert x_{t-1},x_0) \\times \\frac{q(x_{t-1} \\vert x_0)}{q(x_t \\vert x_0)}
\\end{aligned}
$$

$x_0$에 조건부가 된 식이라서, $p_\\theta(x_t \\vert x_{t-1})$와 달라진 것 아니냐는 생각이 들 수도 있지만 위의 식은 $t > 1$인 모든 latent에 대해 $x_0$에 독립 분포를 가진다는 특징이 있다(마르코프 프로세스니까). 그래서 최종 마무리가 된 loss function을 다음 식처럼 나타낸다.

$$
D_{KL}(q(x_T \\vert x_0) \\vert\\vert p_\\theta(x_T)) -\\sum_{t > 1} D_{KL} (q(x_{t-1} \\vert x_t, x_0) \\vert\\vert p_\\theta(x_{t-1} \\vert x_t)) -\\mathbb{E}_q\\left(\\log p_{\\theta}(x_0 \\vert x_1) \\right)
$$

유도 과정은 앞서 본 식에서 $t>1$인 부분에 대해 posterior를 조건부로 바꿔서 tractable하게 만들 수 있으니까, 정리하게 되면 다음과 같이 나온다.

$$
\\begin{aligned}
\\mathcal{L} \\le& \\mathbb{E}_q\\left(-\\log (p_\\theta(x_T))-\\sum_{t=2}^T \\log \\frac{p_\\theta(x_{t-1} \\vert x_t)}{q(x_t \\vert x_{t-1})} -\\log \\frac{p_\\theta(x_0 \\vert x_1)}{q(x_1 \\vert x_0)} \\right) \\newline
\\le& \\mathbb{E}_q\\left(-\\log (p_\\theta(x_T))-\\sum_{t=2}^T \\log \\left( \\frac{p_\\theta(x_{t-1} \\vert x_t)}{q(x_{t-1} \\vert x_t, x_0)} \\times \\frac{q(x_{t-1} \\vert x_0)}{q(x_t\\vert x_0)}\\right) -\\log \\frac{p_\\theta(x_0 \\vert x_1)}{q(x_1 \\vert x_0)} \\right) \\newline
\\le& \\sum_{t > 1} D_{KL} (q(x_{t-1} \\vert x_t, x_0) \\vert\\vert p_\\theta(x_{t-1} \\vert x_t))+\\mathbb{E}_q\\left(-\\log (p_\\theta(x_T))-\\log \\frac{q(x_1 \\vert x_0)}{q(x_T \\vert x_0)}-\\log \\frac{p_\\theta(x_0 \\vert x_1)}{q(x_1 \\vert x_0)} \\right) \\newline
\\le& D_{KL}(q(x_T \\vert x_0) \\vert\\vert p_\\theta(x_T)) +\\sum_{t > 1} D_{KL} (q(x_{t-1} \\vert x_t, x_0) \\vert\\vert p_\\theta(x_{t-1} \\vert x_t)) -\\mathbb{E}_q\\left(\\log p_{\\theta}(x_0 \\vert x_1) \\right)
\\end{aligned}
$$

### ELBO into objective function

위의 식은 수식일 뿐이고 실제로 최적화 과정에서 사용되는 식은 이를 가우시안 분포에 대해 다시 정리한 식이다. 위의 ELBO를 보게 되면 크게 세 부분으로 정리되는데, 이 중 가장 앞부분의 KL divergence는 diffusion process를 통해 **자연스럽게 만족되는 식**(gaussian 분포를 따라가는 것)이기 때문에 실제 loss function에서 제외된다. 원래 VAE에서는 해당 부분이 encoder의 output을 가우시안 분포로 정규화하는 KL divergence term으로 사용되는데, diffusion에서는 굳이 $q$를 학습시킬 필요가 없다보니 빠진다고 생각할 수 있다.

$$
D_{KL}(q(x_T \\vert x_0) \\vert\\vert p_\\theta(x_T))
$$

그리고 가장 마지막 부분인 log likelihood 식이 가장 이해하기 어려운데, 이를 먼저 언급하고 넘어가도록 하겠다.

$$
\\mathbb{E}_q\\left(\\log p_{\\theta}(x_0 \\vert x_1) \\right)
$$

이 식을 다시금 이해하자면, noise가 아주 조금 더해진 $x_1$에서 $x_0$로 reconstruction할 때의 모든 확률을 구해야한다. 중간 과정의 경우에는 분포 사이의 KL divergence 식이기 때문에 확률 분포를 굳이 확률값으로 매핑할 필요가 없었지만, 이 식에서는 확률값 자체를 구해야하기 때문에 적분을 취해야한다.

우선 단순화시켜 생각하기 위해, $\\log$ likelihood의 likelihood 자체만 따로 빼내서 보도록 하자.

$$
p_\\theta(x_0 \\vert x_1)
$$

$p_\\theta$는 언급했던 것과 같이 $x_1$를 통해 $x_0$을 구성하는 각 요소들을 추출할 확률 분포를 구하는 diffusion process의 역과정을 학습한 neural network이다.

$$
p_\\theta(x_0 \\vert x_1) \\sim \\mathcal{N}(x_0;~\\mu_\\theta(x_1, 1), \\sigma_1^2)
$$

확률값을 구하기 위해서는 이미지 상의 ‘모든 픽셀’에 대해 주어진 확률 분포를 적분해야만 구할 수 있다. 이때, 이미지가 $[-1, 1]$로 정규화된 상태라고 생각한다면 가우시안 분포에서 $-1$보다 작은 value는 모두 $-1$로 확률을 매핑하고 $1$보다 큰 value는 모두 $1$도 확률을 매핑한다고 생각할 수 있다. 또한 원래 이미지 전체는 $0 \\sim 255$ 의 discrete RGB value로 표현되기 때문에 다음과 같이 변수를 $1/255$로 끊어서 확률매핑이 가능하다.

예를 들어 $x_1$에서 $x_0$으로 매핑될 때, ground truth에 따르면 $0$이라는 값이 나와야한다고 가정해보자.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/232325472-ed7e91ec-cd58-4826-8641-e3e110aee05d.png" width="1000"/>
</div>


그렇다면 $p_\\theta(x_0 \\vert x_1)$는 해당 값 근방을 적분 구간으로 삼아 확률분포를 적분한 값으로 정의할 수 있는 것이다(아래 그림에서 노란색 부분의 넓이).


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/232325475-c42d8a7f-800e-4567-91a2-f2951bcf2314.png" width="900"/>
</div>


이를 모든 픽셀에 대해 곱하게 되면, 전체 픽셀 ($1\\sim D$)에 대한 joint probability를 구할 수 있다.

$$
p_\\theta(x_0 \\vert x_1) = \\prod_{i=1}^D \\int_{\\delta_{-}(x_0^i)}^{\\delta_+(x_0^i)} \\mathcal{N}(x; \\mu_\\theta^i (x_1, 1), \\sigma_1^2) dx
$$

이때 적분 구간인 $[-\\delta_-(x_0^i),~+\\delta_+(x_0^i)]$는 앞서 말했던 것과 같이 $-1$보다 작은 값들과 $1$보다 큰 값들을 각각 원래 이미지의 $-1$과 $1$에 매핑하고 나머지는 $1/255$만큼 간격을 준다고 생각할 수 있다.

$$
\\delta_+(x) = \\begin{cases}
\\infty,&\\text{if }x=1 \\newline
x+\\frac{1}{255},&\\text{if }x<1
\\end{cases},~~\\delta_-(x) = \\begin{cases}
-\\infty,&\\text{if }x=-1 \\newline
x-\\frac{1}{255},&\\text{if }x>-1
\\end{cases}
$$

### 중간과정 Optimization($L_{1:T-1}$)

결국 위에서 쭉 말한 내용은 $L_T$는 굳이 필요없다는 내용이었고 $L_0$는 확률 분포의 적분을 통해 구할 수 있다는 말이었다. 그렇다면 실제로 더해진 노이즈를 예측해서 빼는 과정인 $L_{1:T-1}$를 KL divergence 식으로 구하면 어떻게 될까?

$$
\\sum_{t > 1} D_{KL} (q(x_{t-1} \\vert x_t, x_0) \\vert\\vert p_\\theta(x_{t-1} \\vert x_t))
$$

해당 식에서 $q(x_{t-1} \\vert x_t, x_0)$를 가우시안 분포로 나타내는 증명과정은 다음과 같다. 앞서 유도한 바와 같이 기존의 prior를 다음과 같이 고쳤었다.

$$
q(x_{t-1} \\vert x_t,x_0) = q(x_t \\vert x_{t-1},x_0) \\frac{q(x_{t-1} \\vert x_0)}{q(x_t \\vert x_0)}
$$

그리고 우리는 식을 구성하는 각각의 분포를 다음과 같은 tractable한 가우시안 형태로 알고 있다.

$$
\\begin{aligned}
q(x_t \\vert x_0) \\sim& \\mathcal{N}(\\sqrt{\\bar{\\alpha}_t}x_0, (1-\\bar{\\alpha}_t)I) \\newline
q(x_t \\vert x_{t-1}) \\sim& \\mathcal{N}(\\sqrt{1-\\beta_t}x_{t-1},\\beta_tI)
\\end{aligned}
$$

오호라 위의 식을 그대로 가우시안 식으로 바꿔쓰게 되면 가우시안 앞부분에 붙는 상수는 무시하고 exponential 안의 식만 따로 정리할 수 있다.

$$
\\begin{aligned}
q(x_{t-1} \\vert x_t, x_0) \\propto& \\exp \\left(-\\frac{1}{2} \\left( \\frac{(x_t - \\sqrt{\\alpha_t}x_{t-1})^2}{\\beta_t}  + \\frac{(x_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}}x_0)^2}{1-\\bar{\\alpha}_{t-1}} - \\frac{(x_t - \\sqrt{\\bar{\\alpha}_t}x_0)^2}{1-\\bar{\\alpha}_t} \\right) \\right) \\newline
=& \\exp \\left(-\\frac{1}{2} \\left( \\frac{x_t^2 - 2\\sqrt{\\alpha_t}x_t x_{t-1}+\\alpha_tx_{t-1}^2}{\\beta_t}  + \\frac{x_{t-1}^2 - 2\\sqrt{\\bar{\\alpha}_{t-1}}x_0x_{t-1} + \\bar{\\alpha}_{t-1}x_0^2}{1-\\bar{\\alpha}_{t-1}} - \\frac{x_t^2 -2\\sqrt{\\bar{\\alpha}_t}x_0x_t + \\bar{\\alpha}_tx_0^2}{1-\\bar{\\alpha}_t}\\right) \\right)
\\end{aligned}
$$

조금 복잡해 보이긴 하는데 여기서 $x_{t-1}$ 부분에 대해서 quadratic(2차 함수) 형태가 되도록 공통 변수로 묶어주게 되면,

$$
=\\exp \\left(-\\frac{1}{2} \\left( ( \\frac{\\alpha_t}{\\beta_t} +\\frac{1}{1-\\bar{\\alpha}_{t-1}} )x_{t-1}^2 - (2\\frac{\\sqrt{\\alpha}_tx_t}{\\beta_t} + 2\\frac{\\sqrt{\\bar{\\alpha}_{t-1}}x_0}{1-\\bar{\\alpha}_{t-1}})x_{t-1} + C(x_t, x_0) \\right) \\right)
$$

위와 같이 정리할 수 있다. 해당 식을 가우시안 분포라고 생각하게 되면 평균은 $ax^2+bx+c$  형태의 이차식에서 $-b/2a$와 같기 때문에

$$
\\begin{aligned}
\\tilde{\\mu}(x_t,~x_0) =& \\frac{\\frac{\\sqrt{\\alpha}_tx_t}{\\beta_t} + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}x_0}{1-\\bar{\\alpha}_{t-1}}}{\\frac{\\alpha_t}{\\beta_t} +\\frac{1}{1-\\bar{\\alpha}_{t-1}}} = \\left( \\frac{\\sqrt{\\alpha}_tx_t}{\\beta_t} + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}x_0}{1-\\bar{\\alpha}_{t-1}}\\right) \\times \\frac{\\beta_t (1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t} \\newline
=& \\frac{\\sqrt{\\alpha}_t(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}x_t + \\frac{\\beta_t \\sqrt{\\bar{\\alpha}_{t-1}}}{1-\\bar{\\alpha}_t}x_0
\\end{aligned}
$$

위와 같이 나타낼 수 있고 분산은 $1/(-2a)$과 같기 때문에

$$
\\tilde{\\sigma}_t^2 = \\beta_t \\times \\left( \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t} \\right)
$$

이처럼 나타낼 수 있다. DDPM 논문에서는 해당 부분을 제대로 증명하지 않고 넘어가서 갑자기 붕 떴던 기억때문에 남겨놓는다.

이제 실제로 $p_\\theta(x_{t-1} \\vert x_t)$와 $q(x_{t-1} \\vert x_t, x_0)$ 간의 KL divergence를 최소화하는 식을 살펴보면 $\\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t,t),~\\Sigma_\\theta(x_t, t))$는 해당 논문에서 미리 스케쥴링해서 학습할 파라미터로 설정하지 않았기 때문에 $\\tilde{\\sigma}_t^2 = \\beta_t \\times \\left( \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t} \\right)$라고 할 수 있다. 실제 실험에서는 단순히 $t$번째 step에서의 variance인 $\\beta_t$를 $\\sigma_t^2$로 사용해도 큰 차이가 없다고 언급하는데, 이는 사실상 variance 누적곱에 해당되는 $\\bar{\\alpha}_t = \\prod_{\\tau=1}^t(1-\\beta_\\tau)$ 가 step 수가 크기 때문에 $t$에 따라 큰 차이를 보이지 않기 때문이라고 생각했다(아니라면 말고). 암튼 결론은 $\\mu_\\theta(x_t,~t)$만 예측하면 된다는 소리..

 KL divergence 식을 가우시안에 적용하게 되면 exponential이 벗겨지면서 다음과 같이 간단하게 표현 가능하다. 아래 식에서 $C$는 파라미터에 무관한 모든 term을 의미한다. 

$$
L_{t-1} := \\mathbb{E}_q \\left( \\frac{1}{2\\sigma_t^2} \\vert\\vert \\tilde{\\mu}_t(x_t, x_0) - \\mu_\\theta(x_t, t)  \\vert\\vert^2 \\right)+C
$$

근데 이 식을 그대로 쓰면 stochastic 부분땜에 미분이 불가능하다. 써 있는 $x_t$를 죄다 reparameterization($x_t = \\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon$)으로 바꾸면 된다. 근데 이렇게 쓰면 너무 장황하니까 $x_t(x_0,\\epsilon)$으로 표현하고자 함. 참고로 $\\tilde{\\mu}_t$에 있는 $x_0$도 process 상에서 $x_t$를 통해 예측되는 $x_0$이기 때문에 reparameterization하여 $x_0 = \\frac{x_t}{\\sqrt{\\bar{\\alpha}_t}}-\\frac{\\sqrt{1-\\bar{\\alpha}_t}}{\\sqrt{\\bar{\\alpha}_t}}\\epsilon$로 쓰면 된다.

$$
L_{t-1} -C = \\mathbb{E}_q \\left( \\frac{1}{2\\sigma_t^2} \\left\\vert\\left\\vert \\tilde{\\mu}_t\\left(x_t(x_0,\\epsilon), \\frac{x_t(x_0,\\epsilon)}{\\sqrt{\\bar{\\alpha}_t}}-\\frac{\\sqrt{1-\\bar{\\alpha}_t}}{\\sqrt{\\bar{\\alpha}_t}}\\epsilon\\right) - \\mu_\\theta(x_t(x_0,\\epsilon), t) \\right\\vert\\right\\vert^2 \\right)
$$

앞서 구했던 식에 대입하면,

$$
\\begin{aligned}
\\tilde{\\mu}(x_t,~x_0) =& \\frac{\\sqrt{\\alpha}_t(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}x_t + \\frac{\\beta_t \\sqrt{\\bar{\\alpha}_{t-1}}}{1-\\bar{\\alpha}_t}x_0 \\newline
=& \\frac{\\sqrt{\\alpha}_t(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}x_t(x_0,\\epsilon) + \\frac{\\beta_t \\sqrt{\\bar{\\alpha}_{t-1}}}{1-\\bar{\\alpha}_t}\\left(\\frac{x_t(x_0,\\epsilon)}{\\sqrt{\\bar{\\alpha}_t}}-\\frac{\\sqrt{1-\\bar{\\alpha}_t}}{\\sqrt{\\bar{\\alpha}_t}}\\epsilon\\right) \\newline
=& \\frac{1}{\\sqrt{\\alpha_t}}x_t(x_0, \\epsilon)-\\frac{\\beta_t}{\\sqrt{(1-\\bar{\\alpha}_t)\\alpha_t}}\\epsilon \\newline
=& \\frac{1}{\\sqrt{\\alpha_t}}\\left( x_t(x_0, \\epsilon)-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon\\right)
\\end{aligned}
$$

위와 같다. 따라서 다시 식에 해당 값을 대입하게 되면,

$$
\\mathbb{E}_q \\left( \\frac{1}{2\\sigma_t^2} \\left\\vert\\left\\vert \\frac{1}{\\sqrt{\\alpha_t}}\\left( x_t(x_0, \\epsilon)-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon\\right) - \\mu_\\theta(x_t(x_0,\\epsilon), t) \\right\\vert\\right\\vert^2 \\right)
$$

요렇게 됨. 그런데 여기서 네트워크는 굳이 $x_t(x_0, \\epsilon)$를 구할 필요가 없게 된다. 왜냐면 time step-$t$에서는 이미 input으로 $x_t$가 주어지고, $x_{t-1}$를 만들기 위한 분포 예측 과정이므로 forward 과정에서 stochastic하게 더해진 epsilon 부분만 예측하면 되는 것.

$$
\\mu_\\theta(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}}\\left( x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}}_t} \\epsilon_\\theta(x_t, t)\\right)
$$

따라서 $x_t$에서 $x_{t-1}$를 샘플링할 때 계산하는 것은 $x_{t-1}$에 대해 예측된 평균인 $\\frac{1}{\\sqrt{\\alpha_t}}\\left( x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}}_t} \\epsilon_\\theta(x_t, t)\\right)$에 사전에 정의된 variance term인 $\\sigma_tz$, $z\\sim\\mathcal{N}(0, I)$을 더하면 된다.

$$
x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\left( x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}}_t} \\epsilon_\\theta(x_t, t)\\right)+\\sigma_tz
$$

그렇기 때문에 $L_{1:T-1}$에 대한 loss는 아주 간단하게 표현 가능하다($x_t$에 대한 부분은 제외하고 예측하면됨).

$$
\\mathbb{E}_{x_0,\\epsilon} \\left( \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t(1-\\bar{\\alpha}_t)} \\parallel \\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t) \\parallel^2\\right)
$$
`,gO=`---
title: "Diffusion LLM의 학습 과정 및 작동 방식에 대하여"
category: "ai theory"
publishedAt: "2025-07-09"
thumbnail: "https://github.com/user-attachments/assets/dc200d92-ac43-4245-af0c-c0a93fdf7b0a"
---

# 일반적인 LLM은 어떻게 동작할까?

**LLM**으로 유명한 일반적인 생성형 트랜스포머(GPT, Claude 등)는 **오토레그레시브(autoregressive)** 방식으로 학습을 진행하고, 추론도 진행하게 된다. 모델은 문장을 왼쪽에서 오른쪽으로, **한 번에 한 단어씩** 생성하며 매 스텝마다 직전까지의 토큰을 조건으로 삼는다. 사람이 글을 순차적으로 쓰는 것과 어찌보면 동일하게 작동하는 것이다. 그렇다면 꼭 ‘한 번에 하나씩’이어야 할까?

## Autoregressive Generation (오토레그레시브 생성)

오토레그레시브 생성에서는 다음의 **토큰 (예측되는 토큰)이 이전 모든 토큰에 의존**한다. 이미 쓴 단어만 볼 수 있는 상태에서 다음 단어를 고르는 것과 같다. 즉, **이후의 예측**은 모두 **이전의 예측을 전제로 수행**하게 된다.

- 토큰 3은 1, 2에, 토큰 4는 1, 2, 3에 의존 …
- 완전히 순차적이므로 **병렬화가 어려운 상황이다.**
- 모델이 커질수록 토큰당 연산량(flops)이 늘어나 전체 응답 시간을 끌어올린다.

학습도 마찬가지다. 모델은 미래 토큰에 **어텐션하지 못하도록 마스킹된** 상태에서 다음 토큰을 예측하는 태스크를 수행한다. LLM의 학습 방식을 그림으로 표현하면 다음과 같다.

### 토크나이징

LLM은 다양한 형태의 입력(사용자의 요청)에 대해 완성된 글을 만들어내는 과정을 수행한다.

<div align="center">
    <img width="1000" alt="Image" src="https://github.com/user-attachments/assets/dc200d92-ac43-4245-af0c-c0a93fdf7b0a" />
</div>

예컨데, 사용자가 위와 같은 요청을 전달했다면 LLM에 추론시키기 전, 이를 \`토크나이즈\`하는 과정이 필요하다. Tokenize란 다양한 형태의 언어(영어, 한국어, 독일어, 스페인어, 일본어, 중국어, …)로 구성된 문장을 만들어낼 수 있는 최소 단위의 토큰에 대한 숫자로 바꾸고, 이를 LLM 모델의 입력으로 사용하는 것이다.

그렇다면 이 ‘최소 단위’는 어떻게 정의할까?

### 가장 이상적인 방식의 토크나이징

물론 위에 나온 그림 예시와 같이 단어 단위로 하면 심플하겠지만, 세상에 존재하는 모든 단어를 하나의 숫자에 매핑하고자 하는 것은 너무나 헤비한 일이다.

<div align="center">
    <img width="600" alt="Image" src="https://github.com/user-attachments/assets/64472678-e7b6-4383-b2f5-9c2d2bcaa499" />
</div>

애초에 단어의 기준을 공백 문자를 기준으로 잡는다고 하여도, 특정 단어에 대해서 표현 방식이 다양해질 수 있는 경우에는 최소 단위를 제대로 정하지 않으면 단어를 숫자로 표현하기 위해 너무 많은 데이터양이 요구될 수 있다.

그렇다고 해서 알파벳 단위 등 하나의 캐릭터 (\`char\` ) 단위로 분절하면 길이가 길어진다는 문제가 있다. 앞서 작성했던 글 중 Transformer가 가지는 최대의 단점 중 하나가 Context 길이가 너무 길어지면 연산량이 극심해진다는 부분이었다.

따라서 대부분의 LLM 구조에서는 BPE 방식을 사용하게 되고, 해당 방식은 UTF-8에서 빈번하게 등장하는 byte pair를 새로운 단어로 지정하는 방식으로 인코딩하게 된다. 해당 알고리즘에 대한 설명은 생략하도록 하겠고, 그냥 

**‘다양한 단어를 표현할 수 있는 최소 단위이면서 context length가 너무 길어지지 않게 할 수 있는 방식’**이라고 이해하면 된다.

### 학습 과정 및 단어 예측

학습 과정은 ‘마스킹 (\`masking\`)’이라는 방식을 통해 병렬 처리가 가능하다. 오토레그레시브 생성 방식의 특성상, 이전 단어를 알아야 다음 단어를 예측하는 Causality(인과성)이 보장되야하기 때문에 추론 과정은 병렬화가 어려우나, 학습 단계에서는 단순이 이후 단어가 없는 것처럼 마스크를 씌우면 되기 때문이다.

예컨데 다음과 같은 문장이 있다고 가정하면,

> **Causality is an influence by which one event,** process, state, or object (*a* *cause*) contributes to the production of another event, process, state, or object (an *effect*) where the cause is at least partly responsible for the effect, and the effect is at least partly dependent on the cause. The cause of something may also be described as the reason for the event or process.
> 

학습하고자 하는 특정 토큰이 있으면 해당 토큰을 포함하여 뒤쪽을 모두 가리는 마스크를 준비하는 것이다. 우리가 어렸을 적 영어 단어를 외우기 위해 한글 뜻 부분을 손으로 가리면서 공부한 것과 같다. 예를 들어 첫 줄에 있는 단어인 ‘\`one event\`’ 뒤쪽의 단어인 ‘\`process\`’를 학습하고자 한다면,

> **Causality is an influence by which one event,** [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]
> 

위와 같이 뒤쪽 단어들을 모두 참고할 수 없게 한 후 직후 단어를 예측하게끔 한다.

<div align="center">
    <img width="600" alt="Image" src="https://github.com/user-attachments/assets/8477f525-9b25-4437-96a8-d9b631e89a99" />
</div>

그런데, 이때 마스킹은 0, 1로 구성된 하나의 array 형태처럼 작동하므로, 동시에 여러 토큰을 예측하게끔 하고, 이 전체의 결과를 통합하여(평균내어) 학습을 진행하는 구조가 된다. 시퀀스 길이 $T$, 토큰 집합 $\\mathcal{V}$ 그리고 모델이 예측한 확률 $hat{p}_{t, v}$와 각 정답에 해당되는 $y_{t, v}$에 대해

$$
\\mathcal{L}_{	ext{CE}}
    = -\\sum_{t=1}^{T}\\sum_{v=1}^{|\\mathcal{V}|} y_{t,v}\\,\\log\\!\\bigl(\\hat{p}_{t,v}\\bigr)
    = -\\sum_{t=1}^{T} \\log\\!\\bigl(\\hat{p}_{t,y_t}\\bigr),
$$

위와 같이 cross entropy가 적용되어 분포를 맞추며 학습이 진행된다. 결국 LLM은 수없이 많이 보게되는 글을 학습하여 특정 단어 이후에 가장 그럴듯한 단어가 오게끔 학습을 수행한다.

# 텍스트 디퓨전은 어떻게 작동할까

Google은 I/O에서 최신 Gemini 제품군을 공개하면서 **Gemini Diffusion** 이라는 새 디퓨전 아키텍처 기반 모델도 함께 선보였다. 발표 자료에 따르면, 이 모델은 추론, 수학, 코딩 등 여러 지표에서 기존 SOTA 모델과 대등한 성능을 보인다.

<div align="center">
    <img width="600" alt="Image" src="https://github.com/user-attachments/assets/6296c491-372e-4528-acc9-9cccf54da636" />
</div>

사실 그동안 텍스트 생성 분야에서 디퓨전 모델은 ‘재미있는 연구’ 정도로만 여겨졌고, 실전 성능은 다른 아키텍처에 뒤처져 왔다. 기존의 LLM이 가지는 Autoregressive한 구조와는 다르게 디퓨전 방식은 다음에 올 토큰 단위로 예측하는 것이 아니라 완성되지 않은 문장이 모두 마스킹된 상태로 가정하고 모든 토큰을 한꺼번에 샘플링하는 방식을 사용한다. 따라서 이렇게 한번에 여러 토큰을 예측하는 과정이 그럴듯한 결과를 낼 수 있을지 명확하지 않았기 때문이다. (과연 언어 생성에 최적화된 샘플링 구조일 것인지 알 수 없다)

## Text Diffusion

이미지 디퓨전은 **무작위 노이즈**에서 출발해 여러 스텝에 걸쳐 노이즈를 제거(denoise)하면서 이미지를 복원한다. 텍스트 디퓨전은 이미지와는 다르게 픽셀 대신 토큰을 ‘denoise’한다.

1. 문장을 \`[MASK]\` 토큰으로 가득 채운다 (텍스트판 노이즈).
2. 여러 스텝에 걸쳐 마스크를 점차 실제 토큰으로 교체해 간다.

Google은 구체적인 학습 절차와 아키텍처를 공개하지 않았지만, **LLaDA** 같은 최근 논문을 통해 대략적인 방식을 짐작할 수 있다.

<div align="center">
    <img width="1000" alt="Image" src="https://github.com/user-attachments/assets/bae0d169-d559-4513-bb17-5ca2df739ae3" />
</div>

이들은 **언어 모델의 핵심은 오토레그레시브 특성이 아니라 ‘다음 단어 분포’를 정확히 모델링**하는 능력이다. 그렇다면 문단의 임의 위치에 있는 단어도 예측할 수 있어야 하며, 나머지 빈칸도 채울 수 있어야 한다.

## Diffusion Pretraining

한 번에 한 단어만 생성하지 않을 것이므로, **시퀀스 전체를 입력**으로 받아 **시퀀스 전체를 출력**으로 내놓는 **seq-to-seq 트랜스포머**를 학습한다.

- 출력층은 모든 위치의 토큰을 예측하지만 **손실(loss)은 오로지 마스크된 위치**만 계산.
- **BERT**의 Masked 모델링과 거의 동일하다고 볼 수 있다.
- 추후 **지도 미세조정(SFT)** 시에는 프롬프트 부분은 그대로 두고, **응답 부분**만 마스크 처리해 학습.

## Diffusion Inference

지금까지는 평범한 seq-to-seq처럼 보이지만, **한 번에 전체 응답을 예측**하면 성능이 떨어진다는 연구가 많다. 여기서 이를 해결하기 위한 수단으로 디퓨전이 등장한다.

1. **전부 마스크**된 상태에서 시작.
2. 모델로 **모든 위치를 예측**.
3. Random 또는 Confidence 등으로 **일부 토큰만 확정(**비율은 time step에 따라 달라짐**)**하고 나머지는 다시 마스크.
4. **다시 추론** → 2~3 단계를 **$T$스텝** 만큼 반복.

해당 과정을 거치게 되면, 초반에는 **약 90%** 정도를 다시 마스크하게 되고, 후반에는 **약 10%** 정도를 다시 마스크해서 추리하게 된다. 이때 한 스텝에 **2개 이상의 토큰을 확정**하고, **총 스텝 수 $T$를 오토레그레시브 길이보다 작게** 잡으면 **추론 속도**를 크게 단축할 수 있다. 즉, 예측 부분을 전체 마스킹 → 2. 예측 → 3. 재마스크 → 루프 (반복)

# 결론 및 개인적인 견해

### 정말 주류가 될 것인가

Google은 앞으로 프런티어 모델의 주류가 디퓨전이 될 것이라 전망한다.

- 속도가 빠르다: LLM이 에이전트·코드 생성처럼 루프형 워크플로에 쓰일수록 속도 이점이 절실해진다.
- GPU 자원 절감: 빠른 추론은 **time-on-chip**을 줄여 **칩 수요**를 낮춘다.

### 그러나 기존의 알고리즘을 사용할 수 없다는 큰 단점

다만 디퓨전이 **오토레그레시브만큼 강력한 언어 모델링**을 구현할 수 있을지는 아직 직관이 없다. **Chain-of-Thought**처럼 이전 논리를 순차적으로 따라야 하는 기존의 LLM 알고리즘을 전혀 사용할 수 없기 때문이다. LLM에서 context engineering과 prompt engineering이 주된 흐름으로 성장한 대표적인 이유가 Autoregressive process에 있었는데, 만약 디퓨전 모델이 주가 된다면 지금 딥시크 형태의 추론 모델은 구현이 어려울 것으로 보인다.

그리고 사실 명확하게 따지자면 이미지 diffusion 모델에서 정의한 학습법과 전혀 다르기 때문에 Diffusion model이라기보단 Autoregressive process를 병렬적으로 진행하되 step을 두어 답변을 개선하는 방식으로 기존의 한계를 해결하려한 것이다.`,bO=`---
title: "Denoising diffusion probabilistic model의 개요"
category: "ai papers"
publishedAt: "2022-11-28"
thumbnail: "https://user-images.githubusercontent.com/79881119/209054523-a52b7452-4e0e-4fa3-8a19-14724a491fc1.png"
---

## Score matching network

Diffusion model은 score matching network로부터 나왔다고 한다. 그렇다면 대체 score matching network는 무엇일까?


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209054523-a52b7452-4e0e-4fa3-8a19-14724a491fc1.png" width="700"/>
</div>
 

Score matching network의 개념을 간단하게 소개하자면 위와 같다. PDF(확률 밀도 함수)의 gradient를 score라고 정의해보자. 이러한 정의에는 energy based 및 normalized probability에 대한 내용이 전제가 되어야 하지만 이거는 아래에서 좀 더 자세히 설명하고 우선 간단하게만 개념을 보자면, gradient는 어떤 함수의 위치에서 정의되고, 그 위치에서 ‘함숫값을 가장 크게 증가시킬 수 있는 방향’으로 정의된다.

그렇기 때문에 흔히 딥러닝에서 사용하는 gradient based learning은 loss 함수의 특정 위치에서의 미분값(그래디언트)을 구하고, 이 그래디언트의 역방향(함숫값을 가장 크게 감소시킬 수 있는 방향)을 통해 loss function을 최적화한다. 이러한 방식을 gradient descent하고 부른다.

마찬가지로 이를 PDF(확률 밀도 함수)에서의 샘플링 관점에서 보면, 확률 밀도 함수의 특정 위치에서 예측한 score(PDF의 그래디언트)가 곧 ‘보다 그럴듯한 샘플’의 방향이 되고, 이 방향대로 샘플을 생성해간다면 궁극적으로는 그럴듯한 샘플을 만들 수 있다는 것이 score matching의 개념이다.

위의 예시 그림을 보면 MNIST(손글씨)를 예시로 가져왔는데, 노이즈로부터 시작하여 점차 gradient를 따라 올라가다보면 깔끔한 샘플이 나오는 과정과 유사하다고 생각하면 된다.

그래서 대체 이런 score matching 개념은 어떻게 나온 것인지, 궁극적으로는 해당 메소드가 DDPM으로 이어지게 된 역사에 대해 설명해드리고자 한다.

## Score matching to NCSN

Energy based 머신러닝 접근법에서는 에너지는 정규화되지 않은 negative log likelihood라고 정의했다. 이에 대한 역함수로 보면 깁스-볼츠만 분포를 정규화하게 되면 확률 분포로 생각할 수 있다는 것

즉 우리가 정말 알고싶은 $p_\\theta(x)$에 대해서 알아내는건 어려워서, $p_\\theta(x) = \\frac{e^{-f_\\theta(x)}}{z_\\theta}$ 이와 같이 우리가 접근할 수 있는 function $f$에 대한 normlalized probability로 접근하고 싶은 것이고, 여기서의 $z_\\theta$는 $e^{-f_\\theta(x)}$를 모든 $x$에 대해 적분한 결과가 된다.

$$
z_\\theta = \\int_{x\\sim X} e^{-f_\\theta(x)}dx
$$

직접 확률을 구하기 힘들어서 결국에 이런 방식으로 접근을 했는데도 여전히 $z_\\theta$를 구하기 힘들다는 문제가 생긴다.

이런 비슷한 문제는 뉴럴 네트워크에서도 똑같이 존재한다. 네트워크를 수많은 파라미터 weight에 대해 최적화하고 싶은데, 결국에 네트워크를 대표하는 ‘함수’ 구조를 analytic하게 표현할 수 없기 때문에 gradient 방식으로 최적화를 진행했던 것이고, 이를 위해 네트워크를 미분 가능한 함수로 정의를 내리고 충분한 데이터로 학습시킨다.

결국 우리가 신경써야할 부분은 모든 $x$에 대해 적분된 $z_\\theta$가 아니라 $p_\\theta(x)$가 최적화될 방향성이다. 그렇다면 위의 식에 negative log를 취하고 $x$에 대해 gradient를 구하게 되면,

$$
\\nabla_x \\log p(x) = -\\nabla_x f_\\theta(x) 
$$

확률의 log likelihood의 gradient에 대해서는 intractable한 부분이 없어진 것을 볼 수 있다. 따라서 좀 더 구체적으로 언급하자면 score matching 방식에서의 score는 probability distribution의 log likelihood를 통해 정의할 수 있는 어떤 함수의 미분이고, 결국 이게 generation의 목적이 된다. 이러한 score matching을 예측하는 score estimation에 대한 방법이 2005년에 소개가 되었고 혹시라도 해당 논문이 궁금한 사람들을 위해 링크를 남겨두도록 하겠다.

[논문링크](https://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf)

논문을 보고 싶지 않은 분들을 위해 간략히 설명드리면, 해당 논문에서는 말 그대로 analytic하게 계산된 결과를 증명해내었고, 증명의 결과 parameter $\\theta$에 대해 관련된 term은

$$
\\int_x p_{data}(x)\\left(\\frac{1}{2} (\\nabla_x \\log p_{\\theta} (x))^\\top (\\nabla_x \\log p_{\\theta} (x)) - Tr(\\nabla_x^2 \\log p_{\\theta} (x))\\right)
$$

위와 같이 정리된다. 결국 문제가 뭐냐면 gradient는 $x$의 차원 수에 따라 연산량이 결정되는데, 이미지와 같이 큰 모달리티에서는 연산 속도가 너무 심하다는 것.

결국 그래서 score matching은 VAE나 flow based, GAN 등 샘플링이 용이한 다른 방법들에 비해 주목받지 못했고, 학습 속도가 생명인 deep learning에서 거의 사용되지 않는 개념으로 마무리될 뻔 했다.

그러다가 이걸 해결하고자 denoising autoencoder 방식을 채택한 NCSN(Noise-conditioning score estimation network)가 등장하였고, 다른 표현으로는 Annealed Langevin dynamics based score matching network라고도 부른다.

해당 논문도 수식이 어마어마하므로 정말 간단하게만 소개하자면, 앞서 score의 jacobian을 구하고 trace를 연산하기가 너무 heavy했기 때문에 Large scale 데이터에 적용하기 힘들었던 기존의 식을 다르게 바꾸었다.

원래의 데이터에 noise 분포를 추가하게 되고(보다 구체적으로는 가우시안 노이즈), 이를 denoising하는 과정으로 원래의 분포를 알 수 있다는 것. 물론 이런 근사화 매커니즘이 성립하려면 각 step에서 추가되는 noise의 크기가 매우 작아야된다.

$$
q_\\sigma(\\tilde{x}) = \\int_x q_\\sigma(\\tilde{x} \\vert x) p_{data}(x) dx
$$

아주 작은 노이즈 분포가 더해졌다는 가정 하에

$$
\\nabla_x \\log q_\\sigma(x) \\approx \\nabla_x \\log p_{data}(x)
$$

이외에도 sliced score matching 방법도 제시되었지만, 해당 방법은 projection에 대해 denoising 방식보다 4배의 cost가 든다는 점에서 따로 설명은 하지 않겠다.

아무튼 이러한 방식을 토대로 원래의 데이터에 미리 정의된 가우시안 분포의 노이즈를 더하고, 이를 제거하는 과정으로 학습을 진행하면 원래의 score matching을 근사시킬 수 있다는 개념이 denoising autoencoder를 활용한 NCSN의 개요.


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209054526-dd0c2779-e177-48a6-a93a-86416c5dfb7a.png"/>
</div>
 

샘플링에서는 Markov process 방식을 활용하는데, noise가 추가된 데이터 $\\tilde{x}_{t-1}$를 통한 score 예측이 해당 위치에서 다음 샘플로의 방향이 될 것이고(앞서 설명했던 것과 같이 보다 그럴듯한 샘플로의 방향) 여기에 diffusion term $z_t$를 더해주는 것이 Langevin dynamics 방식이다.

$$
  x_t = x_{t-1}+\\frac{\\epsilon}{2}\\nabla_x \\log p(x_{t-1}) + \\sqrt{\\epsilon}z_t
$$

랑쥬뱅 동역학은 brownian motion에서의 SDE solution과 같은 형태다. 많은 증명 과정이 생략된 식인데, 결론적으로 말하자면 SDE solution을 discrete approximation한 형태 중 가장 유명한 식이 위와 같이 표현된 식이다  $z_t$는 평균이 0, 분산이 1인 normal distribution이라고 보면 되고 $\\sqrt{\\epsilon}$이 표준편차로써 re-parameterization하는 구조가 된다. 그러나 위와 같은 동역학이 가지는 문제

1. Manifold hypothesis
2. Inaccurate score matching in low density region
3. Slow mixing of Langevin dynamics

때문에 실질적으로는 위와 같은 방식 대신 Annealed Langevin sampling 방식을 채택한다.


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209054528-c0577f2d-d1ef-4cd4-b887-7ab175ca8002.png" width="700"/>
</div>
 

Manifold 문제란 위와 같이 3차원 공간 상에서 실질적으로 데이터가 2차원 manifold(저차원의 특정 위상이라 보면 된다)에 놓이는 문제로, 이럴 경우 score matching은 고차원의 ambient space에서 정의되기 때문에 제대로 된 샘플링이 힘들다는 가정이다.  Low density region이란, 학습 과정에서 분포의 주가 되는 위치에서는(샘플이 몰려있는 부분) score 예측이 잘되는데, 그게 아닌 부분에서 score 예측이 부정확해진다는 문제고, Slow mixing of Langevin dynamics는 서로 다른 분포가 다른 scale로 weighted mixing되어있는 경우 이에 대한 차이를 샘플링 과정에서 증명할 수 있어야 하는데 기존 Langevin dynamics는 작은 크기의 step size로 이를 증명해낼 수 없다는 문제다.

위의 내용들은 너무 복잡하기 때문에 패스하고, 암튼 그래서 노이즈를 점차 줄여가며 샘플링한다고 생각하면 된다.


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209054531-8fb22553-cf96-4d3d-b0d4-f1b934f321c3.png" width="700"/>
</div>
 

더 얘기하고 싶은게 많지만 암튼 이런 denoising하는 형태로 score estimator를 학습할 수 있다고 싶고 넘어가도록 하자.

## What diffusion actually learns?

Diffusion(디퓨전) 모델은 학습에 있어 다음과 같은 목적을 지닌다. 최대한 간단하게 짚고 넘어가기 위해 생성 모델 종류에 대한 그림을 보면 좋을 것 같다.


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209054535-562ac052-0e4a-44f2-9e29-7b05e07b69f1.png" width="500"/>
</div>
 

GAN은 생성 모델이 만들어낸 이미지를 기준으로 생성된 가짜 이미지인지, 진짜 데이터 분포에 존재하는 이미지인지 구별자(discriminator)가 구분하는 형태로 생성자를 학습시킨다. 그리고 VAE는 그와는 다르게 embedding space $Z$로 매핑하는 인코더의 도움을 통해 데이터 $x$를 manifold space로 보내고, 이를 활용하여 생성자인 decoder를 학습한다. 이 때, 실제 데이터의 분포인 $p(x)$를 구할 수 없다는 점을 토대로, 실제로 간단하게 정의 가능한 latent space $Z$의 분포를 조건부로 학습하게 된다. 또한 VAE식에서 볼 수 있는 ELBO(Evidence Lower Bound)를 통해 간접적으로 확률을 maximize하게 된다. Flow-based model은 특정 샘플을 latent로 인코딩할 때 사용되는 flow 함수를 정의하고, 만약 이를 정의할 수 있다면 역으로 latent로부터 샘플링이 가능하다는 가정 하에 제안된 형태가 되겠다. 

Diffusion은 이러한 전반적인 생성 모델의 아이디어와 크게는 다르지 않다. Diffusion의 뜻은 확산을 뜻하는데, 흔히 물에 잉크를 한 방울 떨어뜨리거나 공기 중에서 향수가 퍼지는 형태를 생각해볼 수 있다. 이는 곧 브라운 운동(Weiner process)와도 연관되는데, 이러한 형태가 바로 Diffusion에서 말하는 ‘Forward process’, 그리고 이를 통해 역으로 학습된 parameterized network를 통해 노이즈로부터 샘플링을 진행하는 과정은 ‘Reverse process’라고 부른다.

## Forward process in DDPM

DDPM paper에서는 샘플에 노이즈를 점차 더해가는 과정을 forward로 정의했다. 노이즈를 더해가는 과정도 어찌 생각해보면 encoder를 학습하는 것과 같이 reparameterization trick을 사용해서 학습될 수  있는 부분이지만, 해당 논문에서는 이를 또 하나의 가능성으로 두었을 뿐 실제로 실험은 고정된 형태로 forward process에서의 노이즈를 정의했다*.

*실제 논문에서 Experimental detail에서 언급한 형태는 $\\beta_t$ scheduling이었는데,  constant, linear, and quadratic($T = 1000$, $\\beta_1 = 10^{-4}$ to $\\beta_T = 0.02$)라고 한다.


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209054536-5010c130-bfea-43a1-8968-9186013c8799.png" width="700"/>
</div>
 

DDPM에서의 forward process는 바로 이전 단계($X_{t-1}$)를 기준으로 다음 단계($X_t$)에 노이즈를 더하게 되는데, 이 과정을 Markov process로 진행하게 된다. 마르코프 과정이란 다음 state가 현재 state에만 의존하는 형태다. 위의 그림에서 볼 수 있듯이 원본 이미지($X_0$)에 아주 작은 노이즈가 더해져서 그 다음 상태인 $X_1$를 만들고, 이를 $T$ 만큼 반복하여 완전한 노이즈($X_T$)가 되도록 하는 과정이다. 논문에서 사용되는 forward process의 term은 $q(X_{t} \\vert X_{t-1})$라 볼 수 있다. 논문에서는 미리 정해둔 variance schedule $\\beta$를 사용, 이를 통해 다음과 같은 가우시안 노이즈를 더해가는 일련의 프로세스를 $q$로 정의하였다.

$$
q(x_t \\vert x_{t-1}) := N(x_t; \\sqrt{1-\\beta_t}x_{t-1},~\\beta_t\\rm{I})
$$

## Reverse process in DDPM

DDPM에서 각 $t$에 대해 생성된 노이즈 첨가 샘플들은 이후에도 설명하겠지만 역과정을 학습하기 위한 조건부로 쓰인다. 즉, 우리가 얻고자 하는 생성 모델은 flow based model과 같이 표현하자면 $q(X_{t-1} \\vert X_t)$와 같지만, 실제로는 이를 바로 얻을 수 없기 때문에 $p_{\\theta}(X_{t-1} \\vert X_t)$를 학습하고자 하는 것이다. Forward와 반대로 Reverse는 노이즈로부터 노이즈를 제거하는 작업이 되는데, 결국 특정 신호에 대해서 그보다 매우 작은 가우시안 형태의 잡음을 지속적으로 더하는 과정이나 빼는 과정은 학습되는 분포에 있어서 크게 다르지 않다는 것을 알 수 있기 때문에, 실질적으로 노이즈로부터 샘플을 만들어내는 과정을 모방할 수 있게 되는 것이다. 논문에 나온 표현을 인용하면,

$$
p_{\\theta}(x_{0:T}) := p(x_T)\\prod_{t=1}^T p_{\\theta}(x_{t-1} \\vert x_t)
$$

$\\theta$로 parameterized된 reverse process network $p_\\theta$는, 각 시점의 latent$(x_{1,~2,~\\cdots,~T})$를 조건부로 그 다음 state로 샘플링하게 된다. 여기서 각 state에 따라 샘플링하는 과정은 denoising score matching에서의  Langevin dynamics와 유사하다. Langevin dynamics가 사용되는 형태의 논문은 score based model에 대한 논문을 참고하면 좋다. 간단하게만 설명하자면 노이즈를 보고 그 다음 step을 결정하는데, 이 때 step의 방향은 최대한 diffusion model이 학습하는 과정에서 참고한 샘플들의 형태와 유사해지는 형태가 될 것이다.

## How to define Loss function?

위에서 간단하게 언급된 내용을 통해 우리는 결국 $p_\\theta$를 최적화하는 게 목적임을 알 수 있다. 모든 딥러닝 모델은 각 학습 형태나 목적에 맞는 loss function(cost function)이 있는데, 이는 바로 VAE에서 사용된 ELBO 식을 참고하면 비교적 쉽게 유도할 수 있다.

$$
  \\begin{aligned}
    \\log p(x) =& E_{z\\sim q(z \\vert x)}(\\log p(x)) \\newline
    =& E_{z} \\left(\\log \\frac{p(x \\vert z)p(z)}{p(z \\vert x)} \\right) \\newline
    =& E_{z} \\left( \\log \\frac{p(x \\vert z)p(z)}{p(z \\vert x)} \\frac{q(z \\vert x)}{q(z \\vert x)} \\right) \\newline
    =& E_z (\\log p(x \\vert z))-E_z \\left( \\log \\frac{q(z \\vert x)}{p(z)} \\right) + E_z \\left( \\log \\frac{q(z \\vert x)}{p(z \\vert x)} \\right)
  \\end{aligned}  
$$

$p(x)$를 직접 구할 수 없기 때문에, 우리는 미리 알고 있는 $z$를 통해 이를 추정 가능하다고 생각하는 것이다.  Prior $z$에 대해서 $p(x)$를 조건부로 알게 되는 $p(x \\vert z)$를 likelihood로 삼아 posterior로 나눠주게 되면 추정할 수 있는 것이다. 그런데 $x→z$ 과정은 encoding, $z → x$ 과정은 decoding으로 정의할 수 있으며, 실제로는 Encoder인 p가 forward process를, Decoder인 q가 reverse process를 담당하기 때문에 이를 사용하여 KL divergence 식을 만들어낼 수 있다. 실제로 획득하기 힘든 역과정에 대해서 decoder의 시작점인 $z$의 분포를 알고 있다면 encoder가 이러한 분포를 따라가게끔 학습하면 될 것이고, DDPM과는 다르게 VAE에서 encoder와 decoder는 모두 학습 가능한 파라미터인 $\\phi$ 와 $\\theta$ 로 정의된다.

$$
  =E_z ( \\log p_{\\theta}(x \\vert z)) - D_{KL} (q_{\\phi}(z \\vert x) \\vert\\vert p(z)) + D_{KL} (q_{\\phi}(z \\vert x) \\vert\\vert p(z \\vert x))
$$

Variational Autoencoder의 주목적인 ‘Variational bayes’는 접근하기 힘든(intractable이라고 한다) posterior $p_\\theta(z \\vert x)$를 encoder의 도움을 통해 획득하고자 하는 것이다. 따라서 Generation 관점에서는 maximum likelihood(MLE) $P_\\theta(x \\vert z)$이지만, 사실 이러한 학습은 굳이 VAE 학습법이 아니더라도 adversarial하게 학습이 가능한 GAN이라던지 등 다른 알고리즘으로 궁리해볼 수 있다. 아무튼 여기서 중요한 것은, VAE는 인코더의 도움을 받아서 디코더의 posterior를 학습시켰다는 것.

이러한 관점을 조금만 denoising diffusion probablistic model로 옮긴다면 비슷한 optimization을 생각해볼 수 있다.

기존의 ELBO 식에서의 latent를 time dependent variable $x_t$로, image domain은 $x_0$로 정의한다.

여기서 time $t$는 $x_0$를 기준으로 노이즈가 ‘얼마나’ 더해졌는지를 나타내는 수치라고 보면 된다.

$$
    =E_z(\\log p_{\\theta}(x_0 \\vert x_t)) - D_{KL} (q(x_t \\vert x_0) \\vert\\vert p_{\\theta}(x_t)) + D_{KL} (q(x_t \\vert x_0) \\vert\\vert p_{\\theta}(x_t \\vert x_0))
$$

아까도 말했듯이 likelihood model에서는 posterior를 획득할 수 없다는 걸 알기 때문에,  식에서의 $p(x_t \\vert x_0)$는 처리할 수 없었다. 따라서 이 뒷부분을 $D_{KL} \\ge 0$로 두고 나머지 항들을 통해 lower bound를 정의한 식이 바로 Variational bound이다. 이를 score matching based인 DDPM에서는 조건부를 통해 처리가 가능하다. 이를 실제 식으로 유도해보기 위해 약간의 트릭을 써보도록 하자.

앞에 있는 $\\log p_\\theta(x_0 \\vert x_t)$를 $z$에 대해 평균을 내는 부분과 바로 뒤쪽에 있는 KL divergence의 분모 부분을 바꿀 수 있다. 이때 뒤에 있는 $D_{KL} \\ge 0$을 무시하고 확인해보도록 하겠다.

$$
    \\ge E_{x_T}(\\log p_\\theta(x_t)) - D_{KL} (q(x_t \\vert x_0) \\vert\\vert  p_{\\theta}(x_0 \\vert x_t))
$$

위와 같이 변하게 되고, 이걸 fianl $T$에 대해서 일반화해서 보면,

$$
  \\begin{aligned}
    \\mathcal{L} :=& E_{x_T}(-\\log p_\\theta(x_0)) \\newline
    \\mathcal{L} \\le& \\mathbb{E}_{x_T}\\left(-\\log \\left(\\frac{p(x_0 \\vert x_T)}{q(x_T \\vert x_0)} \\right) \\right) \\newline
    \\mathcal{L} \\le& \\mathbb{E}{x_T}\\left(-\\log \\left(\\frac{p_\\theta(x_T \\prod_{t=1}^T p_\\theta(x_{t-1}\\vert x_t))}{\\prod_{t=1}^T q(x_t \\vert x_{t-1})} \\right) \\right)
  \\end{aligned}  
$$

처럼 graphical modeling이 가능하고

이를 쭉 정리하다보면,

$$
    \\mathcal{L} \\le \\mathbb{E}{x_T}\\left(-\\log (p_\\theta(x_T))-\\sum_{t=2}^T \\log \\frac{p_\\theta(x_{t-1} \\vert x_t)}{q(x_t \\vert x_{t-1})} -\\log \\frac{p_\\theta(x_0 \\vert x_1)}{q(x_1 \\vert x_0)} \\right)
$$

와 같이 정리 가능하다.

여기서 정리한 부분에서 가장 메인이 되는 건 t = 2인 부분인데, 이는 t=2일 때부터 $x_t$는 $x_0$에 무관한 조건화가 가능하다. 따라서 위의 식에서의 pre-defined posterior를 likelihood로 대체 가능하다.

$$
    \\mathcal{L} \\le \\mathbb{E}{x_T}\\left(-\\log (p_\\theta(x_T))-\\sum_{t=2}^T \\log \\left( \\frac{p_\\theta(x_{t-1} \\vert x_t)}{q(x_{t-1} \\vert x_t, x_0)} \\times \\frac{q(x_{t-1} \\vert x_0)}{q(x_t \\vert x_0)} \\right) -\\log \\frac{p_\\theta(x_0 \\vert x_1)}{q(x_1 \\vert x_0)} \\right)
$$

요 식을 다시 잘 지지고 볶아서 다음과 같이 만들면, loss에 대한 공식 유도는 모두 끝난다.

$$
\\mathcal{L} \\le \\mathbb{E}{x_T}\\left(-\\log \\left(\\frac{p_\\theta(x_T)}{q(x_T \\vert x_0)} \\right)-\\sum_{t=2}^T \\log \\left( \\frac{p_\\theta(x_{t-1} \\vert x_t)}{q(x_{t-1} \\vert x_t, x_0)} \\right) -\\log p_\\theta(x_0 \\vert x_1) \\right)
$$


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209054537-af41bf59-2883-4ad9-8cc8-fedb83fb35ee.png" width="700"/>
</div>
 

맨 앞에 있는 negative log likelihood는 결국 x_0로부터 노이즈를 쭉 만드는 과정에서 gaussian 분포를 따라가게끔 하고, 역과정에서는 gaussian 분포를 시작으로 샘플링을 하겠다는 것, 즉 VAE에서의 regularization term이라고 보면 된다( forward process → gaussian).

두 번째 $\\sum$이 있는 식은 중간의 초록색 부분으로, 각 reverse process 에 대한 예측을 이미 정의된 forward process에 맞추는 과정이다. 그리고 마지막 항은 $x_1$로부터 진짜 이미지에 해당하는 $x_0$로 reconstruction하는 부분이라고 보면 된다.

$$
  q(x_{t-1} \\vert x_t,~x_0) = q(x_t \\vert x_{t-1}) \\times \\frac{q(x_{t-1} \\vert x_0)}{q(x_t \\vert x_0)} = N(x_{t-1}; \\tilde{\\mu}_t(x_t, x_0), \\tilde{\\beta}_t I)  
$$

$$
  \\tilde{\\mu}_t(x_t, x_0) := \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1-\\bar{\\alpha}_t}x_0 + \\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}x_t  
$$

$$
  \\tilde{\\beta}_t := \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t}\\beta_t
$$
 

Tractable한 항에 대한 정리는 위와 같다. 위 공식에 대한 유도는 논문을 참고하면 단순히 gaussian 분포의 조건부로 해결 가능한 수식이다.

$$
  L_{t-1}:= \\mathbb{E}_{x_T} \\left( \\log \\left( \\frac{q(x_{t-1} \\vert x_t, x_0)}{p_{\\theta}(x_{t-1} \\vert x_t)} \\right) \\right) = \\mathbb{E}_{x_T \\sim q(x_t \\vert x_0)} \\left( \\frac{1}{2\\sigma^2_t} \\parallel \\tilde{\\mu}_t (x_t, x_0) - \\mu_\\theta (x_t, t) \\parallel^2 \\right) + C
$$

따라서 결국에 위와 같은 식을 최적화하는 것이 목표가 되고, 각 step $t$에서 다음 방향으로의 gaussian 분포를 예측하게 된다. 기존의 loss function은 가우시안 분포 사이의 KL divergence로 구성이 되는데, 이는 결국 noise의 variance를 최소화시키고 time step $T$를 키움으로써 reverse process가 효과적으로 가우시안 분포를 따를 수 있기 때문이다. 모델이 예측해야할 요소는 input에 대한 평균값인데, 이 또한 다음과 같이 단순화할 수 있다.

$$
  L_{t-1} -C := \\mathbb{E}_{x_0, \\epsilon} \\left( \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t (1- \\bar{\\alpha}_t)} \\parallel \\epsilon - \\epsilon_\\theta (\\bar{\\alpha}_t x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon, t) \\parallel^2 \\right)  
$$

$$
  \\mathcal{L}_{simple} := \\mathbb{E}_{x_0, \\epsilon} \\left( \\parallel \\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1- \\bar{\\alpha}_t}\\epsilon, t) \\parallel^2 \\right)
$$

위의 식과 같이 $\\epsilon \\sim N(0, I)$에 대한 식으로 정리할 수 있는데, 이를 그대로 최적화에 사용하는 것과 simple(normalization term $\\gamma$를 없애는 식)을 최적화해도 괜찮다고 했다. 이는 NCSN과는 다른 접근이었던게, NCSN에서는 각 step을 모두 정규화하여 weighted loss를 구해주었는데, 굳이 그럴 필요 없다는 것을 DDPM에서는 실험적으로 증명하였다.

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209054546-0c6c4106-b3ab-4f39-b96a-baba143c252b.png" width="500"/>
</div>
 

그게 바로 baseline과 다른 요 방식. 성능이 더 잘나왔다고 한다.

## Limitations

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209054548-87a67aa8-fdc5-4f4d-9f3f-deea212d5b20.png" width="900"/>
</div>
 

DDPM은 초창기 논문이다 보니 한계점이 되게 많은데, 그 중 하나가 pixel level interpolation이다. 보통 GAN같은 generative model에서는 latent space에서 interpolation을 진행하더라도 생성 이미지의 consistency(일관성)이 유지가 되었는데, DDPM은 어쩐 이유에서인지 낮은 level에서 interpolation을 진행하면 아예 다른 이미지가 만들어진다고 한다. 아무래도 조금씩 노이즈를 제거해가는 샘플링 방식을 사용하는 DDPM에서는 implicit model이 하나의 step($t → t-1$)에 대해서만 최적화가 가능하기 때문에 최종적으로 생성되는 이미지인 $x_0$에 대해서는 낮은 level의 latent가 이를 인지할 수 없다는 생각이다. 이러한 문제를 이후 DDIM에서 implicit, non-Markovian sampling 방식을 통해 해결할 수 있다고 주장하였다.

또다른 한계점으로는 sampling 속도 및 학습 속도라 할 수 있겠다. 물론 학습 과정에서 시간이 소모되는 건 어느 딥러닝 과제에서나 매한가지이지만, 샘플링 자체가 오래 걸리는 건 생성 모델로 사용되기에 매우 큰 단점으로 꼽힌다. 이러한 문제 역시 이후 DDIM이 해결하였고, 현재 대부분의 diffusion sampler 방식은 DDIM을 사용하고 있다(DDIM도 이후 글로 리뷰해서 올려볼 예정이다).
`,vO=`---
title: "Simple explanation of NCE(Noise Contrastive Estimation) and InfoNCE"
category: "ai papers"
publishedAt: "2022-12-02"
thumbnail: "https://user-images.githubusercontent.com/79881119/209057175-14b2eb0c-a332-4d8d-b2a2-9e1e0d195812.png"
---

해당 내용은 Representation Learning with Contrastive Predictive Coding에서 소개된 InfoNCE를 기준으로 그 loss term의 시작에 있는 InfoNCE에 대해 간단한 설명을 하고자 작성하게 되었다. [논문링크](https://arxiv.org/abs/1807.03748)   

InfoNCE는 contrastive learning의 기본에 있는 연구가 되며, 흔히 multimodal(멀티모달)이라 불리는 AI의 새로운 지평을 열기 위해 보다 다양한 task에서도 공통적으로 학습 가능한 형태의 representation space를 찾기 위한 방법 중 하나라 볼 수 있다. 그렇다면 구체적으로 InfoNCE를 알아보기 전, Noise Contrastive Estimation에 대해서 간단하게 소개해보도록 하겠다. NCE가 궁금하지 않다면 그냥 넘겨서 InfoNCE 부분만 읽어도 된다.
다음 링크의 글을 참고하였다. [링크](https://www.kdnuggets.com/2019/07/introduction-noise-contrastive-estimation.html)

## Background
이 개념을 설명하기 위해서는 먼저 문제를 상정해야하는데, 이는 NLP task로 예를 드는 것이 좋다. 가장 유명한 접근 방식으로는 Word2Vec이 있는데, 다음과 같은 process를 따라서 문제를 이해해보도록 하자.


"빠른 주황색 여우가 점프를 한다." 라는 문장이 있다.

Sliding window 방식으로 문장의 각 단어들에 대해 (문맥, 타겟)의 pair를 생성한다. 여기서 타겟은 sliding window가 포함하는 영역에서의 중간 단어를 의미하고, 문맥은 그 주변 단어를 의미한다. 간단한 문제 제시를 위해 여기서는 문맥을 neighboring 1 word라고 가정한다.

- ((빠른, 여우가), 주황색),

- ((주황색, 점프를), 여우가),

- ((여우가, 한다), 점프를)

각 문맥에 대한 word를 vector로 바꾸는데, 이러한 방식은 lookup table를 사용하는 방법이 될 수 있다. 자세한 방법론에 대한 부분은 tensorflow가 제시하는 튜토리얼을 참고하면 좋다. 임베딩은 보통 벡터 형태로 바꾸는 것을 의미하는데, 이렇게 되면 전체 context를 대표하는 context embedding은 각 context word의 임베딩의 평균으로 생각할 수 있다.

이렇게 임베딩의 평균을 사용한 context vector를 MLP(Fully connected NN layers)에 넣게 되고, softmax를 토대로 target word에 대한 확률 매핑을 추출하게 된다. 즉 output이 target 단어들의 후보군에 대한 확률 map이라 생각하면 된다.

맞는 단어에 대한 one-hot encoding에 대해 CrossEntropyLoss 최적화를 진행하면 된다.


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209057172-5ca85d2a-98ee-4e32-973b-4b154a7130bc.png"/>
</div>

이러한 방식을 통해, sentence 내에서 특정 단어들의 추출 빈도나 통계를 학습하게 된다. 또한 lookup table의 경우 학습이 가능하므로, 최종적으로 학습이 마무리된 경우 embedding space 상에서 엇비슷한 단어들은 비슷한 위치에 있게 되고, 서로 다른 단어들은 동떨어져서 존재하게 될 것이다.   
그러나 위에서 4번째 process를 잘 보다보면, 네트워크는 dense layer 구조를 weight matrix of (임베딩의 차원 수, 단어 수)로 가지게 된다. 즉, 우리가 각 vocabulary 단어에 대한 예측을 위해서는, 먼저 각 단어에 대한 layer output을 구하고 (​$\\text{for } i : \\text{ index of word, }z_i = Wx_i$), 이에 softmax transformation을 통해 확률 값으로 매핑한다.

$$
    p(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{\\vert V \\vert} e^{z_j}}​
$$

​​​물론 이와 같이 계산하기 위해서는 기존의 vocabulary size가 지정되어 있어야 한다. 그리고 이를 기반으로 cross entropy loss를 계산하는 것이다.

$$
    L = -\\sum_j^{\\vert V \\vert} y_j \\log (p_j) = -\\log (p_{\\text{target}})    
$$

여기서 중요한 점은 loss function이 모든 prediction에 대해서 더해지는 것처럼 보이지만 사실은 0이 아닌 term은 결국 실제 라벨에 해당되는 $y_i$​​ 가 1인 지점만 생각하게 되는  것이다. 즉, actual target word에 대한 확률값만 고려하게 된다는 의미. 그런데 $p(z_i)$를 보면 모든 위치의 단어에 대한 probability는 항상 같은 분모를 가지게 된다. 이 분모의 term을 보다보면,

$$
    \\sum_{j=1}^{\\vert V \\vert} e^{z_j}
$$

인데, 바로 이 denominator 덕분에 굳이 매번 parameter가 모든 training example에 대해 non-zero gradient term을 가지게 되는 것이다. Noisy한 학습이 진행된다고 생각해주면 될 것 같다.

## Negative sampling
이러한 문제를 해결하기 위해 한가지 고려할 수 있는 것은 잘못된 vocabulary(negative term)을 모두 더하는 것이 아니라 이 중에 일부를 선택하는 것이다. 이렇게 선택된 일부의 non-target word를 negative samples라 부를 것이다. 이 글 전체에서 쓰이기도 하고, 실제로 infoNCE를 언급할 때 사용할 용어 전반은 앞에서 정의한 단어 그대로를 사용할 것이다.   
앞서 언급한 모든 process를 동일하게 진행하되, negative sampling만 추가된 것이다. 즉 이를 다시 풀어쓰게 되면,


"빠른 주황색 여우가 점프를 한다." 라는 문장이 있다.

Sliding window 방식으로 문장의 각 단어들에 대해 (문맥, 타겟)의 pair를 생성한다. 여기서 타겟은 sliding window가 포함하는 영역에서의 중간 단어를 의미하고, 문맥은 그 주변 단어를 의미한다. 간단한 문제 제시를 위해 여기서는 문맥을 neighboring 1 word라고 가정한다.

- ((빠른, 여우가), 주황색),

- ((주황색, 점프를), 여우가),

- ((여우가, 한다), 점프를)

각 문맥에 대한 word를 vector로 바꾸는데, 이러한 방식은 lookup table를 사용하는 방법이 될 수 있다. 자세한 방법론에 대한 부분은 tensorflow가 제시하는 튜토리얼을 참고하면 좋다. 임베딩은 보통 벡터 형태로 바꾸는 것을 의미하는데, 이렇게 되면 전체 context를 대표하는 context embedding은 각 context word의 임베딩의 평균으로 생각할 수 있다.

이렇게 임베딩의 평균을 사용한 context vector를 MLP(Fully connected NN layers)에 넣게 되고, softmax를 토대로 target word에 대한 확률 매핑을 추출하게 된다. 이 때, 전체 단어에 대한 확률 매핑이 아닌, 일부 추출된 negative sample과 positive sample을 entire sample space라 가정하고 계산한다.

맞는 단어에 대한 one-hot encoding에 대해 CrossEntropyLoss 최적화를 진행하면 된다.


물론 이렇게 일부 샘플들을 추출할 경우 매번 denominator가 달라지게 되므로 normalize가 제대로 진행되지 않을 수도 있다는 문제가 있지만, 이는 수많은 학습을 통해 approximation이 가능하다고 보고, 이러한 학습법에서의 가장 주요한 점은 gradient update의 수를 줄일 수 있다는 것이다. 즉,

$$
    \\vert Embedding \\vert \\times \\vert V \\vert \\rightarrow \\vert Embedding \\vert \\times \\vert N+1 \\vert    
$$

negative samples N에 대해서 위와 같이 전체 space(vocabulary samles)이 아닌 일부 영역에 대해서만 최적화가 진행된다.
이는 상당히 많은 vocabulary가 존재하는 NLP task에서 그럴듯하게 들리는게, 애초에 문맥상 **"얼룩말"**이 전혀 쓰이지 않는 상황에서까지 해당 embedding을 최적화하는 것은 학습에도 악영향을 미치기도 하며, 추가적인 메모리 손실을 불러온다.

바로 Noise Contrastive Estimation(NCE)는 이러한 negative sampling을 일부 이론을 추가하여 구현한 형태가 되겠다.

## Learning by comparison

Negative Sampling에서는, true target을 1, 그리고 random samples의 target을 0s로 지표화한다. 이러한 지표화는 네트워크로 하여금 자연스럽게 어떤 샘플이 real이고 어떤 샘플이 noise인지 구분하게끔 한다. NCE는 바로 logistic regression 모델링을 토대로 이러한 문제에 답을 하게끔 해준다. Logistic regression modeling은 input이 다른 클래스가 아니라 해당 클래스에서 왔을 log-odds 비율과 같다.

$$
    logit = \\log (\\frac{p_1}{p_2}) = \\log (\\frac{p_1}{1-p_1})   
$$
여기서 우리는 log-odds가 true word distribution P에서 왔을 확률과 noise distribution Q에서 왔을 확률의 비를 정할 수 있다.
$$
    logit = \\log (\\frac{P}{Q}) = \\log (P)- \\log (Q)   
$$
즉, negative sampling인 noise에 대해 real distribution을 상대적으로 logit 학습을 통해 접근하기 때문에 noise contrastive estimation이라는 용어로 표현 가능한 것이다.   
우리는 실제 distribution인 P는 intractable하지만, noise distribution Q는 어떤 식으로든 정의할 수 있다. 예컨데 만약 all vocabulary를 동일 확률로 샘플링하거나, training data에서 word의 출현 빈도를 고려하는 방식으로 샘플링할 수도 있다. 이런 저런 방법을 떠나 여기서 중요하게 적용할 점은 우리에게 있어서 $\\log (Q)$ 계산에 대한 명확한 방향을 제시해준 셈이다.   
다시 한 번 앞서 소개한 word2vec network를 살펴보고 Q가 어떤 방식으로 사용될 수 있는지 살펴보도록 하자.

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209057172-5ca85d2a-98ee-4e32-973b-4b154a7130bc.png"/>
</div> 

우리는 context vector를 네트워크의 입력으로 사용할 것이다. 그러나 vocabulary 모든 단어에 대한 output 계산이 아니라, 우리가 미리 정의한 distribution 'Q'로부터 무작위 샘플링 된 단어(random samples)에 대해 계산할 것이다. 그렇게 되면 network의 출력값에 대한 계산은 target 단어와 noise distribution으로부터 샘플링된 $N$개의 단어에 대해 진행되는 것이다. 즉, network evaluation은 $N+1$(random samples + target)에 대해 진행된다고 생각하면 된다.   
우리가 negative sampling을 진행하게 될 noise distribution을 정의하였고, 해당 분포로부터 추출된 noise를 사용하기 때문에 Q에 따라 analytically하게 각 단어의 확률을 계산할 수 있다.   
예컨데 만약 "고양이"이 10%의 확률로 샘플링이 되고, "네코"가 90%의 확률로 샘플링이 된다면 "고양이"가 샘플로 추출되었을 때의 Q는 0.1인 것이다. 결국, Q는 각 단어 샘플에 대한 추출 빈도라고 인식하면 된다. 그리고 앞선 수식에서의 probability $P$는 네트워크를 통과한 형태의 prediction이다. 학습은 일반적인 logistic regression task와 같이 target word는 1로, negative samples은0s로 되게끔 할 것이다.   


앞서 조금 길게 NCE에 대한 내용을 짚고 넘어왔다. 앞서 noise constrastive estimation(NCE)에 대해 길게 설명을 했던 이유는 이 논문에서 제시하는 loss의 형태와 intuition이 그로부터 나왔기 때문이다.   
AI, 딥러닝이라 불리는 분야가 성공하는데 있어서는 gradient based optimization을 기준으로 SGD, RMSprop, Adam 등 다양한 알고리즘을 통해 훌륭한 최적화 알고리즘이 나왔으며, labeled dataset을 통한 supervised learning이 주된 역할을 담당했다고 볼 수 있다. 그러나 여전히 딥러닝은 다양한 모달리티(modality)에 적용되기 힘들다는 점에서 한계가 있다. 여기서 modality는 무언가를 나타내는 형식/형태로 해석하면 될 것 같다. 즉 데이터셋의 유형이라고 생각해보자.   
사람의 목소리를 구분하는 task에서(화자 인식) 학습된 데이터셋은 음악 장르를 구분하거나 번역하는 task에 적용되기 힘들다. 비슷한 형태의 representation이 학습되었다는 가정에 대해서는 transfer learning이나 domain adaptation과 같은 방법이 있지만, vision/audio 및 vision/text와 같이 전혀 무관한 modality에서는 이러한 가정이 전혀 성립될 수 없다는 것이다.   
이러한 측면이 가지는 또다른 문제점은 unsupervised learning에서도 나타나는데, 결국 high-level representation을 나타내는데 유의미한 representation을 별다른 supervision 없이 학습할 수 있을지에 대한 궁금증이 생긴다.   
비지도 학습에서는 별다른 지표화 진행 없이 supervision을 가해야하기 때문에 흔히 predictive coding이라는 기법이 사용되는데, 이는 causal한 형태의 데이터를 가정하고 빈 부분이나 미래의 값을 예측하는 형태로 학습을 하게 된다. 이 논문에서는 바로 이런 predictive coding과 NCE를 수식화한 형태의 objective function을 모티브로 삼는다.   
해당 논문에서 제시하는 방법은 다음과 같이 세 가지로 나눌 수 있다.   
- High dimensional data를 보다 compact한 latent embedding space로 mapping, 이에 따라 conditional prediction이 모델로 하여금 더 간단하게 수행될 수 있게 하는 것이 목적이다.
- 이러한 latent condition을 통해 prediction을 함에 있어서 powerful autoregressive(AR) 모델을 사용한다.
- NCE(Noise Contrastive Estimation)을 loss function에 활용함으로써 모델 전반이 end-to-end로 학습되게 한다.
   
길고 길었던 소개글이고, 이제 본격적으로 Contrastive predicting coding에 대해 살펴보도록 하자.

## Main intuition, motivation
이 논문에서 기억해야할 것은 등장하는 수식보다는, 바로 어떻게 최적화를 고안했는지에 대한 intuition이다. 공유되는 정보를 high dimensional signal이라고 했을 때, high dimensional signal의 서로 다른 부분 정보들의 인코딩 방식이 된다. 즉 modality의 자체(low level information, raw dataset)에 대한 representation을 학습하는 것이 아닌, 각 도메인 latent space에서의 relation을 모델로 하여금 implicit하게 학습하고 싶다는 것이 주목적이 되겠다.   
그렇기에 새로운 형태의 loss를 고안해야 했고, 그 방법으로 NCE가 제안이 된 것. 그리고 보통 prediction을 할 때(regressive model과 같은 구조에서), 보다 먼 미래의 값을 예측할 때 global information을 보아야하는 경우가 있는데, 이를 slow feature라고 부른다. Slow feature란 오디오 데이터에서는 억양이나 분위기가 될 수 있고 이미지에서는 object 자체가 될 수도 있으며 텍스트에서는 줄거리나 큰 맥락에서의 주제 등과 같이 보다 데이터 전반에 걸친 inference가 필요한 경우를 의미한다.   

고차원 데이터셋에서의 어려운 점을 꼽으라면, 단연 MSE나 Cross Entropy Loss가 크게 효과적이지 않다는 점을 들 수 있다. 예컨데 이미지 생성 모델을 기준으로 1024 x 1024 샘플을 생성할 때, per pixel loss를 사용하게 되면 penalty가 커지는 문제 때문에 예측 형태가 edge 부분을 명확하게 그리지 못하고 뭉개지는 현상(blurry)이 생긴다. Cross Enropy Loss 또한 번역 모델에서 예측 단어의 수가 증가할수록, 모든 class에 대해 gradient를 먹여줘야한다는 문제랑, 수많은 probability label이 sparse하게 분포된다는 점에서 학습 성능이 저하된다는 문제가 생긴다.   

또다른 문제점으로는 powerful conditional generative model이 필요하다는 점이고, 이는 모델의 학습에 있어서 학습하고자 하는 encoder($E$)와 함께 GPU 상에서 학습이 되어야하기 때문에(비록 parameter update는 되지 않더라도) 그만큼의 하드웨어를 차지한다는 것이 문제가 되며, 또한 generation time에 따라 학습 bottleneck이 생긴다는 단점이 있다. Conditional generative model의 또다른 문제로는 context $c$에 대한 ignorance인데, 이는 고차원 데이터셋인 이미지를 생성함에 있어 class label과 같은 high-level latent 변수는 적은 수의 정보를 담고 있기 때문이다. 즉, decoder를 설계함에 있어 conditional한 확률 분포 $p(x \\vert c)$를 직접적으로 모델링하는 것은 사실은 $x$와 $c$의 공유된 정보를 통해 데이터를 생성하는 것이 아니라 단순히 $c$를 무시한 채 생성을 하도록 학습이 진행되었을 가능성이 있다는 것이다.   

따라서 해당 논문에서는 future information을 예측할 때 target x(미래의 값)와 context c(현재의 값)을 compact한 distribution으로 학습하여, 서로 정보 공유가 되게끔 non linear mapping을 고안하였다. 이는 Mutual information식을 통해,
$$
    I(x; c) = \\sum_{x,~c} p(x, c) \\log \\frac{p(x \\vert c)}{p(x)}    
$$

위와 같이 표현할 수 있다. Mutual information(MI)는 인자가 되는 두 분포$(x, c)$의 joint distribution인 $p(x, c)$가 단순곱 $p(x)p(c)$와 얼마나 비슷한지 측정하는 척도로 쓰이며, 아래와 같이 유도된다.

$$
    I(x, c) = \\sum_c \\sum_{x \\in X} p(x,c) \\log \\frac{p(x, c)}{p(x)p(c)}    
$$

위의 식은 KL divergence 식과 똑같이 유도되고, 이를 bayes rule을 통해 간단히 표현하면,

$$
    I(x, c) = \\sum_c \\sum_{x \\in X} p(x,c) \\log \\frac{p(x, c)}{p(x)p(c)} = \\sum_{x, c} p(x, c) \\log \\frac{p(x \\vert c) p(c)}{p(x) p(c)} = \\sum_{x, c} p(x, c) \\log \\frac{p(x \\vert c)}{p(x)}  
$$

위와 같이 표현 가능하다. $x$와 $c$의 dependent한 척도를 나타내는데, 만약 $x$가 $c$에 대해 independent하다면 위의 값은 작아지고, 반대로 dependent할수록 그 값이 커지게 된다. KL divergence와는 다르게 위의 식은 commutable하기 때문에 대칭성이 성립한다.

## Contrastive predictive coding

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209057175-14b2eb0c-a332-4d8d-b2a2-9e1e0d195812.png"/>
</div> 

위의 그림은 CPC(Contrastive Predictive Coding) model의 구조를 보여준다. 먼저, non-linear encoder로 표현된 $g_{enc}$가 input sequence인 $x_t$를 latent representation의 sequence로 바꾼다.

$$
    g_{enc}(x_0, x_1, \\cdots, x_t) = (z_0, z_1, \\cdots, z_t)    
$$

여기서 autoregressive model $g_{ar}$은 causal한(현재 시점으로 과거의) input들을 활용하여 context latent representation을 생성한다.

$$
    g_{ar}(z_{\\leq t}) = c_t    
$$

여기까지는 흔한 AR 구조의 흐름과 사실상 동일하다. 그러나 위에서 언급했던 것과 같이 future observation을 직접적으로 예측하는 것이 아니라, future prediction과 context vector 사이의 mutual information에 비례하는 density ratio를 모델링한다.

$$
    f_k(x_{t+k,~c_t}) \\propto \\frac{p(x_{t+k} \\vert c_t)}{p(x_{t+k})}    
$$

위에서 설명한 식에서 $\\log$ 내부에 들어가는 값이 결국 $x$, $c$의 joint distribution에 대한 $x$, $c$각각의 분포의 곱과의 차이를 보여주는 비율이 되겠다. 그렇기 때문에 위의 값에 비례하는 형태로 모델을 구성하게 되면 자동적으로 future prediction이 유의미하게 context를 보도록 강조할 수 있다는 것!(density ratio를 최적화함으로써, 모델한테 "이걸 보고 배우면 돼" 식으로 과외 선생님처럼 가르쳐줄 수 있다는 것이다)   

density ratio $f$는 정규화되지 않은 값으로, 다음과 같이 $\\log$ bilinear model이 될 수도 있고 non linear network 및 RNN 구조로 대체될 수 있다.

$$
    f_k(x_{t+k}, c_t) = exp(z_{t_k}^T, W_kc_t)    
$$

이러한 density ratio를 활용, encoder와 함께 다음 $z$를 예측함으로써 model이 고차원의 데이터인 $x$를 예측해야하는 문제를 해결해줄 수 있다. Encoding된 latent space 상에서의 $z$를 예측하는 방식은 직접적으로 $p(x)$나 $p(x \\vert c)$를 건들 수 없지만, 우리가 가정할 수 있는 분포를 가지고 sampling을 진행하는 Noise contrastive estimation과 importance sampling이 사용될 수 있는 것이다. 드디어 우리가 고생고생해서 이해한 NCE를 써먹을 타이밍이 왔다.

## InfoNCE, Mutual information estimation
위의 구조를 잘 보면 결국 우리가 훈련해야 하는 것은 encoder, autoregressive model에 대한 두 개의 모델이다. 두 모델은 모두 NCE를 기반한 loss function에 의해 최적화가 될 것이고, 이를 InfoNCE라 명명하였다.   
앞서 봤던 NCE와 동일한 방법으로, $N$개의 무작위 샘플을 가정할 건데, 이 중에 1개는 posivie sample이 될 것이고 $N-1$개는 proposal distribution을 통해 추출된 negative sample이 될 것이다.

$$
    X = (x_1, x_2, \\cdots, x_N)    
$$
$$
    \\begin{cases}
        positive~x, & x \\sim p(x_{t+k} \\vert c_t) \\newline
        negative, & x \\sim p(x_{t+k})
    \\end{cases}   
$$
그래서 최적화할 loss function은 결국,

$$
    \\mathcal{L} = -E_X (\\log \\frac{f_k(x_{t_k}, c_t)}{\\sum_{x_j \\in X} f_k(x_j,c_t)}) 
$$

요렇게 된다. 근데 사실 이 objective function만 보면, 단순히 categorical cross entropy를 표현한 것처럼 보여 크게 다를 것이 없어 보이는데, 대체 왜 이게 density estimation에 근접한 형태로 학습이 되는지 혼란스러울 수 있다. 왜냐면 내가 그랬는데 똑똑이인 분들은 이걸 보고 바로 이해할 수도 있겠지만 암튼 결국 우리가 얻고자 하는 최종 optimal probability를 생각해볼 수 있는데, 이는 $p(d = i \\vert X, c_t)$라고 할 수 있겠다.   
위에서 말했던 것과 같이 모델이 예측하는 것은 특정 샘플이 context를 보고 나온 녀석인지 아니면 context를 보지 않고 나온 녀석인지에 대한 비율이고, 이 값이 고대로 Mutual information의 logarithm을 통해 수치화된다고 했었는데, 결국 우리는 optimal probability를 주어진 샘플 X와 context vector $c$에 대해서 다음 식으로 나타내볼 수 있다.   
$$
    p(d = i \\vert X, c_t) = \\frac{p(x_i \\vert c_t) \\prod_{l \\ne i} p(x_l)}{\\sum_{j=1}^N p(x_j \\vert c_t) \\prod_{l \\ne j} p(x_l)} = \\frac{\\frac{p(x_i \\vert c_t)}{p(x_i)}}{\\sum_{j=1}^N \\frac{p(x_j \\vert c_t)}{p(x_j)}}
$$
따라서 앞서 본 식에서 네트워크 f가 예측하는 값이 probability ratio에 비례함을 알 수 있으며, 이는 negative sample의 개수와는 무관한 것을 확인할 수 있다. Training에는 단순히 loss만 사용되지만, mutual information은 다음과 같은 lower bound를 통해 확인할 수 있다.

$$
    I(x_{t+k}, c_t) \\geq \\log(N) - L_N    
$$

이는 $N$이 커질수록 tight해진다. 여기서 tight의 의미는 lower bound가 실제 mutual information의 infimum에 가까워진다는 것이다. 즉 실제 값에 근사한다는 의미. 사실 더 쓰자기엔 Appendix가 있는데 이걸 여기서 다 풀어쓰기에는 무리가 있어 여기서 마무리하도록 하겠다.

## Experiments
단연 이 모델의 가장 큰 장점이라고 한다면 encoder를 arbitrary한 구조를 가져와 적용 가능하다는 점과 representation learning에 대한 loss의 기준을 마련했다는 점이 될 수 있다. 그렇기에 해당 논문에서도 여러 modality에 대한 실험을 함께 진행했다.

## Audio dataset
<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209057177-0574457c-a2bc-4272-979e-d6e811a04ada.png"/>
</div> 
Librispeech는 총 251명의 발화자에 대한 음성 파일로 구성되고, 각각에 대한 text script가 주어진다. 이러한 text script는 실제 phone sequence와는 alignment가 되어있지 않기 때문에 추가적인 annotation이 필요하다. 필자는 이걸 모 랩실에서 구현할 때 노답 알고리즘을 파이썬으로 구성해서 만들었었는데 [Kaldi Toolkit](https://kaldi-asr.org/)이란게 있다고 한다. 이런 좋은게 있으면 미리 알려주지.. 흔히 CV에서 opencv-python 모듈을 많이 사용하는데, 이건 그거의 오디오 버전이라고 보면 될 것 같다. 좋은 사실 하나 알아갑니다...

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209057181-2a6a7337-5fc4-454a-af9a-173a7846960e.png" width="400"/>
  <img src="https://user-images.githubusercontent.com/79881119/209057183-c781b0d7-318c-42a9-b4f4-ed2510522eb3.png" width="400"/>
</div> 

아니 세상에 심지어 이 논문에서 분리하고 alignment한 데이터셋을 구글 드라이브로 친절하게 공유도 해주셨다. 본인 speech transformer 학습시킬 때 이런 게 있는 줄 진작 알았더라면 이거 다운받아서 할 걸 그랬다. 후회가 막심하구만..   
암튼 이렇게 구성한 데이터셋으로 할 수 있는 것은 음성을 듣고 발화자를 분류하는 speaker classification, 각 시점에서의 phone(음절)을 분류해내는 phone classification이 있다. 두 task 모두 CPC 방식이 supervised 방식에 대해 좋은 성능으로 보답하는 걸 볼 수 있다. 우측 표는 Phone classification task에 대해 CPC 모델에 대한 ablation을 이것저것 보여준다.

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209057187-038f0dc9-cdbe-42df-a45c-0743bcf343fd.png" width="400"/>
</div> 

이건 단순히 loss 그래프처럼 보이는데 그게 아니고 현재 시점을 기준으로 latent step을 몇 번 거치냐에 따른 phone prediction 평균 정확도를 보여준다. Phone 개수는 41 possible classes가 있으니, 아무런 예측을 하지 못하는 확률값 기준은 대강 0.025로 생각하면 될 듯하다. 한 step의 latent은 10ms를 차지하기 때문에, 약 20 step을 기준으로 해당 범위까지 예측이 이어질 수 있음을 그래프로 보여준 것 같다.

## Vision
<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209057189-814e8e88-7ae3-483c-8bec-9d7eecfc09e2.png" width="400"/>
</div> 
Vision은 좀 특이하게 ResNetv2 101 backbone을 이미지용 인코더로 사용하되, 원래 구조에서 batch normalization을 제외하고 사용하였다. Batch normalization은 classification과 같이 정해진 확률 범위 내에서 결과값을 뽑는데는 학습이 안정적이고 좋지만, representation을 학습해야 하는 generation이 연관된 학습 과정에서는 주로 쓰이지 않는 편이 낫다는 해석이 있다. 이건 뭐 믿거나 말거나인데 실제로 batch normalization이 들어가면 feature map 정규화가 진행되면서 보다 distribution collapse가 발생하기도 한다. Unsupervised learning이 끝난 뒤의 linear layer는 ImageNet labels를 추정하기 위해 따로 학습을 진행한다.

학습 과정은 다음과 같다.

- $256 \\times 256$ 이미지로부터 $7 \\times 7$ grid of $64 \\times 64$ crops를 뽑아낸다. 어? 그럼 $64 \\times 7 = 448$ 이어서 개수가 안맞는데요?? 싶은 사람들을 위해 말씀드리자면 각 patch는 32픽셀 만큼 오버랩되어서 추출한다. 즉 $256+(32 \\times 6)/64 = 4+3 = 7$ 이 되는 것
- 각 crop은 인코딩되고 나서, 각 채널 별로 mean pooling하면 1024 벡터가 나오게 된다. crop이 총 $7 \\times 7$ 만큼 있었으니까 결국 output은 $7\\times 7 \\times 1024$ 짜리 텐서가 나오게 된다.
- PixelCNN 형태의 AR 모델(궁금하면 찾아보면 되는데 간단하게 설명하자면 이전의 픽셀들을 보고 다음 픽셀을 예측하는 형태의 generative model) 해서 다음 픽셀들을 예측하는 형태로 unsupervised learning을 진행한다. 이건 위의 그림을 보면 이해가능

- Linear classifier는 앞서 훈련해놓은 CPC feature map을 통해 학습하는데, CPC 학습에는 Adam optimizer를 사용하고 Linear classifier는 SGD를 활용했다고 한다.

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209057191-a0a8b0cd-d182-4f25-9d5a-2a8c697b0275.png" width="400"/>
  <img src="https://user-images.githubusercontent.com/79881119/209057192-6fec3629-5d57-44c0-8409-00a9e026bbdb.png" width="400"/>
</div> 

결과는 매우 좋았던 것으로 보인다. 참고로 여기서의 Top-1 ACC, Top-5 ACC는 classification task와는 좀 다르게 앞서 언급한 procedure 처럼 unsupervised하게 학습된 feature map을 기준으로 성능을 평가한 것이다.

## Natural Language
이번엔 또 NLP다. Mobality 하나하나에 대한 실험 과정을 설명할라다보니 내가 인간 멀티모달이 되어가는 것 같긴한데, 요즘같은 AI 블루오션 시대에는 이렇게 여러 분야 찍먹하면서 살아남아야지 어쩌겠나 싶다. 암튼,,   

먼저 모델을 BookCorpus dataset에 대해 학습시킨다. NLP task 자체가 autoregressive하므로 학습 과정에 대해서는 앞서 word2vec과 관련된 NCE를 참고하면 될 듯하다. 학습 자체에서 학습되지 못한 단어들을 evaluation하기 위해서, word2vec과 모델이 학습한 embedding 사이에 linear mapping이 추가된다.   

저자들이 사용한 분류 task에 대한 데이터셋은 MR(영화 리뷰 감정), CR(고객 상품 리뷰), 주관적/객관적 평가, 의견(MPQA) 그리고 질문 타입(TREC) 분류 등등을 사용하였다.   
<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209057193-fe7bad9c-fa88-4b46-9170-5376e0c60084.png" width="400"/>
</div> 
학습에 있어서 transfer learning setup은 skip-though vectors 논문에서 사용한 방식과 동일하게 했으며, 해당 논문을 기준으로 비교 대상을 선정한 것을 확인할 수 있다. 암튼 어느 정도 잘 된다는 것을 결과로써 보여준 모습.

## Reinforcement learning
하다하다 이제 RL(강화학습)까지 흘러들어왔다. 이쯤이면 저자들이 변태라는게 학계의 정설은 아니고 Deepmind 최고   
<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209057194-b5e93178-d0c9-4178-8275-e2a0885a8bef.png" width="400"/>
</div> 
참고로 RL은 딥러닝과 그 결이 조금 다르기 때문에 objective를 다르게 줘야 한다. A2C agent를 base model로 사용하고 CPC를 auxiliary loss로 주었다. 자세한 내용은 사실 나도 잘 몰라서 넘어가도록 하겠다. 빨간색이 잘 나온걸 보면 효과적이라고 결론을 낸 것 같다.   

## Appendix

$$
    \\begin{aligned}
        \\mathcal{L}_N^\\text{opt} =& -\\mathbb{E}_X \\log \\left( \\frac{\\frac{p(x_{t+k} \\vert c_t)}{p(x_{t_k})}}{\\frac{p(x_{t+k} \\vert c_t)}{p(x_{t+k})} + \\sum_{x_j \\in  X_\\text{neg}}\\frac{p(x_j \\vert c_t)}{p(x_j)}} \\right) \\newline
        =& \\mathbb{E}_X \\log \\left( 1+\\frac{p(x_{t+k})}{p(x_{t+k} \\vert c_t)} \\sum_{x_j \\in X_\\text{neg}} \\frac{p(x_j \\vert c_t)}{p(x_j)} \\right) \\newline
        \\approx& \\mathbb{E}_X \\log \\left( 1+\\frac{p(x_{t+k})}{p(x_{t+k} \\vert c_t)} (N-1) \\mathbb{E}_{x_j} \\left( \\frac{p(x_j \\vert c_t)}{p(x_j)} \\right) \\right) \\newline
        =& \\mathbb{E}_X \\log \\left( 1+\\frac{p(x_{t+k})}{p(x_{t+k} \\vert c_t)} (N-1) \\right) \\newline
        \\geq& \\mathbb{E}_X \\log \\left( \\frac{p(x_{t+k})}{p(x_{t+k} \\vert c_t)} N \\right) \\newline
        =& -I(x_{t+k}, c_t)+\\log(N)
    \\end{aligned}
$$

Loss function의 lower bound 유도 과정

$$
  \\begin{aligned}
    \\mathbb{E}_X \\left( \\log \\frac{f(x, c)}{\\sum_{x_j \\in X} f(x_j,c)} \\right) =& \\mathbb{E}_{(x, c)} \\left( F(x, c) \\right) - \\mathbb{E}_{(x, c)} \\left( \\log \\sum_{x_j \\in X_\\text{neg}} e^{F(x_j, c)} \\right) \\newline
    =& \\mathbb{E}_{(x, c)} \\left( F(x, c) \\right) - \\mathbb{E}_{(x, c)} \\left( \\log \\left( e^{F(x, c)} + \\sum_{x_j \\in X_\\text{neg}} e^{F(x_j, c)} \\right) \\right) \\newline
    \\leq& \\mathbb{E}_{(x, c)} \\left( F(x, c) \\right) - \\mathbb{E}_c \\left( \\log \\sum_{x_j \\in X_\\text{neg}} e^{F(x_j,c)} \\right) \\newline
    =& \\mathbb{E}_{(x, c)} \\left( F(x, c) \\right) - \\mathbb{E}_c \\left( \\log \\frac{1}{N-1} \\sum_{x_j \\in X_\\text{neg}} e^{F(x_j, c)} + \\log (N-1) \\right)
  \\end{aligned}
$$

Mutual information neural estimation과의 연관성을 보여줌. 결국  mutual information에서 intuition을 어떠한 방식으로 얻으셨는지 수식을 통해 보여주는 것 같다.
`,xO=`---
title: "NeRF-Representing Scene as Neural Radiance Fields for View Synthesis에 대해서"
category: "ai papers"
publishedAt: "2022-12-05"
thumbnail: "https://user-images.githubusercontent.com/79881119/209057571-56b18740-a937-4bee-9719-600215be1ec7.png"
---

## Abstract

  이 논문은 input으로 한정된 수의 3D scene을 획득, 이를 활용하여 continous volumetric scene function에 입각한 여러 방향에서의 scene 정보를 얻는 것에 SOTA가 되었다. 알고리즘은 convolution을 사용하지 않고 FC layer를 사용했으며, input은 spatial location(공간 좌표)인 x, y, z와 해당 위치에서의 scene을 관찰하는 방향 파라미터인 $(\\theta, \\phi)$ 를 사용하게 된다(5차원 좌표).

  이러한 5차원 좌표를 사용, camera ray를 따라 특정 값(RGB, density)들을 예측하고, 이를 volume rendering 방법을 사용하여 color와 density를 다시 image로 합성해낸다. Volume rendering의 경우 미분 가능한 공식이므로, 이렇게 생성된 synthesized scene과 실제 GT와의 비교를 통해 최적화가 가능하다.


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209057567-9cdd8d53-6ccc-4d79-a8b9-06e084788da9.png" width="800"/>
</div>
 

위와 같은 그림을 참고해보자.

## Introduction

  View synthesis와 관련된 문제 해결 방법은 NeRF 이전에도 많이 존재했다. 나머지 방법은 간단한 task 설명 후에 related works에서 보다 자세히 풀어보도록 하자. 우선 이 논문에서 View synthesis를 해결하고자 한 방향은 optimizing parameters, 즉 딥러닝 base라는 것이다. 따라서 저자들은 우선 static scene을 연속된 5차원 함수로 표현하고자 하였다. 여기서 말하는 차원은 특정 scene을 대표할 수 있는 인자로, 해당 논문에서는 앞서 설명했듯이 공간 상에서의 위치 $(x, y, z)$ 와 관찰 방향 $(\\theta, \\phi)$ 을 함께 사용한 좌표계를 정의하였다. FC layer, MLP로 최적화하는 방향성은 바로 이 5차원의 input을 토대로 volume density(의미는 밀도이지만, 실질적으로는 해당 위치에 volume이 존재할 확률로 해석된다)과 RGB를 예측한다. RGB의 경우 뒤에서도 설명하겠지만 관찰 위치에 따라서 달라지는 값이 되기 때문에 view-dependent RGB color로 생각하면 된다.

  NeRF는 일종의 Few-Shot learning과 같이 생각하면 되는데(물론 실제로 필요한 샘플 수는 few가 아니긴 하지만), 이는 하나의 scene에 대한 continous representation을 학습하기 위해서는 공간 전반에 걸쳐 카메라를 정렬(혹은 위치)시키고, 이러한 세팅 속에서 생성한 데이터가 최적화에 필요하기 때문이다. 이렇게 사용된 샘플, 그리고 그에 상응하는 5차원 벡터는 앞서 설명한 딥러닝 네트워크를 최적화한다. 우선 모델이 특정 위치에서의 camera ray를 따라 output query를 생성하면 이를 종합하여 scene 정보를 합성하는 형태로 함수를 구성하고, 실제 GT 와의 비교 및 미분 최적화를 통해 파라미터 세팅을 진행한다.


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209057570-4fad8804-9064-4980-9929-31e8f1c90e94.png" width="800"/>
</div>
 
모델의 최적화 과정을 가장 잘 보여주는 그림

이 논문이 가장 큰 contribution을 가지는 점은 간단한 네트워크를 사용했다는 점이 아닐까 싶다. 또한 최적화 과정에서 3D represesntation에 대한 GT를 전혀 사용하지 않았다는 점 또한 놀랍다. 단순히 3D scene을 촬영한 2D projected image만을 사용, 최적화하여 연속적인 공간 상의 물체를 표현하려 했다. 5차원 벡터를 input으로 사용함에 있어 생성 이미지의 fidelity를 높이는 방법으로 positional embedding을 사용하였다. 

## Related works

실제로 3D scene을 표현하려는 방법으로 MLP 최적화를 사용한 방법은 NeRF가 유일하지는 않다. 이전에는 특정 위치($x, y, z$)에서의 거리(distance)를 통해 간단한 3D 형태를 학습 또는 최적화하는 연구도 있었다. 그러나 이러한 방법들은 3D representation(voxel grid 및 mesh)를 활용해 최적화하던 방법에 비해 얻을 수 있는 scene의 퀄리티가 상당히 저하되는 문제점이 있었다. 이 논문에서 소개한 related works를 살펴보면 크게 두 갈래로 나뉘는 것을 알 수 있는데, 이를 하나씩 살펴보도록 하자.

### *Neural 3D shape representations*

NeRF와 비슷한 시기에 연구되었던 내용 중 하나는 앞서 언급했던 $xyz$ 좌표에 따라 signed distance(물체가 존재하는 표면과의 거리) 혹은 occupancy fields(물체가 차지하는 평면)을 예측하는 형태로 진행되었다. 그러나 이러한 연구들은 전체적인 윤곽만 잡을 뿐 실제 형태에 대해서 디테일하게 예측하지는 못한다는 단점이 존재하며, 추가적으로 3D representation이 필요하다는 문제가 있다. 3D occupancy field를 딥러닝으로 최적화하는 경우는, 3D occupancy field의 표현과 각 ray 와의 교차점을 찾는 것이고 이렇게 예측된 각 포인트마다 color를 예측하게 된다. 만약 이(3D occupancy field)를 직접적으로 사용하지 않는다면 단순히 feature vector와 각 좌표에서의 연속적인 RGB color를  내보내고, RNN을 통해 ray의 각 지점마다 surface 존재 여부를 판별하게 된다.

  물론 위와 같은 연구들이나 기술들 또한 복잡하거나 고화질의 형태를 예측할 수 있지만, 간단한 형태에 대해서만 최적화가 된다던가 rendering 시에 표면이 제대로 표현되지 않고 oversmooth되어 보인다. 이는 loss를 최적화하는 과정에 있어 distance 및 occupancy field와의 loss를 최적화할 때, overfitting을 방지하기 위한 방향성 때문이라고 생각된다.

  따라서 NeRF에서는 3차원 공간 좌표가 아닌 이에 추가적으로 방향성에 대한 정보를 더한 5차원 벡터 $(x, y, z, \\theta, \\phi)$ 를 사용하여, 특정 방향에서 바라본 보다 photorealistic하고 고화질의 2D representation을 획득할 수 있게끔 하였다.

### *View synthesis and image-based rendering*

 만약 시점에 대한 샘플링이 많이 진행된다면(보다 연속적인 view에 대한 샘플링이 이루어진다면), 단순히 특정 시점에서의 photorealistic view는 light field의 보간을 통해 획득할 수 있다(구체적인 방법은 복잡하기 때문에 생략하지만, Camera geometry와 관련된 내용이라고 간단하게 짚고 넘어가도록 하겠다). 그렇지 않고 만약 보다 적은 샘플링이 진행되었을 경우, computer vision 및 graphic 연구에서는 관찰된 이미지들을 통해 geometry를 예측하게 되고, 이는 충분히 많은 성과를 이뤄가고 있었다.

  예측할 수 있는 representation 중 가장 유명한 방법은 mesh-based로, diffusion 방식(generative model에서 해석하는 diffusion의 뜻과 동일)으로 표면을 색칠하거나, view 방향에 따라 어떻게 보이는지 결정하는 형태의 연구로 구분될 수 있다.

  컴퓨터 그래픽스(graphics)에서 Clipping(clip space로 옮긴 다음 공간 밖의 물체를 자르는 작업), Perspective division(원근법을 구현), Back-face culling(실제 사물을 보는 시각에서 메쉬의 각도에 따라서 시각적인 처리해주는 것), Surface normal(메쉬를 구성하는 vertex의 순서에 따라 앞면과 뒷면을 구분하는 것) 그리고 viewpoint transform(clip space에서 시각적인 공간으로 옮기는 것)과 마지막으로 Scan conversion(메쉬 삼각형 내부의 fragment를 생성하는 것 - 보통 interpolation으로 이해하면 된다)를 진행하는 Rasterizer가 있는데, 메쉬를 시각화하는 프로세스 자체를 미분 가능한 형태로 만들기도 하며, 언리얼 엔진에서 많이 사용되는 Path tracing 또한 미분 가능한 형태로 만들어 최적화하기도 한다([링크](https://en.wikipedia.org/wiki/Path_tracing)). 미분 가능한 형태로 image reprojection을 진행(tracer의 역과정으로 backpropagation)하는데, 이때 local minima에 빠질 확률이 높고 loss landscape가 복잡한 형태에 대해 최적화하기 힘들다는 단점이 있다. 또한 mesh 형태로 scene이 구성되어야 한다는 제약 때문에, 배경이나 물체의 형태가 다양하게 등장하는 real-world scene에 대해서는 synthesizing하기 힘들다는 문제도 존재한다.

  앞선 방법과 다른 형태의 방법으로는 volumetric representation을 사용하여 최적화하는 방식이 있는데, mesh based에 비해 artifact가 현저히 줄어들 수 있는 형태가 되며 보다 사실적인 디테일을 살릴 수 있는 방법이다. 초기 연구들은 관찰된 이미지와 color voxel grid를 직접적으로 접근시키는 방법을 사용했으며, 보다 최근에는 여러 방향에서의 scene을 딥러닝을 통해 최적화, 이를 토대로 수집된 이미지에 대한 입체 representation를 생성하는 연구도 진행되었다. 다중 시각에 대해서 최적화한 representation을 alpha summation을 하거나 학습된 summation을 통해 새로운 view를 생성하게 된다. 다른 형태의 연구로는 CNN과 특정 scene에서의 voxel grid를 융합하여 최적화하는 방식이 있다.

  이렇게 부피(공간)를 대표하는 형태를 기반으로 최적화하는 기술들은 좋은 결과를 보여주었지만, 보다 고화질의 이미지로 확장하는 면에 있어서 sampling과 시간 사이의 trade off가 심하다는 문제가 있다.

## Scene Representation

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209057571-56b18740-a937-4bee-9719-600215be1ec7.png" width="800"/>
</div>
 

앞서 사용된 그림 중 일부를 발췌하였다.

  이 논문에서 5차원 벡터를 기반으로 연속적인 이미지를 구상하는데 있어서, 각 위치에 대해 RGB 값과 density를 함수의 output으로 가지게 된다. 물론 딥러닝(MLP)를 사용하기 때문에, deterministic한 함수가 아닌 parameterized function인 $F_{\\Theta}$ 를 최적화하게 된다. 논문에서는 위치 벡터에 해당되는 3차원 좌표 $(x, y, z)$ 를 $\\rm{x}$, 방향에 대한 정보인 2차원 벡터 $(\\theta, \\phi)$ 를 $\\rm{d}$ 라고 명시한다. 이렇게 공간 전반에 걸쳐 representation이 학습되면, 이 학습된 전체 정보는 네트워크 내부에서 처리되는 것이다. 또한 네트워크 학습에 있어 constraints를 주기 위해 RGB color인 $\\rm{c}$ 는 위치 및 방향에 대해서 모두 예측되게끔 하고, 밀도인 $\\sigma$ 는 위치에 대해서만 예측되게 하였다. 이는 물체의 scene을 합성하는데 있어서 **RGB는 방향에 따라 달라질 수 있지만, 같은 위치에서의 density는 같아야 하기 때문**이다.

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209057572-0d8f6a71-a2aa-4d6c-b8f3-f350803a49bf.png" width="800"/>
</div>
 

  이를 단적으로 보여주는 예시가 바로 위와 같은데, View 1에서의 다이아몬드와 View 2에서의 사각형 위치의 색이 서로 다른 것을 확인할 수 있다. 실제로 학습된 네트워크를 기반으로 전체 방향에서의 interpolation을 (c)에서 Radiance Distribution으로 그렸는데, 색 변화가 viewing direction에 대해 연속적으로 변해가는 것을 확인할 수 있다.

## Volume rendering

  위와 같이 네트워크가 학습되면 위치에 따라서 연속적인 공간에 대한 정보를 학습하게 된다. 그렇다면 이렇게 학습된 값들을 기반으로 scene을 구상하는 법을 생각해보도록 하자.

  5차원 neural radiance field는 volume density와 특정 포인트에서의 색으로 표현된다고 하였다. 즉, 어떠한 ray가 있다면, 그 ray의 모든 점에 대해서 volume density $\\sigma(\\rm{x})$ 는 location $\\rm{x}$ 까지(그 위치에서 끝나는) 향하는 ray  상에서의 확률 차이(정확히 말하자면 이를 연속적인 미소 dt에 대해서 분배)라고 볼 수 있다. 이게 무슨 말이냐면, 관찰을 시작하는 부분을 기준으로 특정 지점($\\rm{x}$) 까지의 확률 누적이라고 볼 수 있다는 것. 이를 식으로 표현하자면 $\\rm{r}(t) = \\rm{o}+t\\rm{d}$ 로 표현되는 ray 직선의 방정식에 대해 가까운 bound $t_n$ 부터 가장 먼 bound인 $t_f$ 까지에서의 color expectation을 구할 수 있다는 것이다.

$$
C(\\rm{r}) = \\int^{\\it{t_f}}_{\\it{t_n}} \\it{T(t)} \\sigma(\\rm{r}(\\it{t})) \\rm{c}(\\rm{r}(\\it{t}), \\rm{d})\\it{dt}
$$

$$
T(t) = \\exp \\left( -\\int_{t_n}^t \\sigma(\\rm{r}(\\it{s})) \\it{ds}\\right)
$$

  바로 위에서 설명했듯, scene에서의 색은 바라보는 방향과 위치 모두의 함수이므로 이 둘을 모두 인자로 가지게 되고 scene에서의 밀도는 위치에 대한 인자만 input으로 가지게 된다. $T(t)$ 는 직선 상의 지점 $t$ 까지의 밀도에 대한 확률 누적으로, 지점 $t_n$ 까지 진행하는 ray가 물체와 충돌하지 않을 확률(사이에 아무런 object가 존재하지 않을 확률)로 해석 가능하다. 다르게 표현하자면 $C(\\rm{r})$은 해당 위치, 방향에서의 RGB 값이 특정 density로 존재할 확률에 대한 expectation을 구한 식과 같다. 흔히 discretized 형태의 grid를 렌더링할 때 구분구적법을 사용하게 되는데(연속된 샘플을 얻을 수는 없으므로), 이 경우 특정 location에 대한 정보만 MLP가 query되므로 representation의 resolution이 앞서 언급한 기존 방식과 크게 다르지 않게 제한되는 문제가 발생한다. 따라서 대신에 해당 논문에서는 stratified sampling을 사용하였고, 이는 ray가 퍼져가는 특정 구획을 N으로 균일하게(각 bin 사이 간격이 일정) 샘플링한 상태에서 각 bin 사이에서 하나의 sample를 uniform하게 가져오는 형태를 취하였다. 즉,

$$
t_i \\sim U \\left( t_n+\\frac{i-1}{N}(t_f-t_n),\\sim t_n + \\frac{i}{N}(t_f-t_n) \\right)
$$

  위와 같은 식처럼, $1/N$로 분할된 영역 내에서 랜덤으로 위치를 뽑는데, 그 랜덤 방식이 uniform distribution을 따른다고 생각하면 된다. 이를 통해 모델이 학습하는데 있어서 보다 다양한 position sample에 대해 최적화가 가능해진다. 비록 계산을 위해 불연속 점들에 대한 cumulation이 적분을 대체하지만, 이를 샘플링으로 어느 정도 커버한 것을 확인할 수 있다. 즉, 실제로 모델이 학습하고, 이를 토대로 렌더링하는 $C(\\rm{r})$의 근사치인 $\\hat{C}(\\rm{r})$은,

$$
\\hat{C}(\\rm{r}) = \\sum_{i=1}^N T_i(1-\\exp(-\\sigma_i \\delta_i))\\rm{c}_i
$$

이고, 여기서 $T_i$는,

$$
T_i = \\exp \\left(  -\\sum_{j=1}^{i-1} \\sigma_j \\delta_j\\right)
$$

  위와 같이 summation으로 전환 가능하다. $\\delta$로 표현된 것은 인접한 두 샘플 사이의 간격으로, 기존의 $dt$를 대체한다고 보면 된다.

## Optimizing a Neural Radian Field

전체적인 개요, 선행연구 그리고 어떤 식으로 Neural network를 최적화할지에 대해 개념적인 리서치는 모두 언급하였다. 그러나, 단순히 이런 이론만 가지로 연구를 진행했을 경우 SOTA 퀄리티가 나오지 않는 것을 확인하였고, 저자들은 이를 해결하기 위해 두 가지 추가 방법론을 제시한다. 그 중 첫 번째는 positional encoding을 추가하여 좌표에 추가 차원(dimension) 효과를 준 것이고, 두 번째는 hierarchical sampling이다.

### *Positional encoding*

Neural network 자체는 함수 형태를 최적화하기 위해 좋은 형태로 작용하지만, 단순히 $F_\\Theta$를 $xyz\\theta \\phi$에 최적화하는 작업은 생각보다 high-quality 정보를 읽어버리는 결과를 낳았다. 이는 deep neural network과 lower frequency function에 치우치게 된다는 문제 때문인데, 일반적으로 딥러닝 모델은 최적화 방향을 향해 찾아가기 때문에 이러한 문제가 발생할 수 있다. 또한 앞선 다른 연구들에 따르면, input에 higher dimensional space를 더해줄 수 있는 함수를 이용하여 모델을 최적화하면 이러한 경향성을 줄일 수 있다는 것이다. 따라서 network를 두 부분으로 분리, 이를 합성하는 형태로 바꾸게 되었는데 $R$로부터 고차원 임베딩 공간 $R^{2L}$로 보낼 수 있는 deterministic encoder를 사용(학습 불가능)한 후, 이를 기반으로 $F’_{\\Theta}$를 최적화하게 된다.

Positional encoding은 가장 잘 알려져있는 sinusoidal encoding인,

$$
\\gamma(p) = ( \\sin(2^0\\pi p), \\cos(2^0 \\pi p), \\cdots, \\sin(2^{L-1}\\pi p), \\cos(2^{L-1} \\pi p) )
$$

을 사용한다. 인코딩 함수인 $\\gamma$는 x, y, z 세 좌표 모두에 각각 적용된다. 이때 좌표는 $[-1, 1]$로 정규화된 상태다. 마찬가지로 인코딩 함수는 방향 벡터인 $\\rm{d}$에도 적용된다. 이 논문에서 실험 세팅은 $\\rm{x}$(위치 벡터)에 대해서는 $L=10$을 적용하였고, $\\rm{d}$(방향 벡터)에 대해서는 $L = 4$를 적용하였다.

이러한 포지션 임베딩을 적용하는 대표적인 예시로는 transformer architecture가 있다. Transformer는 이를 inductive bias를 더해주는(각 토큰 위치 정보를 통해) 형태로 작용하게 되었으나, 이 논문에서는 좌표를 보다 고차원의 정보로 임베딩하기 위해 사용했다는 점이 차이가 될 수 있겠다.

## Hierarchical volume sampling

계층적 샘플링은 고전적으로 많이 사용되는 샘플링 방법 중 하나다.  결론부터 말하자면 이를 활용하는 이유는 실제 ray 상에서 물체가 존재하는(density가 있는) point는 극히 일부에 해당되며, 이는 마치 이미지에서 foreground보다 background 영역이 큰 것과 비슷한 상황이다. Object detection 네트워크에서도 학습할 때 foreground, background 샘플의 분포가 많이 차이나는 것 때문에 bias가 생기는 것을 우려하여 계층적 샘플링을 통해 분포를 맞춰주는 작업을 하는 것을 확인할 수 있다.

따라서 단순히 하나의 네트워크를 최적화하기 보다, 두 네트워크를 동시에 최적화하는 방법을 사용하였다. 이를 각각 ‘Coarse(거친, 날것의)’ 네트워크와 “fine(정교한, 세심한)” 네트워크로 명명한다. 먼저 위에서 언급했던 것과 같이 $N_c$ 개의 location을 stratified sampling을 통해 랜덤으로 뽑아내고, $\\hat{ C}(\\rm{r})$를 예측하는 형태로 coarse network를 evaluate하게 된다. 이렇게 추출된 output을 기반으로, volume에 보다 관련이 있는 영역으로 샘플을 맞춰주게 된다. 이를 위해 가장 먼저,

$$
\\hat{C_c}(\\rm{r})=\\sum_{i=1}^N \\it{w}_i\\rm{c}_i,~\\it{w}_i = T_i(1-\\exp(-\\sigma_i \\delta_i))\\rm{c}_i
$$

위와 같이 추출된 color를 ray를 따라서 오는 모든 sample color $c_i$의 weighted sum으로 표현하고, 이러한 weight 전체를 normalize하여 확률값으로 바꿔준다.
$$
\\hat{w_i} = \\frac{w_i}{\\sum_{j=1}^{N_c} w_j}
$$

각 위치에 대한 확률값으로 바뀐 이 value를 통해 다시 샘플링하게 된다. weight가 큰 값을 가지는 곳이 곧 coarse network가 evaluate했을 때 물체가 있을 확률(밀도)가 큰 영역이므로, 이 부분 위주로 샘플링하는 것이다. 이렇게 추가로 추출된 $N_f$ 샘플을 추가로 사용하여 fine network를 evaluate하면, 결론적으로 최종 rendered color인 $\\hat{C}_f(\\rm{r})$을 획득하게 된다. 이는 비균일한 확률 분포를 가지는 샘플링과 비슷하지만, 샘플된 value 자체를 전체 적분 영역에서 non uniform 이산화(quantization)하기 위한 확률 값으로 사용하였다.

## Optimization

  최적화하기 위해서 요구되는 것은 RGB image scene과 각 scene에 대한 camera pose, intrinsic, bound 등 여러 corresponding 정보들이다. 최적화 단계에서는 camera rays의 배치에서 랜덤하게 추출하고, coarse network와 fine network를 기반으로 계층적 샘플링을 진행한다. Loss는 정말 간단하게도 추정된 color(렌더링 과정을 거쳐서 생성된 pixel 값)과 실제 값을 비교하는 squared error loss를 사용한다.

$$
L = \\sum_{r \\in R} \\left( \\parallel \\hat{C_c}(\\rm{r}) - \\it{C}(\\rm{r}) \\parallel_2^2 + \\parallel \\hat{\\it{C}}_{\\it{f}}(\\rm{r}) - \\it{C}(\\rm{r}) \\parallel_2^2 \\right)
$$

각 notation이 의미하는 바는 다음과 같다.

- $\\it{C}(\\rm{r})$ : Grount truth color
- $\\it{\\hat{C}}_c(\\rm{r})$ : coarse volume color prediction
- $\\it{\\hat{C}}_f(\\rm{r})$ : fine volume color prediction
- $R$ : rays pool(batch 내부의 모든 ray를 의미)

## Experiment setting detail

  실험에서는 4096 batch size의 ray를 사용, 이는 coarse 하게 추출된 $N_c = 64$와 이를 토대로 추출된 $N_f = 128$을 기반으로 한다. Optimizer는 Adam을 사용하였으며, $5 \\times 10^{-4}$ 부터 $5 \\times 10^{-5}$ 까지 exponentially 줄어들게끔 learning rate을 사용하였다. 하나의 scene을 최적화하는데 약 100~300k iteration이 들며, 이는 저자들이 실험한 세팅에서 약 1~2일 걸렸다고 한다(오래 걸리는게 이 모델의 단점인 것 같다).


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209057573-05991343-976d-4722-9f28-b44b0b22cea8.png" width="800"/>
</div>
 

## Results

  실제로 위의 표를 보면, 성능 지표로 사용된 PSNR, SSIM 그리고 LPIPS 등 여러 부문에서 NeRF가 좋은 성능을 보이는 것을 볼 수 있다.  [NeRF project page](https://www.matthewtancik.com/nerf)에 들어가보면 보다 자세한 결과를 확인해볼 수 있다.

### *Dataset*

### *Synthetic renderings of objects*

이 논문에서 결과를 보이기 위해 사용한 데이터는 크게 두 가지다. 첫 번째는 DeepVoxels dataset으로, 이는 총 4개의 Lambertian(어느 방향에서 보아도 조광이 일정하다(균일하다)는 뜻) 물체가 간단한 geometry로 구상된 형태의 데이터셋이다. 각 object는 $512 \\times 512$ 픽셀로 viewpoint에서 렌더링되며(반구의 위쪽에서 관찰된 이미지 형태), 479 샘플은 input으로, 1000 샘플은 test에 사용되었다. 또한 추가적으로 보다 사실적인 scene에 대한 데이터셋인 복잡한 기하학적 형태를 가진 non-Lambertian(방향에 따라 물체의 겉면 색이 달라진다는 뜻)물체 8개와, 각각의 pathtraced image를 포함한 형태의 데이터셋을 구성하였다. 여섯은 반구 위쪽에서 관찰된 이미지 형태를 렌더링하고, 둘은 구 전체(물체의 아래 부분까지 포함)에 대한 viewpoint를 렌더링하였다. 모두 $800 \\times 800$ 의 픽셀 사이즈를 가지며, 100개의 view를 input으로 사용하고 200개를 test에 사용하였다. 위의 표에서 **Diffuse Synthetic**이 의미하는 것이 곧 **Lambertian**, **Realistic Synthetic**이 의미하는 것이 **non-Lambertian**이라고 보면 된다.

### *Real images of complex scenes*

또한 이 논문에서는 복잡한 형태의 현실 scene을 앞쪽에서 본 모습을 사용하였는데, 총 8개의 scene을 사용하였다. 이 중 5개는 LLFF paper에서 온 장면이고, 3개는 저자들이 직접 캡처했다고 한다. 20~62개의 이미지가 캡처되었고, 이중 1/8은 test에 사용하였다. 모든 렌더링 이미지의 크기는 $1008 \\times 756$ 픽셀 사이즈를 가진다.

## Model comparisons

NeRF 모델의 성능을 확인해보기 위해, 다른 view synthesis 기술 중 좋은 성능을 보이는 모델들과 비교하였고, 이를 실제로 시각적으로 페이퍼에 기재하였다. 모든 기술들은 같은 input view에 대해 학습하고(Local Light Field Fusion은 제외) 그 성능을 비교하였다.

### Neural Volumes(NV)

[Neural Volumes](https://research.facebook.com/publications/neural-volumes-learning-dynamic-renderable-volumes-from-images/)는 분리된 배경(무조건 object를 제외하고 촬영이 되어야 함) 정면에 존재하는 bounded volume의 view를 합성한다. 최적화는 deep 3D convolutional network를 사용하고, 불연속적인 RGB$\\alpha$ 값을 voxel grid마다 ($128^3$ 샘플 사용) 예측하고 동시에 3D warp grid를 $32^3$ 샘플로 예측하게 된다. 전체적인 알고리즘은 아래 그림에서 볼 수 있듯이, marching camera ray를 warped voxel grid에 렌더링하는 방식으로 진행된다.


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209057576-77ac467c-690e-4b30-88a9-41d49c3272a1.png" width="800"/>
</div>
 

### Scene Representation Networks(SRN)

SRN은 연속적인 장면을 불투명한 형태의 표면(surface)로 표현하고자, MLP를 활용하여 각 $(x, y, z)$ 좌표에 대한 feature vector를 추출한다. 그리고 RNN 구조를 활용하여 ray를 따라 진행하게 되고, 이때 앞서 추출된 feature vector를 사용한다. 각 state에서 내보내는 output은 곧 matching step size를 결정하게 된다(ray를 따라서 이동하는 형태로 생각). 최종단에서 추출되는(decoding 되는) feature vector는 output으로써 마지막 위치에서의 surface color를 디코딩한다.

### Local Light Field Fusion

LLFF는 forward facing scene(앞에서 본 이미지)를 기반으로 사실적인 novel view를 만들기 위해 디자인된 모델이다. 학습된 3D convolutional network를 사용하여 RGB$\\alpha$ 그리드를 생성하고, alpha compositing과 nearby MPI를 활용하여 novel scene을 렌더링하는 방식이다.


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209057577-a9bfa3db-3ad4-4ea2-971b-8763a94ff10d.png" width="700"/>
</div>
 

MPI 예시 그림. marching된 씬을 따라 움직이면 multiplane image를 생성할 수 있다는 형태로 이해하면 쉽다

## Comparison results


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209057578-5f76a207-a13d-4c62-a0c8-813c761f06b1.png" width="700"/>
</div>
 


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209057580-9d57a4da-9e39-4480-8f37-1895441d6407.png" width="700"/>
</div>
 

결과도 잘 나오는 것을 확인 가능.

## Discussion

 잘 나왔다는 점이 눈에 가장 먼저 보인다. 디테일한 부분들을 살리지 못했던 기존 방식에 비해 NeRF는 3D representation 정보를 사용하지 않고도 충분히 detail을 살린 결과들을 보여주었다. 그러나 실용적으로 사용되기 힘들었던 점은 시간과 공간 표현 대비 trade-off가 너무 큰 문제로 작용했기 때문인데, 예를 들어 기존 방식의 LLFF와 같은 경우 시간은 다소 적게 걸리지만 그만큼 Realistic Synthetic scene을 위한 메모리 차지가 심하다는 단점이 있었다. 그에 비해 NeRF는 네트워크 자체는 상당히 간단한 편에 속하기 때문에, 다양한 데이터셋에 대해서도 최적화가 가능하다는 장점이 있다.

## FC layer structure


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209057581-93b0de01-f6ca-47bf-bc65-733dfe5d7bcb.png" width="700"/>
</div>
 

위는 fully-connected network 구조,  positional encoding이 더해진 형태의 위치 정보$(\\gamma(\\rm{x}))$ 를 input으로 넣어주게 되고, 256 채널과 ReLU로 엮인 총 8개의 네트워크를 통과하게 된다. 해당 논문에서는 DeepSDF 구조를 따르고, skip connection을 5번째 layer의 activation에 넣어주게 된다.

또한 추가 레이어는 volume density인 $\\sigma$ 를 output으로 내보내게 되는데, 이때 output은 non-negative임이 보장되어야 하므로 ReLU를 사용하여 rectify한 output을 사용하였다. density와 함께 추출된 256의 채널로 구성된 feature 정보는 이후 방향 벡터가 인코딩된 정보$(\\gamma(\\rm{d}))$ 와 함께 마지막 레이어를 통과하면서 RGB 값을 추출해낸다.

## NDC ray space derivation

forward facing scene을 만들기 위해서 NDC space라는 개념이 사용된다. 행렬이 무자비하게 나오는 파트라서 간단하게만 짚어보고 넘어가는게 좋을 것 같다.


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209057582-3857242c-b016-4b2f-b57b-bd74db2ef457.png" width="800"/>
</div>
 

NDC는 쉽게 말해서 카메라가 인지하는 공간적인 형태를 고르게 펴서 cube 형태로 맞춰주는 작업을 의미한다. 우리가 흔히 알고 있는 기본적인 3D perspective projection matrix는 homogeneous 좌표계에 따라서,

$$
M = \\left(
\\begin{array}{cccc}
 \\frac{n}{r} & 0 & 0 & 0 \\newline
 0 & \\frac{n}{t} & 0 & 0 \\newline
 0 & 0 & \\frac{-(f+n)}{f-n} & \\frac{-2fn}{f-n} \\newline
0 & 0 & -1 & 0
\\end{array}
\\right)
$$

위와 같이 표현 가능하다. $n, f$ 는 가각 clipping plane 까지의 거리라고 보면 되고, $r, t$ 는 각각 오른쪽과 위쪽 bound(가까운 clipping plane)를 의미한다. 위의 그림에 얼추 나와있으니 보면서 이해하면 좋다.

Homogeneous point인 $(x, y, z, 1)^T$ 를 프로젝션하기 위해, M을 곱하고 마지막 coordinate으로 정규화를 진행한다. 자세한 내용은 Homogeneous 관련 lecture를 들으면 된다.

$$
M =
\\left( \\begin{array}{cccc}
 \\frac{n}{r} & 0 & 0 & 0 \\newline
 0 & \\frac{n}{t} & 0 & 0 \\newline
 0 & 0 & \\frac{-(f+n)}{f-n} & \\frac{-2fn}{f-n} \\newline
0 & 0 & -1 & 0
\\end{array} \\right)
\\left( \\begin{array}{c}
x \\newline
y \\newline
z \\newline
1 \\newline
\\end{array}\\right) = 
\\left( \\begin{array}{c} \\frac{n}{r}x \\newline
\\frac{n}{t}y \\newline
\\frac{-(f+n)}{f-n}z-\\frac{-2fn}{f-n} \\newline
-z \\end{array} \\right)
$$

여기서, 프로젝션된 좌표를 다시 원래의 좌표로 바꾸게 되면

$$
\\text{Project} \\rightarrow
\\left( \\begin{array}{c} \\frac{n}{r}\\frac{x}{-z} \\newline \\frac{n}{t} \\frac{y}{-z} \\newline \\frac{(f+n)}{f-n}-\\frac{2fn}{f-n} \\frac{1}{-z} \\end{array} \\right)
$$

이제 NDC space에 놓여있게 되므로, 우리는 특정 ray 벡터 $\\rm{o}+\\it{t}\\rm{d}$ 상의 모든 point를 NDC space인 $\\rm{o^\\prime}+\\it{t^\\prime}\\rm{d^\\prime}$ 옮기는 것이 목표다. 앞서 구한 projection point들을 다음과 같이 바꿔쓰도록 하자.

$$
\\text{Project} \\rightarrow
\\left( \\begin{array}{c} \\frac{n}{r}\\frac{x}{-z} \\newline \\frac{n}{t} \\frac{y}{-z} \\newline \\frac{(f+n)}{f-n}-\\frac{2fn}{f-n} \\frac{1}{-z} \\end{array} \\right) = 
\\left( \\begin{array}{c} a_xx/z\\newline a_yy/z \\newline a_z+b_z/z \\end{array} \\right)
$$

그렇게 되면 projection 되는 $o^\\prime$ 그리고 direction $d^\\prime$ 은 다음을 만족해야한다.

$$
\\left( \\begin{array}{c}
a_x \\frac{o_x+td_x}{o_z+td_z} \\newline
a_y \\frac{o_y+td_y}{o_z+td_z} \\newline
a_z+\\frac{b_z}{o_z+td_z}
\\end{array} \\right) = 
\\left( \\begin{array}{c}
o^{\\prime}_x+t^{\\prime}d^{\\prime}_x \\newline
o^{\\prime}_y+t^{\\prime}d^{\\prime}_y \\newline
o^{\\prime}_z+t^{\\prime}d^{\\prime}_z
\\end{array} 
\\right)
$$

그러나 위의 식은 constraint가 없어 너무 변수가 많아지므로 식이 복잡하다. 간단하게 표현하기 위해 starting point를 기준으로 식을 바꾸면,

$$
\\left( \\begin{array}{c}
a_x \\frac{o_x}{o_z} \\newline
a_y \\frac{o_y}{o_z} \\newline
a_z+\\frac{b_z}{o_z}
\\end{array} \\right) =
\\left( \\begin{array}{c}
o^{\\prime}_x   \\newline
o^{\\prime}_y \\newline
o^{\\prime}_z
\\end{array}
\\right) = o^\\prime = \\pi(o)
$$

위와 같고, 정확히 origin $o$ 를 projection한 대칭 point가 $o^\\prime$ 임을 확인할 수 있다. 이를 다시 역으로 적용해서 변수 $t$ 가 그대로 살아있는 형태로 적용하면,

$$
\\begin{aligned}
\\left( \\begin{array}{c}
t^{\\prime}d^{\\prime}_x \\newline
t^{\\prime}d^{\\prime}_y \\newline
t^{\\prime}d^{\\prime}_z
\\end{array} 
\\right) =&
\\left( \\begin{array}{c}
a_x \\frac{o_x+td_x}{o_z+td_z}-a_x \\frac{o_x}{o_z} \\newline
a_y \\frac{o_y+td_y}{o_z+td_z}-a_y \\frac{o_y}{o_z} \\newline
a_z+\\frac{b_z}{o_z+td_z}-a_z-\\frac{b_z}{o_z}
\\end{array} \\right) \\newline
=& \\left( \\begin{array}{c}
a_x \\frac{td_z}{o_z+td_z}\\left( \\frac{d_x}{d_z}-\\frac{o_x}{o_z}\\right) \\newline
a_y \\frac{td_z}{o_z+td_z}\\left( \\frac{d_y}{d_z}-\\frac{o_y}{o_z}\\right) \\newline
-b_z\\frac{td_z}{o_z+td_z}\\frac{1}{o_z}
\\end{array} \\right)
\\end{aligned}
$$

위의 식에서 $t$ 에 대한 부분을 제외하고 나면 남는 $d^\\prime$ 의 형태는 다음과 같다.

$$
\\left( \\begin{array}{c}
a_x \\left( \\frac{d_x}{d_z}-\\frac{o_x}{o_z}\\right) \\newline
a_y \\left( \\frac{d_y}{d_z}-\\frac{o_y}{o_z}\\right) \\newline
-b_z\\frac{1}{o_z}
\\end{array} \\right)
$$

다시 원래 사용했던 original projection matrix의 constants로 식을 정리하면,

$$
o^{\\prime} = \\left( \\begin{array}{c}
a_x \\frac{o_x}{o_z} \\newline
a_y \\frac{o_y}{o_z} \\newline
a_z+\\frac{b_z}{o_z}
\\end{array} \\right) = 
\\left( \\begin{array}{c}
-\\frac{f_{cam}}{W/2} \\frac{o_x}{o_z} \\newline
-\\frac{f_{cam}}{H/2} \\frac{o_y}{o_z} \\newline
1+\\frac{2n}{o_z}
\\end{array} \\right)
$$

$$
d^\\prime = \\left( \\begin{array}{c}
-\\frac{f_{cam}}{W/2} \\left( \\frac{d_x}{d_z}-\\frac{o_x}{o_z}\\right) \\newline
-\\frac{f_{cam}}{H/2} \\left( \\frac{d_y}{d_z}-\\frac{o_y}{o_z}\\right) \\newline
-2n\\frac{1}{o_z}
\\end{array} \\right)
$$

위와 같이 표현 가능하다.
`,$O=`---
title: "NSVF-Neural Sparse Voxel Fields에 대해서"
category: "ai papers"
publishedAt: "2022-12-06"
thumbnail: "https://user-images.githubusercontent.com/79881119/209058084-f7bf887e-0a84-4cec-8955-67f93cbec2b5.png"
--- 

## Abstract

사실적인 형태의 real-world scene을 만들어내는 것은 상당히 어려운 일이다. 이전 세션에서 살펴본 NeRF의 경우, 간단한 구조의 Neural Network을 사용하여 학습되지 않은 방향 및 좌표에서의 scene rendering을 어느 정도 성능 이상으로 증가시켰지만, 여전히 학습이 오래 걸린다는 점, 다량의 데이터 수집(특히 실험 의도에 따라 적절한 camera marching이 제대로 구성되어야 하는 실험적 constraint가 존재한다)이 주요 bottleneck으로 작용한다는 단점이 있다. 또한 다양한 방법의 3D supervision에서 제시한 방법들은 추론 시간을 줄이기 위해 network의 capacity에 대해 많은 여유를 확보하지 못하고, blurry rendering 결과에서 만족하기도 한다. 이처럼 3D scene rendering(generation)은 성능을 향상시키기도 어렵고, 그만큼 데이터 수집이나 적절한 metric을 찾기 곤란한 경우가 많다.

이번 논문에서는 기존 방식보다 rendering 성능을 높이며(high quality) 그와 동시에 속도도 증가시킨(fast) NSVF(*Neural Sparse Voxel Fields*)를 제시한다. 뒤에서도 설명하겠지만 논문 제목에서 알 수 있듯이 Voxel 기반의 학습법을 제시했으며, sparse voxel octree(물체가 있을법한 장소에 대해서만 rendering 및 학습을 진행)함으로써 학습 시간 단축과 샘플링 퀄리티를 dense하게 가져갈 수 있다는 장점을 보여준다. 또한 explicit한 3D representation을 사용함으로써 scene editing 및 scene composition 등에 사용될 수 있는 가능성도 보여준다.

## Introduction

컴퓨터 그래픽 분야에서 scene rendering, realistic rendering이 흔히 활용될 수 있는 형태로는 AR, VR 뿐만 아니라 영화 사업이나 방송에서 활용될 수 있는 비쥬얼 이펙트가 될 수도 있고 연구 분야로는 데이터 생성에 큰 기여를 할 수 있다. 그러나 실제로 활용될 수 있을 정도의 고퀄리티 샘플은 얻기가 힘들다는 challenge가 있다. 이는 실제 scene에 대한 3D representation을 얻기 힘들 뿐만 아니라(LiDAR와 같은 센서는 너무 비싸다) 이를 해결하고자 연구한 IBR(Image Based) 접근법은 결과에 대한 조작이 어려우며(scene의 다양성이 떨어짐), 획득이 간편함에 따라 퀄리티는 보장할 수 없다. 이러한 한계점 때문에 최근의 연구들은 딥러닝을 활용, embedding space에서 가상의 geometry와 appearance를 학습하게 된다. 이러한 연구들은 training을 위해 3D geometric model인 voxel grid, textured mesh를 활용하기도 하며, multi-plane image(depth 정보가 있는 multi-channel이라고 보면 됨), point cloud 등등과 함께 활용되기도 한다. 이러한 explicit geometric representation을 사용하는것보다, neural implicit을 사용하는 방식은 보다 부드럽고 연속적이며(representation은 computing 환경에서 가상으로 생성한 synthesized geometry이므로 discrete하다),  신호처리 관점에서는 high spatial resolution, free-aliasing이라는 장점이 있다. 그러나 실제로 학습 과정에서는 network가 충분한 파라미터를 갖지 못해(learning capacity 부족) 그만큼 성능을 내지 못하는 것이 알려졌고, scene geometry에서 camera ray의 교차점을 찾기 힘들다는 점이 문제로 발생한다.

NSVF 논문에서도 마찬가지로 implicit function을 기반으로 한 Neural Sparse Voxel Fields를 제시한다. 물론 여기서의 implicit field는 sparse voxel이기 때문에 3D geometry가 포함된 것처럼 생각될 수 있겠지만, sparse voxel은 학습 과정에서의 sampling을 위한 임베딩 공간 상에서 가상으로 정의한 부분에 해당된다. 기존 NeRF에서는 각 포인트 및 방향 자체가 MLP에서의 단서(key feature)로 활용되었다면, Voxel에서는 각 point를 대표하는 정육면체의 8개의 꼭짓점(vertices라 부른다) 좌표가 요약된 형태의 feature vector가 사용된다. Query point의 대표성으로 사용되는 형태는 다르지만, 결론적으로는 NeRF에서의 방식과 동일하다는 것이 수식적으로 증명되는데, 이는 뒤에서 한번 더 언급하도록 하겠다.

학습 과정에서 scene 정보가 희박한 voxel grid는 puning되고, 남은 tree node들에 대해서 추가적인 sampling을 통해 디테일한 구조를 잡아가고, 이러한 구조 속에서의 representation을 학습, 최종적으로는 디테일한 부분을 보다 효율적으로 학습하는 방법을 제시한다. 실제로 NeRF 모델에 비해 10배 가량 빠른 성능을 보인다.

## Neural rendering with implicit fields

Implicit function을 학습 가능한 neural network인 $F_\\theta$ 에 대해서 $F_\\theta : (p, v) \\rightarrow (c, \\omega)$ 라 두면, 핀홀 카메라의 포지션  $p_0 \\in R^3$ 에 따라 render color $C$ 를 다음과 같이 표현할 수 있다.

$$
C(p_0,~v) = \\int^{+\\infty}_0 \\omega(p(z)) \\cdot c(p(z),~v)dz
$$

,where $\\int^{+\\infty}_0 \\omega(p(z))dz = 1$ 각 노테이션에 대해 간단하게 짚자면,  $v$ 는 바라본 방향에 대한 정보, $p(z)$ 는 카메라의 원점부터 시작하여 ray를 따르는 방향으로 뻗은 벡터에 해당된다. 즉,

$$
p(z) = p_0+z\\cdot v
$$

이고, 물론 공간 상에서 이 벡터 전부가 물체와 intersection하는 representation이 되진 않으므로 weight가 곱해져서 적분되는 형태로 나타나는 것이다. Weight의 경우 물체의 density, 혹은 ray가 그 지점까지 도달할 동안 아무런 물체와도 마주치지 않을 확률 등으로 해석된다. 따라서 weight는 방향 벡터($v$)에 대해 무관한 함수로 표현되며 color는 non-Lambertian surface(reflection)의 경우에 대해 고려해야 하기 때문에 방향 벡터에 대해 유관한 함수로 표현된다. 이는 실제로 학습 네트워크 구조에서도 반영되는 것을 확인할 수 있다.

### Surface rendering

표면을 렌더링하는 것은 앞서 렌더링한 것과 다르게 ray와 물체의 겉면이 만나는 부분을 찾아내는 것이기 때문에 weight가 총합 1로 렌더링되는 것이 아닌 디렉델타 function으로 표현된다.

$$
\\omega(p(z)) = \\delta(p(z)-p(z^*))
$$

where  $p(z^*)$ is the intersection of the camera ray with the scene geometry

### Volume rendering

volume을 렌더링하는 것은 앞서 말한 개념이랑 거의 동일하다. NeRF에서는 샘플링을 통해 렌더링하는데, 다음과 같은 식을 보도록 하자.

$$
C(p_0,~v) \\simeq \\sum_{i = 1}^{N} (\\prod_{j = 1}^{i-1} \\alpha(z_j,~\\Delta_j)) \\cdot (1-\\alpha(z_i,~\\Delta_i))\\cdot c(p(z_i),~v)
$$

식에서의 alpha는 $\\alpha(z_i,~\\Delta_i) = exp(-\\sigma(p(z_i) \\cdot \\Delta_i))$, 그리고 $\\Delta_i = z_{i+1}-z_i$(sampling 간격)
을 의미한다.

## Limitations

표면을 렌더링하는 것은 여러 view에 대해 학습된 color가 일정하게끔 학습되므로 렌더링이 흐려지는 문제가 발생한다. 부피를 렌더링하는 것은 표면을 렌더링하는 것보다 많은 정보가 필요하므로 높은 성능을 위해서는 dense sampling이 필요하다는 한계가 있다. 실제로 속도가 많이 느리다(NeRF는  $800 \\times 800$ 크기의 이미지를 렌더링하기 위해 30초 가량이 소모된다).

## Definition

그래서 결국 이 논문에서 소개하고자 하는 NSVF가 정확하게 어떤 의미로 적용되는 것일까? 이를 보기 위해서 먼저 Voxel bounded implicit field를 보도록 하자.

### Voxel-bounded implicit fields

다음과 같이 가정해보자. 특정 방향에서 바라본 scene의 모습에서 공간 상의 non-empty part를 각각 특정 크기의 voxel로 대표하고, 이러한 K개의 voxel를 하나의 set로 생각해보자.

$V = \\{V_1,~\\cdots,~V_K\\}$ 이와 같이 표현되는 voxel representation에 대해서 scene은 여러 개의 voxel-bounded implicit function을 통해 예측되는함수로 표현 가능하다. 각 voxel에 대한 다변수 함수라고 생각하면 된다.

$$
F_\\theta(p, v) = F_\\theta^i(g_i(p), v), \\forall p \\in V_i
$$

즉, 각 point가 포함되는 voxel에 대한 parametric estimation으로 표현된다. 그렇다면 각 3D point $p$ 에 대한 color 및 density, 그리고 해당 point에 대한 representation에 대해 함수를 분리할 수 있다.

$$
g_i(p) = \\zeta(\\chi(\\tilde{g_i}(p_1^*), \\cdots, \\tilde{g_i}(p_8^\\ast)))
$$

point가 속한 vertex의 8개의 vertices를 input으로 각 vertex에는 $\\tilde{g_i}$ 를 통해 feature vector를 저장한다. 추가로 $\\chi(\\cdot)$ 함수는 trilinear interpolation이고 $\\zeta(\\cdot)$ 는 positional encoding이다 positional encoding은 한정된 차원에서의 manifold의 학습이 어렵다는 측면에서 고차원으로 끌어올려주기 위해 NeRF에서도 사용했던 방법이다.

이러한 식으로 표현되는 NSVF에서, 결론적으로는 만약 vertex를 활용하지 않고 point 자체가 함수에 사용된다면 이는 특별한 케이스인 NeRF가 된다. 즉,

$$
g_i(p) = \\zeta(\\chi(\\tilde{g_i}(p_1^\\ast), \\cdots, \\tilde{g_i}(p_8^\\ast)))
$$

$$
= \\zeta(p)
$$

이면 그냥 원래의 NeRF와 동일하다. 그리고 $\\tilde{g_i}(p) : p \\rightarrow (c, \\sigma), \\zeta(\\cdot)$ 이며 $F_\\theta^i$가 그냥 identity function이라면 해당 모델은 단순히 explicit voxel에 color 및 density를 담는 NV(Neural Volumes)와 같은 식이다.

## Volume rendering

NSVF는 voxel에 포함된 scene의 어떠한 점에 대해서도 color 및 density를 예측한다. NeRF와 같이 전체 space에 대한 implicit representation을 렌더링하는 것과 다르게, NSVF를 렌더링하는 것은 빈 부분에 대한 고려가 들어가지 않으므로 훨씬 효율적이다. Rendering은 두 step으로 분리되어 진행된다.

1. Ray와 Voxel의 intersection을 찾는다.
2. Voxel 내부에서의 ray-marching을 진행한다.


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209058084-f7bf887e-0a84-4cec-8955-67f93cbec2b5.png" width="700"/>
</div>
 

### Ray-voxel intersection

먼저, AABB(Axis-Aligned Bounding Box) intersection을 적용한다. 이는 좌표계에 축을 맞춘 bounding box(voxel)와 ray에 대해서 voxel의 6개의 face(면)들과의 ray origin 사이 거리를 측정한다. 이러한 AABB 체크 방식은 hierarchical octree structure를 가지는 NSVF와 같은 task에서 잘 적용된다. 실제 실험을 통해 약 10k~100k의 sparse voxel를 통해 충분히 complex scene rendering 을 최적화할 수 있음이 보였다.

### Ray Marching inside Voxels

$$
C(p_0,~v) \\simeq \\sum_{i = 1}^{N} (\\prod_{j = 1}^{i-1} \\alpha(z_j,~\\Delta_j)) \\cdot (1-\\alpha(z_i,~\\Delta_i))\\cdot c(p(z_i),~v)
$$

위와 같은 volume rendering 식을 통해 $C(p_0, v)$를 샘플링한다. Ray가 모든 object를 놓치는 경우를 배제하기 위해서, background term인 $c_{bg}$를 붙여 사용한다.

Transparency $A(p_0,~v) = \\prod^N_{i=1} \\alpha(z_i, \\Delta_i)$와 $C_{bg}$는 background에 대해 학습이 가능한 RGB value가 된다. 앞서 논의했던 것과 같이, 전체 공간에 대해서 rendering하는 것은 실제로 물체가 있는 영역에 대한 sampling이 부족하기 때문에 시간적으로도 비효율적이고 resolution이 높지 않다는 단점이 있다. 따라서 NeRF에서는 이를 coarse network와 fine network로 분리해서 학습하는데, 

$$
\\hat{C}c(\\rm{r}) = \\sum{i=1}^N \\it{w}_i\\rm{c}_i,~\\it{w}_i = T_i(1-\\exp(-\\sigma_i \\delta_i))\\rm{c}_i
$$

이도 결국 단일 네트워크로 학습이 불가능하고 두 네트워크가 서로 직렬로 구성되어 학습 속도 및 추론 속도의 저하를 불러온다는 문제가 있다. 이와는 다르게 NSVF에서는 두 번째 sampling 과정이 필요하지 않을 뿐더러 같은 evaluation cost를 가지면서도 관심 있는 영역에서의 샘플링이 가능하기 때문에 훨씬 효율적이다.

### Early termination

NSVF는 불투명한 물체, solid한 물체 모두 잘 표현할 수 있다. 그러나 solid surface의 경우, 사실상 겉으로 보이는 부분에 대한 voxel만 처리하면 되고 내부의 불필요한 accumulation은 시간만 잡아먹게 된다(accumulated transparency $A(p_0,~v) = 0$). 따라서 해당 실험에서는 $\\epsilon = 0..01$이라는 threshold를 두고, 만약 이보다 transparency가 작아진다면 rendering process를 조기에 끝내는 방식을 사용, 품질 저하 없이 속도를 증가시킬 수 있었다. 

## Training

식을 보면 알 수 있듯이, $C_{bg}$ term 이외에는 NeRF와 동일한 형태를 사용, 모든 rendering process가 미분이 가능하기 때문에 reference image와의 정량적인 cost function을 정의할 수 있다.

$$
L = \\sum_{(p_0,~v) \\in R} \\parallel C(p_0,~v)-C^*(p_0,~v) \\parallel_2^2 + \\lambda \\cdot \\Omega(A(p_0,~v))
$$

R은 batch sampled ray를 의미하고, $C^*$은 camera ray의 GT color를, $\\Omega(\\cdot)$는 beta-distribution regularizer이다. Beta regularizer는 간단하게  0~1 사이의 변수에 대한 정규화를 진행하는 함수라고 보면 된다.

### Voxel initialization

부피 V를 가지는 초기 bounding box를 가로, 세로, 높이 각각 10등분한 sub voxel 들로 초기화 한다. 이 과정이 결국 coarse sampling이라고 보면 되는데, 여기서부터 Self-pruning, rendering 등등 거치면서 세부화된다. Self pruning은 말 그대로 non essential한 voxel들을 가지치기, 없애는 작업이 되고, density probabilty가 특정 threshold보다 크다면(해당 위치에서 ray가 물체와 만날 확률을 의미한다고 NeRF에서 해석), 이 부분의 voxel은 아예 computation에서 제외하는 것이다.

$$
V_i \\text{ is pruned if} \\min_{j = 1, \\cdots, G} exp(-\\sigma(g_i(p_j))) > \\gamma,~p_j \\in V_i,~V_i \\in \\nu
$$

$\\{p_j\\}^G_{j = 1}$는 voxel $V_i$ 내부에서 uniformly sampled된 point에 해당된다(실험에서는 voxel 내부에서 가로, 세로, 높이 각각 16등분한 $G = 16^3$ 사용). Threshold인 $\\gamma = 0.5$를 모든 실험 환경에서 사용했다고 한다.

## Progressive training

사실 이 부분에서 조금 띠용했던 건, 시작부터 NeRF에서의 hierarchical network를 비판했으면서 여기서도 iteration을 돌면서 detail한 부분을 학습해나간다는 것이다. 솔직히 속도가 빨라진 것은 pruning 영향이 크고, 성능 향상은 cost를 동일하게 가져가면서 sampling을 보다 dense하게 할 수 있다는 장점이 있는 것 같은데, 아무리 봐도 이 부분은 NeRF의 방식과 거의 유사하다고 생각된다.


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209058401-0e8d1f3f-446f-43b8-a591-3673b83c9c67.png" width="700"/>
</div>
 

그림에서 보이는 것과 같이, 한 번 iteration을 돌고 나면 각 voxel을 axis 기준으로 2등분($2^3$)하여 sub-voxel에 대해 다시 pruning 및 이것저것 진행된다. 물론 세분화되기 때문에 중간 vertex에는 feature가 없다는 문제가 있지만 이를 간단하게 trilinear interpolation을 통해 해결한다. Trilinear interpolation은 bilinear interpolation이랑 같은 원리인데 축이 하나만 더 추가된 것이다.


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209058405-bcc9fd77-4440-49f4-9564-49791adce570.png" width="700"/>
</div>
 
`,yO=`---
title: "MixMatch-A holistic approach to semi-supervised learning에 대하여"
category: "ai papers"
publishedAt: "2022-12-07"
thumbnail: "https://github.com/user-attachments/assets/d48ce4c4-29fb-434a-a05a-959252e064bf"
---


준지도학습이란 흔히 일부 레이블이 존재하는 데이터에 대해 학습한 모델을 토대로 레이블이 없는 데이터셋에 대해서 학습을 진행하는 것을 의미한다. 즉, unsupervised learning과 동일하게 representation mapping을 어떠한 방식으로 해결할 것인지에 대한 분석이 중요하며, 본 게시글에서는 SSL(Semi-supervised learning)에서 가장 성능을 끌어올렸던 유명한 논문 중 하나인 MixMatch에 대해 리뷰하도록 하겠다. [논문링크](https://arxiv.org/abs/1905.02249)

## Deep learning의 구성 요소
뜬금없이 딥러닝의 구성 요소를 언급하고자 한 이유는 바로 그 안에 supervised, semi-supervised 등 딥러닝 연구 분야에 대한 갈림길이 내재되어있기 때문이다. 딥러닝이 성공한 주요 이유는 머신 러닝의 한 기법 중 하나인 Neural Network를 보다 효율적인 형태로 구성하고, 이를 gradient based 방식으로 최적화하기 위해 SGD, Adam 등 다양한 optimization 방법이 제안되었다. 또한 ReLU function이나 residual learning과 같이 현재에도 SOTA로 쓰일 정도로 좋은 모델링 방법이 많이 연구되었고, 여전히 transformer 구조 및 multimodal 등 다양한 형태의 연구가 진행되는 중이다.
이러한 서사를 막론하고, 결국 우리가 목표로 잡는 학습을 cost function, objective function이라 부르고, 이를 최적화하기 위해서는 충분히 많은 데이터가 필요하다. 예를 들어 이미지 분류 작업같은 경우, 단순히 이미지를 n개의 class로 구분하는 지표화 작업이 필요하지만, segmentation과 같은 경우 이미지 각 픽셀에 대한 지표화가 필수적이고, 이런 경우 time consuming 문제가 있다. 다른 문제로는 만약 의학적인 지식이 필요한 상황이라면(의료 CT, MRI 이미지를 통한 진단 딥러닝 알고리즘을 설계하려고 한다면), 레이블링에는 충분히 많은 사전 지식(도메인 지식)이 필요하기도 하다. 또한 만약 어떠한 영상이나 이미지를 보고 등장하는 사람이나 사물에 대한 private information이 지표화에 필요할 경우, 사생활 문제가 야기될 수 있다.
결국 길게 말하고자 한 것은 방대한 데이터셋과 augmentation 방법으로 딥러닝 네트워크 파라미터로 하여금 보다 일반화에 가까운 representation을 학습한 것이 딥러닝이 좋은 성능을 낼 수 있는 키포인트인데, 이러한 데이터셋을 얻기 위한 과정이 순탄치가 않다는 것.

## SSL : Semi-Supervised Learning
위에서도 사용했지만, 준지도 학습에 대한 단어는 SSL로 통일하도록 하겠다. 준지도 학습에서 지표화된 데이터셋의 필요를 줄이기 위해서 접근한 연구 방식은 모델로 하여금 unseen data에 대해 unlabeled dataset이 보다 일반화에 도움을 줄 수 있게끔 해주기 위한  "loss term"을 잘 설정하는 데에 있었다.
예를 들어 라벨링이 되어있지 않은 그림에 대해서 우리는 고양이 사진을 보고 당연히 "고양이"라고 오차 없이 예측할 수 있지만, 네트워크는 모든 분류 작업을 확률 기반(softmax)으로 계산하기 때문에 이러한 형태의 확신을 가지기 힘들다. 이는 이후 SSL을 MixMatch라는 알고리즘으로 접근한 해당 논문의 내용에서 더 자세히 다루도록 하고, loss term을 바꾸는 연구의 세 가지 큰 방식을 구분하도록 하겠다.

### Entropy minimization
첫 번째는 Enropy minimization이다. 일반적으로 지표화가 진행되지 않은 데이터를 모델이 예측하면 확률값으로 매핑이 된다.

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209059028-f13d36bd-c752-4c19-9cc8-b016185d4f75.jpg"/>
</div>
 
만약 지표화가 된 상황에서의 데이터라면 "개, 고양이, 원숭이"의 3가지 클래스를 구분하는 작업에 있어서 1-hot encoding(이를 hard label이라고도 부른다)을 수행한다. One-hot encoding이란 정답인 확률이 1이고 나머지가 0인 상황이다. 따라서 위의 그림을 그대로 지표화하게 되면 (0, 1, 0)이 되는 것이다.

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209059035-69102d07-26c6-48fb-9b8d-2b15aa0ac992.png"/>
</div>

그러나 이를 네트워크에 통과시킨 결과는 다르다. 매우 잘 학습한 모델이 세 클래스에 대해 결과를 아무리 잘 예측하더라도 (0, 1, 0)이 되기 힘들다.
이는 cross entropy loss의 특성상 softmax를 포함하는 데에서 그 한계를 찾을 수도 있는데, 점수표가 어떤 방식으로 설정되든 이에 대한 CE loss를 계산하기 전 softmax 연산을 통해 확률값으로 매핑한다.

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209059037-8d487115-45d6-46e6-88d7-920feb69f796.png"/>
</div>

그러나 softmax 함수의 경우 0과 1을 점근선으로 가지기 때문에, 실제로는 점수표 상의 그 어떠한 value도 softmax 상에서 0과 1의 절대적인 값을 가질 수 없고, 결론적으로는 잘 학습된 모델의 경우에도 '고양이일 확률이 가장 높다' 정도의 예측이 최선인 것이다.
결국 네트워크를 통과한 각 클래스에 대한 예측값은 확신이 없다고 볼 수 있는데, 이는 지표화되지 않은 데이터셋에 대해 학습을 하게 될 경우 문제가 생긴다. 그렇기 때문에 entropy minimization을 통해 애매한 확률값들을 확실한 값으로 바꿔준다. 이에 대한 내용은 이후 모든 loss term에 대해 설명한 후에 종합적인 분석이 필요한 부분이 있어 뒤로 넘기도록 하겠다.

### Consistency regularization
두 번째는 Consistency regularization이다. 이는 생각보다 간단한 개념인데,

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209059039-fb2d2135-7f02-46e5-b713-c73cb6a67e41.png"/>
</div>

예를 들어 위와 같이 고양이 사진을 90도 회전시킨 augmented sample을 unlabeled dataset으로 사용한다고 생각해보자. 서로 다르게 augmented(노이즈 추가, 컬러 변경, 회전 등)된 두 이미지는 사실 서로 같은 probability distribution을 가져야 한다. 즉 지표화되지 않은 샘플에 대해 예측이 들쭉날쭉하게 변하지 않게 하는 것이 정규화 방식이다.

### Generic regularization
Model의 overfitting을 방지하는 정규화 방식으로, 이후 설명할 MixMatch에서의 MixUp과 관련이 있다. 예를 들어 두 이미지에 대해 지표화가 되어있다고 가정하자. 강아지의 경우 (1, 0, 0)의 라벨을 갖게 되고, 고양이의 경우 (0, 1, 0)의 라벨을 가지게 된다. 이 두 샘플에 대한 convex sample은

$$
    \\theta \\cdot cat + (1-\\theta) \\cdot dog,~(0<\\theta<1)
$$

라고 볼 수 있다. 여기서 convex sample이란 두 샘플을 convex set의 한 지점으로 보고, 그 사이를 보간하는 모든 sample이 포함되는 convex set의 정의를 그대로 따른다고 볼 수 있다. 이를 실제로 시각화하면,

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209059042-6f90bb89-7f19-4a0f-b7cc-a8756fd5c746.png"/>
</div>

이렇게 개냥이가 각각 50%씩 첨가된 샘플이 생성된다. 해당 샘플에 대한 라벨은 마찬가지로 convex sample과 같은 공식에 따라

$$
    \\theta \\cdot (0,\\ 1,\\ 0) + (1-\\theta) \\cdot (1,\\ 0,\\ 0)\\ = (0.5,\\ 0.5,\\ 0)
$$

이러한 방식을 MixUp이라 부른다.
앞서 여러 가지의 정규화 방식을 소개하였고, 이제 본격적으로 MixMatch에서 어떠한 방식을 통해 위와 같은 여러 알고리즘을 통합하여 준지도학습을 진행할 수 있었는지 천천히 소개하도록 하겠다.

## Related works
준지도 학습의 경우 관련 내용이 좀 있는데, MixMatch 논문에서는 전혀 언급하지 않는 분야도 있다. 이 중에 가장 유명한 transductive model, graph-based model 그리고 generative model에 대해 간단하게 소개하도록 하겠다.

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209059045-b04b1a3a-5b87-42c1-8b93-0d4ca790cc43.png"/>
</div>

Transductive learning은 Inductive learning과 다르게, 각 노드(데이터셋)와 엣지(라벨)에 대해 일부 노드에 대한 엣지 정보만 가지고 나머지 노드에 엣지를 부여하는 작업이다. 따라서 그래프 개념으로 해석한 SSL 그 자체로 보면 된다. 그래프 based model도 비슷한 형태로 생각해주면 된다. 물론 위와는 다르게 노드와 엣지의 느낌이 약간 다른데, Graph-based modeling에서 각 노드를 데이터셋으로 보는 방식은 transductive learning에서 해석하는 것과 같지만, 엣지는 유사성을 나타낸다.

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209059047-34b7ea16-352b-4d71-84f2-f5109687188d.png"/>
</div>

간단하게 MNIST 데이터셋으로 기준을 보인다면, 같은 숫자일수록 그래프 상에서 엣지(선으로 표현된 부분)가 강하게 나타날 것이고 이는 곧 유사한 클래스의 데이터일수록 높은 유사도(그래프 상에서는 거리가 가깝다고 역으로 이해할 수 있다)를 보인다고 생각할 수 있다. 에너지 based로 생각하는 것, Hessian과 관련된 수식 증명의 경우 나중에 기회가 된다면 따로 다룰 것이고 오늘 언급할 페이퍼는 해당 내용을 신경쓰지 않기 때문에 넘어가도록 하겠다.

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209059049-7a484a0f-631e-47a8-89a2-629b361e2fe9.png"/>
</div>

Generative modeling 방식은 말 그대로 생성 모델링을 통해 heuristic한 준지도 학습을 진행하게 된다. 이를 테면 노이즈를 제거하는 방식이 될 수도 있고, 이미지에 color를 입히는 작업이 될 수도 있으며 perturbation(빈 부분, 손상된 부분)을 복원하거나 서로 다른 채널을 예측하는 형태로 진행된다.


## Build up for MixMatch
Mixmatch를 언급하기 전에 관련된 준지도학습 관련 내용을 간단하게 언급했다. 위의 내용은 사실상 related works라고 보기는 힘들되, semi-supervised learning을 풀어가려는 다양한 방법론으로 제시가 되고 있다.
그렇다면 MixMatch에서 아이디어로 삼게 된 여러 알고리즘에 대한 기본 내용을 보다 자세히 언급하도록 하겠다.
그 중 가장 첫 번째는 Consistency regularization으로, Augmentation이 서로 다르게 적용되었다고 하더라도 같은 라벨을 예측해야한다는 것을 네트워크 학습에 이용하게 된다.
따라서 stochastic한 함수 Augment(x)가 존재하고, 만약 같은 input image X에 대해 랜덤한 augmentation을 적용하면, 이에 대한 parameterized 모델의 예측은

$$
    p_{model}(y \\vert \\text{Augment}(x); \\theta), p_{model}(y \\vert \\text{Augment}; \\theta)    
$$

와 같이 두 개로 나온다. 여기서 주의할 점은 Augment() 함수 자체가 stochastic하다고 했으므로, 두 개의 term은 서로 다른 예측값을 가진다(같은 value가 아님). 따라서 모델은 다음과 같은 loss term을 최소화하는 방향으로 학습된다.

$$
    \\parallel p_{model}(y \\vert \\text{Augment}(x); \\theta) - p_{model}(y \\vert \\text{Augment}; \\theta) \\parallel_2^2
$$

Mean Teacher 방식에서는 두 개의 term을 서로 다른 모델링을 통해 해결하는데, 바로 아래와 같은 그림을 보면 student model의 경우에는 똑같은 방식으로 최적화가 진행되지만, teacher model은 student model의 parameter를 exponential moving average 방식으로 가져와 사용한다. Exponential moving average를 잘 모른다면 그냥 단순히,

$$
    w_{k+1}^{teacher} = \\beta w_k^{teacher} + (1-\\beta)w_k^{student}    
$$


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209059052-e06308cd-62b4-4969-8a05-1be0d97cb338.png"/>
</div>

처럼 기존 weight에 student weight를 업데이트하는 방식을 사용한다고 생각하면 된다. 자세한 내용은 [여기](https://arxiv.org/abs/1703.01780)를 참고. 그리고 VAT라는 방식(Virtual Adversarial Training)에서는 Adversarial sampling 방법 중에 maximally changes output class distribution을 이용한 perturbation 방식(모델을 가장 혼란스럽게 만드는 augmentation이라고 보면 된다)을 사용, hard sampling을 통해 같은 방식으로 최적화를 한다. MixMatch에서는 단순한 data augmentation 방식으로 볼 수 있는 random horizontal flips and crops를 사용한다.

두 번째는 엔트로피 최소화이다. 사실 그냥 Entropy minimization을 단순 번역한 것. 곰곰히 생각해보면 정보 이론에서 엔트로피가 어떤 식으로 정의되는지 혹시 기억할지 모르겠다. 만약 랜덤 변수 space X에서 각 랜덤 변수가 추출될 확률을 $P = (p_1, p_2, p_3, ...)$ 등으로 정의한다면 해당 space에서의 엔트로피는

$$
    \\begin{aligned}
        \\text{for }X =& (x_1,~x_2,~\\cdots,~x_N)\\text{ where each random variable }x_i(i=1,~\\cdots,~N)\\text{ has a probability }P = (p_1,~\\cdots,~p_N), \\newline
        H(X) =& -\\sum_{i=1}^N p_i \\log(p_i)
    \\end{aligned}    
$$

라 할 수 있다. 물론 지금 이 상황에서는 이산 확률에 대한 가정이지만, 결론적으로 말하자면 얼마나 분포가 고르냐/고르지 않냐의 문제로 귀결된다.

앞서 설명했던 바와 같이 지표화가 진행되지 않은 샘플에 대한 예측은 실제 one-hot encoding 방식에 비해 라벨링 자체의 엔트로피가 높게 생성된다. 또한 앞서 언급한 여러 data augmentation을 거친 샘플들에 대한 예측 결과는 더욱 entropy를 증가시키는 요인이 될 것이다.

이렇게 라벨 대신에 사용할 모델의 예측 확률값들을 Pseudo-Label이라 부르기로 했고, 우리는 이러한 유사 라벨들을 실제 학습에 활용하기 위해 다음과 같은 전략을 세운다.

1. $K$개의 augmentation을 같은 데이터 $X$에 취한다.

2. 각각의 augmented data $K$개를 모델에 통과시킨 예측 확률 map에 평균을 취하고, average label을 생성한다.

3. Average label에 앞서 언급한 entropy minimization을 수행한다.

Sharp label을 만들기 위한 entropy minimization은 temperature hyperparameter $T$에 의해 결정된다. 일반적인 probability에,

$$
    Sharpen(p,~T)_i = \\frac{p_i^{1/T}}{\\sum^L_1 p_j^{1/T}}
$$

이와 같이 적용한 새로운 확률 맵을 이용하는 것이다. 이를 실제로 시각화하여 보면 다음과 같다.

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209059056-4f683a5d-ce95-4a72-8248-e3a1dd1abb4c.png"/>
</div>

$T = 1$이면 원래의 확률 맵과 동일하다. 위와 같이 균등균등하게 설정한 확률 맵에서는 유사한 확률값(0.15, 0.13, 0.12)가 실제 모델 학습에서 dense region problem을 일으킬 수 있다. 이게 무슨 소리냐면,

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209059057-a0ece861-efba-405b-b135-deb59f99f388.jpeg"/>
</div>

위와 같은 그림에서, 진하게 표시된 십자가 모양과 삼각형 모양이 라벨링 된 데이터고 이에 대해 학습을 진행한 후에 unlabeld 샘플(파란색/주황색 점들)에 대해 decision boundary를 고려하는 상황이라면, 실선으로 나와있는 경계선보다 점선으로 나와있는 경계선이 분포 상으로 덜 밀집된 부분을 지나가기 때문에 적절한 경계선으로 보인다. 이렇듯 경계선이 밀도가 높은 지점을 지나게 되면, 해당 경계선 위치에 있는 샘플의 경우 보다 확률이 애매하게 매핑되기 때문에 이를 방지하기 위한 minimization 방법을 고안하게 된 것이다.

<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209059060-042b9c8c-5546-443b-bd44-6b8050fba5ca.gif"/>
</div>

실제로 $T$값을 점차 감소시키면서 위의 식을 적용해보는 모습이다. $T$가 $0$에 가까워질수록 분포는 one-hot encoding에 가까워지고 $T \\rightarrow 0$가 되면 one-hot encoding에 수렴한다.
<figure class="half">
    <img src="https://user-images.githubusercontent.com/79881119/209059063-258cba67-de1e-4de7-ada3-b6d32971acdb.png" width="600" />
    <img src="https://user-images.githubusercontent.com/79881119/209059066-0e54d4e6-842d-4fc8-9ed7-3545f8d9a574.png" width="600" />
</figure>

## MixMatch algorithm

<div align="center">
  <img src="https://github.com/user-attachments/assets/d48ce4c4-29fb-434a-a05a-959252e064bf"/>
</div>

MixMatch 알고리즘이 사용하는 loss objective는 크게 두 가지로 구분된다. 준지도 학습을 구성하는 labeled dataset과 unlabeled dataset 각각에 대해 적용되는 loss(CE loss for labeled sample, consistency loss for unlabeled sample)이 서로 다르기 때문이다.

뒤이어 알고리즘 전반에 대해 디테일하게 설명하기 전에, $X', U'$는 각각 labeled dataset으로부터의 augmented dataset 그리고 unlabeled dataset으로부터의 augmented dataset을 의미한다.

$$
    X',~U' = MixMatch(X, U, T, K, \\alpha)    
$$

$X, U$는 augmentation이 진행되기 전 각 dataset을 의미하고 $T$는 entropy minimization에 사용되는 temperature, $K$는 unlabeled dataset에 적용될 augmentation 개수, $\\alpha$는 MixUp에 사용될 convex coefficient에 해당된다.

$$
    L_X = \\frac{1}{\\vert X' \\vert} \\sum_{x, p \\in X'} H(p, p_{model}(y \\vert x; \\theta))   
$$

당연하게도 라벨이 존재하는 데이터에 대해서는 원래의 label에 대한 cross entropy loss를 적용하게 되고,

$$
    L_U = \\frac{1}{L\\vert U' \\vert} \\sum_{u, q \\in U'} \\parallel q-p_{model}(y \\vert u; \\theta) \\parallel_2^2    
$$

라벨이 존재하지 않는 데이터세 대해서는 pseudo label $q$를 적용한 consistency regularization loss를 사용한다. 이 두 개를 잘 섞어서 사용한다고 생각하면 된다. 사실 수식만 봐서는 아직 잘 이해가 안될 분들을 위해 직접 알고리즘 코드 한 줄 한 줄 설명해드리도록 하겠다.


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209059068-c301914f-e460-4e6f-accd-0c745e74107f.png"/>
</div>

1~6번째 줄을 먼저 보도록 하자. 입력으로는 같은 배치 크기의 labeled dataset과 unlabeled dataset을 사용하고, labeled dataset $x$에 대해서는 stochastic augmentation을 한 개 적용하고, unlabeled dataset $u$에 대해서는 stochastic augmentation을 $K$개 적용한다.


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209059073-bbebd988-7b15-4cef-9606-556bb05ac16c.png"/>
</div>

요 부분에서 pseudo label이 결정되는데, $K$개의 augmented된 unlabeled sample인 $\\hat{u}$ 애들을 가지고 각각 모델의 예측값을 뽑아낸 뒤, 이를 $K$로 나누어 평균 예측값을 구하게 된다. 논문에서도 설명하겠지만 Pseudo-labeling 과정에 대해서는 최적화를 먹이지 않는다고 한다. 즉 오로지 현재 모델의 예측값을 기준으로 삼는다는 것. 그런 뒤 temperature hyperparameter $T$에 대해 sharpening을 진행하면 pseudo label $q_b$를 생성할 수 있게 된다.


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209059075-530284c1-90de-4a4e-ab0f-9139e508f01f.png"/>
</div>


10~12번째 줄은 label data(augmentation 이후) + unlabeled data(augmentation 이후)를 서로 합친 뒤에 셔플링하는 과정이다. 섞게 되면 총 $B + B \\times K$개의 샘플이 무작위로 나열되고, 이를 하나의 queue 혹은 dequeue 자료형으로 생각한다.


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209059078-b9d40da6-ec11-4bab-904f-dad2d09ae580.png"/>
</div>


무작위로 나열된 샘플 배치 내에서 X'(labeled dataset)과 MixUp을 진행하고, 나머지 샘플들을 이용하여 U'(unlabeled dataset)과의 MixUp을 진행한다. 이렇게 진행된 MixUp은 lambda 값에 따라 labeled data 혹은 unlabeled data와의 중요도를 결정하는데,
만약 단순히 샘플링한 W를 MixUp에 사용하면, 구체적으로 labeled dataset과의 MixUp, 혹은 unlabeled dataset과의 MixUp에 대한 중요도가 사라지게 된다. 따라서

$$
    \\begin{aligned}
        \\lambda =& Beta(\\alpha,~\\alpha) \\newline
        \\lambda' =& \\max (\\lambda,~1-\\lambda) \\newline
        x' =& \\lambda'x_1 + (1-\\lambda')x_2 \\newline
        p' =& \\lambda'p_1 + (1-\\lambda')p_2
    \\end{aligned}
$$

이와 같이 샘플을 MixUp하게 되면 Vanila MixUp(lambda를 서로 같은 값으로 둠)에서 무시했던 batch order를 유지하면서 MixUp sample를 생성할 수 있게 된다.


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209059081-b3812560-90f9-4fed-954b-52d6519ecc9c.png"/>
</div>


실제로 CIFAR-10, SVHN에 대해 250~4000 label를 가지고 SSL을 진행한 MixMatch 방식과 오차율을 비교하게 된다. Supervised method는 당연히 다른 방법들에 비해 좋은 것이 맞고, 검은색(제안된 방법)이 적은 라벨을 가지고도 representation을 효과적으로 학습할 수 있음을 보인다.


<div align="center">
  <img src="https://user-images.githubusercontent.com/79881119/209059086-cc8045b7-cc5a-4a0c-844e-f43dd1dfeba7.png"/>
</div>


주요 contribution이라 함은 이런 저런 loss term과 관련된 SSL 방식을 최적화하는 알고리즘을 효율적으로 잘 설계했다는 점이 될 수 있겠다.
`,_O=`---
title: "Efficient deep neural networks에 대하여"
category: "ai papers"
publishedAt: "2022-12-08"
thumbnail: "https://user-images.githubusercontent.com/79881119/209087790-826f9967-3a81-42c9-9bd3-a05f5b75485b.png"
---


## Efficient Network

딥러닝 네트워크를 구성하는데 있어서 고려해야할 점은, 학습에 사용될 dataset의 크기나 특성 그리고 궁극적으로 해결하고자 하는 task일 것이다. 그리고 이러한 것들을 모두 신경쓰고도 최종적으로 확인해야할 부분은, 내가 가진 resource(GPU와 같은 장비)를 사용해서 학습이 가능한가?라고 볼 수 있다.   
만약 누군가 우리에게 외주를 맡겼다고 가정해보자. 학습 가능한 GPU 성능도 있고, 굳이 저용량의 모바일 기기나 임베딩에 런칭하고 싶은 DNN이 아니라면 더 좋은 성능을 내기 위해 network 구조를 scaling up할 필요가 있을 것이다.   
바로 이러한 관점에서 **'효율적으로'** 네트워크의 크기를 키우자!며 등장했던 것이 efficient network의 개념이며, 해당 연구에서는 CNN의 storage, computational complexity에 대해 다음과 같은 세 가지의 변인을 정의했다.
1. Resolution : 이미지의 차원 수, feature map size를 의미한다.
2. Width : 네트워크의 채널 수
3. Depth : 네트워크의 레이어 수

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209087790-826f9967-3a81-42c9-9bd3-a05f5b75485b.png" width="800"/>
</div>



## EfficientNet v1
[EfficientNet v1](https://arxiv.org/abs/1905.11946v1?fbclid=IwAR15HgcBlYsePX34qTK2aHti_GiucEYpQHjben-8wsTf7O83YPhrJQgXEJ0)에서 고려한 Baseline architecture의 경우 NAS라는 AutoML(간단히 설명하자면, 좋은 성능을 보이는 deep learning network를 찾는 과정이나 hyper-parameter를 세부 조정하는 것을 자동화하는 알고리즘)을 통해 새로운 baseline network를 디자인하고 이를 scaling up하는 방식을 사용한다.   
이때, MobileNet-v2에서 사용되는 MBConv(mobile inverted bottleneck convolution) Block을 기본으로 한다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209087817-8f617ea2-fbd8-4a6a-8709-cce98db3dd76.png" width="400"/>
</div>

위의 그림 참고, lightweight model에 대해서도 포스팅한 글이 있으므로 해당 글을 참고해주면 감사하겠다. 암튼 그래서 baseline network 구조는 다음과 같이 잡고 시작한다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209087794-466ce338-0831-43ef-a859-b1288afae880.png" width="400"/>
</div>



## Depth scaling($d$)
앞서 scaling factor 변인이 총 세가지 제시가 되었다고 했는데, 그 중 가장 단순하게 네트워크의 깊이에 대한 변인으로 생각할 수 있는 depth에 대한 scaling이다.   
물론 ResNet의 등장 이후, 네트워크가 깊어지면 깊어질수록 receptive field의 크기도 커지고 서로 다른 여러 image feature간 상관관계를 잡아낼 확률도 높아진다는 사실은 어느 정도 당연하게 받아들이고 있을 것이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209087798-705fc27b-304e-4d98-b1a6-900e430e26b2.png" width="400"/>
</div>

그러나 ResNet에서나 VGG Network에서 보였던 경향성과 같이, 네트워크가 지나치게 깊어지면 accuract 향상에 큰 도움이 안된다. 이는 학습 과정에서 파라미터 수가 많아지더라도 이 모든 representation을 최적화할 정도로 많은 데이터가 없을 수도 있고, 그게 아니라 단순히 네트워크가 깊어지면 input과 output 간의 거리가 멀어지면서 네트워크가 학습해야할 optimal solution까지 도달하기 어려워질 수 있다는 것이다.   
일련의 예로 ResNet-1000은 ResNet-101과 유사한 accuracy를 가지는 것을 보면 알 수 있다. 또한 위의 그래프를 보아도 $d = 8.0$ 부터는 accuracy가 수렴하는 것을 확인할 수 있다. 아, 그래프에 대해 설명을 안했는데 가로축은 연산 수랑 비례하는 FLOPS이고 세로축은 ImageNet Top-1 Accuracy를 의미한다.


## Width scaling($w$)
그 다음으로 생각해볼 수 있는 것은 network의 깊이를 유지한 채로 wide하게 만드는 것이다. Wider network는 spatial dimension으로 커지는게 아니라, channel 수를 증가시키는 개념으로 보면 된다.   

채널 수가 많은 네트워크일수록 fine-grained(디테일하다고 보면 됨)한 특징들을 더 잘 잡아낼 수 있고, 네트워크 깊이가 얕기 때문에 excessive depth에 의한 학습 저하 효과가 덜하다. 

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209087799-93edd523-edaf-47d2-9a74-193d8b71c5df.png" width="400"/>
</div>

그럼에도 불구하고 문제가 될 수 있는 것은 너무 얕으면서(depth $d$ $\\downarrow$) 지나치게 넓은(width $w$ $\\uparrow$) 네트워크는 쉽게 saturate된다는 것이다. 그래프에서 볼 수 있듯 대략 $w=5$ 정도에서 성능이 수렴한다고 한다.


## Resolution scaling($r$)
Spatial dimension을 더 크게 가져가는 resolution factor에 대한 부분에서, 앞선 내용과 유사하게 high resolution image는 fine grained feature를 얻는 데에 유리하다. 그러나 resolution이 증가할수록 FLOPS도 증가할 뿐더러 대략 $r = 2.5$ 정도가 되면 성능이 수렴한다고 한다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209087802-30e5a146-1035-46bd-bc39-333fd54be84d.png" width="400"/>
</div>



## Observations
이런 저런 실험을 통해 두 가지의 결론에 다다를 수 있었다. 그것은 바로
1. 네트워크의 깊이/너비/차원 등 모든 factor에 대해 키우면 키울수록 성능은 증가했지만, <U>증가폭은 점차 감소하더라</U>.
2. FLOPS나 네트워크의 용량 등 효율적인 측면을 고려했을 때 네트워크의 <U>모든 dimension에 대한 밸런스를 맞추는 것</U>이 중요하다.

따라서 이러한 조건 속에서 다음과 같은 objective를 가지게 된다.

$$
    \\begin{aligned}
        &\\max_{d, w, r} Accuracy(\\mathcal{N}(d, w, r)) \\\\
        &\\text{s.t.}~\\mathcal{N}(d, w, r) = \\bigodot_{i=1 \\cdots s} \\hat{F_i}^{d \\cdot \\hat{L_i}}(X_{(r \\cdot \\hat{H_i}, r \\cdot \\hat{W_i}, w \\cdot \\hat{C_i})}) \\\\
        &\\text{Memory}(\\mathcal{N}) \\le \\text{target\\_memory} \\\\
        &\\text{FLOPS}(\\mathcal{N}) \\le \\text{target\\_flops}
    \\end{aligned}
$$

식을 간단히 해석하자면 $\\mathcal{N}$이라 표시된 것이 바로 비교하고자 하는 CNN 모델을 모아놓은 함수의 집합이라 생각하면 된다. 이상한 기호로 표시된 애는(두번째 줄에 s.t.하고 써있는 부분) ($d, w, r$)로 스케일링 된 CNN의 모든 레이어를 함수로 보고, 일련의 output을 input에 대한 합성 함수의 연산으로 해석한 것이다. 따라서 $\\mathcal{N}(d, w, r)$은 input에 대한 output, 그리고 우리는 해당 output을 토대로 supervision이 주어지는 어떠한 task에 대해 최대의 정확도를 얻고 싶은 것.   
그러는 와중에 threshold memory나 flops를 넘지 않는 선에서 네트워크를 서칭하고자 하는 것이다.   
근데 이렇게 보면 수학적인 모델링이 아니라 그냥 "그런 모델을 만들고 싶다"니까, 실질적으로 수치화할 필요가 있다. 즉, $d, w, r$과 같은 weight factor가 memory나 flops에 어떤 영향을 미칠 것이고, 우리가 가용하고 싶은 resource의 개수를 어떤 방식으로 searching 하느냐이다.

$$
    \\begin{aligned}
        \\text{depth: }&d = \\alpha^\\phi \\newline
        \\text{width: }&w = \\beta^\\phi \\newline
        \\text{resolution: }&r = \\gamma^\\phi \\newline
        \\text{s.t. }&\\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2 \\newline
        &\\alpha \\ge 1, \\beta \\ge 1, \\gamma \\ge 1
    \\end{aligned}    
$$

우리는 애초에 스케일을 키우면서 서칭할 것이기 때문에(첫번째 observation에 의한 결정) 모든 인자는 1보다 크거나 같아야 하고, $\\phi$는 compound coefficient로서 얼마나 많은 resource를 사용할 지 지수로 결정한다. 그리고 FLOPS는 각각 network의 depth $d$에 대해서는 linear하게, 그리고 width와 resolution $w, r$에 대해서는 거의 quadratic하게 비례하므로 전체 FLOPS가 $2^\\phi$가 넘지 않도록 하는 것이다. 결국 결정되어야 할 파라미터는 총 4개 $(\\alpha,~\\beta,~\\gamma,~\\phi)$이다.


## Implementation

실제로 어떤 식으로 구현하였는지 확인해보자. 앞서 baseline 모델은 MBConv(MobileNet v2에서 사용한 구조)를 이용했다고 했고, 이를 B0라고 하자. 구조는 NAS를 통해 구했다(효율적인 네트워크 시작점을 찾는 알고리즘이라고 생각하면 된다).

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209087794-466ce338-0831-43ef-a859-b1288afae880.png" width="600"/>
</div>


그 다음으로는 $\\phi = 1$로 고정하고, 두 배의 resource가 더 가용된다고 가정하고 $\\alpha,~\\beta$ 그리고 $\\gamma$에 대해 grid search를 진행한다. 앞서 constraints에 $2^\\phi$로 설명했던 부분.   
B0에 대해 optimal value는 $\\alpha = 1.2$, $\\beta = 1.1$, 그리고 $\\gamma = 1.15$가 가장 optimal value로 결정되었다.   
다음에는 $\\alpha,~\\beta$ 그리고 $\\gamma$를 상수로 고정시킨 채(위에서 얻은 값들로) 서로 다른 $\\phi$에 대해 테스트해본다.
바로 이 $\\phi$를 변화시켜 얻은 것이 EfficientNets B1 부터 B7이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209087804-57e25abc-ef8c-4854-92b8-1e23253f0074.png" width="600"/>
</div>


처음에 $\\alpha,~\\beta,~\\gamma$를 잘 정하고 시작했기 때문에 parameter 수를 많이 증가시키지 않고도 기존 네트워크들과 비교하여 쉽게 우위를 점할 수 있는 것을 볼 수 있다. ~~역시 무지성으로 네트워크 구조 짜는게 답은 아니라는 것을 알 수 있다.~~


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209087808-02559f28-75a9-4a8d-bda5-763c06ff224b.png" width="800"/>
</div>


각 클래스 별로 네트워크가 어디를 중점적으로 보는지 시각화하는 CAM을 사용해서 본 결과다. **compound scaling**은 보다 디테일한 부분을 잡아내는 걸 확인할 수 있다.


## EfficientNet v2
아직 안끝났다. EfficientNet v1이 있다는 것은 v2도 있다는 것이다. 어벤져스 1편이 있으면 2편도 있고 그런거다(이게 뭔 소리지). 아무튼 [EfficientNet v2](https://arxiv.org/abs/2104.00298)는 비교적 최근에 나왔다(2021). 과거의 EfficientNet v1이 영광을 누리던 시절은 ViT같은 새로운 구조의 computer vision network도 없었고 그랬는데 암튼 시대가 많이 변했다. 발전이 필요해서 더 좋은 네트워크를 고안하기 시작한 것이다.   
EfficientNet v2에서 네트워크 구조를 서칭하는 과정은 정확히 EfficientNet v1과 동일하다. 다만 EfficientNet v2에서는 architecture(baseline)이 바뀐 것이 첫번째 contribution이고, **progressive learning**을 활용했다는 것이 두번째 contribution이다. 즉, image size에 대한 정규화를 진행했다는 것이다([PGGAN](https://arxiv.org/abs/1710.10196) 참고).   
이러한 방법을 토대로  EfficientNet v1에 대해 $11\\times$ 빠른 학습 속도와 $6.8\\times$ 나은 parameter efficiency를 보여주었다.


## Fused MBConv

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209087812-b386bf1e-9a5d-4b68-9d78-25f66889915a.png" width="600"/>
</div>

사용한 구조에서 fused MBConv를 간단히 설명하면, depthwise $3\\times 3$ convolution과 expansion $1 \\times 1$ convolution이 기존 MBConv를 구성하는데, 이걸 그냥 $3 \\times 3$ convolution으로 바꾼 것이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209087817-8f617ea2-fbd8-4a6a-8709-cce98db3dd76.png" width="600"/>
</div>


그래서 모든 레이어를 다 Fused MBConv로 한 건 아니고, 표에서 보는 것과 같이 기존 Efficient v1의 구조에서 일부분(표에서 stage로 표시된 부분이 바뀐 부분)을 바꿨을 때의 효과를 보여준다.   
그러나 저자들은 정확히 <U>어떤 부분에서 fused MBConv를 써야할 지</U> 확실한 답을 얻을 수 없었기에 이를 NAS에 맡긴다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209087818-e12926fd-96cf-4155-a4ca-e59d98f293ac.png" width="800"/>
</div>


그래서 얻은 baseline 구조(EfficientNetV2-S라 명명)에 대한 결론은,
1. 초반 layer에서 fused MBConv를 많이 사용하자.
2. EfficientNet v2($3 \\times 3$)는 v1($5 \\times 5$)에 비해 작은 kernel size가 좋았다.
3. 마지막 stride-1 stage를 없애버림

이다. 그냥 복잡하니까 NAS가 알아서 제일 좋은거 갖다줬다고 이해하면 될듯.  


## Progressive learning
점진적 학습의 가장 메인 아이디어는 작은 image size부터 차근차근 학습시키자는 것이다. 이를테면 총 10 epoch 동안 학습하는 과정에서 초반 6 epoch는 $224 \\times 224$의 이미지 크기로 학습을 하고 남은 4 epoch은 좀 더 큰 $256 \\times 256$에 대해 finetune하는 느낌.   
그러나 단순히 smaller image로부터 larger image로 점차 키우는 건 accuracy drop을 불러일으켰다고 한다.   
성능 더 좋게 할라고 한건데 왜...? ㅠㅠ 라고 하며 저자들이 생각했던 것은 image size가 달라지면, regularization strength도 그에 따라 변화시켜줘야 한다는 것이다. 즉 regularization을 무지성으로 똑같이 적용하다보니 unbalance 문제가 발생했다는 것.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209087820-d759593e-41bd-4e95-b59e-f2f7fa6811dc.png" width="800"/>
</div>


그래서 개선 방법으로 떠올렸던 것이 이미지 크기도 점차 증가시키고 그에 따른 정규화도 점차 키우는 것이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209087823-69796a5f-d41d-4292-9cf4-8a2dfc2d91ad.png" width="800"/>
</div>


알고리즘을 보면 $R_i$가 regularization strength고 $S_i$가 이미지 크기를 의미한다. 되게 간단해서 딱히 설명 안해도 괜츈할 거 같다. iteration이 M번 증가하면서 이미지 크기는 초기값 $S_0$에서 linearly 증가시키고 마찬가지로 regularization strength도 초기값에서 linearly 증가시킨다.   
슬슬 여기서 들 수 있는 의문은 "그래서 대체 그 정규화가 어떤 정규화인데?" 싶을거다. 논문에서는 이러한 학습법이 존재하는 대부분의 정규화 작업에 적용 가능하며, 총 세 개의 정규화에 대해 실험하였다.

- **Dropout** - 네트워크 레벨에서의 정규화, 일부 채널을 학습 과정에서 dropping 한다. 즉 노드를 일부 끊어버린다고 생각하면 된다. Dropout rate $\\gamma$가 각 노드가 버려질 확률을 의미하는데, 이를 조절하였다.
- **RandAugment** - 이미지별 data augmentation 방법에 해당된다. 말 그대로 랜덤으로 augmentation을 하는건데, 여기서 magnitude $\\epsilon$을 조절했다.
- **Mixup** - 이미지 간의 data augmentation에 해당된다. 예컨데 개 이미지 $(x_i, y_i)$ 와 고양이 이미지 $(x_j, y_j)$ 가 있으면 이걸 0부터 1 사이의 mixup ratio인 $\\lambda$를 기준으로 개냥이 이미지 $\\tilde{x_i} = \\lambda x_j + (1-\\lambda)x_i$ 그리고 그에 해당되는 라벨 $\\tilde{y_i} = \\lambda y_j + (1-\\lambda)y_i$를 만들어내는 정규화 작업. 여기서 mixup ratio $\\lambda$를 조절했다고 한다.   

암튼 요로코롬 조로코롬 잘 조절해서 실험하니, 다음과 같은 결과를 얻었다고 함.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209087825-cff21495-b8b3-4916-bde0-6be8418d9efe.png" width="800"/>
</div>
`,wO=`---
title: "About light weight CNNs(MobileNet 시리즈들)"
category: "ai papers"
publishedAt: "2022-12-08"
thumbnail: "https://user-images.githubusercontent.com/79881119/209088800-7ac3c32b-e720-43ce-b42b-efe2992d27d0.png"
---

## 경량화 네트워크
Deep learning의 발전은 하드웨어와 함께 시작되었고, 충분히 좋은 성능을 가지는 CPU 혹은 GPU 없이는 뛰어난 추론 능력을 가진 네트워크를 fully 활용할 수 없는 것이 사실이다.   
그러나 언제까지나 무거운 서버컴에서 딥러닝을 돌릴 순 없기 때문에 딥러닝 네트워크의 경량화, 간소화가 필수적이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209088788-348d060b-19ba-432c-8b74-6578d47becb7.svg" width="650"/>
</div>


따라서 경량화 네트워크의 목적은 두 가지로 나눌 수 있다.
1. CNN을 applicable하게 만들고, 이를 모바일 device나 embedding system에 적용이 가능하게끔 한다.
2. 경량화 과정에서 accuracy나 performance를 적어도 유지, 가능하다면 improve하고 싶다.

경량화 자체는 어렵진 않다고 생각할 수 있다. 그러나 Deep neural network의 구조를 좀 더 간결하게 구성한다던지 filter kernel의 채널 수를 줄여 가용 파라미터 수를 줄인다는 식의 접근이 있지만, 함부로 구조나 파라미터를 축약하게 되면 학습 가능한 representation이 제한되기 때문에 이마저 그리 간단하지 않다.   
따라서 이러한 네트워크 구조를 제안한 여러 논문들을 살펴보고, 어떤 식으로 파라미터 수를 줄일 수 있었는지 확인해보도록 하자.


## MobileNet
유명한 논문 중 하나인 [MobileNet](https://arxiv.org/abs/1704.04861)은 google로부터 제안된 방법이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209088800-7ac3c32b-e720-43ce-b42b-efe2992d27d0.png" width="650"/>
</div>

바로 depthwise seperable convolution인데, 이는 depth-wise convolution과 point-wise convolution을 서로 연결한 구조가 된다.   
이름에서 알 수 있듯 depthwise convolution과 pointwise convolution은 콘볼루션이 적용되는 영역에 대한 내용인데, 흔히 우리가 처리하고자 하는 image tensor는 각 레이어를 통과하며 하나의 batch 당 $C \\times H \\times W$로, 각각 채널, 높이, 너비를 가지는 3차원의 데이터로 구성된다. 그리고 이러한 이미지 텐서에 적용되는 필터의 크기는 $C \\times D_K \\times D_K$로, 각각 채널, 커널의 높이, 커널의 너비를 가지는 3차원의 weight로 구성된다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209088798-c6daff25-46dc-4b2b-b080-875bc57e585c.png"/>
</div>

따라서 위의 그림을 참고하게 되면(실제 mobilenet 논문에 첨부된 사진을 인용하였다), $D_K$로 표시된 dimension이 커널의 높이와 너비에 대해 $M$은 필터가 적용될 데이터의 채널 수가 되고, $N$은 필터의 개수가 된다. notation에 일부 혼란이 있을 수 있어 미리 말하자면 그림에서의 $M$이 내가 언급한 $C$와 같은 값이라고 생각해주면 된다.   
연산 후에는 데이터가 $N \\times H' \\times W'$이 된다.   
이런 식으로 convolution이 진행되는데, 각 커널 필터의 파라미터 수를 확인해보면,

$$
    {D_K}^2 \\times M \\times N
$$

이 되고, 이러한 커널이 각 레이어의 convolution마다 존재하기 때문에 parameter 수가 dimension value $D_K$에 따라 급격히 증가하는 구조가 된다. 심지어 segmentation과 같이 high level computer vision에서는 이러한 부분이 네트워크 경량화에 큰 bottleneck으로 작용한다.   

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209088807-6ae8a0c4-9464-4da2-ba85-6a4aeef33b69.png" width="450"/>
    <img src="https://user-images.githubusercontent.com/79881119/209088809-677c3e21-6da7-4a64-b450-4a9f67223ab3.png" width="450"/>
</div>

따라서 이를 위와 같이 두 개로 분리하자는 것이 mobilenet에서 제시한 방법이다. 좌측에 보이는 $D_K \\times D_K \\times 1$ 크기의 커널을 $M$개 만듦으로써 <U>각 데이터의 채널 별로 연산</U>을 진행하고자 하는 것이 depthwise convolution이고, 우측에 보이는 $1 \\times 1 \\times M$ 크기의 커널을 $N$개 만듦으로써 <U>각 데이터의 픽셀 별로 연산</U>을 진행하고자 하는 것이 pointwise convolution이다.
Depthwise convolution을 제시한 [논문](https://arxiv.org/abs/1610.02357)에서는, feature map(데이터)에서의 채널 간의 상관관계와 spatial(같은 채널에서의 픽셀 간) 상관관계가 완전히 분리될 수 있다고 가설을 세웠다.   
따라서 depthwise convolution은 computational complexity를 줄일 수 있지만 channel 사이의 correlation을 관장할 수는 없고, 이러한 문제를 pointwise convolution을 통해 채널 간 상관관계를 채워주는 느낌이다. 이러한 가설을 토대로 분리한 두 convolution step에 대한 parameter수는,

$$
    ({D_K}^2 \\times M) + (M \\times N) < {D_K}^2 \\times M \\times N
$$

위와 같이 감소할 수 있는 것이다.   
MobileNet 구조는 신기하게도 pooling layer를 전혀 적용하지 않았는데, 그 대신 depth-wise seperable convolution을 stride 2를 적용해서 sub-sampling하는 방식을 채택하였고, 전체 layer 수는 28개다.


## ReLU6
기존 CNN 구조를 보면 각 convolution block에서의 activation function으로 대부분 ReLU()를 사용한다. 물론 현재는 ReLU()도 이런 저런 문제들 때문에 잘 안쓰이긴 하지만 그래도 조상격인 스테디셀러나 마찬가지다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209088813-1d95c642-fdb8-4d3c-a79b-b41f6556355b.png.png"/>
</div>

그러나 ReLU의 경우 value가 양수라면 모든 값을 내보낼 수 있기 때문에 <U>상한선이 없다</U>는 문제점이 생긴다. 이게 왜 큰 문제가 될 수 있냐면, 값의 상한선이 없다면 이를 표현할 메모리를 한정적으로 사용할 수 없다는 것이다. 이러한 문제에서 제시된 것이 바로 fixed point 관점, training 관점에서 모두 좋은 평가를 받은 ReLU6 activation function이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209088819-ea3c1e5e-b846-4273-9e2a-fba3c1877e4c.png"/>
</div>

딥러닝 모델 최적화에 있어 fixed point로 변환해야 하는 경우가 생기는데, 6으로 상한선을 두게 되면 3개의 bit만 가지고 모든 가짓수를 표현 가능하기 때문에 최적화 관점에서 큰 도움이 된다. 또한 상한선을 두게 되면 딥러닝 모델이 학습할 때 퍼져있는 feature들을 더 일찍 학습할 수 있다는 [분석](http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf)도 존재한다. 여러 테스트를 통해 상한선을 결정할 때, 6이 가장 성능이 좋았다는 결과를 토대로 ReLU6를 제안했다고 한다.


## MobileNet v2
유명한 논문의 다음 논문인 [MobileNet-v2](https://arxiv.org/abs/1801.04381)도 보도록 하자.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209088821-fae1de5e-b3d0-4bf2-b344-3fbe24a082fc.png" width="200"/>
    <img src="https://user-images.githubusercontent.com/79881119/209088825-0dd7fd6b-0c8f-4a83-b04f-98fd747364d9.png" width="300"/>
</div>

가장 왼쪽에 보이는 그림이 MobileNetv1의 framework이고, 중앙과 우측의 두 구조가 바로 MobileNetv2의 framework다.   
핵심부터 미리 말하자면 MobileNetv2에서는 **inverted residual** 구조를 사용했다. MobileNetv2의 좌/우 block은 모두 pointwise($1 \\times 1$) + Convolution + ReLU6로 시작한다. 그리고 depthwise convolution을 수행할 때 왼쪽 block은 stride 1을 적용하여 spatial dimension을 그대로 가져가고, 오른쪽 block은 stride 2를 적용하여 downsampling한다. 세번째에 다시 pointwise convolution을 적용하는데, 이때는 activation function을 적용하지 않는다. Downsizing이 될 경우 skip connection이 이루어질 수 없기 때문에 skip connection(residual connection)이 없고 왼쪽 block에는 residual connection이 있는 것을 확인할 수 있다.   
대강 구조를 확인해보았는데, 이를 좀 더 풀어서 설명하자면 expansion convolution으로 채널 수를 뻥튀기 시키고, 거기에 depthwise convolution을 적용한 뒤에 이를 pointwise convolution으로 production하는 구조와 같다. 즉 맨 앞단의 $1 \\times 1$ convolution은 expanding 역할, 뒷단의 $1 \\times 1$ convolution이 projection 역할을 한다. 또한 projection 뒤에는 activation function을 적용하지 않은 이유는 ReLU를 통한 feature 왜곡을 최소화하기 위함이라고 한다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209088829-4773c43a-319f-4561-b1cb-899e359e8445.png"/>
</div>

그림을 참고하면 우측이 inverted residual 구조를 나타낸 것이고 좌측이 원래의 residual 구조를 나타낸 것이다. 채널 수가 확장되는 차이가 있다. 좌측에 보이는 구조가 곧 ResNet 구조라 보면 되고, 우측이 MobileNetv2에서 사용한 구조라 보면 된다.


## MobileNet v3

유명한 논문의 다음 논문의 다음 논문인.. [MobileNet-v3](https://arxiv.org/abs/1905.02244)도 보도록 하자.   
우선 심플하게 해당 논문은 inverted residual 구조를 고대로 사용했고, MobileNet v1에서의 pointwise, depthwise 개념이랑 MobileNet v2의 inverted residual은 유용하다고 판단하여 계속 사용한다.   
다만 조금 달라진 점이라면 새로운 activation function을 제시했다는 점 그리고 SE(Squeeze and Excitation network) 모듈을 사용했다는 점이 될 수 있겠다. 각각 순서대로 설명하면,

## h-swish
h-swish는 swish function의 개량된 버전인데, 솔직히 본인은 swish도 잘 몰라서 찾아봤다. 다들 그렇다고 답해주길 바라고 있다.
$$
    \\text{swish}(x) = \\frac{x}{1+e^{-x}}
$$
그래서 찾아봤는데 오잉? 그냥 시그모이드에다가 $x$ 곱한 녀석이더라. 생각보다 별볼일 없는 친구라 생각이 들었다.
암튼 sigmoid를 요즘은 잘 안쓰니까, 어찌 보면 sigmoid를 조금 바꿔놓은 swish라는 애를 ReLU 식에도 적용이 가능하지 않을까? 라는 생각이다.
는 바로 적용.
$$
    \\text{h-swish}(x) = x\\frac{\\text{ReLU6}(x+3)}{6}    
$$

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209088832-0c395635-00d3-4c61-ad85-ccd8569df4c4.png"/>
</div>

pytorch에서는 hardswish란 이름으로 모듈이 있는 듯하다. 참고해보실 분들은 참고!

## SE(Squeeze and Excitation)
사실 [이 친구](https://arxiv.org/abs/1709.01507)는 굉장히 유명하기도 하고 그래서 읽어보는 걸 추천한다. 내용도 엄청 복잡복잡하지 않고 간단명료하게 딱 그림, 설명 이 정도면 대강 어떤 목적으로 구현된 모듈인지 이해할 수 있다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209088835-5de42d5d-57b9-406e-a3da-a8986ebc2da2.png"/>
</div>

[STN](https://arxiv.org/abs/1506.02025) 논문을 읽어보신 분들은 아실 수도 있는데, 얘도 궁극적으로 추구하고자 하는 것은 모듈을 사용함으로써 자동적으로 layer 별로 attention이 최적화가 되는 걸 바라는 것이다. 보통 우리가 어떠한 feature map을 쓸 때 해당 feature map의 어느 채널이 유용한 정보를 담고 있는지 신경 쓰지 않기 때문에 효율적인 학습이 어렵다는 관점에서 나온 모듈인데, MobileNet에서 pointwise convolution이 채널 간의 연관성을 되살려주는 역할을 하고 있다는 가정 하에 요런 보조적인 장치 하나가 성능에 큰 효과를 줄 수 있다는 것이다.   
심지어 그림을 보면 알 수 있지만 weight를 도출하는 방식은 단순히 squeeze(spatially average)한 다음에 함수를 통과시키는 구조다. 매우 간단쓰

## NAS(Neural Architecture Search)
딥러닝 네트워크를 만들 때 네트워크 구조 짜는 거나, 하이퍼 파라미터 튜닝하는 거나 이 모든게 자동화가 완전히 되지 못한다. AutoML은 이러한 디자인/경험적인 문제를 해결하고자 하는 process라 보면 된다.
- Search space : 시도할 아키텍쳐의 집합
- Search algorithm : search space를 어떤 식으로 explore할 것인지. Randomly하게 할 수도 있고 Bayesian optimization 관점으로 접근할 수도 있고 아니면 강화 학습 등등...
- Evaluation strategy : NAS 알고리즘을 어떤 방식으로 학습할 것인지. 흔히 하는 Training/Validation 작업을 할 것인지 혹은 fully-supervision, few-shot 등등...

대표적인 용어에 대해 설명하자면 위와 같다.

NAS 방법이 엄청 다양하고, 이거에 대해서 한나절 설명하자니 lightweight 모델에 대한 내용이 부수적이게 될 것 같아 여기서 멈추도록 하겠다.. 암튼 NetAdapt 알고리즘을 통해 network 구조를 최적화했다고 한다. 그래서 그런지 MobileNetv3 논문 제목부터가 "Searching for MobileNetV3".
`,TO=`---
title: "Generative adversarial networks(GAN)에 대하여"
category: "ai papers"
publishedAt: "2022-12-09"
thumbnail: "https://user-images.githubusercontent.com/79881119/209089640-17906d02-f7bf-42e6-ad86-01d2a8c5d04c.png"
---


이번 게시글에서는 생성 모델 중 하나인 GAN(Generative Adversarial Networks)에 대해 다룰 예정이다. 앞서 다른 생성 모델들인 VAE, Diffusion에 대해서는 모두 소개했었는데 가장 유명하고 가장 많이 연구된 GAN을 빼먹었다는게 아쉬워서 들고 왔다. GAN 연구는 여전히 활발히 진행 중이며 StyleGAN과 같이 스타일에 따른 이미지 생성 모델 뿐만 아니라 3D scene aware GAN이나 depth map generation 등 다양한 분야에서 활용되고 있다. GAN이 가장 좋은 점은 샘플링 속도가 빠르다는 점에 있다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209054535-562ac052-0e4a-44f2-9e29-7b05e07b69f1.png" width="600"/>
</div>


## Background knowledge
생성 모델은 원하는 데이터의 분포를 얻는 과정이다. 그렇기 때문에 likelihood based network는 아니지만 GAN 또한 approach 자체는 유사한 분포를 구성하고자 하는 방향이고, 그렇기 때문에 흔히 latent space라고 하는 $\\mathcal{Z}$로부터의 $\\mathcal{X}$ mapping이 정의된다. 이러한 부분에서 꼭 짚고 넘어가야할 확률에 대한 사전 지식에 대해 보도록 하자.

## Probability


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209089630-707703f9-2742-4366-b3ef-30b00bdc23a4.png" width="600"/>
</div>

위의 그림에서 붉은색으로 표시된 부분이 확률 변수 $y \\sim \\mathcal{N}(0, 1)$이라고 보고 파란색으로 표시된 부분이 확률 변수 $x \\sim \\mathcal{N}(0, 0.5)$라고 해보자. 둘 다 간단하게 이해해보기 위해 가우시안 분포로 가정하였다.   
우리는 두 변수 $x,~y$에 대해 중간 분포를 정의할 수 있고, 이를 joint probability라 부르는 $p(x,~y)$로 정의할 수 있다. 여기서 중요한 점은 변수 $x$와 $y$가 서로 관련될 수도 있고, 그렇지 않을 수도 있다. 두 확률 분포 $p(x)$와 $p(y)$의 joint는 그림에서 보는 것과 같이 중간 평면 상에서의 입체적인 가우시안 형태로 나타난다. 가우시안은 평균을 기준으로 대칭이기 때문에 해당 joint는 isotropic한 특징을 가진다.   
해당 joint distribution($p(x,~y)$)을 각 축으로 projection하게 되면 원래 분포인 $p(x)$와 $p(y)$를 얻을 수 있다. 이는 다음과 같은 식으로 표현 가능하다.
$$
    \\begin{aligned}
        \\int_{-\\infty}^{\\infty} p(x,~y) dy =& p(x) \\newline
        \\int_{-\\infty}^{\\infty} p(x,~y) dx =& p(y)
    \\end{aligned}    
$$
물론 이는 연속인 확률 밀도 함수에 대한 표현이고, 만약 이산 확률 질량 함수에 대한 표현으로 바꾸면,
$$
    \\begin{aligned}
        \\sum_{y} p(x,~y) =& p(x) \\newline
        \\sum_{x} p(x,~y) =& p(y) \\newline
    \\end{aligned}    
$$
위와 같다. 다차원에서 특정 축을 기준으로 적분한다는 것은 해당 축을 따라서 모두 더하는 것이므로 projection이 된다.   
앞서 말했던 것과 같이 두 개의 변수는 서로 연관되어 있을 수도 있다. 만약 서로 무관하다면(지나가는 사람이 내 이상형일 확률과 내일 비가 올 확률 정도의 관계) 상관없지만, 그렇지 않을 경우 조건부 확률이 marginal이나 joint와는 다르게 유의미한 형태로 나타난다. 이를테면 변수 $x$가 어떤 값을 가질 때의 변수 $y$의 확률을 나타내는 식은,
$$
    p(y \\vert x) = \\frac{p(x,~y)}{p(x)}    
$$
위와 같다.

## PDF(확률 밀도 함수) modeling and sampling
그렇다면 특정 분포를 모델링하고, 모델링된 파트에서 샘플링을 어떻게 해야할까? 다음과 같은 순서로 볼 수 있다.
1. 분포를 예측하고 싶은 Samle data가 있다.
2. Sample data로부터 empirical하게 PDF를 구한다.
3. Empirical하게 구한 PDF를 Standard PDF와 피팅시킨다.
4. 이 과정에서 parameter를 예측한다. 예측하는 방식은 Maximum likelihood estimation(MLE), Bayesian estimation 등등...

그리고 샘플링은 이렇게 구한 PDF로부터 역으로 구하는 것이다. $p(x) \\rightarrow x$.   
사실 이렇게만 말하면 아직 MLE나 Bayesian에 대해서 제대로 짚고 넘어오지 않았기 때문에 이해가 잘 되지 않는다. 일단 넘어가도록 하자.


## Generative and Discriminative models

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209089636-7a1f4e2e-6f0d-4af2-97c6-fe44d0271457.png" width="600"/>
</div>

GAN을 그냥 간단하게 한마디로 설명하자면 생성자라고 불리는 Generator($G$)와 구별자라 불리는 Discriminator($D$)가 서로 치고박고 싸우게 하면서 성능을 높이는 것이다. 바쁘다바빠 현대사회 경쟁사회에서 다들 취업이니 스펙이니 신경쓰면서 자기개발에 힘쓰는 시대이지 않은가. ~~뭔 관계인지는 모르겠지만~~   
암튼 GAN을 구성하는 요소들 중에서 generator와 discriminator에 대해 알아볼 필요가 있다. Generative model은 joint PDF인 $p(x,~y; \\theta)$로 표현되거나 marginal PDF $p(x; \\theta)$로 표현된다. 모든 딥러닝 네트워크는 implicit하게 함수를 학습하는데, 이때 generator는 분포를 만드는 $\\theta$를 학습한다고 생각해주면 된다. 그와는 다르게 discriminative model은 conditional PDF인 $p(y \\vert x; \\theta)$를 학습하게 되고, generative model이 내보낸 그럴 듯한 모달리티(데이터) $x$에 대해 구분하는 작업이 되겠다. 그래서 표현 자체로는 parameter $\\theta$를 동일하게 사용했지만, 좀 더 엄밀히 말하자면 generative model과 discriminative model은 서로 다른 parameter set $\\phi \\in \\Phi$, $\\theta \\in \\Theta$에 대한 최적화라 보면 된다. Generative model에는 [GMM](https://jontemple.org.uk/wp-content/uploads/2020/06/bht10.pdf), [HMM](https://web.stanford.edu/~jurafsky/slp3/A.pdf) 그리고 [Bayesian Networks](https://www.bu.edu/sph/files/2014/05/bayesian-networks-final.pdf), [Deep belief networks](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf), 뭐 [Deep Boltzmann machine](http://www.cs.toronto.edu/~fritz/absps/dbm.pdf) 등등이 있다.   
그리고 discriminative model에는 흔히 알고있는 low level task를 담당하는 MLPs, CNNs, Logistic regression, SVM 등등 모든 형태의 encoder로 볼 수 있다.


## General concept of data generation with generative models
그냥 일반적인, 모든 형태의 생성 모델이 하고자 하는 방향성에 대해 일반적인 컨셉을 설명하자면 다음과 같다. 우리는 디코더 형태를 지닌 generator network를 통해 그럴듯한 데이터 $x$를 만들고 싶고, 앞서 설명했던것과 같이 generator는 $p(x; \\theta)$를 학습한다. 이때, 실질적인 real dataset probability를 구하기 힘들기 때문에, 이에 대한 joint $p(x,~z)$를 상정하고 싶은 것이고, Bayesian에 대해 우리가 접근할 수 없는 확률들과는 관계없이 최적화하자는 것이다.
$$
    p(z \\vert x) = \\frac{p(x \\vert z)p(z)}{p(x)}    
$$

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209089637-f5d977b6-2a78-4338-9ae1-b880fa281f83.jpg" width="600"/>
</div>

다만 Likelihood($p(x \\vert z)$) 최적화 관점이 아니라 여기서는 $p(x)$에 집중한다. 즉 joint를 모든 $z$에 대해 적분한 것은 곧 likelihood를 우리가 미리 정해놓을 수 있는 marginal likelihood 변수 $z \\sim p(z)$에 대한 평균을 보자는 것이다.
$$
    p(x) = \\int_z p(x \\vert z) p(z) dz
$$
그렇게 정하는 방식이 곧 normal distribution에서의 샘플링 $p(z) = \\mathcal{N}(0, I)$이고, 이렇게 추출된 $z$를 **latent vector** 혹은 **latent variable**이라 부른다.   
추출된 latent를 generator에 통과시킨 결과가 $p(z)p(x \\vert z)$에 대한 샘플이 되니까, 아까 앞서 말했던 background에서와 같이 empirical PDF에 피팅하는 과정이라 볼 수 있다.


## GAN의 특징
[GAN 가장 첫 논문](https://arxiv.org/abs/1406.2661)을 보면 모든 컨셉이 다 나와있다. 가장 먼저 모델은 implicit PDF를 학습한다. $x$를 만들기 위해 따로 joint probability에 대한 supervision을 주지 않기 때문에 unsupervised approach라고 할 수 있다.   
그리고 네트워크는 총 두 개로 구성이 되고, generative network인 $G$와 discriminative network인 $D$가 서로 경쟁하는 구조가 된다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209089638-4006e341-97bf-45f2-9204-8e23b3a0c4e4.png" width="600"/>
</div>

이걸 흔히 위조지폐범과 이를 잡아내는 경찰로 비유하는데, generator(위조지폐범)는 그럴 듯한 거짓 데이터(위조지폐)를 만들어내기 위해 노력하고, discriminator(경찰)은 진짜 데이터와 거짓 데이터(위조지폐와 실제 화폐)를 구분하는 형태로 경쟁적인 학습을 한다. 이를 표현한 것이 mini-max GAN이며 optimization loss는 다음과 같다.
$$
    \\begin{aligned}
        &\\min_G \\max_D V(D,G) \\newline
        V(D,G) =& E_{x \\sim p_{data}(x)}(\\log D(x)) + E_{z \\sim p_z(z)}(\\log (1-D(G(z))))
    \\end{aligned}    
$$
표현식에서 $\\log (\\cdot)$는 흔히 확률을 energy based로 최적화할 때 사용하는 log liklihood라 보면 되고, 확률은 $0 \\sim 1$의 값을 가진다고 생각하면 된다.
  
## Discriminator의 입장
실제 데이터 분포인 $p_{data}(x)$ 로부터 온 $x$는 discriminator로 하여금 진짜다(True = 1)라고 판별되어야 하기 때문에, discriminator 입장에서 최적화가 되었을 때 가장 이상적인 $\\log D(x)$은 0이다.   
마찬가지로 latent variable $z \\sim p(z)$를 사용해서 생성한 가짜 데이터 $G(z)$는 discriminator로 하여금 가짜다(False = 0)라고 판별되어야 하기 때문에, discriminator 입장에서 최적화가 되었을 때 가장 이상적인 $\\log(1-D(G(z)))$는 0이다.   
즉 Discriminator는 $V(D, G)$ 식을 최대화하고 싶어한다. 여기서 0을 만드는게 왜 최대화지?라고 고민이 된다면, $\\log$ 함수에 들어가는 값이 $0 \\sim 1$ 사이의 확률값이기 때문에 치역이 $-\\infty \\sim 0$임을 고려해주면 좋을 것 같다.   

## Generator의 입장
Generator는 $V(D ,G)$의 두 term 중에서 뒤의 term에만 관련이 있다. Generator의 입장에서는 자신이 latent variable $z$로부터 생성한 가짜 데이터 $G(z)$가 discriminator를 속여야 하므로(가짜 데이터를 보고 '어 이건 찐인데?' 해야 한다는 것), generator 입장에서 최적화가 되었을 때 가장 이상적인 $\\log (1 - D(G(z)))$는 $-\\infty$이다.


위의 두 내용을 종합했을 때 mini-max optimization 식을 이해할 수 있다. 서로 최적화하고자 하는 방향성이 다르기 때문에 치고 박고 싸우는 과정에서 파라미터가 알아서 optimization된다는 것.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209089640-17906d02-f7bf-42e6-ad86-01d2a8c5d04c.png" width="400"/>
</div>

따라서 한쪽 축으로 보면 convex optimization, 다른 축으로 보면 concave optimization이 되어 궁극적으로는 global point로 하여금 saddle point에 머무르는 것이다.   
이를 보다 직관적으로 보여주는 그림이 GAN 논문에 첨부되어있다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209089642-86c8cfec-0767-498e-b5ae-63892418363d.png" width="800"/>
</div>

파란색으로 나타나있는 분포가 discriminator가 그리는 분포이며, 검은색으로 나타나있는 분포가 real data의 distribution, 초록색으로 나타나있는 분포가 generator가 생성한 데이터에 대한 분포이다. (a)를 보면 학습 초반에는 generator, discriminator 모두 최적화가 안된 상태이므로 $z \\rightarrow x$ mapping(초록색 분포)도 실제 데이터(검은색 점)를 잘 따라가지 못하며, discriminator도 얼추 구분은 하지만 완벽하게 진짜/가짜 샘플에 대한 확신을 못하고 변동이 큰 것을 볼 수 있다.   
Discriminator 학습이 어느 정도 진행되면 (b), discriminator는 이제 진짜/가짜 샘플에 대한 구분을 잘 하게 된다.   
이 상태에서 Generator를 학습시키면 (c), $z \\rightarrow x$ mapping이 보다 초록색 분포를 검은색 점에 가깝게 만드는 걸 확인할 수 있다.
학습이 마무리되면, generator는 완전히 실제 분포에 가깝게 mapping을 할 수 있게 되며 discriminator는 진짜/가짜 샘플에 대해 완전히 구분할 수 없게 된다. 바로 이 지점이 위에서 언급한 saddle point, global optimization이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209089644-847cd2b8-e9e0-4c2b-a8db-6dd7149edf66.png" width="800"/>
</div>

알고리즘을 보면 위와 같다. 실제로 학습할 때는 discriminator를 generator에 대해 $K$ 번 optimize하고, generator를 1번 optimize하는 방식을 사용했다. 그래서 위에서 봤던 분포를 기준으로 하면 실제 학습은
$$
\\text{Training iterations} \\times (K \\times \\text{((a)} \\rightarrow \\text{(b))} \\rightarrow 1 \\times \\text{(c))} = \\text{(d)}
$$
라고 볼 수 있다.


## Optimize expression as JS(Jensen-Shannon) divergence
앞서 언급했던 식을 유도하면 JS divergence 식으로 만들 수 있다.
$$
    \\begin{aligned}
        C(G) =& \\max_D V(G,~D) \\newline
        =& E_{x \\sim p_{data}}(\\log D_G^\\ast (x)) + E_{z \\sim p_z}(\\log(1-D_G^\\ast(G(z)))) \\newline
        =& E_{x \\sim p_{data}}(\\log D_G^\\ast (x)) + E_{x \\sim p_g}(\\log(1-D_G^\\ast(x))) \\newline
        =& E_{x \\sim p_{data}} \\left( \\log \\frac{p_{data}(x)}{p_{data}(x)+p_g(x)} \\right) + E_{x \\sim p_g} \\left( \\log \\frac{p_g(x)}{p_{data}(x)+p_g(x)} \\right)
    \\end{aligned}    
$$
앞선 식과 차이가 있다면 generator가 만드는 분포 자체를 $p_g$로 상정해버린 것이다. 실제로는 $z \\rightarrow x$ mapping이 전제되는 부분을 generator term을 없애버리면서 이처럼 바꿀 수 있게 된다. 여기에 우리가 알고 있는 사실 하나는, 최적화가 되었을 때 $p_g = p_{data}$라는 것이고, $D_G^\\ast = \\frac{1}{2}$이다. 해당 value들을 위의 식에다가 적용하면,
$$
    \\begin{aligned}
        C(G) =& E_{x \\sim p_{data}} \\left( \\log \\frac{p_{data}(x)}{p_{data}(x)+p_g(x)} \\right) + E_{x \\sim p_g} \\left( \\log \\frac{p_g(x)}{p_{data}(x)+p_g(x)} \\right) \\newline
        =& E_{x \\sim p_{data}} \\left( \\log \\frac{p_{data}(x)}{\\frac{p_{data}(x)+p_g(x)}{2}} \\times \\frac{1}{2} \\right) + E_{x \\sim p_g} \\left( \\log \\frac{p_g(x)}{\\frac{p_{data}(x)+p_g(x)}{2}} \\times \\frac{1}{2} \\right) \\newline
        =& -\\log(4) + D_{KL}\\left( p_{data} \\parallel \\frac{p_{data}(x)+p_g(x)}{2} \\right) + D_{KL} \\left( p_g \\parallel \\frac{p_{data}(x)+p_g(x)}{2} \\right) \\newline
        =& -\\log(4) + 2 \\cdot D_{JS}(p_{data} \\parallel p_g)
    \\end{aligned}    
$$



<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209089648-4373578b-2637-4b8c-8c1a-b01a834f48a7.png" width="800"/>
</div>

생성된 결과물이다. MNIST, TFD 그리고 CIFAR-10에 대해 실험하였다.


## Measure metrics for GANs
GAN은 생성 모델이다. 그렇기 때문에 discriminator인 모델들이 supervision을 통해 얻을 수 있는 정량적 평가 방식이 통하지 않는다. 생성 모델이라고 하면 단순히 이미지를 잘 만드는 것에 그치는게 아니라 얼마나 다양하고, 그러면서도 있을 만한 이미지를 만들어야 하기 때문에 이를 측정할 수 있는 지표가 필요하다. GAN evaluation에는 일반적으로 약 3개의 meric이 사용된다.

## IS: Inception Score
IS라 불리는 inception score는 quality와 diversity를 측정한다.
$$
    IS(G) = exp(E_{x \\sim G} (D_{KL}(p(y \\vert x), p(y))))    
$$
만약 생성된 이미지가 다양하다면, 그에 따른 분포 $p(y)$는 고르게 퍼져 있어야 한다. 즉, 엔트로피가 커야 한다. 해당 부분이 판단하는 것이 곧 이미지의 다양성이다. 마찬가지로 generated image $x$와 이미지에 대한 라벨 $y$는 정확하게 측정되어야 한다. 무슨 말이냐면 $x$라는 이미지가 정말로 $y$처럼 보여야한다는 것이다. 이때는 샘플에 대한 확신이 곧 이미지의 퀄리티를 반영하므로 $p(y \\vert x)$는 low entropy를 가져야하고, 해당 term이 image quality를 나타낸다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209089655-7128e7d6-88e8-4e93-895b-891c010ef22a.png" width="800"/>
</div>


확률맵 $p(y \\vert x)$를 측정하는 방식은 해당 sample을 통해 학습시킨 inception network를 기준으로 softmax vector를 사용하며, $p(y)$는 모든 샘플에 대한 marginal distribution으로 구한다($p(y) = \\frac{1}{N} \\sum_{i=1}^N p(y \\vert x_i)$). 만약 diversity가 적다면 $p(y)$는 sparse vector(희박한 벡터, one-hot encoding과 같다고 보면 된다)가 되고 반대로 diversity가 크다면 $p(y)$는 dense vector(밀도있는 벡터, entropy가 높은 uniform distribution을 생각하면 된다))가 된다.   
이런 식으로 각 샘플인 $x_i$에 대해 구할 수 있는 inception score IS는,
$$
    IS = exp \\left( \\frac{1}{W} \\sum_{i=1}^N D_{KL} (p(y \\vert x_i), p(y)) \\right)    
$$

## FID: Frechet Inception Distance
그러나 Inception Score에는 큰 문제가 있는데, 그것은 바로 각 class 별로 단 하나의 이미지만 만들어낼 수 있어도(이런 문제를 mode collapse라고 부른다) diversity가 높게 측정된다는 것이다.   단순히 generator가 각 클래스마다 생성하기 쉬워보이는 샘플에 overfitting되면 IS score가 높다. 그렇기 때문에 정말로 다양한 샘플을 만들 수 있는지에 대한 기준이 되기 힘들다. 이러한 측면에서 inception network를 사용하여 중간 layer에서 feature를 뽑고, 평균인 $\\mu$와 공분산 $\\Sigma$를 사용하여 다변수 가우시안 분포를 모델링하는 FID 방식이 소개되었다.
$$
    FID(x, g) = \\parallel \\mu_x - \\mu_g \\parallel^2_2 + Tr(\\Sigma_x + \\Sigma_g - 2(\\Sigma_x \\Sigma_g)^{1/2})    
$$
즉 실제 데이터와 생성 데이터 간의 분포를 비교하고자 하는 식이고, 해당 메트릭은 Inception score에 비해 noise에 강하다는 특징이 있다. 또한 만약 generator가 단 하나의 이미지를 각 클래스마다 만들어내면 이를 IS 지표상으로는 구분을 못했는데, FID에서는 실제 데이터의 분포와 비교를 하다보니 FID 값이 더 안좋게(크게) 나온다. 따라서 다양성에 대한 특성도 반영할 수 있다는 것이다. 그래서 Inception score와 비교하여 FID가 다양성에 대한 지표로서 잘 작동한다.

## LPIPS: Learned Perceptual Image Patch Similarity

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209089658-4e9b9b38-40a4-4325-b1a4-88356fe002d1.png" width="800"/>
</div>

LPIPS라는 메트릭은 비교적 간단하게 확인할 수 있다.
$$
    d(x,~x_0) = \\sum_l \\frac{1}{H_l W_l} \\sum_{h, w} \\parallel w_l \\odot (\\hat{y_{hw}}^l - \\hat{y_{0hw}}^l) \\parallel_2^2    
$$
Feature extraction model인 AlexNet, VGG, SqueezeNet을 사용하여 각 feature map과의 차이를 구하는 것이다. $x_0$이 기준이 되는 이미지이고 $x$가 측정하고 싶은 이미지라면, 각각의 activation map인 $\\hat{y_{hw}}^l, \\hat{y_{0hw}}^l$ 사이의 Euclidean distance를 계산한 뒤에 layer weight $w_l$에 대해 이를 각 채널에 대해 평균을 낸 뒤 레이어 별로 구해진 거리들에 대해 평균을 내면 된다. Feature map 간의 거리를 계산한다는 점에서 image patch similarity를 거리 메트릭으로 사용했다고 한다. 여기서 각 레이어별 weight인 $w_l$은 논문을 보면 학습시키는 파라미터로 작용하고, 아마 이러한 내용 때문에 paper 제목이 지어지지 않았나 싶다.
`,EO=`---
title: "GAN variations(DCGAN, CGAN, PGGAN and StyleGAN)에 대하여"
category: "ai papers"
publishedAt: "2022-12-11"
thumbnail: "https://user-images.githubusercontent.com/79881119/209126974-59cf4988-bf47-407a-8a36-df5bf70644a2.png"
---


## DCGAN: Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks
GAN(Generative Adversarial Network)에 대해서는 이미 기존에 다룬 적이 있다. 간단하게 요약하자면 생성자인 generatoe($G$)와 판별자인 discriminator($D$)가 서로 적대적으로 학습하는 구조가 된다고 했는데, 앞서 소개했던 GAN에서의 구조는 MLP였다.   
그런 와중에 처음으로 convolutional network를 사용하여 좋은 성능을 낼 수 있는 GAN 구조에 대해 연구를 했던 것이 바로 [DCGAN](https://arxiv.org/abs/1511.06434)이며 GAN 특성상 네트워크나 최적화 구조에 따라 학습 형태가 급격하게 변하기 때문에 학습이 힘든데, 이러한 부분에서 많은 ablation이 진행된 paper라고 볼 수 있다.   

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209126944-58190ead-5728-4e4c-8d20-9fb4a06ca86b.png" width="600"/>
    <img src="https://user-images.githubusercontent.com/79881119/209126947-14d04c65-b54f-4817-8f4d-4d5906abf26d.png" width="600"/>
</div>

좌측 그림이 살짝 깨져보여서 죄송스럽지만 슬프게도 논문에서는 generator 구조만 그려서 보여주었고 discriminator 구조는 인터넷에 떠도는 그림을 가져왔다. 암튼 구조를 보면 알 수 있지만 각각 upsampling하는 convolutional networks, downsampling하는 networks로 구성되어있는 걸 알 수 있다.   
DCGAN 구조에서 가장 주목할 점은 max pooling layer가 전혀 사용되지 않는다는 점이다. 아무래도 high level, low resolution feature를 사용하는 classification이나 detection 등등과는 다르게 실제로 이미지를 생성해야하기 때문에 filter로 작용되는 pooling layer가 정보를 많이 손실시키지 않을까 싶다.   
Activation function은 generator의 경우에는 ReLU activation function을 사용하였고, discriminator에서는 마지막 layer에서 확률 범위인 $0 \\sim 1$을 맞춰주기 위해 sigmoid function을 사용한 것 이외에는 Leaky ReLU function을 사용한다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209126950-566cf8d1-15f4-4d71-b305-2d91c771c378.png" width="300"/>
    <img src="https://user-images.githubusercontent.com/79881119/209126953-be29dc02-ca8d-4a41-a8f4-361ba6d18ca5.png" width="300"/>
</div>

$$
    \\begin{aligned}
        ReLU(x) =& \\begin{cases}
            x, & \\text{if }x \\geq 0 \\newline
            0, & \\text{otherwise}
        \\end{cases} \\newline
        LeakyReLU(x) =& \\begin{cases}
            x, & \\text{if }x \\geq 0 \\newline
            -0.01x, & \\text{otherwise}
        \\end{cases}
    \\end{aligned}
$$
Leaky ReLU는 단순히 음수 부분에 대한 학습을 무시하는게 아니라, 작은 값으로 gradient를 주게 된다. 아마 초반 generator의 분포에 따라 discriminator가 먼저 수렴해버리거나 overfitting이 되면 서로 균형된 학습을 하지 못하게 될 수 있기 때문에 이렇게 구성하지 않았나 싶다. 이건 개인적인 견해.   
다시 generator 구조를 자세히 보면 처음에는 100 차원의 latent vector가 input으로 들어가게 되는데, 이를 convolution 연산이 가능하게끔 확장하여 넣어주게 된다.

\`\`\`python
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()

        self.init_size = opt.img_size // 4
        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))

        self.conv_blocks = nn.Sequential(
            nn.BatchNorm2d(128),
            nn.Upsample(scale_factor=2),
            nn.Conv2d(128, 128, 3, stride=1, padding=1),
            nn.BatchNorm2d(128, 0.8),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Upsample(scale_factor=2),
            nn.Conv2d(128, 64, 3, stride=1, padding=1),
            nn.BatchNorm2d(64, 0.8),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),
            nn.Tanh(),
        )

    def forward(self, z):
        out = self.l1(z)
        out = out.view(out.shape[0], 128, self.init_size, self.init_size)
        img = self.conv_blocks(out)
        return img
\`\`\`
위는 오피셜 Pytorch 코드에서 generator를 잠시 가져온건데, 코드를 보면 latent $z$를 받은 뒤 Linear 연산을 통해 차원을 확장시키고, 이를 view를 통해 resize하는 형태를 가진다. 즉 1차원의 텐서의 차원 수를 확장 시킨 뒤에 이를 적절히 나누어 (Batch)$\\times$(Channel)$\\times H \\times W$ 형태로 만들어주게 된다. 이를 project and reshape이라 한다.   
그 다음부터는 fractionally strided convolution을 통해 spatial dimension을 확장한다. 

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209126955-526a9750-ccb2-4d6e-860f-34c33315ebd0.gif" width="300"/>
    <img src="https://user-images.githubusercontent.com/79881119/209126957-b19e4e5f-0d14-4d50-bd67-ad0f8db8cd49.gif" width="300"/>
</div>

기존의 convolution(우측)과 fractionally strided(좌측)이 다른 점은 input(아래쪽 사각형)에 대해 convolution은 그대로 연산을 진행하지만 fractionally strided는 input 사이사이 padding을 넣어 연산을 진행한다. 즉 일단 쭉 늘려놓은 다음에 convolution을 하는 형태다. Max-unpooling하는 형태나 bilinear interpolation같은 방식보다 해당 메트릭이 더 효과적이었던 것 같다. 암튼 DCGAN에서의 생성자는 이런 구조를 가지고 최종적으로는 generated image의 scale을 맞춰주기 위해 $\\tanh$ activation을 사용했다.   
그리고 Batch Normalization을 사용했는데, 여기서 중요한 점은 generator는 <U>가장 마지막 convolutional layer에는</U> normalization을 사용하지 않았고 discriminator에서는 <U>가장 처음 convolutional layer에</U> normalization을 사용하지 않았다.


## Conditional GAN
앞서 DCGAN을 살펴보았다. 해당 paper는 어떤 방식으로 generative adversarial network를 convolution 구조로 풀어냈는지 설명하였고, 그 다음으로 살펴볼 것은 생성자가 '원하는 샘플'을 생성할 수 있을까에 대한 논문인 [conditional GAN](https://arxiv.org/abs/1411.1784)이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209126960-8f7fffe3-dc7d-44be-91c0-cff948c75af7.png" width="500"/>
</div>

기존의 GAN 방식을 보면 $z$ space에서 임의의 latent를 샘플링하고, 해당 latent를 generator에 통과시켜 얻은 이미지가 타겟 도메인으로 하는 modality에 실제로 있을법한지 구분하는 형태였다. 그러나 이러한 방식은 $z$ space에서의 latent를 유의미하게 분리할 수 있는 supervision이 없으며, 실제로 생성하고 싶은 이미지가 어떤 class의 이미지인지(예를 들어 MNIST라면, 숫자 $1$을 생성하고 싶은지 아니면 $8$을 생성하고 싶은지 등등) control할 수 없다는 문제가 생긴다.   
따라서 cGAN에서는 간단하게 이를 label $y$를 추가 parameter로 generator에 주면서 조건부 확률을 만들어내고자 했다. 이전에 보았던 mini-max optimization GAN 식에서,
$$
    \\begin{aligned}
        &\\min_G \\max_D V(D,G) \\newline
        V(D,G) =& E_{x \\sim p_{data}(x)}(\\log D(x \\vert y)) + E_{z \\sim p_z(z)}(\\log (1-D(G(z \\vert y))))
    \\end{aligned}    
$$
각 term에 label $y$에 대한 조건만 추가해주면 된다. 이렇게 최적화가 되고 나서는 generator에 latent sample $z$과 생성하고픈 class를 같이 parameter로 넣어주면 된다.

\`\`\`python
class generator(nn.Module):
    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)
    # Architecture : FC1024_BR-FC7x7x128_BR-(64)4dc2s_BR-(1)4dc2s_S
    def __init__(self, input_dim=100, output_dim=1, input_size=32, class_num=10):
        super(generator, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.input_size = input_size
        self.class_num = class_num

        self.fc = nn.Sequential(
            nn.Linear(self.input_dim + self.class_num, 1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(),
            nn.Linear(1024, 128 * (self.input_size // 4) * (self.input_size // 4)),
            nn.BatchNorm1d(128 * (self.input_size // 4) * (self.input_size // 4)),
            nn.ReLU(),
        )
        self.deconv = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 4, 2, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.ConvTranspose2d(64, self.output_dim, 4, 2, 1),
            nn.Tanh(),
        )
        utils.initialize_weights(self)

    def forward(self, input, label):
        x = torch.cat([input, label], 1)
        x = self.fc(x)
        x = x.view(-1, 128, (self.input_size // 4), (self.input_size // 4))
        x = self.deconv(x)

        return x
\`\`\`
Official code는 아닌 것 같은데 [해당 링크](https://github.com/znxlwm/pytorch-generative-model-collections/blob/master/CGAN.py)를 참고하였다. 구조를 보면 concatenate을 통해 latel과 input을 <U>옆으로 이어붙인다</U>(첫번째 차원 기준). 그런 뒤에 나머지 구조는 GAN과 동일하게 진행된다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209126961-ace055a5-0920-4603-8b1a-26d29f707bd1.png" width="400"/>
</div>

비슷한 형태로 deep convolutional GAN 구조에 적용될 수도 있다. 


## PGGAN: Progressive Growing of GANs for Improved Quality, Stability, and Variation
사실 설명하고 싶었던 논문 중에 [WGAN(Wassertein-1 distance GAN)](https://arxiv.org/abs/1701.07875)도 있는데, 이건 수학이 너무 많이 필요해서 나중에 post 하나에 담아보기로 했다.   
그렇다면 그거 대신에 볼만한 논문 중에서 GAN이 low resolution에만 한정될 수 밖에 없는 문제를 해결해주었던 [PG(progressive-growing)-GAN](https://arxiv.org/abs/1710.10196)에 대해 리뷰해보도록 하겠다. 사실 왜 WGAN을 같이 다루고 싶었냐면 PGGAN에서 학습 전략으로 사용했던 것이 WGAN-GP(Gradient penalty) loss였는데, 이는 이전까지 설명했던 mini-max optimization과는 조금 다르기 때문이다.   
초반 GAN 논문들은 아카이브에서 로드할 때 금방 됐었는데 생성 네트워크가 보다 high resolution sample들을 만들어내면서 논문 용량들이 쉽지가 않다. ~~그렇다고 저자들을 탓하는건 아니고...~~   

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209126962-66718039-bec2-4efd-bad5-b70776dc69b4.png" width="700"/>
</div>

PGGAN은 GAN 학습을 안정적으로 바꾸기 위해 위와 같은 구조의 학습법을 제시했으며, $1024 \\times 1024$의 고화질 이미지를 생성하면서도 안정적인 학습을 할 수 있다는 장점이 있다. 말 그대로 PGGAN은 단계별로 학습을 한다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209126964-9413601b-f116-4c6d-a16a-7473dcd219af.gif" width="700"/>
</div>

학습 과정은 위의 그림과 같다. 처음에는 low resolution image에 대한 GAN 구조를 학습시키고, 거기다가 layer를 추가해서 학습시키고 다음 네트워크를 학습할 때 보조적으로 사용하게 된다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209126969-c480fb6d-6213-4c4c-8210-c27a40738263.png" width="700"/>
</div>

바로 $\\alpha$ 값이 resolution transition을 할 때에 residual connection 처럼 구성하기 위한 친구로, 새롭게 추가된 layer의 학습을 안정적으로 만들 수 있다. $alpha$는 낮은 resolution일 때 $0$부터 높은 resolution일 때 $1$까지 linearly 증가시켰다고 한다. 초반에는 이전 레이어가 upsampling된 녀석이 학습에 보조적인 역할을 해야하지만, 궁극적으로 최종으로 얻고자하는 generator는 오직 마지막 레이어의 output만을 고려해야하기 때문이다.   
참고로 $2\\times$나 $0.5\\times$로 된 부분은 각각 scale을 키우는 입장($G$)에서는 Nearest Neighborhood interpolation 방식을, scale을 줄이는 입장($D$)에서는 Average pooling 방식을 사용하였다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209126971-22f3c731-f39c-4ada-adba-6aeb4f9fd81d.png" width="700"/>
</div>

학습에 사용된 구조는 위와 같다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209126974-59cf4988-bf47-407a-8a36-df5bf70644a2.png" width="700"/>
</div>

참고로 해당 논문에서 소개한 데이터셋이 그 유명한 [CelebA-HQ](https://github.com/tkarras/progressive_growing_of_gans)이며, StyleGAN에서 소개된 FFHQ-1024와 같이 다양한 형태로 활용되고 있다. 참고르 바로 다음에 소개할 논문이 StyleGAN이다.


## StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks
[StyleGAN](https://arxiv.org/abs/1812.04948)은 PGGAN structure를 가지되, 다음과 같은 구조적 변경이 있다. 물론 그 구조적 변경이 해당 논문에서 가장 중요한 부분이고, 어떻게 style에 따른 disentanglement(feature 분리)와 image 생성을 했는지 설명해보도록 하겠다.

- Nearest Neighborhood interpolation 대신 Bilinear upsampling 사용
- Mapping network($\\mathcal{Z} \\rightarrow \\mathcal{W}$)와 AdaIN 추가
- Generator에서의 latent vector를 없앰
- 각 block에 noise 추가
- Mixing regularization 추가


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209126978-fd3f8bd7-ed75-4dd7-911f-321f3a494e11.png" width="700"/>
</div>

기존 구조와 차이점은 StyleGAN에서 만들고자 한 generator 구조는 style image 생성에 특화된다는 점이다. 가장 눈에 띄게 다른 점은 Latent $z \\in \\mathcal{Z}$ 부분이 synthesis network $g$에서 사라진다는 것인데, 왜냐하면 해당 부분이 style에 대한 정보를 추출해서 이를 Adaptive Instance Normalization하는 구조로 바뀌었기 때문이다. 이런 관점에서 StyleGAN에서의 generator $G$는 이미지를 "생성"한다는 느낌보다는 도화지에 그림을 입혀나가는 느낌이다.   
그리고 특이하게도, StyleGAN에서는 $z \\sim \\mathcal{Z}$를 바로 affine하여 style에 사용하지 않고 <U>8개의 linear layer로 구성된 MLP</U>를 통해 생성된 새로운 space $\\mathcal{W}$를 기준으로 한다. 이 내용이 바로 위에서 언급했던 Mapping network의 추가와 관련된 내용인데, 다음 그림을 보도록 하자.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209126979-a909fed3-4f17-4b06-93d7-845b4e4aed0a.png" width="700"/>
</div>


만약 데이터셋에 안경을 쓴 남자만 있고, 안경을 쓰지 않은 남자는 없었다고 가정해보자. 그러면 그림 (a)에서 가로축이 안경의 유무, 세로축이 성별이라고 가정하면 안경을 쓴 여자, 안경을 쓰지 않은 여자 그리고 안경을 쓴 남자는 있는데 **안경을 쓰지 않은 남자**만 없는 것이다. 해당 부분이 아예 비어있는 것으로 표현된다.   
이를 단순히 $z \\sim \\mathcal{Z}$에 최적화하면 가우시안 distribution에 그대로 mapping되므로, 비어있는 부분이 서로 엉겨붙게 된다(그림 (a)에서 보라색 윗부분이랑 노란색 좌측부분이 서로 이어붙여졌다고 생각하면 됨). 여기서 생기는 문제점은 이럴 경우 style image 생성 시에 특정 style에 대한 latent interpolation을 진행하게 되면 다른 style latent까지 영향을 받는다는 것이다.   
<U>예컨데, 안경을 쓰지 않은 남자가 데이터셋에 없었기 때문에, 안경을 쓴 여자의 성별을 안경을 쓴 남자로 바꾸려고 했더니 갑자기 안경도 같이 사라지는 문제가 생기는 것</U>이다.   
바로 이런 문제점을 **Entanglement**라고 부르며, 저자들이 8개의 MLP layer를 사용한 이유가 바로 여기에 있다. 그림 (c)를 보게 되면 데이터셋의 분포를 고려한 space를 만들어, 부족한 feature에 대한 엉겨붙는 현상(entanglement)를 해결(disentanglement)할 수 있는 걸 볼 수 있다.   
그리고 "Synthesis network"라는 이름을 통해 실질적으로 추출된 style vector를 constant에 단계적으로 입히는 과정(PGGAN이랑 유사하다)을 명시한다. 즉 어떠한 스타일의 이미지를 만들고 싶어서 latent로부터 만들겠다라는 느낌보다는 low level부터 style을 계속 합성(얼굴 구조, 성벌, 머리 색, 눈 크기, 입 모양 등등)시켜서 원하는 이미지를 만들고자 하는 것이다.   
구조를 보면 각 블록에서 style이 affine된 벡터가 AdaIN에 사용된다. Adaptive Instance Normalization이라 부르는 식은 다음과 같다.
$$
    \\text{AdaIN}(x_i,~y) = y_{s,~i}\\frac{x_i - \\mu(x_i)}{\\sigma(x_i)}+y_{n,~i}    
$$
$i$번째 feature map을 평균과 분산을 통해 standard gaussian으로 정규화해준 뒤, style vector를 통해 얻은 style scale과 style bias로 다시 normalize한다. 참고로 affine transform은 단순히 style vector $w \\sim \\mathcal{W}$를 weight $A$에 곱하여 얻은 $y = Aw$와 같다. 이를 통해 추출된 $y$에는 style scale factor $y_s$와 style bias factor $y_b$이다.


<div align="center">
   <img src="https://user-images.githubusercontent.com/79881119/209126982-af1317f5-14d4-42f6-a3da-40ce743a9646.png" width="700"/>
</div>

합성 네트워크에서 style이 들어가는 부분에 대해 조절할 수 있는 feature에 대해 저자들은 다음과 같이 소개한다. 낮은 resolution에서의 style feature($4 \\times 4$ 혹은 $8 \\times 8$)는 얼굴 모양, 포즈 그리고 헤어스타일과 같이 큰 맥락을 잡는데 사용하고, 중간 resolution에서의 style feature($16 \\times 16$ 혹은 $32 \\times 32$)는 얼굴의 이목구비, 보다 자세한 헤어스타일에 대해 조절할 수 있다고 한다. 마찬가지로 높은 resolution($64 \\times 64$ 에서 $1024 \\times 1024$)까지의 resolution은 미세한 디테일에 영향을 준다. 근데 그림에서 보면 알 수 있듯이 middle, fine detail을 바꾸는 과정에서 배경과의 entanglement는 해결하지 못한게 한계점으로 보여진다.   
Style mixing은 단순히 latent $z$ 하나를 $\\mathcal{W}$로 매핑한 $w$ 하나만 style synthesis에 사용하는 것이 아니라 중간 부분에서 다른 latent인 $z'$를 사용하여 cross-over되는 부분을 만들겠다는 것이다. 위에서 봤던 그림을 그대로 사용해서 설명하자면, 각각의 이미지가 행과 열로 표현된 table에서 가장 윗 가로줄이 source 1, 가장 좌측 세로줄이 source 2라고 생각해보자.   
Source 1은 latent $z_1$로부터 생성된 이미지, source 2는 latent $z_2$로부터 생성된 이미지라면, 두 스타일을 섞을 때 다음과 같이 진행한다.

<div align="center">
   <img src="https://user-images.githubusercontent.com/79881119/209126985-87a50086-3a4b-4c25-80d5-4a4c2b38b48c.png" width="300"/>
</div>

바로 해당 cross-over 지점을 조절해주는 것이 각 소스에서 fine detail을 가져올지, coarse feature를 가져올지 결정되는 부분이다.   
마지막으로 볼 것이 noise를 injection하는 파트이다.

<div align="center">
   <img src="https://user-images.githubusercontent.com/79881119/209126988-0e9230b4-d7ff-4620-b246-8aa414ebc6b5.png" width="300"/>
</div>

Gaussian noise tensor를 input map과 같은 사이즈로 조절해준 뒤에 이를 feature map에 그대로 더해준다. 이를 사용하는 이유는 noise가 이미지의 세부 디테일을 변화시켜주고, 항상 같은 이미지가 아닌 무작위의 style이 샘플링될 수 있는 방법으로 사용된다.

<div align="center">
   <img src="https://user-images.githubusercontent.com/79881119/209126992-02fc9130-8a9b-4707-b9d0-ba9ccba8138e.png" width="400"/>
   <img src="https://user-images.githubusercontent.com/79881119/209126994-242f9fce-d395-4413-a160-56febf58a481.png" width="400"/>
</div>

보통 coarse level에서 사용되는 noise는 보다 큰 형태의 variation으로 나타나고, fine level에서 사용되는 noise는 디테일에 대한 variation으로 나타난다. 예컨데 우측 이미지에서, (a)는 모든 레이어에 대해 노이즈를 추가한 샘플, (b)는 노이즈가 아예 없는 샘플 (c)는 fine layer에만 노이즈를 추가한 샘플, (d)는 coarse layer에만 노이즈를 추가한 샘플이다. 노이즈가 없었던 것과 비교했을 때 coarse layer에 노이즈를 추가한 샘플 (d)는 머리 스타일을 바꾸거나 하는 큰 형태의 stochastic을 볼 수 있고, fine-layer에 노이즈를 추가한 샘플(c)에서는 피부톤, 전체적인 색감 그리고 머리카락 하나하나의 디테일이 보이는 걸 확인할 수 있다.   
그리고 또 중요하게 살펴볼 paper detail 중 하나는 CelebA-HQ의 경우에는 학습할 때 WGAN-GP loss를 사용했었지만 이를 FFHQ 데이터셋 학습 시에는 non-saturating loss로 바꿨다는 점이다. Non-saturating loss는 기존 GAN loss에서 생성자의 loss만 바꾼 식이다.
$$
    \\begin{aligned}
        \\mathcal{L_D} = \\max_D V(D,G) \\newline
        V(D,G) =& E_{x \\sim p_{data}(x)}(\\log D(x)) + E_{z \\sim p_z(z)}(\\log (1-D(G(z)))) \\newline \\newline
        \\mathcal{L_G} = \\max_G V'(G) \\newline
        V'(G) =& E_{z \\sim p_z(z)}(\\log (D(G(z)))) + \\frac{\\gamma}{2} E_{p_D(x)}(\\parallel \\nabla D_\\psi(x) \\parallel^2)
    \\end{aligned}   
$$
Generator는 더이상 negative log likelihood에 대한 minimization이 아닌 discriminator에 의해 실제 이미지 분포로 측정될 가능성을 높이는 방향으로 학습된다. 뒤에 붙어있는 $\\gamma$ term은 $R_1$ regularization이고 $\\gamma = 10$을 사용했다.   
또한 낮은 probability density region에 해당되는 $z$ 혹은 $w$ space로부터의 샘플은 해당 네트워크를 최적화하기 힘들다. 그렇기 때문에 sampling region을 한정적으로 사용하는 것이 샘플링 퀄리티를 높이게 된다. 이는 $z$나 $w$를 truncating하는 것으로 해결했다.
$$
    \\begin{aligned}
        w' =& \\bar{w} + \\psi(w-\\bar{w}) \\newline
        \\bar{w} =& E_{z \\sim p(z)}(f(z))
    \\end{aligned}  
$$ 


<div align="center">
   <img src="https://user-images.githubusercontent.com/79881119/209127000-7988b3d5-9e1c-4ea0-ba58-bac41a8ab856.png" width="600"/>
</div>

이를 간단히 설명하자면 평균 face를 구하고, 해당 face로부터 $\\psi$ 값을 조정하면서 feasible space를 서칭하고자 하는 것이다.
`,kO=`---
title: "GAN을 활용한 이미지 조작(image to image 그리고 GAN inversion까지)"
category: "ai papers"
publishedAt: "2022-12-12"
thumbnail: "https://user-images.githubusercontent.com/79881119/209128164-6af25b70-ff59-423a-b9bf-8a785910a622.png"
---


Generative model인 GAN은 여러 방면에서 활용될 수 있다.   
- 대표적인 Image synthesis(합성)의 경우 Texture synthesis([PSGAN](https://arxiv.org/abs/1909.06956), [TextureGAN](https://openaccess.thecvf.com/content_cvpr_2018/papers/Xian_TextureGAN_Controlling_Deep_CVPR_2018_paper.pdf), [Texture Mixer](https://arxiv.org/abs/1901.03447))이 있으며,   
- Image super resolution(화질 높이는 것)([ProGAN](https://arxiv.org/abs/1710.10196), [Progressive face super-resolution](https://arxiv.org/abs/1908.08239), [BigGANs](https://arxiv.org/abs/1809.11096), [StyleGAN](https://arxiv.org/abs/1812.04948))이 있다.
- Image impainting이라는 task는 미완성된 그림이나 사진을 완성하는 작업으로, [Deepfillv1](https://arxiv.org/abs/1412.7062), [ExGANs](https://arxiv.org/abs/2009.08454), [Deepfillv2](https://arxiv.org/abs/1806.03589), [Edgeconnet](https://arxiv.org/abs/1901.00212), [PEN-Net](https://arxiv.org/abs/1904.07475)이 있다.
- Face image synthesis는 얼굴 이미지 합성과 관련된 task로, [ELEGANT](https://openaccess.thecvf.com/content_ECCV_2018/papers/Taihong_Xiao_ELEGANT_Exchanging_Latent_ECCV_2018_paper.pdf), [STGAN](https://arxiv.org/abs/1904.09709), [SCGAN](https://arxiv.org/abs/2011.11377), [Example guided image synthesis](https://arxiv.org/abs/1911.12362), [SGGAN](https://ieeexplore.ieee.org/document/8756542), [MaskGAN](https://arxiv.org/abs/1907.11922)
- Human image synthesis로는 사람의 포즈나 전체적인 윤곽을 생성하는 task가 된다. [Text guided](https://arxiv.org/abs/1904.05118), [Progressive pose attension](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhu_Progressive_Pose_Attention_Transfer_for_Person_Image_Generation_CVPR_2019_paper.pdf), [Coordinate-based](https://openaccess.thecvf.com/content_CVPR_2019/papers/Grigorev_Coordinate-Based_Texture_Inpainting_for_Pose-Guided_Human_Image_Generation_CVPR_2019_paper.pdf), [Semantic parsing](https://arxiv.org/abs/1904.03379)

하지만 오늘 살펴볼 내용은 이것과는 다르게 이미지를 바꾸는 작업, 즉 image manipulation과 관련된 것들을 볼 예정이다. 오늘 게시글과 관련된 내용들을 언급해보자면
1. Image to image translation([CycleGAN](https://arxiv.org/abs/1703.10593), [MUNIT](https://arxiv.org/abs/1804.04732), [DRIT](https://arxiv.org/abs/1808.00948), [TransGaGa](https://arxiv.org/abs/1904.09571), [RelGAN](https://openreview.net/pdf?id=rJedV3R5tm))
2. Image editing([SC-FEGAN](https://openaccess.thecvf.com/content_ICCV_2019/papers/Jo_SC-FEGAN_Face_Editing_Generative_Adversarial_Network_With_Users_Sketch_and_ICCV_2019_paper.pdf), [FE-GAN](https://ieeexplore.ieee.org/document/9055004), [Mask-guided](https://arxiv.org/abs/1905.10346), [FaceShapeGene](https://arxiv.org/abs/1905.01920))
3. Cartoon generation([CartoonGAN](https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf), [PI-REC](https://arxiv.org/abs/1903.10146), [Internal Representation Collaging](https://arxiv.org/abs/1811.10153), [U-GAT-IT](https://arxiv.org/abs/1907.10830), [Landmark Assisted CycleGAN](https://arxiv.org/abs/1907.01424))


## Image translation
위에서 많은 task를 언급했던 것은 GAN으로 이만큼이나 많이 할 수 있다는 걸 보여줄라고 한 것이고, 사실 이 게시글의 주 목적은 단순히 image manipulation과 관련된 초기 논문 아이디어에서 insight를 얻어보기 위함이다.   
Image-to-image translation이라 함은 input image로부터 output 이미지를 생성하는 task가 되며, 이때 output과 input은 서로 어떠한 관계에 놓이게 된다.   
이를 테면 computer vision이나 machine learning task에서 주로 나오는 semantic labeling이나 boundary detection이 될 수도 있고,

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209128127-87d17af5-7a5c-4286-88b0-892aacb811df.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/209128133-59673788-fece-4176-bab6-2485f6049932.png" width="400"/>
</div>

Computer graphics나 computational photography에서 다루는 image colorization, super-resolution이 될 수도 있다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209128137-a759fc58-54ff-4881-af7c-579c27016684.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/209128139-4ab35598-1900-41eb-bac2-c097da070b6d.png" width="400"/>
</div>


즉, 해당 task의 supervision은 source로부터 target을 만드는 과정이 되며, generator $G$는 source domain $S$의 이미지를 사용하여 target image $T$를 만들게끔 학습된다.   
어떠한 문제가 되던, 위와 같은 task는 다음과 같은 objective로 귀결된다.
- Objective function $\\mathcal{L}$ 설정
- Training data $(x, y)$ 설정
- Network $G$ 학습하기
- Image translation $G(S) = T$ 정의하기

$$
    \\arg \\min_{\\mathcal{F}} \\mathbb{E}_{x,y}(\\mathcal{L}(G(x), y))    
$$

그러나 기존의 GAN 방식을 바로 적용하기에는 문제가 있는데, 이는 바로 <U>image generation의 mode(modality)를 제어할 수 없다는 것</U>이다. 즉 우리는 아무런 이미지만 만들면 되는 게 아니라 input으로 넣은 이미지의 translation 버전을 원하는데, 이걸 generator가 인지할 수 없다는 것이 첫번째 문제다. 다음은 generator로 생성한 이미지의 low-resolution 문제가 있다.   
이러한 task를 다룬 총 세 개의 대표적인 paper가 바로 pix2pix, cycleGAN 그리고 pix2pixHD이다. 그 중 가장 유명한 논문인 pix2pix와 cycleGAN에 대해서 먼저 살펴보도록 하자.


## pix2pix: Image-to-Image Translation with Conditional Adversarial Nets

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209128143-1afe1947-10ea-4004-a5a1-2e6034374d6d.png" width="600"/>
</div>

pix2pix는 대표적인 image to image translation을 GAN으로 해결한 연구이다. 가장 유명한 figure인 sketch to real image에 대한 framework는 위와 같다.   

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209128145-2aa6c562-4816-434c-9417-047d39df9ac2.png" width="600"/>
</div>

만약, generator가 단순히 sketch image를 가지고 real image를 만들어내는 task에 대해서 생각해보면, 위의 그림과 같이 '그럴듯한 가방'을 만들어내는 것도 가능하지만 그럴 경우 실제 sketch와의 correspondence 문제까지 고려해야한다. 즉, 위쪽 row에 대해서는 sketch 부분에 잘 맞게끔 이미지가 생성되지만, 아래쪽의 row에서는 sketch는 거의 무시한 채 이미지를 생성해낸다.   
이러한 문제를 기존 GAN loss에서는 고려할 수 없었으며(아래쪽 식을 참고),
$$
    \\begin{aligned}
        &\\min_G \\max_D V(D,G) \\newline
        V(D,G) =& \\mathbb{E_{x \\sim p_{data}(x)}}(\\log D(x)) + \\mathbb{E_{z \\sim p_z(z)}}(\\log (1-D(G(z))))
    \\end{aligned}    
$$
이는 generator가 만든 이미지에 대해 loss를 적용할 때 단순히 real distribution에 의한 결과인지 cross-entropy로 구분했기 때문이다. 물론, 이 식을 그대로 적용하지는 않고 input으로 sketch image를 주기 때문에 데이터셋 $(x, y)$를 적용한 GAN loss를 고려해보면,

$$
    \\begin{aligned}
        &\\min_G \\max_D V(D,G) \\newline
        V(D,G) =& \\mathbb{E_{x \\sim p_{data}(x)}}(\\log D(y)) + \\mathbb{E_{z \\sim p_z(z)}}(\\log (1-D(G(x, z))))
    \\end{aligned}    
$$

Generator에 input $x$가 latent vector $z$와 함께 주어지는 구조인 걸 확인할 수 있다. 그러나 이러한 식은 실질적으로 discriminator가 generator에게 줄 수 있는 학습 정보는 "진짜같은" 이미지인지에 대한 loss 뿐이므로 correspondence를 해결할 수 없다는 문제가 생긴다.   
그래서 제시된 objective function이 conditional GAN의 방법을 이용한 loss이며, 여기에 추가적으로 MAE(Mean absolute error)를 생성된 이미지와 정답(GT) 이미지 사이에 줌으로써 low resolution result 문제와 correspondence 문제 모두 해결하려 했다.

$$
    \\begin{aligned}
        &\\arg \\min_G \\max_D \\mathcal{L} (G,D) + \\lambda \\mathcal{L}(G) \\newline
        \\mathcal{L}(G, D) =& \\mathbb{E}(\\log D(x, y))+\\mathbb{E}(\\log(1-D(x, G(x, z)))) \\newline
        \\mathcal{L}(G) =& \\mathbb{E}(\\parallel y-G(x, z) \\parallel_1)
    \\end{aligned}  
$$

위에 표현된 식에서 $\\mathcal{L}(G, D)$는 conditional GAN loss에 해당되며, 앞서 설명했던 것과 같이 discriminator에 input 정보를 함께 줌으로써 생성된 이미지가 입력된 이미지에 대한 조건을 가지게끔 해준다. 그러나 해당 loss는 content가 유지된다는 보장을 줄 수 없기 때문에 여기에 추가적으로 $\\mathcal{L}(G)$로 표현된 식을 통해, 원본 이미지와 유사하게 생성되게끔 만들어준다. $\\lambda = 100$ 정도로 크게 생각해주면 된다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209128147-a661f2ef-4c41-4177-ac9d-f97edfccd894.png" width="600"/>
</div>


생성자 구조는 이와 같이 [U-Net](https://arxiv.org/abs/1505.04597) 형태를 이용하였다. 또한 이 논문에서 특별한 점은 $x$에 추가적으로 latent vector $z$를 넣어주지 않고, decoder part의 dropout으로 해당 stochastic한 부분을 충당할 수 있다고 한다. 그래서 사실상 loss 식에서 표현된 $z$를 따로 넣어주지는 않는다. 추가로 넣어주더라도 별로 효과적이진 않다고 판단했다. 즉, 넣어주어도 결국 $z$에는 아무런 영향을 못받는다고 했다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209128149-7dd5735c-0f0b-4e9f-abfd-af21d2df494c.png" width="600"/>
</div>


각 loss term에 의한 효과를 보여주는 그림이다. L1 loss를 쓰지 않았을 때는 GT의 전체적인 틀을 잃어버리는 문제가 발생하고, cGAN loss를 쓰지 않았을 때는 blurry한 결과가 나오는 것을 확인할 수 있다. Discriminator 구조는 appendix에 따로 나와있는데, 간단하게만 설명하면 모든 ReLU는 LeakyReLU(기울기 0.2)를 사용하였고 [DCGAN](https://arxiv.org/abs/1511.06434)에서와 같이 첫번째 layer에서는 BatchNorm을 사용하지 않았다. 이러한 pix2pix는 두 개의 paired dataset만 있다면 한쪽을 source, 다른 쪽을 target으로 삼아서 다양한 image to image translation task에 적용될 수 있다는 장점이 있다.   
그러나 여기서 생길 수 있는 문제는, 과연 input-output 간에 paired dataset이 없다면 어떻게 될까이다. 이를테면 sketch dataset에 그에 상응하는 사물 이미지가 있어야 sketch to image generation 학습이 가능하고, scene에 대한 depth map이 존재해야 depth estimation 생성이 가능하기 때문이다. 즉 한계점은 pair를 모으기 힘들고, 몇몇 task에 대해서는 아예 불가능할 수도 있다는 것이다. 바로 이러한 관점에서 제시된 것이 [cycleGAN](https://arxiv.org/abs/1703.10593)이다.   
이를테면 사진이 있는데, 그걸 모네 화풍으로 바꾸고 싶다고 하자. 모네의 화풍에 대한 image를 구하기 위해 우리가 직접 사진을 찍을 수도 없고, 정말 그림을 그린 그 풍경이 현재도 존재한다고 가정할 수도 없다. 심지어 조경이나 날씨 등 환경도 달라지고, 동일 시간대 내에서 사진을 찍은게 아니라면 paired dataset을 구축할 수 없다. ~~그렇다고 모네를 환생시켜서 그림 그려달라고 할 수도 없는 노릇~~


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209128152-72f039eb-b155-4cb6-9885-bb78eacdf647.png" width="600"/>
</div>

이 그림을 보게 되면, pix2pix는 paired dataset에 대해서만 적용될 수 있고, 우측과 같은 unpaired dataset에서는 구현이 불가능한 것을 알 수 있다. 그렇다면 cycleGAN의 intuition을 알아보도록 하자.


## CycleGAN: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks
생성 과정을 간단하게 번역 과정으로 생각해보자. 영어를 한국어로 번역하고, 번역된 한국어를 다시 영어로 번역하면 원래의 문장이 나와야한다. 바로 이것이 cycleGAN의 주된 메커니즘이며, 여기서 번역기 기능을 하는 것이 generator라고 생각하면 된다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209128154-d19c06b6-27cf-44a6-939c-5d59d9e055aa.png" width="600"/>
</div>

예를 들어 말 이미지에서 얼룩말 이미지로 바꾸는 task에 대해서 설명하면, 말을 얼룩말로 바꾼 이미지를 다시 말 이미지로 되돌렸을 때 원래의 말 이미지가 나와야 한다는 것이다. 이러한 방법을 통해 unpaired dataset을 가지고 있더라도 원래의 컨텐츠를 유지하면서 이미지를 생성할 수 있게 된다는 것이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209128157-9abd55c3-9674-4c03-8e01-2b219a7019c5.png" width="1000"/>
</div>

이 위에 나타난 그림이 되게 중요한데, cycleGAN의 프레임워크를 이 그림만 보면 모두 이해할 수 있기 때문이다. $X$를 한쪽 도메인이라고 생각하고 $Y$를 다른쪽 도메인이라고 생각하자. 여기서 도메인이 의미하는 것은 데이터셋이 포함되는 하나의 집합이다.   
$X$ 도메인에 포함된 데이터셋 $x$와 $Y$ 도메인에 포함된 데이터셋 $y$에 대해서, 각 방향에 대한 generator를 함수로 표현할 수 있다. $X$ 도메인의 데이터셋을 받아 $Y$ 도메인의 데이터를 생성하는 네트워크를 $G$라고 하고, 이렇게 생성된 $G(x)$를 $\\hat{y}$라 표현한다. 마찬가지로 $Y$ 도메인의 데이터셋을 받아 $X$ 도메인의 데이터를 생성하는 네트워크를 $F$라고 하고, 이렇게 생성된 $F(y)$를 $\\hat{x}$라 표현한다. 두 네트워크에 대해 $X$ 도메인 이미지에 대해 F, G를 최적화하는 과정은 다음과 같다. 우선 $X \\rightarrow Y$로 생성된 이미지에 대한 adversarial loss는 다음과 같이 표현된다. 

$$
    \\mathcal{L}_{GAN}(G, D_Y, X, Y)    
$$

Adversarial loss에서 각 notation이 의미하는 바는 다음과 같다.
- $X$ 에서 $Y$ 이미지를 생성하는 forward process에 대해서,
- $Y$ 도메인의 실제 데이터와 가짜로 생성된 데이터를 비교하는 discriminator $D_Y$가 있고,
- 가짜로 데이터를 생성하는 generator $G$에 대해서 adversarially 최적화를 하겠다.

요약하자면, $D_Y$와 $G$가 서로 경쟁하면서 학습하는 구조가 된다. 이렇게 되면 문제는 $D_Y$에 대해서나 $G$에 대해서는 학습이 가능한데, 역과정에 대해서는 학습이 안된다. 그렇기 때문에 우리는 추가로 경쟁 구조를 하나 더 만들 것이다.

$$
    \\mathcal{L}_{GAN}(F, D_X, Y, X)    
$$

위의 식에서 각 notation이 의미하는 바는 다음과 같다.
- $Y$ 에서 $X$ 이미지를 생성하는 reverse process에 대해서,
- $X$ 도메인의 실제 데이터와 가짜로 생성된 데이터를 비교하는 discriminator $D_X$가 있고,
- 가짜로 데이터를 생성하는 generator $F$에 대해서 adversarially 최적화를 하겠다.

요약하자면 $D_X$와 $F$가 서로 경쟁하면서 학습하는 구조가 된다. 이제 모든 domain에 대한 generator, discriminator 학습이 가능해진다.
그리고 추가적으로 여기에 앞서 언급한 cycleGAN에서의 주요 intuition인 <U>"다시 돌아왔을 때 원래와 같아야 함"</U>을 적용한 cyclic loss는 다음과 같다. 정말 간단하게도 다시 역방향 generator를 사용해서 생성된 이미지를 기준으로 원래 이미지와의 L1 loss를 계산한다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209128161-3a5726a6-706f-4190-a084-7686f293a540.png" width="600"/>
</div>


구현은 pix2pix에서와 동일한 generator과 discriminator를 사용했으나, cycleGAN original paper에서는 Instance Normalization을 사용했으며 modified ResNet based generator를 사용한 점이 살짝 다르다.
 
<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209128162-454b45de-740b-4f14-ad57-bc636cbe9f30.png" width="800"/>
</div>

위의 그림이 cycleGAN에서 사용된 generator 구조라고 보면 된다.
학습 알고리즘을 글로 풀어쓰면 다음과 같다.

1. $x$, $y$ 두 개의 이미지를 서로 다른 도메인 $X$, $Y$로부터 하나씩 가져온다.
2. 각각 방향에 맞는 generator를 통과시켜 fake image를 얻는다. $(x,~y) \\rightarrow (G(x),~F(y)) = (\\hat{y},~\\hat{x})$
3. 각각 도메인에 맞는 discriminaor를 통해 discriminator loss를 계산한다. $D_X$는 $(x, x')$을, $D_Y$는 $(y, y')$을 토대로 계산한다.
4. 다시 각자의 방향으로 가는 generator를 통과시켜 fake image에 대한 reconstructed image를 얻는다. $(\\hat{y},~\\hat{x}) \\rightarrow (F(\\hat{y}),~G(\\hat{x})) = (\\tilde{x},~\\tilde{y})$
5. 다시 reconstructed된 이미지에 대해서 cyclic L1 loss를 계산해준다. 계산은 $(x,~\\tilde{x})$ 그리고 $(y,~\\tilde{y})$에 대해서 진행한다.
6. Generator loss를 최종적으로 계산한다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209128166-90c08835-28d1-4694-91e3-48724217422a.png" width="700"/>
    <img src="https://user-images.githubusercontent.com/79881119/209128164-6af25b70-ff59-423a-b9bf-8a785910a622.png" width="450"/>
</div>


위의 좌측 이미지를 보면 알 수 있듯이, 해당 task는 굉장히 다양한 형태로 구현할 수 있고, 서로 연관짓고 싶은 domain에 대한 데이터만 있으면 어떠한 학습도 가능하다는 장점이 있다. 그러나 limitation으로 등장한 것은 GAN 자체가 사물에 대한 인식을 할 수 없기 때문에 얼룩말로 바꾸는 task와 같은 경우 배경에 무늬가 들어가거나, 심지어 사람이 타고 있다면 사람에도 얼룩말 무늬가 들어가는 일이 발생한다. 보통 모든 데이터셋에 말을 타고 있는 사람이 있다면 이런 일은 일어나지 않겠지만, 학습할 때 사람이 추가로 들어있는 경우가 거의 없을 경우에 inference하면 이와 같은 artifact가 발생하게 된다.


## GAN inversion
다음으로 볼 내용은 [GAN inversion](https://arxiv.org/pdf/2101.05278.pdf)이다. GAN을 뒤집는다라는 간단한 제목으로 소개되는데, 해당 task는 image manipultation에 있어 다음과 같이 접근한다.
 
<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209128169-d2f99af6-41d4-48e6-82e1-2f906f2c6267.png" width="700"/>
</div>

학습된 decoder(혹은 generator $G$)에 대해 $z$라는 latent space 상의 한 점은 fake image인 $x = G(z)$를 만들어낸다. 흔히 latent vector를 뽑는 과정은 Normal distribution으로부터 추출한다($z \\sim \\mathcal{N}(0, I)$).   
그렇다면 이러한 접근은 어떨까? Real image가 있는데, 이 real image와 비슷한 이미지를 만들어낼 수 있는 $z$를 찾는 것이다. 이를 수식으로 표현한 것이 바로
$$
    z^\\ast = \\arg \\min_z (G(z),~x)    
$$
따라서 우리는 **이상적인 $z^\\ast$를 찾고 싶고**, real image $x$와 거의 똑같은 이미지를 만들어낼 수 있는 $z$를 찾을 수 있으면, 이 <U>$z$를 조금씩 바꿔가면서 image editing이 가능</U>하다는 것이다. Concept은 GAN을 이해할 수 있다면 받아들일 수 있을 정도로 simple하다.   
이러한 image manipulation에서 활용되기 쉽게 여러 도메인에 대한 style을 학습시킨 pre-trained styleGAN이 있다. 다음 깃허브 링크를 참고하면 좋을 것 같다. [사전 학습된 StyleGAN 링크](https://github.com/justinpinkney/awesome-pretrained-stylegan)   


## Image to styleGAN
위에서 언급했던 것과 같이 사전 학습된 StyleGAN이 있다고 하자, image to style이란 GAN이 주어진 이미지를 styleGAN의 latent space로 보내는 task다. StyleGAN에 대해서는 이미 다뤄서 대충 알고 있겠지만, $\\mathcal{Z}$ space에서 $\\mathcal{W}$로 보내는 MLP 구조가 있다. 그런데 [Image to style](https://arxiv.org/pdf/1904.03189.pdf)에서는 이를 좀 다르게 $\\mathcal{W^+}$ space로 보낸다. 기존의 $\\mathcal{W}$ space에서의 벡터는 <U>똑같은 벡터를 18개 복사</U>하여 각 styleGAN layer에 넣어주는데, 이와는 다르게 $\\mathcal{W^+}$ space에서는 $18 \\times 512$ 크기의 벡터를 업데이트하는 것이기 때문에, 보다 세부 조정된 inversion이 가능하다. 즉, <U>18개의 서로 다른 row vector들이</U> style layer에 들어간다고 생각하면 된다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209128172-15bc984f-5d16-48cd-a13e-cf8e6eb26e9d.png" width="700"/>
</div>

대충 그림으로 나타낸 것이 바로 위쪽 그림이다. StyleGAN에 의해 임의로 생성되는 이미지를 사용하는게 아니라, latent를 최적화하여 그럴싸한 원본 이미지를 만드는 latent를 찾은 뒤에, 그 latent를 활용하여 image를 editing하겠다는 전략이다.   
그렇다면 $\\mathcal{W^+}$를 어떤 식으로 최적화할까?

1. Initial latent code $w^\\ast$로부터 시작한다.
2. Image $I^\\ast = G(w^\\ast)$를 생성한다. 여기서 $G$는 사전 학습된 styleGAN generator이다.
3. $I^\\ast$를 원래 생성하고 싶었던 reference 이미지 $I$와 비교하고, loss function $\\mathcal{L}$을 계산한다.
4. 위에서 정의한 Loss function을 토대로 처음에 guess한 latent code $w^\\ast$를 gradient descent 방법을 사용하여 업데이트한다.
5. 위의 과정을 몇번의 iteration을 통해 반복한다.

위의 과정을 보면 성능에 중요한 영향을 미치는 것은 딱 두 가지로 정할 수 있다. 애초에 decoder는 미리 학습된 styleGAN과 같은 generator를 사용하기 때문에 딱히 건드릴 순 없고, image 특성을 잘 반영해서 생성해줄 latent space를 잘 찾는 것과 그 latent space에 속해있는 latent vector를 찾아낼 수 있는 loss function이다.   
초기화의 경우 무작위의 latent를 샘플링하는 방법도 있으나, 이전 글에서 styleGAN에서 소개했던 바와 같이 **sampling이 분포가 희박한 곳에서 발생할 경우에 문제가 발생한다**. 이게 무슨 의미냐면, initial point에서 gradient descent가 일어나는 feasible direction이 최적화가 힘든 길이면 원하는 이미지가 잘 생성되지 않을 수도 있다. 그렇기 때문에 단순히 랜덤으로 초기화하는 것보다는 평균치를 의미하는 mean latent code(mean face)로부터 시작하는 것이 낫다는 것이다. 또 loss function에 대해서 언급하자면, 단순히 MSE를 계산하는 것은 pixel-wise error를 계산하기 때문에 high quality embedding을 찾기 힘들다. 왜냐하면 MSE loss가 가지는 값이 실제로 그 이미지를 대변할 수 없기 때문이다. 그렇기 때문에 perceptual loss를 사용하여 latent space를 잘 찾을 수 있게 보조해주는 loss를 생각해볼 수 있다.
$$
    w^\\ast = \\min_w L_\\text{percept} (G(w), I)+\\frac{\\lambda_{mse}}{N} \\parallel G(w) - I \\parallel_2^2   
$$
[Perceptual loss](https://arxiv.org/abs/1603.08155)는 pre-trained VGG-16(ImageNet에 대해 사전 학습된 네트워크)의 feature extraction 부분을 활용하며, 두 이미지 간의 hidden feature 사이의 유사도를 측정한다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209128173-bbb6ae26-9699-4b74-9900-cc8bf25f4108.png" width="700"/>
</div>

위의 그림이 perceptual loss가 제시된 논문에서 발췌한 그림이다. Style에 대한 정보를 VGG-16을 활용한 loss를 토대로 최적화하였고, 이를 컨셉으로 생각하게 되면 우리가 하고자 하는 이미지의 content나 style을 이해하고 최적화하는 상황에서 사용될 수 있음을 시사한다.

$$
    L_\\text{percept}(I_1, I_2) = \\sum_{j=1}^4 \\frac{\\lambda_j}{N_j} \\parallel F_j (I_1) - F_j (I_2) \\parallel_2^2
$$


## Various applications
다음부터는 여러 적용 방법들에 대해서 소개소개..

## Morphing
Morphing은 image processing technique으로, 흔히 한쪽 이미지에서 다른쪽 이미지로 점진적 변화를 할 때 사용한다. 두 개의 이미지 $I_1$ 그리고 $I_2$에 대해 각각 latent space로 embedding된 코드 $w_1$와 $w_2$를 사용, morphing은 다음과 같이 두 latent code의 convex set의 임의의 점을 가리킨다.

$$
    w = \\lambda w_1 + (1-\\lambda)w_2,~\\lambda \\in (0, 1)
$$

이렇게 구한 $w$로 morphed image $G(w)$를 구하면 된다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209128176-ffdf81a3-daa6-4137-845b-307a053eea45.png" width="700"/>
</div>

좌측 이미지와 우측 이미지의 morphed image가 $\\lambda$에 따라 나타난 그림은 위와 같다.

## Expression transfer
이 개념은 사실상 latent arithmetic와 같다. 예를 들어 이런 개념이다.   
- 무표정의 고양이 사진($I_1$)이 있고, 이를 latent에 embedding한 vector $w_1$이 있다고 하자.
- 무표정의 강아지 사진($I_2$)가 있고, 이를 latent에 embedding한 vector $w_2$가 있다고 하자.
- 웃는 강아지 사진($I_3$)가 있고, 이를 latent에 embedding한 vector $w_3$가 있다고 하자.
- 그렇다면, $w = w_1 + \\lambda(w_3 - w_2)$를 통해 만든 이미지는 어떤 모습일까?

사실 이쯤만 되어도 어느 정도 감이 좋은 사람이라면 **"웃는 고양이 사진"** 이 의도한 정답이라는 것을 알아채주셨을 것이다. 결국 expression transfer에서 하고자 하는 것은 latent에서의 연산이 해당 feature를 반영하는 latent space에서의 방향이라는 것.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209128177-8eba5ab9-dab9-4e4f-b516-91e97c694788.png" width="700"/>
</div>


## Style transfer
StyleGAN에서 했던 style transfer 방식과 완전 동일한데, 다만 이걸 $\\mathcal{W}$ space가 아니라 $\\mathcal{W^+}$ space에서 하고자 하는 것이다.

- Image $I_1$에 대해 최적화 및 임베딩된 $18 \\times 512$ 크기의 latent vector $w_1$를 생각할 수 있다.
- Image $I_2$에 대해 최적화 및 임베딩된 $18 \\times 512$ 크기의 latent vector $w_2$를 생각할 수 있다.
- 중간 부분까지 $w_1$를 가져다 쓰고, 그 다음부터는 $w_2$를 가져다 쓴다. 즉 새로운 latent code $w$는 $w_1$과 $w_2$의 몇개의 row를 concatenate한 구조가 된다(아래 그림 참고).


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209128180-092a4a59-8600-408f-8f84-49cbd36033eb.png" width="700"/>
</div>


그렇게 되면 다음과 같이 정말 의미 없는 두 도메인에 대해서도 style transfer가 일어날 수 있다. 단순히 얼굴 이미지에 대한 style synthesis를 제시했던 styleGAN에서 더 확장성 있는 연구 가능성을 제시해준 것과 같다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209128183-9bb9979c-9bd6-440c-a30a-dc5eb64832ae.png" width="700"/>
</div>


이후 [Image2StyleGAN++](https://arxiv.org/abs/1911.11544)를 통해 더 확장성 있는 application을 보여준다. mask based style transfer, image impainting 그리고 local edit 등등 여러 application이 제시가 되었다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209128190-242c9a82-0e6f-445a-bbcb-f75adee7d0cd.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/209128193-3b0bb6ea-3799-4501-b91d-04a49314c345.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/209128194-337469ac-f014-4894-9d17-d82394f7d058.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/209128196-c247f424-e564-424a-81bd-9c15601f7bcb.png" width="400"/>
</div>
`,SO=`---
title: "그래프를 활용한 neural network"
category: "ai papers"
publishedAt: "2022-12-17"
thumbnail: "https://user-images.githubusercontent.com/79881119/209130941-9d4a3e87-4fbb-4c99-a579-2f53d1a52f94.png"
---


우리가 지금까지 봐왔던 전반적인 모델들의 특징은, 따로 설명하지는 않았지만 데이터셋 전체에 대해 <U>i.i.d</U>(independent and identically distribution)을 가정한다. 즉 어떠한 modality에 대해 얻어진 데이터셋 각각은 서로 독립적으로 존재하며, 어떠한 추론 과정에서도 다른 데이터셋에 의한 영향을 받지 않는다는 의미가 된다. 이번에 살펴볼 graph를 통한 neural network의 접근은 이와 차이가 있다.


# 그래프란?

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209130897-9265a3ee-9b74-4f4a-ae5f-2712328c9b5b.png" width="500"/>
</div>

위의 그림에서 보이는 것과 같이 <U>Graph</U>란 개별적인 vertices 혹은 nodes(점)과 각각을 잇는 edges(선)으로 이루어져있다. Node/Vertex는 각각의 item이나 entity를 나타내고 edge는 그들 간의 관계를 나타낸다.


# Euclidean domain(유클리드 정역)이란?

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209148483-6c47c2cc-9be8-4f34-9173-fa5f33c6aa22.png" width="600"/>
</div>

어떠한 domain $R$ 위에 정의된 유클리드 함수 $f : R \\rightarrow \\mathbb{N}$은 다음과 같은 성질을 만족해야한다.
$$
    \\begin{aligned}
        \\text{임의의 }&a\\in R \\text{ 및 }b \\in R \\text{에 대하여,} \\newline
        &a = bq+r \\newline
        \\text{이며, }&r = 0 \\text{ 또는 }f(r) < f(b)>인 q, r \\in R \\text{가 존재한다.}
    \\end{aligned}
$$
여기서 <U>유클리드 정역</U>은 위와 같은 유클리드 함수가 **적어도 하나** 존재하는 정역이라고 생각하면 된다. 사실 위의 내용만 보면 어려운 말이 많아서 바로 이해가 가질 않는데, 유클리드 정역에 놓일 수 있는 데이터셋의 예시를 보면 크게 어렵지 않다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209130902-e2496b8d-1890-4a8a-869e-e717cf2ec48c.png" width="400"/>
</div>

위와 같은 이미지는 <U>2D grid 상</U>의 데이터로 표현될 수 있다. 2D grid는 유클리드 정역의 대표적인 예시이므로(우리가 흔히 볼 수 있는 좌표계는 모두 해당된다고 생각하면 된다) 유클리드 정역의 데이터라고 말할 수 있다. 마찬가지로 어떠한 문장이나 음성 신호를 token화하고 이를 embedding으로 처리하는 것 모두를 <U>1D grid 상</U>에서의 유클리드 정역이라고 볼 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209130906-34e27adc-f386-4bdb-81c0-7f126f071384.png" width="600"/>
    <img src="https://user-images.githubusercontent.com/79881119/209130908-525c95a8-efb3-45d2-9e9f-0b75bc57c13c.png" width="700"/>
</div>


그러나 그래프 구조는 유클리드 정역이 될 수 없다. 이를테면 각 face의 vertex와 edge로 이어지는 3D mesh 구조도 이러한 비유클리드 정역의 데이터 중 하나며, SNS와 같이 유저와 유저와의 관계로서 정의되는 network 구조도 마찬가지이다. 인스타그램을 예로 든다면, 유저 A, B, C, ... 등등에 대해 '서로 친구인 관계', '친한 친구인 관계', 'DM을 자주 보내는 사이', '친구도 아닌데 DM을 보내는 사이' 등등 서로간의 관계로 정의가 되며, 각 유저에 대해서는 '비공개 계정', '공개 계정', '스토리를 자주 올리는 계정' 등등 다양한 property를 가질 수 있다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209130909-d1518d9c-7143-447a-b656-48e70b91e6e9.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/209130911-4d3cd3fd-14f8-4ed3-a5f0-752590c03e3e.png" width="400"/>
</div>

다른 데이터로는 pose estimation에서 사용되는 structure를 예시로 들 수 있다. 이렇듯 grid에 놓일 수 없는 <U>비-유클리드 데이터</U>의 특징으로는 각 점들간의 관계로 정의되는 모든 modality를 상상해볼 수 있다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209130913-54f07431-d122-4b60-a7a1-6d7872c5c381.png" width="700"/>
</div>



# 그래프에서의 notation
앞으로 이런저런 내용들을 설명함에 있어 짚고 넘어가야할 여러 notation 정의들에 대해 먼저 살펴보도록 하자.

- $V$ : A set of vertices(node)
- $N$ : The number of vertices in a set of vertices $V$
- $v_i$ : The $i^{th}$ vertex in a set of vertices $V$
- $v_i^F$ : The feature vector of vertex $v_i$
- $ne(v_i)$ : The set of vertex indicies for the vertices that are direct neighbors of $v_i$
- $E$ : A set of edges
- $M$ : The number of edges in a set of edges $E$
- $e_{i,j}$ : The edge between the $i^{th}$ vertex and the $j^{th}$ vertex, in a set of edges $E$.
- $e_{i,j}^F$ : The feature vector of edge $e_{i,j}$
- $h_i^k$ : The $k^{th}$ hidden layer's representation of the $i^{th}$ vertex's local neighborhood.
- $o_i$ : The $i^{th}$ output of GNN(indexing is framework dependant)
- $G = G(V, E)$ : A graph defined by the set of vertices $V$ and the set of edges $E$

Vertex의 특징 벡터란, 해당 노드가 가지고 있는 property를 의미한다. 특징 벡터에는 vertex의 번호(인덱스), 사람에 대한 정보를 예시로 들게 되면 해당 노드가 가지는 이름이나 국적 그리고 나이를 포함할 수 있다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209130918-3248e0f8-ff3e-4890-8eec-749cc6987fe2.png" width="500"/>
</div>


그래프에 대한 또다른 사전 정의로는 연결성에 대한 representation이 될 수 있다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209130921-6089b27d-36da-442b-b144-2a5e19bd8823.png"/>
</div>

- $A$ : The adjacency matrix; each element $A_{i, j}$ represents if the $i^{th}$ vertex is connected to the $j^{th}$ vertex by a weight
- $W$ : The weight matrix; each element $W_{i,j}$ represents the 'weight' of the edge between the $i^{th}$ vertex and the $j^{th}$ vertex. The 'weight' typically represents some real concept of property. For example, the weight between two given vertices could be inversely proportional to their distance from one another(i.e., close vertices have a higher weight between them). Graphs with a weight matrix are referred to as *weighted graphs*, but not all graphs are weighted graphs.
- $D$ : The degree matrix; a diagonal matrix of vertex degrees or valencies(the number of edges incident to a vertex). Formally defined as $D_{i,j} = \\sum_j A_{i, j}$
- $L$ : The non-normalized graph Laplacian; defined as $L = D-W$. For unweighted graphs, $W = A$.
위의 정의들을 활용하여 위쪽에 있는 graph에 대해 각각의 요소들을 구해보면,

$$
    D = \\begin{bmatrix}
        2 & 0 & 0 & 0 & 0 & 0 \\newline
        0 & 3 & 0 & 0 & 0 & 0 \\newline
        0 & 0 & 2 & 0 & 0 & 0 \\newline
        0 & 0 & 0 & 3 & 0 & 0 \\newline
        0 & 0 & 0 & 0 & 3 & 0 \\newline
        0 & 0 & 0 & 0 & 0 & 1
    \\end{bmatrix}    
$$
우선 $D$의 경우에는 $d_{i, j},~(i = j)$ 요소만 있는 diagonal matrix로, 각 요소들의 의미하는 것은 각 노드와 연결되어있는 다른 노드의 개수가 된다.

$$
    A = \\begin{bmatrix}
        0 & 1 & 0 & 0 & 1 & 0 \\newline
        1 & 0 & 1 & 0 & 1 & 0 \\newline
        0 & 1 & 0 & 1 & 0 & 0 \\newline
        0 & 0 & 1 & 0 & 1 & 1 \\newline
        1 & 1 & 0 & 1 & 0 & 0 \\newline
        0 & 0 & 0 & 1 & 0 & 0
    \\end{bmatrix}   
$$
그리고 $A$의 경우에는 $a_{i,j}$ 요소가 의미하는 바가 $i$번째 vertex와 $j$번째 vertex의 연결성 유무$(0/1)$이다. 따라서 위의 matrix는 보는 바와 같이 대칭 구조(symmetric matrix)를 가진다.
$$
    L = \\begin{bmatrix}
        2 & -1 & 0 & 0 & -1 & 0 \\newline
        -1 & 3 & -1 & 0 & -1 & 0 \\newline
        0 & -1 & 2 & -1 & 0 & 0 \\newline
        0 & 0 & -1 & 3 & -1 & -1 \\newline
        -1 & -1 & 0 & -1 & 3 & 0 \\newline
        0 & 0 & 0 & -1 & 0 & 1
    \\end{bmatrix}   
$$
edge가 weighted되지 않은 graph에 대해서 non-normalized Laplacian은 단순히 degree 행렬에서 adjacent 행렬을 빼면 된다. 이외에도 다양한 형태의 라플라시안 행렬이 정의될 수 있다.

- $I_n$ : An $n \\times n$ identity matrix; all zeros except for one's along the diagonal.
- $L^{sn}$ : The symmetric normalized graph Laplacian; defined as $L = I_n - D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$
- $L^{rw}$ : The random-walk normalized graph Laplacian; defined as $L = I_n - D^{-1}A$


# Graph Neural Network(GNN)
Graph neural network(GNN)이란 그래프를 처리하기 위한 학습 구조이며, training 및 evaluation 과정에서 그래프 구조가 neural network에 들어가게 된다. 이때, 그래프의 vertices나 edges가 가변적이므로 구조적으로 이를 제한하지 않으며, 가장 중요한 건 GNN 구조는 <U>structured, Euclidean</U> domain의 데이터를 처리할 수 있으면서도 동시에 <U>non-Euclidean</U> domain의 데이터를 처리할 수 있다는 것이다.   
   
GNN은 기존 Neural Network에서의 MLP(Multilayer Perceptron)와 동일하며, GNN이 하고자 하는 것인 <U>여러 번의 feature extraction</U>를 통해 그래프로부터 유의미한 high level feature representation을 얻고자하는 것이다. 이렇게 획득된 **high level feature representation**은 우리가 흔히 알고있는 deep learning framework의 decoder output과 같다.   
즉 다시 말해서 GNN의 목표는 크게 두 가지로 정의될 수 있다.
1. 각 vertex의 High level hidden feature vector를 계산하는 것에 집중하고, transition function $f$를 사용하여 연산하게 된다.
2. 이렇게 획득된 hidden feature vector $f(\\cdot)$를 토대로 유의미한 output을 뽑아내는 과정이고, 이는 output function $g$를 사용하게 된다. 과정 하나하나를 자세히 살펴보도록 해보자.

## Step 1: Transition
각 vertex $v_i$의 인접 노드를 생각해보자. 이를 위에서 언급하기로 $ne(v_i)$로 나타내었다. 이러한 neighborhoods를 가지고 타겟이 되는 노드의 hidden representation을 구해야 한다. 물론 각 vertex마다 neighbor의 개수가 다르기 때문에, transition process는 <U>neighbors of vertex $ne(v_i)$의 aggregation(요약본)</U>을 구하는 것과 같다. 이렇게 하면 각 vertex에 대해 동일한 크기의 hidden feature vector를 구할 수 있다. Vertex $v_i$의 $k$번째 state에서의 hidden state $h$ 혹은 embedding은 다음과 같이 formulation된다.

$$
    h_i^k = \\sum_{j \\in ne(v_i)} f(v_i^F, e_{i,j}^F, v_j^F, h_j^{k-1}), \\text{Where all }h_i^0 \\text{ are defined upon initiallization}    
$$

위의 식을 보면 알 수 있듯이 formulation에서의 aggregation은 summation으로 나타내어지고, 함수는 'hidden state를 구하고 싶은 vertex의 feature', 'hidden state를 구하고 싶은 vertex와 인접한 vertex의 feature', '그 사이의 관계를 나타내는 edge feature', '인접한 vertex의 hidden state'를 input으로 삼는다. 식을 천천히 보다보면 왜 <U>Vertex 자기 자신의 이전 hidden state</U>를 input으로 가지지 않을까라는 의문이 생겼는데, 그건 formulation을 단순히 위와 같이 했기 때문이라고 한다. 아마 구체적인 formulation은 각 상황에 맞게 달라질 수 있지 않을까 생각했다. 위에서의 $f$는 non-linear transformation 구조를 가진다. Activation function을 가지는 단순하게 구성된 MLP를 생각하면 된다.   
   
이렇게 정의된 transition 과정에서의 $k^{th}$ state란,

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209130924-98229b95-743e-4be9-8d08-42ad02480451.png"/>
</div>

노드들은 각 레이어에서 $v_i^F$를 가지고 있을 것이고, $0^{th}$ state란 0번째 레이어에서 각각의 vertex $A = v_A^F$가 가지는 hidden state $h_A^0$가 된다. 이런 맥락으로 쭉 이어가다보면, $n$번째 state에서는 타겟이 되는 vertex와 $n$ 만큼 가까운 vertex들의 정보들을 aggregation(요약)한 형태가 된다. 따라서 GNN transition에서의 stage는 곧 <U>'얼마나 멀리 있는 vertex의 정보까지 활용하여 현재 vertex의 state를 나타내었는가'</U>에 대한 지표가 된다.   
여기서 $k$는 임의의 큰 숫자도 가능하며, 일반적으로 transition function $f$를 반복적으로 적용하여 $k^{th}$ state가 안정적인 상태에 들어갈 때까지 연산하게 된다.

## Step 2: Output
안정적인 상태에 접어들 때까지 $k$번 function $f$를 적용하고 나면 graph에는 연산된 feature vector가 implicit하게 존재하게 된다. Output function은 이렇게 <U>수렴된 hidden state를 사용하여</U> 유의미한 <U>output을 뽑아내는</U> 연산 과정이다.   
유의미한 output은 vertex-level, edge-level 그리고 graph-level 이렇게 세가지로 구분될 수 있다. 각각이 적용될 수 있는 task가 조금씩 다르다고 보면 된다.   

가장 먼저 vertex level framework는 output value를 내보내기 위해 오직 <U>vertex 정보</U>만 사용하게 된다. 따라서 해당 구조에 요구되는 인자는 오직 vertex의 feature vector($v_i^F$)와 hidden state information($h_i^{kmax}$)가 된다.
$$
    o_i = g(v_i^F,~h_i^{kmax})    
$$

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209130929-e238133e-230b-4e40-a416-8fbf6d8694d4.png" width="400"/>
</div>

주로 활용될 수 있는 task는 몇몇 노드에 labeling이 되어있지 않은 경우 주변 노드들을 활용하여 labeling을 하는 node classification이나 regression이다.

두번째로 edge level framework에서는 output value를 내보내기 위한 input으로 총 다섯 개의 정보가 필요하다. 하나의 Edge는 우선 두 개의 vertex와 연결되어있기 때문에 두 개의 vertex에 대한 feature vector($v_i^F$, $v_j^F$) 그리고 hidden state($h_i^{kmax}$, $h_j^{kmax}$)가 필요하며, 그와 함께 edge 자체의 feature vector($e_{ij}^F$)도 필요하다.
$$
    o_i = g(v_i^F,~h_i^{kmax},~v_j^F,~h_j^{kmax},~e_{ij}^F)
$$

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209130934-f06f85d7-1002-4da7-9948-ca1d4e216ccc.png" width="500"/>
</div>

주로 활용될 수 있는 task로는 각 edge를 분류하는 작업(edge classification)이 될 수도 있고(관계에 대한 분류라고도 볼 수 있음), 이를 보다 확장시켜 생각하면 link prediction에도 활용될 수 있다. Link prediction이란 <U>두 node가 미래에 유의미한 관계를 가질 수 있을지</U>에 대한 내용으로 흔히 Amazon과 같은 온라인 마켓에서 물건을 구매하기 시작하면 이후에 내가 관심가질만한 품목들을 추천해주는 것과 같다. 이외에도 컨텐츠 추천이 가능한 다양한 플랫폼에서 활용될 수 있다.

세번째로 graph level framework에서는 output value를 내보내기 위한 input으로 네트워크 전반에 대한 정보를 활용한다. 물론 그래프의 모든 정보를 가질 필요는 없지만 final hidden state of vertices or edges 혹은 initial state도 선택에 따라 활용할 수 있다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209130937-bdad24a5-5068-4b10-a385-8f02f060ef81.png" width="500"/>
</div>

주로 활용되는 task로는 위와 같이 서로 다른 그래프에 대해 어떠한 집단인지 분류하는 graph classification이 될수도 있으며,

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209130941-9d4a3e87-4fbb-4c99-a579-2f53d1a52f94.png" width="500"/>
</div>

Euclidean domain의 데이터인 이미지의 각 픽셀에 대한 graph를 학습한 뒤에 이를 토대로 분류할 수도 있다.


# Reformulation to Neural Network form
앞서 보았던 식은 단순히 $f$, $g$로 각 transition 및 output에 대한 함수를 표현했고, 구체적으로 neural network에서는 이를 어떠한 방식으로 수식전개를 했는지 나타내지는 않았다.   
Neural network에서는 preceptron의 정의에 따라 학습 가능한 parameter를 가지는 weight와 bias를 통해 next state에 대한 affine을 연산하고, 이 결과에 non linear activation을 통해 여러 layer를 통과했을 때 연산의 complexity를 높이는 방식이다.
$$
    \\begin{aligned}
        h_v^0 =& x_v \\newline
        h_v^k =& \\sigma\\left( W_k \\sum_{u \\in N(v)} \\frac{h_u^{k-1}}{\\vert N(v) \\vert} \\right),~\\forall k \\in (1, \\cdots, K) \\newline
        z_v =& h_v^K
    \\end{aligned}
$$ 
위의 수식에서 볼 수 있듯, $k$번째 state($h_v^k$)를 구하고 싶은 vertex($v$)의 인접한 vertex($u$)의 $k-1$번째 hidden state($h_u^{k-1}$)를 평균을 통해 aggregation을 하고, 이를 k번째 weight term $W_k$에 곱한다. 그리고 나서 vertex $v$의 $k-1$번째 hidden state $h_v^{k-1}$에 k번째 bias term $B_k$를 곱해서 더하고, 이 전체 연산 결과에 activation function을 적용하는 구조가 된다.
$$
    \\begin{aligned}
        H^{(l+1)} =& \\sigma(H^{(l)}W_0^{(l)}+\\tilde{A}H^{(l)}W_1^{(l)}) \\newline
        \\text{with } \\tilde{A} =& D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}
    \\end{aligned}    
$$
위와 같이 vector form으로도 표현 가능하다. 이렇게 구해진 hidden state 혹은 embedding을 loss function을 활용하여 학습 가능한 weight 및 bias parameter에 대해 gradient based optimization이 가능하다.

# Graph embedding
이렇게 학습된 neural network를 통해 embedding space로의 변환이 가능하다. Node를 vector로 보내는 변환이 될 수도 있고 graph의 일부 구조(substructure)를 vector로 보내는 변환이 될 수도 있으며 graph structure 자체를 vector로 변환할 수도 있다. Embedding mapping의 퀄리티를 측정하는 주요 방법은 기존에 그래프 구조에서 가지고 있는 similarity(node간의 유사성 등)을 $d$ dimensional embedding space에서 유지할 수 있는지 보는 것이다. 추출된 embedding은 다양한 downstream task에서 활용될 수 있다.


# Convolutional GNNs
앞서 계속 풀었던 내용은 Graph 구조를 어떤 방식으로 neural network를 통해 inference할 수 있는지에 대한 부분이고, 지금부터 살펴볼 내용은 이를 어떤 방식으로 convolutional 구조를 사용하여 해결할 수 있는지에 대한 부분이다. CGNNs라 불리는 convolutional GANN은 두 가지의 type으로 정의할 수 있는데, 첫번째는 <U>spatial domain</U>, 그리고 두번째는 <U>spectral(frequency)</U> domain이다.

## CGNNs in spatial domain
예를 들어 다음과 같은 그래프 구조를 가지는 숫자 이미지를 생각해보자,

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209130942-9c784fe9-81fe-4e21-8e99-3d263da00856.png"/>
</div>

이미지는 graph를 특별하게 구성한 형태로 가정하고, 여기에 convolution 연산을 하는 것은 정해진 크기의 filter($3 \\times 3$)를 input에 대해 곱한 뒤 aggregation을 하는 것과 같다. 그런 뒤 filter를 shift하여 같은 과정을 반복한다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209130945-e7e71ffd-bb60-4fec-ac8c-4d8af70f758f.png" width="500"/>
</div>

여기서 알 수 있는 사실은 단순히 convolution 연산을 image와 같은 modality에 적용할 수 있지만 spatial order를 가지는 다른 graph를 적용하기 힘들다는 문제와, Grid 구조에 맞게 설계가 된 이미지와는 다르게 vertex neighbors가 달라질 수 있는 non-Euclidean domain dataset에는 균일한 형태로 aggregation이 힘들다는 것이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209130946-72b3cf07-6457-4687-a220-8b30b67feaed.png" width="500"/>
</div>

위의 그림을 예시로 보면, R, G, B의 색을 가지는 target vertex와 각각의 neighborhoods로 정의된 dotted box가 있다. Spatial convolution에서는 neighborhood가 선택되고 해당 vertices들의 feature vector가 aggregate된다. 그리고 이렇게 aggregate된 value는 target vertex의 다음 embedding을 결정하는 요소로 사용된다. 이러한 process는 graph의 모든 neighborhoods에 대해서 반복된다. 이렇게 구해진 embeddings는 다음 layer의 spatial convolution에 활용될 수 있는 value가 된다. 따라서 hierarchical feature extraction이 가능하다. 이를 정리해서 순서로 나타내면,

1. Using spatial connectivity to define graph neighborhoods around all vertices, select the first neighborhood in the input graph - 그래프가 연결되어있는 구조를 확인하여 모든 vertex에 대해서의 neighborhoods 집합들을 정의하고 이 중 첫번째 neighborhood를 고른다.
2. 이 neighborhood에 있는 value를 축약한다. 앞서 봤던 연산과 같이 합 연산이나 평균 연산을 진행한다.
3. 이렇게 얻어진 value를 통해 vertex hidden state(embedding)을 update한다.
4. 해당 process를 모든 neighborhood에 대해 진행한다.

[GraphSAGE](https://arxiv.org/abs/1706.02216) 논문에서는 node에 대한 embedding을 추출하고, node attribute information이 풍부한 경우에 사용되었다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209130948-524d7bbe-1ab6-4ec3-9fb7-289efa788ccd.png" width="500"/>
    <img src="https://user-images.githubusercontent.com/79881119/209130951-72cf9bb8-abbe-4393-b3e3-faadf5871a10.png" width="500"/>
</div>

결국 GNN에서의 aggregation식과 거의 유사하게 작동하는 것을 확인할 수 있다. Sptial CGNN에서 활용되는 다양한 aggregator는 다음과 같다.

|Variant|Aggregator|Updator|
|:---:|:---:|:---:|
|[Neural FPs](https://arxiv.org/abs/1509.09292)|$h_{\\mathcal{N_v}}^t = h_v^{t-1}+\\sum_{k=1}^{\\mathcal{N_v}}h_k^{t-1}$|$h_v^t = \\sigma(h_{\\mathcal{N_v}}^t W_L^{\\mathcal{N_v}})$|
|[DCNN](https://arxiv.org/abs/1707.01926)|Node classification:<br> $N = P\\astX$ <br> Graph classification:<br>$N = 1_N^T P\\ast X/N$|$H = f(W^c \\odot N)$|
|[GraphSAGE](https://arxiv.org/abs/1706.02216)|$h_{\\mathcal{N_v}^t} = \\text{AGGREGATE}_t(h_u^{t-1}, \\forall u \\in \\mathcal{N_v})$|$h_v^t = \\sigma(W^t \\cdot (h_v^{t-1} \\parallel h_{\\mathcal{N_v}}^t))$|

이러한 CGNN이 기존 GNN과 비교해서 다른 점은 다음과 같다. GNN의 경우 embedding이 stable한 value를 가질 때까지 transition function $f$를 반복해서 적용하였다. 따라서 앞서 표현했던 바와 같이 $h_v^{kmax}$까지 최적화가 진행된다. 그와는 다르게 CGNN은 고정된 수의 layer를 가지게 되고, 보통 간단하게 업데이트될 때 단일 layer를 가지게 된다($k = 2$).

## CGNNs in spectral domain

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209148489-63f693f6-5e0e-4e61-9bb4-ec31ee9aaa09.png" width="400"/>
</div>

Spectral domain이란 <U>주파수</U>를 의미한다. 흔히 시간에 대한 temporal information이 주어진 음성과 같은 신호에 대해서는 주파수를 생각해보기가 쉽다. 왜냐하면 소리가 들리는 방식이 흔히 우리가 알고 있는 것과 같이 공기(매질)의 진동을 통한 정보 전달이며, 이를 주파수 영역에서 분석하기 위해 fourier transform($\\mathcal{F}$)을 하는 것이 일반적이다. 그러나 이미지에서도 이처럼 convolution을 적용한 형태의 spectral을 활용할 수 있다.   
이미지를 fourier transform하게 되면 주파수 영역에서는 convolution 연산이 multiplication이 된다.
$$
    \\mathcal{F}(f \\ast g) = \\mathcal{F}(f) \\times \\mathcal{F}(g)    
$$
이러한 background를 기반으로 그래프 신호에 대해 논의해보도록 하자.

## Graph signals
그래프에서의 <U>신호</U>는 곧 모든 vertex에 대해 매핑된 embedding이 real value를 가질 경우에 정의될 수 있다. 여기서 함수 $f$가 특정 vertex를 기준으로 $f: V \\rightarrow \\mathbb{R},~\\forall V \\in G$라면, 곧 이 함수가 신호가 된다. 해당 함수를 통해 모든 vertex는 $N$ 크기의 vector(여기서 $N$은 vertex의 개수를 의미)로 표현되며, $i^{th}$ 요소는 곧 vertex $v_i$의 signal value이다. 여기서 $f$는 앞서 말했던 것과 같은 neural network는 아니고 단순히 각 vertex에 대한 feature map이라고 보면 된다.

## Laplacian
라플라시안 연산자($\\Delta$)는 2차 gradient($\\nabla^2$)를 의미한다. graph signal $f$가 있고, 여기서 구할 수 있는 gradient는 다음과 같다.
$$
    \\nabla f(i) = (f(i+1)-f(i))/\\delta    
$$
여기서의 $\\delta$와 $i$는 각각 vertex의 index($i,~j$)와 edge weight(w)로 바꿀 수 있다.
$$
    \\nabla f(i,~j) = (f(i)-f(j))w(i,~j)
$$
그렇게 되면 Laplacian은 다음과 같이 정의될 수 있다.
$$
    \\begin{aligned}
        \\Delta f(i,~j) =& \\sum_j \\frac{\\partial^2 f(i)}{\\partial j^2} \\newline
        \\Delta f(i,~j) \\simeq& \\sum_{j \\in \\Omega_i} (f(i)-f(j))w(i,~j)
    \\end{aligned}
$$
이를 조금 더 전개해서 matrix form에 대해서 나타내면,
$$
    \\begin{aligned}
        \\Delta f(i,~j) =& \\sum_j \\frac{\\partial^2 f(i)}{\\partial j^2} \\newline
        \\simeq& \\sum_{j \\in \\Omega_i} (f(i)-f(j))w(i,~j) \\newline
        =& (\\sum_j w_{ij})f(i) - \\sum_j w_{ij}f(j) \\newline
        =& (Df)_i - (Wf)_i = ((D-W)f)_i
    \\end{aligned}
$$
앞서 notation에서 설명했던 Laplacian operator에 대해서 graph Laplacian이 degree matrix와 adjacency(혹은 weight) matrix로 표현될 수 있는 이유가 된다.

## Graph Laplacian
위에서 얻은 $L$에 대해 eigendecomposition을 진행하면 $L = U\\Lambda U^\\top$로 표현되며, 여기서 eigenvectors $u_i,~i=0,\\cdots,N$은 graph $G$의 fourier bases가 되고 eigenvalues $\\lambda_i,~i=0,\\cdots,N$은 graph의 frequency components가 된다. 이렇게 표현할 수 있게 되면 graph signal $f$에 대한 푸리에 변환($f \\rightarrow \\hat{f}$)과 그 역변환($\\hat{f} \\rightarrow f$)은 각각,

$$
    \\begin{aligned}
        \\hat{f} =& U^\\top f \\newline
        f =& U\\hat{f}  
    \\end{aligned}
$$

위와 같이 표현 가능하고 해당 내적을 각각의 fourier bases에 대해 표현하면,

$$
    \\begin{aligned}
        \\hat{f} =& U^\\top f = \\sum_{i=1}^N f(i) u_k^\\ast (i) \\newline
        f =& U\\hat{f} = \\sum_{k=1}^N \\hat{f}(k) u_k(i)  
    \\end{aligned}
$$
위와 같다. 그리고 앞서 background로 짚고 넘어온 내용 중에, vertex에서의 multiplication은 frequency domain에서 convolution과 같다는 것이 있었다. 원래는 spatial domain에서의 convolution이 frequency domain에서의 multiplication이 된다는 내용인데, duality에 의해 반대도 성립하는 것이다.

$$
    (\\hat(f) \\ast \\hat{g})(i) = \\sum_{k=1}^N \\hat{f}(k) \\hat{g}(k) u_k(i)    
$$
이를 이용하여 임의의 filter $g$를 정의만 할 수 있다면, 다음과 같이 hidden channel 연산이 가능해진다. Filter $g$ 대신 학습 가능한 weight $\\Theta$로 생각한다면,

$$
    \\begin{aligned}
        H_j^k =& \\sigma(\\sum_{i=1}^{f_{k-1}} U(\\Theta^k(U^\\top H_i^{k-1}))), \\newline
        \\text{where }j =& 1,2, \\cdots, f_k
    \\end{aligned}    
$$

해당 식을 다음과 같은 그래프에 대해서 적용해보도록 하자.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209148493-fbabedeb-a16f-4280-bca9-5be28e29fe30.png" width="600"/>
</div>

총 vertex의 개수 $N = 5$, edge의 개수 $M = 5$이며 각 vertex에서 처리할 graph signal의 개수 $f_k = 2$이다. 여기서 $k$는 레이어 인덱스이다. 위의 그림에서 보이는 feature vector는 초기 상태를 의미한다. 따라서,

$$
H^0 = \\begin{bmatrix}
    0.2 & 8 \\newline
    0.4 & 6 \\newline
    0.3 & 7 \\newline
    0.3 & 12 \\newline
    0.1 & 4
\\end{bmatrix}
$$
위와 같이 나타낼 수 있다. 업데이트할 각각의 column $H^k_1$ 그리고 $H^k_2$에 대해서,

$$
    H^0_1 = \\begin{bmatrix}
        0.2 & 0.4 & 0.3 & 0.3 & 0.1
    \\end{bmatrix}^\\top
$$

$$
    H^0_2 = \\begin{bmatrix}
        8 & 6 & 7 & 12 & 4
    \\end{bmatrix}^\\top
$$

$$
    \\begin{aligned}
        H_j^k =& \\sigma(\\sum_{i=1}^2 U(\\Theta^k(U^\\top H_i^{k-1}))), \\newline
        \\text{where }j =& 1,2
    \\end{aligned}
$$
위와 같이 표현된다.그러나 위의 식을 보면 알 수 있듯이, graph의 구조에 따라 eigen-system을 계산해야한다는 문제가 있다. 그렇기 때문에 $1^{st}$ generation of spectral CGNN이었던 모델에서는 노드의 개수인 $N$이 커지면 그에 따라 연산해야할 eigen-system이 커진다는 문제가 있었다. 이를 계속 해결하기 위한 연구가 이후에 진행되었고, $2^{nd}$ generation인 ChebNet 그리고 $3^{rd}$ generation인 GCN이 연구되었다.

|Variant|Aggregator|Updator|
|:---:|:---:|:---:|
|[ChebNet](https://arxiv.org/abs/1606.09375)|$N_k = T_k(\\tilde{L})X$|$H = \\sum_{k=0}^K N_k \\Theta_k$|
|$1^{st}$ order model|$N_0 = X$<br>$N_1 = D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}X$|$H = N_0 \\Theta_0 + N_1 \\Theta_1$|
|Single parameter|$N = (I_N + D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}})X$|$H = N\\Theta$|
|[GCN](https://arxiv.org/abs/1609.02907)|$N = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}X$|$H = N\\Theta$|

## GCN
위에서 언급했던 GCN이 spectral domain에서의 convolutional GNN에서 가장 유명한 형태가 된다. GCN은 $l$-layer network로, 보다 우리에게 친숙한 neural network 구조를 가진다. 위의 표에서도 언급했지만 Hidden layer는 Aggregator $\\hat{A}$에 대해서,

$$
    \\begin{aligned}
        H^{(l+1)} =& \\sigma(\\hat{A}H^{(l)}W^{(l)}), \\newline 
        \\text{with }\\hat{A} =& \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}X \\newline
        \\tilde{D_{ij}} =& \\begin{cases}
        \\sum_{k=1} \\tilde{A_{ik}} & \\text{if }i = j \\newline
        0 & \\text{otherwise}
    \\end{cases}
    \\end{aligned}
$$

위와 같이 나타낼 수 있다. 이렇게 연산된 각 hidden embedding에 대해서 output layer는,

$$
    Z = f(X, A) = softmax(\\hat{A}\\text{ReLU}(\\hat{A}XW^{(0)})W^{(1)})    
$$
위와 같이 나타낼 수 있다.


<div align="center">
    <img src = "https://user-images.githubusercontent.com/79881119/209148496-25f9fd06-63ec-4f82-a779-b63b7fe27518.png" width="500"/>
</div>



그래프 관련 네트워크에 대해서는 아직 이해를 완전히 하지 못했다. 나중에 기회가 된다면 더 공부해볼 수도 있겠지만 아쉽게도 아직까지는 나한텐 딥하게 다뤄보기가 애매한 분야인 것 같다.
`,NO=`---
title: "다양한 Transfer Learning 기법들에 대한 리뷰"
category: "ai papers"
publishedAt: "2022-12-18"
thumbnail: "https://github.com/user-attachments/assets/ab75aa42-1a07-48ae-b4ec-bd334c3114f0"
---


이번 게시물은 딥러닝 네트워크를 학습시킬 수 있는 다양한 학습법에 대한 내용이다. 단순히 단일 task에 대한 학습법이 아니라 다양한 환경에서 적용될 수 있는 방법론에 대해서 다루기 때문에 이번 게시글에는 다양한 내용을 담게 되었다. 그래서 우선 글을 본격적으로 시작하기 전에 어떤 내용을 다룰 것인지 간단하게 소개를 하고 시작하도록 하겠다.

1. Transfer learning : DL optimization in different tasks
2. Knowledge distillation : Increase representation generalization with soft label
3. Continual learning : Learn various representations without losing previous tasks' performances
4. Self-supervision : Learn representations with no/little supervision


# 딥러닝 학습이란?

딥러닝은 머신러닝의 여러 기법들 중 하나인 <U>Neural Network</U>를 대량의 데이터셋을 통해 보다 깊은 layer를 학습시키는 법을 gradient based로 제안하였다. 그렇기 때문에 ImageNet에서 우승했던 기본 classification 모델부터 시작해서 현재 급속한 발전을 이루었다.   
결국 하고자 하는 말은 딥러닝이 학습하기 위해서는 <U>Task가 정의</U>되어야 하며, 해당 task를 해결하고자 할 때 사용될 modality에 의한 <U>dataset이 필요</U>하며, 이러한 dataset 형태에 따라 최적화하고자 하는 <U>loss function</U>, 그리고 gradient based로 학습시킬 때 적용할 <U>optimization method</U>로 귀결된다. 


# 만능 인공지능을 만들 순 없을까?

그렇다면 결국 각 task가 정의되어야하고, 그 task에 최적화된 딥러닝 모델을 만들 수는 있으나 과연 정말 인간과 같이 <U>말도 잘하고</U>, <U>사물 구분도 잘하면서</U> 어떠한 장면을 보고 <U>질문에 대한 대답도 합리적으로 잘하면서</U> 인지에 대한 실시간 대응이 가능한 네트워크를 구성할 수 있을까?   
요즘 "그림 그리는 AI"나 "ChatGPT"가 유명해지며 사람들의 관심이 끌리기 시작했다. 그만큼 생성 AI의 성능이 무서울 정도로 성장세를 보이고 있고, 다양한 modality에 대해 적용 가능한 네트워크 구조가 많이 발전한 것은 사실이지만, 그럼에도 불구하고 여전히 효율적으로 모든 실생활의 문제에 적용 가능한 딥러닝은 개발되지 않은 상태다. 그렇다면 <U>하나의 task에 대해 최적화된</U> 딥러닝을 <U>다른 task에 적용할 때</U>는 어떤 방법들을 사용할 수 있는지 소개해보도록 하겠다.


# Transfer learning(전이 학습)
cs231n에서도 소개되는 내용인 <U>전이학습</U>은(혹은 transfer learning), 현재 연구되고 있는 다양한 딥러닝 연구 전반에서 활용될 수 있는 기법이다.   
전이학습의 정의는 'previous task'에 대해 딥러닝에 학습된 지식이나 기술이 'novel task'에서도 활용될 수 있을까 혹은 적용할 수 있을까?에 대한 접근법이다.   
예를 들어 사람을 예시로 들어보자. Chess를 잘 두는 선수는 Checkers나 바둑과 같이 비슷한 게임에 더 적응을 잘할 수 있을 것이다.

다른 예시로는 수학을 잘하는 사람이 인공지능 공부를 더 잘하는 것이나, 코딩을 잘하게 되는 현상 그리고 tennis를 잘치는 사람이 탁구나 배드민턴을 금방 익히는 것과 비슷하다.   
결국 길게 소개했지만 말하고자 하는 것은 <U>'어떠한 task'</U>에 대해 최적화된 네트워크에 학습된 parameter를 이용해서 '비슷하지만 살짝 다른 task'에 대해 적용할 수 있지 않을까이다.

일반적인 imageNet에 대해 학습된 네트워크는 학습된 필터들을 통해 이미지에서 유의미한 feature를 추출할 수 있다(General images). 이 네트워크를 보다 특성화된 <U>downstream task</U>인 classification에 적용하기 위해서 기존의 feature extractor를 가지고 파라미터 수정을 하게 된다. 처음부터 학습하는 것보다, 기존의 학습된 representation이 학습에 있어 안정적인 역할을 해줄 수 있다. 마지막으로는 support vector machine(SVM)을 사용, 기존 feature extractor를 그대로 사용하되 마지막 부분에 task-specified head를 사용하여 학습하는 것을 보여준다. 이처럼 tansfer learning 혹은 fine tuning을 하는 과정은 <U>기존 task와 새롭게 접근할 task의 관계</U>에 따라서 다른 전략으로서 정의된다. 물론 dataset의 크기도 중요한 척도가 된다.   
이렇게 기존에 학습된 딥러닝 네트워크를 활용할 때 기준점이 되는 네트워크를 <U>"pre-trained model"</U>이라 부른다. 한국말로는 '사전 학습된 네트워크'라 부른다. 사전 학습된 모델에 접근하는 법을 크게 3개의 step으로 표현하면 다음과 같다.
- Select source model : 가능한 네트워크 중에서 가장 잘 적용될 수 있는 구조를 고르는 작업. 현재 [paper with code](https://paperswithcode.com/)와 같은 플랫폼에서 다양한 연구 기관(Facebook AI, Google 등등)에서 출시한 pre-trained network나 pytorch, tensorflow에서 기본으로 제공하는 pre-trained model 등등 활용할 수 있는 network pool이 넓어졌다.
- Reuse model : 위에서 고른 pre-trained network를 학습에 전반적으로 학습하는 과정이다. 일종의 <U>'from scratch'</U>사전 학습된 네트워크를 '초기화'상태로 두고 새로운 task에 적용하는 것이다. 흔히 모델의 일부분을 사용하거나 전체를 사용하는 등 모델링 기법에 따라 학습 전략은 달라진다.
- Tune model : 경우에 따라 관심이 있는 새로운 task에 대해 input-output 관계로 네트워크를 tuning하는 것이다. 흔히 classifier 부분을 SVM으로 바꾸거나 낮은 learning rate로 학습하는 방법이 이 단계에 해당된다고 보면 된다.

결국 네트워크를 새로운 task에 학습시킬 때 '학습하고자' 하는 부분이나 방법에 대한 구분이 필요하다. 이에 대해 <U>각 레이어에 대한 gradient optimization 여부</U>를 다음과 같이 구분할 수 있다.

- Re-train : 기존에 학습된 weights를 모두 버린 뒤 새로운 weight로 업데이트하는 것
- Fine-tuning : 학습된 weight를 그대로 두고, 학습 데이터를 통해 파라미터 조정을 가하는 것
- Frozen : 학습된 weight를 그대로 유지하고, 학습 과정에서도 이를 그대로 보존하는 것

일반적으로 fine-tuning은 기존에 학습될 때 사용하는 <U>learning rate</U>의 $1/10$배 혹은 $1/100$배의 크기를 적용한다. Learning rate이 곧 prediction과 ground truth 사이에 발생한 오차에 대해 얼만큼 weight update를 진행할 지에 대한 척도가 되는데, 이 값을 줄임으로써 기존에 학습된 weight를 어느 정도는 유지하고자 하는 것이다.


# 데이터셋의 유사도와 크기에 따른 transfer learning

앞서 말했던 것과 같이 사전학습된 모델을 고르고, 어떤 레이어는 처음부터 다시 학습시키거나(retrain), weight를 낮은 learning rate로 살짝 조정하거나(fine-tuning) 혹은 weight를 그대로 사용할 수 있다(frozen). 이러한 전략은 사전 학습된 모델과 새로 학습할 task의 유사도(dataset similarity), 그리고 학습 시에 사용될 새 데이터셋의 규모(dataset size)에 따라 4가지로 구분될 수 있다.

## 데이터셋의 크기는 크지만, 기존 task와의 유사도는 떨어질 경우(Quadrant 1)
이럴 경우에는 기존의 task의 representation(학습된 weight가 implicit하게 가지고 있는 mapping 능력이라고 생각하면 된다)를 그대로 사용하는 것보다는 전체 네트워크를 학습하는 것이 성능 향상에 더 도움이 될 수 있다. 이유는 네트워크 parameter 전체를 전체적으로 다 학습할 정도로 충분한 양의 데이터셋이 있다는 가정 하에 <U>overfitting이 일어나지 않을 것</U>이고, 기존 task에 대해 최적화된 weight는 <U>유사도가 낮은 새로운 task</U>에서는 적용 가능성이 낮기 때문이다.

## 데이터셋의 크기도 크고, 기존 task와의 유사성도 충분할 경우(Quadrant 2)
사실 이럴 경우가 가장 좋긴한데, 여기서 딜레마가 발생한다. 사실상 데이터셋이 충분하다는 가정만 있다면 Quadrant 1과 같이 <U>처음부터 학습하더라도 큰 문제는 없다</U>. 그러나 기존에 학습된 parameter의 representation이 새로운 task에 대해서도 좋은 supervision을 제공할 수 있다면 초반에 local optimal에 빠지거나 optimizer가 무의미한 탐색을 하지 않고도 **빠른 속도로** 수렴 가능한 parameter를 찾을 수 있을 것이다. 따라서 이런 경우에는 주로 가장 최하층(feature extraction 뒤쪽과 classifier 부분)을 fine-tuning하거나 re-training하는 형태로 학습이 진행되며, 나머지 layer에 대해서는 학습된 parameter의 도움을 받기 위해 frozen(유지)하는 방법을 선택한다.

## 데이터셋의 크기도 작고, 기존 task와의 유사성도 떨어지는 경우(Quadrant 3)
4사분면 중 여기가 가장 까다롭다. 사실 해당 문제에 대해서는 정답이라고 명확하게 정의할 수 있는 방법은 없으며, 데이터셋이 적기 때문에 새로운 task에 대해 overfitting 없이 generalization하기 곤란하다. 보통 quadrant 2에서 다룬 것처럼 일부 레이어만 학습하는데, 여기서는 frozen하는 이유가 위의 경우와는 다르다.   
Quadrant 2에서는 학습된 weight을 통해 <U>최적화 속도를 높이거나</U> <U>성능을 향상시키기 위해</U> 일부 layer만 학습하게 되지만, Quadrant 3에서는 dataset에 specific한 네트워크를 만들기 위해서 모든 레이어를 학습하게 되면 비교적 <U>적은 양의 데이터셋에 대해</U> 네트워크가 학습될 수 밖에 없기 때문에 <U>overfitting의 위험</U>이 있다.

## 데이터셋의 크기는 작지만, 기존 task와의 유사성이 높은 경우(Quadrant 4)
이럴 경우에는 quadrant 3와 마찬가지로 학습했을 때의 <U>overfitting 문제</U>를 신경써야한다. 그렇기 때문에 보통 데이터셋의 크기가 작긴 하지만 사전 학습된 네트워크의 representation mapping이 novel task에 적용할 수 있다고 판단되는 경우(<U>similar task</U>) 굳이 classifier를 제외한 부분까지 학습시키지 않는다. 일반적으로 classifier 부분은 파라미터 수가 상대적으로 적거나 학습했을 때 task-specific하게 fine-tuning이 될 수 있으므로 적은 데이터셋으로 학습하기 용이하다.


# Knowledge distillation
그 다음으로 다룰 내용은 knowledge distillation이다. 기존에 학습된 네트워크를 활용하여 새로운 task에 적용하기 위한 방법으로 제시된 transfer learning과는 살짝 다르게, knowledge distillation은 단일 task에 대해 최적화되어 좋은 성능을 보이는 pre-trained model(<U>teacher network</U>)이, 보다 얕고 가벼운 light-weight network(<U>student network</U>)를 학습시키는데 보조적인 역할을 해줄 수 있다는 것이다. 지식을 전달한다는 의미의 knowledge distillation은 temperature($T$)라는 hyperparameter를 통해서 one-hot encoding에 비해 보다 합리적인 soft label을 만들어낼 수 있고(higher entropy), 이를 학습에 활용했을 때 hard label을 사용했을 때와 비교하여 '<U>가벼운 모델로 하여금 최적화 성능을 높일 수 있다</U>'는 접근법이다.

즉 똑똑한 네트워크(Teacher network)의 output을 따라가게끔 Student network가 학습하는 과정을 의미하며, 이 학습법의 경우 굳이 convolutional neural network 구조가 아니어도 모든 형태의 deep network에서 활용될 수 있다는 범용성이 있다. 실제로 [DeiT](https://arxiv.org/abs/2012.12877)와 같은 transformer based approach에서도 사용될 수 있는 방법이다. 해당 논문에서는 inductive bias에 대한 의존성을 가진 convolutional neural network에 비해 적은 데이터로는 추론 능력이 떨어지는 transformer 모델을 학습하는 과정에서 <U>CNN의 학습된 지식을 주는</U> 형태로 knowledge distillation을 사용하였다.   
최적화에 사용되는 loss는 다음과 같다. 학습하고자 하는 데이터셋과 hard label $(X,~Y)$과 temerature $T$에 대한 prediction student network $S(\\cdot, T)$ 그리고 teacher network $R(\\cdot, T)$에 대해서,

$$
    \\mathcal{L} = T^2 \\lambda \\mathcal{L_t} + (1-\\lambda) \\mathcal{L_s}
$$

위와 같이 나타낼 수 있고, 각각의 loss term(student loss, teacher loss는)

$$
    \\mathcal{L_s} = \\mathcal{L}_{CE}(Y, S(X, 1))
$$

$$
    \\mathcal{L_t} = \\mathcal{L}_{CE}(S(X, T),~R(X, T))    
$$

위와 같다. 여기서 temperature에 의한 prediction은 hard label을 soft label로 만들기 위한 과정으로, 다음과 같이 원래의 softmax 대신 temperature normalized softmax 값을 사용한다.
$$
    q_i = \\frac{exp(z_i/T)}{\\sum_j exp(z_i/T)}
$$

$z_i$가 네트워크의 prediction에 의한 logit값이 된다고 생각하면 된다.

## Why knowledge distillation??
그러면 대체 왜 knowledge distillation을 사용해야하는 것이 실제 네트워크 학습에 도움이 되는 것일까? 단순히 classification이라면, 원래의 hard label에 대해 빡세게 트레이닝하는 것이 결국 성능을 높일 수 있는 방법이지 않을까 싶다.   
Soft target의 효과를 설명하기 전에 우선 딥러닝에서 <U>왜 knowledge distillation 방법</U>이 제시가 되었는지 짚고 넘어가도록 하자. 딥러닝을 사용하며 모바일이나 임베디드 환경에서도 충분히 적용 가능한 최적화 네트워크를 구축하고 싶다.
즉, 다양한 edge device(핸드폰, 카메라, 게임기 등)에 딥러닝을 적용할 수 있기 위해서는 그만큼의 경량화 모델이 필요하고, 이는 이전에 다루었던 [MobileNet 관련 게시글](https://6unoyunr.github.io/blog/light)을 참고해보면 더 좋다.   
아무톤 이런 방식으로 네트워크를 경량화하고자 하는 다양한 방법들이 소개가 되었으며, 단순히 성능이 좋은 무거운 네트워크를 학습하는 방법으로는 좋은 성능을 내기가 힘들다는 문제에 부딪히게 된다.

## Attribute based soft label?
고양이 사진에서의 특징을 나열해보면, 스핑크스가 아니라면 털이 복실복실하고, 귀엽고, 위의 사진과 같은 경우에서는 '모자'를 쓰고 있으며, 눈망울이 크고 똘망똘망하다는 것이다. 이러한 세부적인 attribute는 대부분의 고양이 사진에서 어느 정도 찾아볼 수 있으며, 이러한 특성을 모두 배제하고 단순히 <U>'모 아니면 도'</U>의 학습을 하는 것은 파라미터 수가 적은 network로 하여금 일반화 성능을 떨어뜨릴 수 있다는 것이다. 그렇기 때문에 teacher network의 예측을 보다 smoothing하여, 잘 학습된 네트워크에서 특정 이미지를 보고 예측했던 <U>다른 class에 대한 logit</U>을 학습에 활용하겠다는 의미가 된다. 결국 cross entropy loss는 one-hot encoding으로 학습되기 때문에 다른 class의 예측값들은 무시되는게 맞는데, <U>이걸 활용할 수 없을까?</U>에 대한 해결책이 되는 것이다. 또다른 측면에서 바라보았을 때, soft target은 network의 prior knowledge를 대변하는 값이므로(사전 학습된 네트워크의 implicit한 함수의 결과값으로 생각할 수 있다) $S(X, T) \\rightarrow R(X, T)$

## Use pre-trained model
결론을 말하자면 transfer learning, knowledge distillation 모두 '<U>사전 학습된 네트워크</U>'를 사용하는 학습법이다. 두 개의 차이점은 transfer learning은 하나의 네트워크를 다른 task에 적용하는 방법에 대해서였고, knowledge distillation은 두 개의 네트워크(deep neural network and shallow network)를 사용해서 단일의 task에 대한 성능 향상 및 학습 보조에 대한 내용이다.


# Continual learning
잘 학습된 자율주행 모델이 있다고 생각해보자. 그러나 자율주행을 하기 위한 학습에 사용된 데이터셋에는 '갑자기 등장하는 고라니' 라던지 '갑자기 등장하는 캥거루'와 같은 데이터셋이 없었다고 가정해보자. 즉, 차량이 조심해야 하는 anormal situation에 <U>동물이 갑자기 등장하는 상황</U>이 포함되어 있지가 않다.   
그렇다면 차량 내부 시스템의 업데이트를 통해 해당 데이터셋에 대해 추가적으로 학습해야한다. 물론, 이런 내용에 대해서 기존에 학습된 네트워크에 단순히 데이터셋만 추가하면 된다고 생각할 수 있지만, 흔히 anormal한 상황은 학습된 네트워크를 기반으로 한다고 하더라도 <U>특별한 상황이기 때문</U>에 representation에 대해 <U>만족할 만한 성능이 나온다는 보장을 할 수 없을 뿐</U>더러, 실제 학습에 사용될 수 있는 데이터셋도 분명 한정적일 것이다.   
기존에 학습된 딥러닝에 대해서 새로운 task(갑자기 등장하는 동물에 대한 차량 통제)를 처리할 수 있게 하고는 싶은데, 원래의 성능(일반적인 상황에서의 주행)을 악화시키고 싶지는 않다. 바로 이러한 접근이 '<U>continual learning</U>'의 시작이다.   
만족스러운 continual learning이 이루어지기 위해서는 다음과 같은 요구사항이 뒷받침된다.

- 네트워크는 새로운 task에 대해 좋은 성능을 보여야한다.
- 네트워크는 기존 task에 대해서 성능을 유지해야한다.
- 네트워크는 기존 task의 데이터셋을 요구하지 않는다(memory efficiency).
- 학습 및 추론 과정에서의 time complexity(FLOPs).
- 네트워크의 storage complexity(학습 가능한 representation의 범위).

## Regularization based method
Continual learning의 다양한 방법론과 그 접근법에 대해 살펴보도록 하자. 위에서 언급했던 것처럼 '사전 학습된 representation'을 사용하는 transfer learning을 remind 해보자. 만약 <U>기존에 학습된 network의 parameter를 크게 바꾸지 않고</U> 새로운 task에 적용 가능한 학습법을 사용한다면, <U>기존 task의 성능을 어느 정도 유지</U>하면서도 <U>새로운 task에 대해 최적화</U>가 가능할지도 모른다. 바로 이러한 방법을 'Regularized based method'라 부른다. 보통 복잡한 모델은 적은 learning rate를 통해 정규화하면서 기존 task 성능을 유지한다는 측면에서 이런 이름이 지어졌으며, 대표적인 방법으로는 '<U>fine tuning</U>'이 있다.

## Rehearsal based method
이러한 방법도 있다. 기존 taks의 data에 대한 sample을 buffer에 기록하고, 지금 task를 학습시키기 위한 dataset과 동시에 활용하는 것이다. 이러한 방식으로 접근하게 되면 기존 task의 supervision을 지속적으로 제공하면서도 새로운 task에 대해 최적화가 가능하다는 장점이 있다. 대표적인 방법으로는 '<U>Learning without forgetting(LWF)</U>'이 있다.

## Architecture based model
다음 방법으로는 학습 구조에 대한 의존성을 주는 부분이다. 이미 학습된 task의 성능을 그대로 유지한 채로 다른 task를 학습하고자 하면, 단순히 네트워크를 확장시키거나 auxiliary head를 활용한 학습을 고려해볼 수 있다. Rehearsal based method와 거의 유사하지만 차이가 있다면 이 task의 경우에는 네트워크의 구조를 여러 task에 맞게끔 바꾸는 것이 주된 목적이기 때문에 기존 task의 dataset도 요구한다는 점이 될 수 있다. 대표적인 방법으로는 '<U>Multitask learning</U>'이 있다.


# Feature extraction
앞서 간단하게 continual learning의 여러 접근 방식에 대해 살펴보았다. 이러한 방법들의 가장 근간이 되는 baseline인 feature extraction은 매우 간단하다. 다만 시작하기 전에 notation을 정리하고 넘어가도록 하자.

- $\\theta_s$ : Shared parameters
- $\\theta_o$ : Task-specific weights for previously(old) trained tasks
- $\\theta_n$ : Randomly initialized task-specific parameters for new tasks.

Feature extraction은 shared parameters(보통 ocnvolutional layers는 모두 해당된다)를 기존 task 그대로 사용하되, 새로운 head를 붙여 new task에 대해 학습하는 것을 의미한다. 즉 앞서 소개했던 <U>transfer learning</U>의 용어로 보면, feature extractor인 convolutional network($\\theta_s$)는 frozen(고정)된 상태로, new task($\\theta_n$)를 최적화하는 구조가 된다. 물론 head가 서로 다르기 때문에 old task에 대한 framework($\\theta_s + \\theta_o$)에 영향을 주지 않기 때문에 old task에 대한 성능을 그대로 유지할 수 있다는 장점이 있으며, 단점은 feature extractor가 새로운 task에 대해 최적화될 수 없기 때문에 new task에 대한 성능 향상이 한정적이라는 것이다.


# Fine tuning
미세 조정이라는 의미대로, fine tuning은 작은 learning rate을 기반으로 기존 parameter를 미세하게 최적화하는 것을 의미한다. feature extraction보다 많은 parameter에 대해 최적화가 가능하기 때문에 new task의 성능을 더 끌어올릴 수 있다는 장점이 있으나, 만약 <U>새로운 task가 기존의 task와 상이한 경우</U> 기존 task의 성능이 급격하게 낮아질 수 있다는 문제가 있다. Feature extraction에서는 parameter $\\theta_s + \\theta_o$ 전체가 freeze된 상태였지만, fine tuning에서는 $\\theta_s(:k) + \\theta_o$(여기서 $k$는 fine tuning하고자 하는 convolutional layer의 첫 인덱스를 의미한다.) 부분만 fixed된 상태고, 나머지 weight 모두 영향을 받기 때문에 문제가 될 수 있다. 물론 fine-tuning에서 fully connected layer만 하게 되는 경우에는 기존의 feature extraction과 동일한 방법으로 취급된다.


# Multitask learning(MTL)
Multitask learning은 우선 <U>학습하고자 하는 모든 task의 데이터셋이 존재하는 경우</U> 사용할 수 있는 방법이다. 기존 방식과는 다르게 $\\theta_s,~\\theta_o$ 그리고 $\\theta_n$이 모두 jointly optimized된다. 일반적으로 feature extraction 부분은 공유가 되면서, head 부분만 각 task에 맞게 조정된다. 

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209148883-fea34e51-ea02-4398-8f77-d7edca5cb44d.png" width="700"/>
</div>

Multitask learning의 목적은 training과 관련된 여러 유사한 domain specific information들을 종합적으로 활용하여 single task generalization(일반화) 성능에 도움을 주기 위함이다. 예를 들어 자율주행에서 가장 주가 되는 task는 <U>운전 손잡이를 상황에 따라 어느 방향으로 틀어야 하는가</U>이고, 여기에 부수적으로 길가에 있는 표지판이나 신호, 교통상황에 따라 엑셀이나 브레이크를 밟는 것이 auxiliary task가 된다. 추가적으로 auxiliary task는 주가 되는 task와 함께 학습될 때 L2 regularization을 통해 weight가 상이하지 않도록 조절된다. 또한 단순히 single task의 성능을 높이는 것과 더불어 auxiliary task 및 main task를 포함한 모든 task의 성능을 동시에 올리는 구조가 된다.   
앞서 설명했던 것에 이어서 MTL이 가질 수 있는 네트워크 공유 구조에 대한 variation은 다음과 같다. 만약 여러 task에 대해 일반화를 진행하는 학습 과정이라면 '굳이' <U>feature extraction parameter</U>가 공유될 필요는 없다(아래 그림 참고).

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209148885-f393d7bc-6ed8-4b2e-ba04-eb7c0ecf3d56.png" width="400"/>
</div>

물론 위와 같이 모든 파라미터를 공유하는 형태의 학습이 존재하고, 이를 'hard parameter sharing'이라고 부른다. 단일 feature extraction network가 모든 task에 대해서 학습이 되기 때문에 overfitting의 위험성이 적고, task마다 parameter를 가져가는 것에 비해 파라미터 수가 $N$배 적기 때문에 그만큼의 overfitting 방지 효과를 획득할 수 있다. 이와는 다르게 <U>각 task마다 네트워크 parameter를 서로 다르게</U> 가져가는 형태도 존재한다. 다음 그림을 보면,

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209148887-86116f5c-abb7-4622-97c7-faa11cd63f06.png" width="400"/>
</div>

위와는 다르게 Task A, B, C가 서로 다른 네트워크를 통과하는 형태다. 그러나 여기서는 cross-talk라는 regularization term이 있게 되는데, 바로 앞서 설명했던 L2 regularization을 통해 weight를 서로 비슷하게 유지해준다는 것이 이 부분이다. 전반적으로 모든 task가 서로 다른 최적화 과정을 거치게 하되, <U>각 layer마다의 parameter가 유사하게끔</U> 학습되는 것이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209148890-4eb26ebe-5de5-404d-84d7-176a4866c407.png" width="800"/>
</div>

여기에 추가로, 서로 다른 task의 activation map이 교차되면서 학습되는 cross-stitch 방식도 존재한다. 논문 제목에서 알 수 있듯이 두 activation map이 서로 교차되어 input activation map과 곱해지게 된다.

$$
    \\begin{bmatrix}
        \\tilde{x_A}^{ij} \\newline
        \\tilde{x_B}^{ij}
    \\end{bmatrix} =
    \\begin{bmatrix}
        \\alpha_{AA} & \\alpha_{AB} \\newline
        \\alpha_{BA} & \\alpha_{BB}
    \\end{bmatrix}
    \\begin{bmatrix}
        x_A^{ij} \\newline x_B^{ij}
    \\end{bmatrix}
$$

이러한 MTL 방식은 모든 network와 dataset이 동시에 GPU에 올라가야 학습이 가능하기 때문에 <U>메모리 소모가 심하다</U>. 그러나 여러 task에 대해서 동시에 최적화가 가능하기 때문에 이를 통해 얻을 수 있는 이점도 있다.   
Multitask-learning에서 사용되는 loss는 모든 task loss를 합해서 사용하게 되는데, 이때 task 종류에 따라 쉬운 task의 경우 상대적으로 적은 gradient를 가지게 되므로 학습 불균형이 발생한다. 따라서 이를 적절하게 조절해주기 위해 weighted sum loss를 사용하게 된다.

$$
    L_{total} = \\sum_i w_i(t) L_i
$$

여기서 중간에 $t$ term은 epoch를 의미하고, 각 task마다 수렴 속도가 다르기 때문에 이를 동적으로 할당해주는 loss를 의미한다. 해당 loss의 specialized version이 단순한 weighted loss라고 보면 된다.


# Learning without forgetting(LWF)
만약 학습 과정에서 모든 데이터셋을 동시에 학습시키기 힘든 상황이라면 어떨까? 그러면서 동시에 old task에 대한 representation을 유지하면서 학습하고 싶다고 해보자. 이러한 관점에서 제시되는 방법이 바로 LWF(Learning without forgetting)이다. 해당 방법을 사용하게 되면 feature extraction 및 fine tuning보다 new task에 대한 적응력도 좋으면서, old task에 대한 성능도 유지할 수 있다. 또한 MTL에서는 <U>기존 데이터셋에 대한 최적화도 함께</U> 진행해야하므로 학습 속도가 느리다는 단점이 있지만 LWF에서는 그럴 필요가 없기 때문에 학습 속도도 향상된다. 무엇보다 학습 framework가 비교적 간단하다는 장점이 있다.   
학습 과정을 여러 단계로 분류해서 나타내면 다음과 같다.

- Pre-train network $F_{\\theta_s,\\theta_o}$ with old task. $\\theta_s$ means a parameter that can be trained in a shared network, and $\\theta_o$ means a parameter that can be trained in a classifier specialized in the old task.
- Generate soft label $y'$ based on pre-trained network $F_{\\hat{\\theta_s},\\hat{\\theta_o}}(x_n)$. Soft label can be generated as $y_o' = \\frac{(y_o^{(i)})^{1/T}}{\\sum_j (y_o^{(j)})^{1/T}}$ and $y_o^{(i)} = F_{\\hat{\\theta_s},\\hat{\\theta_o}}(x_o^{(i)})$.
- Freeze $\\hat{\\theta_s},~\\hat{\\theta_o}$ and train new task classifier $\\theta_n$ with new task supervision $(x^i_n,~y^i_n),~i=1,~\\cdots,~l$ and criterion of cross-entropy loss $CE(y_n, F_{\\theta_s,\\theta_n}(x_n))$.
- Jointly train $\\theta_s,~\\theta_o,~\\theta_n$ with low learning rate with soft label of old task supervision and new task supervision. The training criterion is $\\lambda_o\\mathcal{L_{old}}(y_o', \\hat{y_o'})+\\mathcal{L_{new}}(y_n, \\hat{y_n})+\\lambda_r \\mathcal{L}_2$.

과정을 전부 영어로 썼는데 간단하게 풀어서 말해보면, 우선 <U>old task</U>에 대해 사전 학습된 네트워크가 있다고 가정해보자. 학습된 네트워크를 통해 <U>new task</U> dataset $x_n$에 대한 soft label을 추출한다. 그렇다면 이 **soft label**은 old task에 대해 최적화된 네트워크로부터 나온 representation을 그대로 유지하고 있게 된다. 그 다음에 학습 과정에서는 이렇게 추출된 **soft label**과 prediction 사이에 대한 loss와, 실제 **ground truth**와 prediction 사이의 loss를 계산한 뒤에 이를 weighted summation하여 최적화한다. 여기에 L2 regularization을 통해 weight가 안정적으로 학습될 수 있도록 해준다.


# 각 접근법의 한계점
앞서 크게 총 세가지의 접근법을 언급하였다. 첫번째로, 정규화 방법으로 사용된 fine-tuning의 경우 새로운 task에 대한 최적화에 있어서는 좋은 효과를 보였지만, 기존의 task의 성능을 거의 잊어버리는 결과를 가져왔다. 특히 <U>복잡한 형태의 multitask</U>를 해결하기에는 불가능한 학습법이라고 할 수 있다.   
두번째로 언급했던 구조적인 접근을 했던 MTL(Multi-Task learning)은 네트워크가 학습되는 상황에서의 task identity를 요구하기 때문에, 실제로 class incremental(분류해야하는 class의 수가 늘어나는 경우)와 같은 상황이나 task-agnostic setting(task가 같이 학습되는 구조에서 상이하면 퍼포먼스가 저하됨)과 같은 부분에서 한정적일 수밖에 없는 것을 알 수 있다. 물론, 메모리 문제가 항상 이슈가 된다.   
마지막으로 LWF(Learning without forgetting) 방법으로 소개된 rehearsal based는 작은 buffer size에서는 사용될 수 없으며(new task에 대한 old representation의 labeling이 필요), 무엇보다 데이터셋에 보안 문제가 있다면 사용될 수 없다는 단점이 있다.


# Self-supervised learning

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209148892-e4e42f95-6a3e-4d91-aa9d-7f6022778d98.png" width="400"/>
</div>

전공 공부를 할 때를 떠올려보자. 만약 전자회로나 전자기학을 수강 중인데, 예제 문제에 풀이가 없다면 우리는 당장 ~~체그~~..랑 구글링을 시작한다. 그런데도 도저히 해당 문제에 대답을 찾을 수 없을 때도 있으며, <U>원하는 정답이 아닌 이상한 답변</U>만 가득 적혀있는 경우가 많은 적이 종종 있다(나만 그런거 아니지??). 

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209148894-517e71e9-097a-42e6-8322-984baa0c49a2.gif" width="400"/>
</div>
  
이런 상황에서 잘 알아차리고 '<U>정답이 잘못되었구나</U>'라는 사실을 잘 인지할 수 있거나 혹은 문제에 대한 정답을 우리가 직접 풀어서 알지 못한다면, 이상한 답변을 학습하거나 혹은 문제에 대한 답을 알지 못해서 시험을 ~~조질 것~~이다 ㅠㅠ.   
딥러닝도 마찬가지다. 우리가 지금까지 정의한 문제는 결국 input에 대해 원하는 output이 있는 경우에만 다루었던 것. 만약 다량의 이미지가 있고, 이 모든 이미지에 대해 사람이 하나씩 분류해서 라벨링을 해야 한다면, 이는 생각보다 힘든 작업이 될 것이다. 사람이 편하자고 인공지능을 만들었는데, 그걸 <U>학습시키기 위해 인간이 고생하는 아이러니한 상황</U>이 빚어지는 것.   

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209148898-61ba7380-dce1-4e4b-952f-0d9a9e4c3218.png" width="400"/>
</div>
  
다들 무신사에서 옷을 사본 적이 있는가? 무신사는 현존하는 온라인 편집숍 중 가히 대학생들의 성지 뿐만 아니라 남녀노소 많이 이용하는 플랫폼이라 할 수 있다. 무신사에서 물건을 사면 <U>후기</U>를 남길 수 있는데, 이게 또 전신이 나와야하고, 후기를 남길 때의 조건에 따라 적립금을 줄 수 없는 대상(미지급 대상)을 선정하게 된다. 이걸 직접 사람이 하나하나 하기에는 노가다성이 짙고, 그러자고 딥러닝 모델을 학습시키자니, 이걸 하나하나 라벨링하기엔 고생스럽다.
해당 내용에 대해 무신사 데이터솔루션 팀에서 [작성한 글](https://medium.com/musinsa-tech/imagelabeling-c351c0258a62)이 있다. 혹시 관심이 생긴다면 해당 글을 읽어보는걸 추천한다.   
사실 갑자기 무신사를 홍보하려고 했던 것은 아니고, 이런 식으로 현업에서도 딥러닝을 사용할 때 supervision 구하기가 힘든 경우가 많다는 것이다. 이러한 측면에서 제시되는 것이 사람의 간섭 없이도 modality에 대한 representation을 잘 학습할 수 있는 방법인 self-supervised learning이다. 그렇다면 결국 <U>label 없이도 이미지에서 feature를 잘 뽑아낼 수 있게 학습해야하는데</U>, 그러면 보통 가지고 있는 (image, label) 페어에서 label이 없이 image만으로도 학습을 해야한다는 의미가 된다. 이를 pre-text task라고 부르고, 이렇게 학습된 representation으로 여러 downstream task를 수행할 수 있게 된다.


# Examples of pre-text task

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209148900-5e05650c-123e-4919-b938-eb37f8c65ff4.png" width="600"/>
</div>

가장 간단한 방법은 이미지를 회전(augmentation) 시키고, 회전 각도를 예측하는 형태로 학습하는 것이다. 회전 각도를 인지하기 위해서는 이미지의 object를 파악하거나 feature를 활용해야하고, 그렇게 학습되다보면 임의의 이미지를 넣어도 representation이 잘 출력될 수 있지 않을까라는 컨셉이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209149817-38a45906-2cec-466d-a9f7-655abc422e7f.png" width="600"/>
</div>


그 다음으로는 퍼즐 맞추기와 같이 patch의 위치를 예측하는 task가 있다. 이미지 상에서 랜덤으로 하나의 patch를 고른다. 그런 뒤 첫번째 패치를 기준으로 주변에 $3 \\times 3$ 크기의 grid를 가정하여 나머지 8개의 patch 위치를 잡아낸다. Model은 8개의 주변 patch들의 위치를 예측하는 task를 해결하면서 representation 학습을 하게 되는 것이다.


# Contrastive learning이란?
Self-supervised learning 방법으로 제시되는 논문 중, contrastive 방식이 아닌(clustering) 경우도 있다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209149822-3f1154b4-aa20-4d06-81ce-00cd25252c2f.png" width="400"/>
</div>

이러한 방식의 경우 supervision을 얻는 대신, 데이터 간의 관계를 사용하여 task를 해결하려는 알고리즘이다. 그러나 이러한 방식에 있어서 한계가 존재하기 때문에, 지금부터 설명할 내용들은 모두 contrastive learning 방식을 활용한 self-supervised learning이라고 생각하면 될 것 같다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209150771-2ce9e043-d12e-4530-a7f7-6f2b89ae5ddb.gif" width="600"/>
</div>

<U>Contrastive learning</U>의 개념은, 간단하게 설명해서 비슷한 샘플들끼리는 모이게 하고 서로 다른 샘플들끼리는 멀어지게 하는 것이다. 위의 그림을 참고해보면 이해가 쉬울 것 같다. 서로 같은 class라면(positive pair) 가까워지게끔, 서로 다른 class라면(negative pair) 멀어지게끔 구성한다. 따라서 Contrastive learning에서는 positive, anchor 그리고 negative라는 개념이 정의된다. **'Anchor'**는 일종의 샘플 query를 의미하고, 예를 들어 새 이미지가 있다면 이에 대응하는 또다른 새 이미지는 **'Positive'**, 같은 class에 속하지 않는 비행기 이미지는 **'Negative'**로 간주한다. 여기서 언급되는 <U>positive pair</U>란 <U>(anchor, positive)</U>의 샘플 쌍을 의미하며, 반대로 <U>negative pair</U>는 <U>(anchor, negative)</U> 샘플 쌍을 의미한다.   
그런데 다들 알다시피 self-supervised learning에서는 각 샘플들의 <U>label을 알 수 없다</U>. 결국, contrastive image pair를 만들기 위해서는 비슷한지 혹은 상이한지에 대한 knowledge 및 supervision이 있어야하는데, 결국 여기서 또다시 모순이 생겨버리는 것이다. 이걸 해결하는 방법은 다음과 같다.   
만약 $N$개의 이미지가 있다면, $N$개의 서로 다른 이미지는 서로 다른 $N$가지의 클래스로 구분된다고 하자. 그렇게 되면 모든 샘플들에 대해서 triplets(positive, negative pair)를 구성할 수 있게 된다. 

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209151055-9d82eef7-c3cd-4921-bd8d-76bd4ec45fab.png" width="600"/>
</div>

Triplet의 예시.. 아무튼 모든 샘플에 대해 세 요소인 negative, positive, anchor를 구성할 수 있게 되는 것이다. 그렇다면 'negative pair'는 그냥 서로 다른 이미지를 갖다 붙이면 되는거고, 'positive pair'는 과연 어떻게 구성해야할까??


# Data augmentation pipeline
바로 여기서 사용할 수 있는 것이 <U>data augmentation</U>이다. 단순하게도, 한 이미지에 augmentation 변형을 적용한 이미지는 결국 원본 이미지와는 다르지만 같은 객체에 대한 정보를 담고 있기 때문에 encoding된 결과가 유사해야한다. 

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209151061-1fe464f6-8dbf-47f9-91cd-e06f5f0194c1.png" width="600"/>
</div>

유명한 논문 중 하나인 [SimCLR](https://arxiv.org/abs/2002.05709)에서 언급된 내용은 이와 같다. 중간에 stochastic한 augmentation module인 $A(x)$ 혹은 $T$가 있고, 서로 다르게 augmented된 이미지인 $x_i$ 그리고 $x_j$는 서로 <U>유사한 인코딩 값을 뽑게끔</U> 학습되어야한다. 다만 SimCLR에서 주장했던 것은 이러한 <U>object의 pose 및 alignment를 유사하게 만들게끔 학습하게 되면</U> representation 학습에 있어서 다양한 샘플을 얻을 수 없다는 문제가 발생하기에, 여기에 추가적으로 Linear layer(Representation extraction)을 통해 추출한 $z_i,~z_j$를 유사하게 만드는 학습법을 제안하였다. 따라서 위의 그림에 대해서 언급하자면 $h_i,~h_j$에 대한 유사도를 메트릭으로 삼게 되면 마치 앞단의 encoder가 spatial transformer network와 같이 동작해서 <U>downstream task</U>를 해결하는데 있어 일반화가 어려워진다는 것. 즉 뒤쪽에 붙은 representation extraction 부분이 일종의 affine 필터(STN) 역할을 해준다고 볼 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209150800-c9a469b8-da9a-47c8-9068-1f8fd5657ea4.gif" width="600"/>
</div>


[Contrastive predictive coding(CPC)](https://arxiv.org/abs/1807.03748) 논문에서 제시된 augmentation은 다음과 같다. 이미지에 color fitter, random grayscale 그리고 random flip과 같이 흔히 볼 수 있는 data augmentation 기법을 적용한 뒤에 image를 overlaying sub patches로 분할한다. 패치 중 하나를 anchor로 삼고, 나머지 패치 중에서 positive pair로 삼게 될 패치 하나와, 자기 자신과 다른 이미지에서 생성된 패치 중 하나를 negative pair로 삼는다. 


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209150790-bc14237d-2686-43e9-8059-8220bb554dd1.gif" width="600"/>
</div>


이외에 [AMDIM](https://arxiv.org/abs/1906.00910), SimCLR 그리고 Moco와 같이 대부분의 유명한 논문들에서 사용하는 augmentation은 위와 같다. 이미지에 jitter, flip과 같이 일반적으로 볼 수 있는 data augmentation을 적용하는 것까진 동일한데, 패치로 나누는 형태가 아니라 같은 이미지에 서로 다른 data augmentation을 적용한 쌍을 positive pair, 서로 다른 이미지에 대해 각각 data augmentation을 적용한 쌍을 negative pair로 삼는다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209150787-8d86b095-75f7-4d9f-859b-cabe5bea9e7b.gif" width="600"/>
</div>


앞서 설명했던 모든 pair에 대해서 이미지를 latent space로 mapping하고, 이렇게 추출된 feature map을 기반으로 contrastive loss를 최적화한다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209150796-de0b585e-b013-4cc5-a026-9501acf2499e.gif" width="600"/>
</div>


CPC의 경우 latent space에서의 future를 prediction한다고 표현된다. 이는 image patch를 일종의 <U>timeline</U>으로 삼아서, <U>top-left 부터 bottom-right 까지</U> 순서대로 구성되었다고 보고, 이를 auto-regressive하게 풀어냈기 때문이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209150793-5a78fcb9-09dd-4340-8f94-d962299ccf98.gif" width="600"/>
</div>


AMDIM에서는 두 개의 augmented sample를 생성하고, 각 이미지에 대해 같은 encoder를 통과시킨 feature map을 <u>서로 다른 feature level</U> 단에서 비교하게 된다. 따라서 spatial scale에 따라 비교가 이루어진다는 장점이 있다.


# Loss function with similarity measure
Latent space, feature map 등 다차원의 벡터에 대해 유사도를 구하는 법은 여러가지가 있지만, 다음과 같은 코사인 유사도를 사용하거나 혹은 내적(inner product)를 사용한다. Cosine similarity가 결국 normalized inner product와 같기 때문에 두 metric을 서로 동일하게 취급 가능하다.

$$  
    \\begin{aligned}
        \\cos \\theta =& \\frac{\\vec{a}\\cdot \\vec{b}}{\\parallel \\vec{a} \\parallel \\parallel \\vec{b} \\parallel} \\newline
        \\parallel \\vec{a} \\parallel =& \\sqrt{a_1^2 + a_2^2 + \\cdots + a_n^2} \\newline
        \\parallel \\vec{b} \\parallel =& \\sqrt{b_1^2 + b_2^2 + \\cdots + b_n^2}
    \\end{aligned}
$$

이렇게 구한 <U>유사도</U>는 특정 범위 내에서 <U>두 벡터가 서로 유사할 확률</U>과 같이 해석되므로, 여기에 그대로 negative log likelihood를 토대로 optimize할 수 있다.
바로 이렇게 유도한 식이 Negative contrastive estimation loss(NCE loss)가 된다.

$$
    NCE_{Loss} = -\\log \\left( \\frac{\\exp (sim(g(x), g(x^+)))}{\\exp (sim(g(x), g(x^+))) + \\sum_{k=1}^K \\exp (sim(g(x), g(x^-_k)))} \\right)
$$


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209151064-d11ceaa3-f7b8-4072-a88b-5f4aded89a2f.png" width="600"/>
</div>

이렇게 학습된 representation을 기반으로 위와 같이 downstream task(분류, 검출 등등)을 fine-tuning해서 사용하게 된다. 그런데 SimCLR에서는 위의 NCE loss가 아닌 NT-Xent(Normalized temperature-scaled cross entropy loss)를 사용하였다.

$$
    1_{i,j}= -\\log \\frac{\\exp (sim(z_i, z_j)/ \\tau )}{ \\sum^{2N}_{k=1} 1_{(k \\neq i)}\\exp (sim(z_i, z_j) / \\tau)} 
$$


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209151068-2835ed0a-8929-43de-b345-06f645b7e4e8.png" width="600"/>
</div>


참고로 SimCLR에서 언급했던 내용 중, random crop + random jitter이 self-supervised learning의 representation 학습에는 큰 도움이 되었으며, 앞서 언급했던 것과 같이 nonlinear projection을 통한 spatial transform 그리고 batch 내에 negative sample 수를 늘리거나 더 큰 네트워크를 사용하기, 혹은 더 긴 epoch 만큼 학습시키기와 같은 scaling up 방법이 성능 향상에 도움이 되었다고 한다.
`,CO=`---
title: "Low shot learning에 대하여"
category: "ai papers"
publishedAt: "2022-12-20"
thumbnail: "https://user-images.githubusercontent.com/79881119/209151534-534fbd4d-e614-4e2d-b9bc-e4219d464120.png"
---


일반적으로 네트워크를 학습시킬 때, 대량의 데이터를 통해 최적의 parameter를 찾는 과정을 생각하게 된다. 그러나 만약 inference에 사용될 데이터셋에 대한 학습 데이터가 없거나 부족하다면, 네트워크는 <U>적은 데이터로도</U> 충분히 최적화될 수 있어야한다. 여기서 출발한 개념이 바로 'Low-shot learning'이며, 여기서의 'shot'은 <U>네트워크에게 제공되는 학습 데이터셋을 의미</U>한다.

# Few shot learning
현재의 딥러닝 방법들은 few examples(적은 학습 데이터셋)을 기반으로 일반화가 불가능하다. 대부분 다량의 parameter를 업데이트하는 방법으로 충분히 <U>많은 training sample을 통한 정규화</U>에 초점이 맞춰져있으며, 특히 overfitting을 방지하기 위해 다양한 data augmentation과 같은 정규화 방식이 제안되고 있다. 그렇기 때문에 데이터가 없는 상황에서 특정 task에 대해 적용 가능한 최적화된 네트워크를 만드는 것은 어쩌면 불가능할 수 있다. 일반적인 few-shot learning(FSL)에는 다음과 같은 예시가 있다.

- Character generation : 캐릭터를 생성하는 작업의 경우, 해당 캐릭터에 대한 example이 많이 존재해야하지만, 저작권 문제나 이런 저런 이슈들로 인해 충분한 샘플을 확보하지 못할 수 있다.
- Advance Robotics
- Training sample을 얻기 힘든 task : drug discovery, FSL translation, cold-start item recommendation

Machine learning에서는 computer program은 특정 <U>task</U> $T$로부터 나오는 $E$라는 <U>experience</U>를 학습하고, 학습 결과로 나오는 <U>performance measure</U> $P$에 대한 성능 향상을 이루는 것이 주 목적이다. 예를 들어 **Image classification**이라는 <U>task</U>가 있다면, 각 클래스 별로 존재하는 **대용량의 labeled image**(클래스 별로 구분된 이미지)가 곧 computer program이 경험할 수 있는 <U>experience</U> $E$가 되고, 네트워크로 하여금 예측된 각 이미지의 class에 대한 정확도(accuracy)가 측정 메트릭, <U>performance</U>가 된다. Few-shot learning에서는 바로 여기서 말하는 experience $E$가 현저히 부족한 상황에서의 문제를 이야기하며, 이를 딥러닝에서 사용하는 용어로 표현하자면 task $T$에 대한 supervision이 limited되었다고 할 수 있다.   
FSL 방법은 주로 사용할 수 있는 <U>supervision dataset</U> $E$를 활용함과 동시에, 이미 가지고 있는 <U>prior knowledge</U>와 함께 결합하여 learning이 feasible하도록 유도하는 것이다. 예를 들어 character generation이라면 supervision은 <U>각 캐릭터에 대해 존재하는 적은 샘플들</U>을 의미하고, 같이 활용될 수 있는 prior knowledge로는 캐릭터를 생성함에 있어서 <U>각 부분이나 관계에 대한 생성법</U>이 될 수 있다. 또다른 예시로 drug toxicity discovery에 대해서는 <U>새로운 분자 구조</U>가 주어지는 환경에서, 이미 알고있는 <U>유사한 형태의 분자 구조</U>를 prior knowledge로 생각해볼 수 있다. 마지막으로 image classification의 경우에는 <U>각 클래스별 라벨링된 데이터셋이 부족한 환경</U>에서, <U>다른 classification task에 대해서 학습된 네트워크</U>가 prior knowledge로 사용될 수 있다.   
또한 이러한 few-shot learning에서의 특별한 케이스로, 학습 가능한 샘플의 수가 하나만 있는 one-shot learning, 그리고 task $T$에 대해서 참고할 만한 example이 아예 없는 zero-shot learning으로 구분될 수 있다. Zero-shot learning에서는 environment $E$가 다른 modality(attribute 혹은 word embedding 등)를 가지고 있어야 하고, 이를 통해 몇몇 supervised information을 transfer하여 inference가 가능하게끔 해야한다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209151534-534fbd4d-e614-4e2d-b9bc-e4219d464120.png" width="600"/>
</div>

일반적으로 사람은 자신이 알고 있던 <U>배경 지식을 토대로 추정</U>하고(고양이를 살면서 한 번도 본적이 없는 사람이 고양이를 보고 강아지라고 한다), 만약 이렇게 추정된 내용이 잘못되었다고 하면(친구가 그건 강아지가 아니라 고양이라고 알려줌) <U>즉각적으로</U> 해당 object에 대한 지식을 얻게 되고(사실 강아지가 아니라 고양이었다는 사실을 알게됨), 다음 번에 다시 해당 object를 보게 되면(길고양이를 다시 마주함) 그때는 <U>잘못된 추론이 아닌 제대로 된 정답을 낼 수 있다</U>. 이를 딥러닝의 일련의 과정으로 나타내면 error를 통한 loss 발생이 한번에 해당 task에 대한 optimization으로 이어져서 한 번의 경험(데이터셋)으로도 일반화가 가능하다. 물론 이는 사람의 경우이고, 딥러닝에서는 단순히 이미지 하나에 대해서만 최적화를 하는 것은 optimal solution이 될 수 없다. 결국 이전에 우리가 진행했던 supervised learning에서의 학습법인 'learn from scratch' 방식을 사용할 수 없다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209151538-3c1ee16f-8ef6-4f65-a6ab-68cf0dc8a732.png" width="600"/>
</div>

따라서 위와 같이 'support set'이라는 학습 가능한 하나의 에피소드를 구성하게 된다. 여기서 $N$-classes $K$-shots라 표현된 부분은, 학습에 사용될 support set의 클래스 개수가 $N$이고 각 클래스 별로 존재하는 샘플의 수가 $K$라는 것이다. 그리고 Query set은 이렇게 최적화된 네트워크를 통해 실제로 추론을 진행할 샘플이라고 보면 된다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209151541-e5100481-f82d-4bc8-92c5-416fb4211a98.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/209151549-6aa5e461-d58a-4008-80fb-15c8f21a7811.png" width="400"/>
</div>


일반적인 supervised learning은 image classification에서 첫번째 이미지와 같이, <U>모든 클래스에 대해</U> $N$개의 샘플이 있다면 그 중에서 $K$개를 training, 나머지 $N-K$개를 testing에 사용한다. 그러나 few-shot learning에서는 이와는 약간 다르게 <U>class중의 일부</U>(위의 예시에서는 5개)를 pre-training 및 training dataset에 사용하고, 나머지 class 5개에 대해서 <U>support set</U>(사전 학습된 데이터셋과 겹치지 않는 class)를 구성하게 되고, task에 맞게 적은 샘플들로만(ex. $3$ samples) 구성한다. 그리고 학습될 때 사용되지 않는 데이터(ex. $N-3$ samples)는 <U>query set</U>으로 이후 few-shot training의 성능 평가에 사용된다. Few-shot learning의 접근법에는 다음과 같은 방식들이 있다.

- Transfer learning
- Data augmentation/Transformation/Synthesis
- Meta Learning(learning to learn)
- Metric based approach(Embedding Learning)
- Multi-task Learning
- Generative learning

여기서 transfer learning, multi-task learning과 관련된 부분은 앞서 게시글 중 [다양한 딥러닝 학습법](https://6unoyunr.github.io/blog/transfer)에서 다룬 내용과 같다. 이 중에서 이번 글에서는 '<U>학습 전략을 위한 학습</U>'인 meta learning 그리고 '<U>metric 기반 추론법</U>'인 embedding learning에 대해 살펴볼 것이다. 그리고 여력이 된다면 마지막 부분인 생성 모델을 활용한 방법에 대해서도 간략하게 정리를 해보도록 하겠다.


# Utilizer prior knowledge for FSL(Few-Shot Learning)
다른 게시글에서도 언급했던 내용인데, transfer learning은 각 클래스 별로 적은 샘플만 가지고는 잘 작동하지 않는다. 그리고 fine tuning을 마지막 레이어에 대해서만 진행한다고 해도 학습에 사용되는 sample 개수가 너무 부족하기 때문에 <U>overfitting</U>의 위험성이 높다. 따라서 다른 유사한 problem들로부터 experience를 획득하고, 이렇게 얻은 prior knowledge를 활용하는 방식을 채택하게 된다. 이러한 prior-knowledge를 각 domain에서 얻는 방법으로는,

1. Dataset으로부터 얻는 방법
2. Similarity로부터 얻는 방법
3. Learning으로부터 얻는 방법

크게 세 방법으로 구분될 수 있다.

## Prior knowledge about data

Dataset으로부터 prior knowledge를 얻는 방법은 비교적 심플하며, 정말 말 그대로 support set의 augmented dataset을 통해 진행된다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209151550-f1a2b664-68b4-4bb0-9712-c9060c9c710c.png" width="600"/>
</div>

예를 들어 학습에 사용될 support set인 $D_{train}$이 있다고 해보자. 여기에서 추출할 수 있는 sample과 label pair에 해당되는 $(x_i,~y_i)$에 대해, 학습된 transformation function $t$를 적용해서 augmented support set $(t(x_i),~y_i)$를 ouput으로 얻을 수 있다. 이에 추가적으로 labeling이 부적합하거나 완료되지 못한 샘플 $(\\bar{x},~-)$에 대해 $D_{train}$을 기반으로 라벨링을 진행한다. 즉, 예측값을 토대로 $(\\bar{x},~t(\\bar{x}))$와 같이 labeling을 하는 것이다. 또한 similar data sets로부터의 샘플 $(\\hat{x}_j, \\hat{y}_j)$를 aggregator $t$를 통해 조합한 output $(t( \\hat{x}_j ),~t( \\hat{y}_j))$를 도출하게 된다. 여기서는 mixup을 사용한다. 이처럼 transformer $t$가 prior knowledge로 사용되어 각 dataset category에 따른 output을 만들어낸다.

## Prior knowledge about learning

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209151554-3947069e-53d8-4172-9200-7ab0deff1d0b.png" width="600"/>
</div>

Transfer learning은 사실 이 카테고리에 속하는 내용이다. 그러나 보다 좋은 방법이 바로 <U>Meta learning</U>이라는 방법이고, 학습 전략을 위한 학습법이다. 만약 task $T$를 해결하고 싶다면, meta-learning 알고리즘은 training task $T_i$의 여러 배치에 대해서 학습이 되며, 이때 중요한 것은 모든 $T_i$에 대해
$$
        T_i \\cap T = \\emptyset
$$
임이 보장되어야 한다. 결국 이러한 다양한 task인 $T_i$에 대해서 학습되면서 정말 풀고자 하는 task $T$를 잘 학습하고자 하는 것이다. 쉽게 비유하자면 수능을 잘 풀기 위해서 모의고사를 많이 푸는 것이다. Training data에 대한 multiple task가 학습이 되고, 이러한 multiple task의 결과로 learning algorithm을 meta-learning한다. 이렇게 최적화된 learning algorithm을 사용해서 network를 학습시키고, 이렇게 <U>최적의 방법으로 최적화된</U> model을 사용하여 실제 task에 대한 성능을 확인한다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209151560-490a34d1-b32b-426f-a561-d5286a28f7ea.png" width="600"/>
</div>

각 base set은 여러가지의 episode로 구성되고, 각각의 episode는 (support set, query set)으로 이루어져있다. 앞서 언급한 learning algorithm을 meta-learning하는 부분이 바로 이 base set을 사용하는 과정이다. 학습법에 대한 최적화가 끝나게 되면, 해당 방법을 통해 support set으로 네트워크를 학습한다. 이때는 실제 task에 대한 $N$-ways $K$-shots support set을 학습에 사용한 뒤 실제 task의 query set에 대한 예측 성능을 확인하게 된다. 모델은 여러 episode가 포함된 batch를 해결하면서 파라미터를 업데이트하고, 이러한 방법을 통해 새롭게 unseen few-shot classification task가 나왔을 때도 안정적이고 효과적으로 학습할 수 있는 학습법을 찾게 된다.   
   
[MAML(Model-Agnostic Meta-Learning)](https://arxiv.org/pdf/1703.03400.pdf)이 이러한 learning 알고리즘 중 하나로 소개되는데, meta learning을 활용하면서도 동시에 neural network를 <U>두 가지의 backpropagation을 진행</U>하면서 최적화한다. 가장 메인이 되는 컨셉은, neural network를 학습하는 과정에서 빠르고 적은 샘플로도 새로운 classification task에 적응 가능한 parameter를 가지게끔 학습하는 구조를 잡는 것이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209151563-ca25dde5-5a2a-485f-9cd9-be11937bd357.png" width="400"/>
</div>

따라서 MAML은 총 두 개의 neural network 구조를 가지고, 두 네트워크는 서로 같은 구조를 공유한다. 즉 하나의 네트워크를 통해 두가지의 역할을 수행하는데, 그 중 하나는 learner(학습자)로서 앞서 언급했던 것과 같이 새로운 task에 대해 잘 학습할 수 있는 학습법(meta-learning)을 학습하고, 나머지는 adapter(적응자)로서 각 task에 빠르게 적응할 수 있는 알고리즘을 학습한다. 서로 다른 task를 해결하는 과정이기 때문에, 각자의 task에 맞는 learning rate이 사용된다. 구체적인 학습 알고리즘을 각 step 별로 나타내면 아래와 같다.

- Learner(학습자)를 랜덤하게 초기화한다.
- meta-training에 사용되는 모든 episode에 대해 아래와 같은 과정을 반복한다. 반복은 특정 epoch 수만큼 진행되거나, meta-parameter가 안정화에 접어들 때까지 진행한다.   

    - Meta-training에 사용될 episode의 batch를 샘플링한다.
    - Adapter의 parameter를 learner의 parameter로 초기화한다.
    - Inner training step이 정해진 횟수만큼 진행될 때까지, adapter를 batch의 support set에 대해서 학습시키고 loss 및 gradient based optimization을 진행한다.
    - 최적화된 adapter의 parameter를 사용하여 batch의 query set에 기반한 meta-loss를 계산한다.

- meta-gradient를 계산한다. 위에서 batch의 query set에 기반한 meta-loss를 계산하였는데, 이를 토대로 meta-parameter를 최적화하고, learner의 parameter를 update한다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209151566-dea16fbd-3baa-4beb-8154-83aea72e01fe.png" width="600"/>
</div>


각 task에 대해서 구체적인 과정을 살펴보면 다음과 같다. 먼저 meta-learning에 있어 <U>학습된 learner</U> $M$을 <U>복사한 adapter</U> $f$를 만들고, $f$의 parameter를 learner의 parameter인 $\\Theta$로 초기화한다. 그런 뒤 meta-training support set으로 adapter $f$를 빠르게 fine-tuning한다. 이렇게 특정 task의 support set에 대해 최적화가 끝난 adapter $f$에 query set을 적용하고, 이 과정에서 추출된 meta-learning loss를 사용하여 $\\Theta$와 함께 meta-learning parameter를 최적화한다. <U>Meta-training</U> 과정에서는 MAML은 <U>initialization parameter</U>를 학습하게 되며, 각 network(adapter)가 새로운 few-shot task에 대해 빠르게 학습 및 최적화될 수 있도록 학습된다. 그리고 <U>Inference</U>에는, meta-trained model을 사용하여 meta-test set를 예측하고, 여기서 중요한 점은 query set에 대한 **추가적인 gradient는 있지만** 직접적으로 learner의 parameter는 이를 통해 **바뀌지 않는다**. 즉 학습된 초기화 지점은 고정으로, adapter 및 meta learning 과정만 학습하는 것이다. MAML 방식은 어떠한 neural network에서도 활용될 수 있기 때문에 model agnostic하다고 할 수 있다.

## Prior knowledge of similarity

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209151570-ff472ca7-4067-4427-9f6e-752e1864e665.png" width="600"/>
</div>

해당 내용에 대한 알고리즘은 compact representation(일종의 embedding으로의 mapping)을 학습하고자 하는 것이고, 여기서 data vector는 intra-class variation에 대해 크게 영향을 받지 않으며 각 class 간의 관계를 잘 유지할 수 있는 space를 구성하게끔 만드는 것이 주된 목적이다. 이를 다른 말로 <U>metric(embedding) learning approach</U>라 부른다. 모델링에 있어, similarity function 혹은 metric을 각 샘플에 대해 적용하여 similar sample의 경우는 closer, 상이한 sample은 take apart하는 형태로 학습한다. 이전에 다뤘던 contrastive 개념과 유사하다.
대표적인 방법으로는 [Siamese](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf) Network가 있다. 이외에도 Matching network, Prototype network 그리고 Relation network 등등이 있다. 간단하게 위의 그림을 참고해서 <U>support set</U>에 '3-way and 3-shots' sample이 있다고 가정해보자. 총 9개의 샘플에 대해 encoder $E$를 통과시킨 결과 embedding($g$)를 획득할 수 있다. 이렇게 획득한 총 9개의 임베딩을 실제 mappind된 embedding space에서 보게 되면, 서로 같은 class에 속하는 샘플 3개씩은 모여있어야 하고, 그와 동시에 서로 다른 class는 일정 거리만큼 떨어져 있는 것이 이상적이다. 이런 식으로 학습된 네트워크에 query set image를 encoder $E$에 통과시킨 결과 embedding($f$)를 얻을 수 있고, 이렇게 매핑된 query embedding이 가장 가까운 거리(nearest neighbour)를 보이는 class가 바로 이 query image가 예측되는 class가 되는 방식이다.   
Siamese network는 두 개의 쌍둥이 형태의 sister network를 구성하고, 두 네트워크는 같은 weight를 공유한다. 해당 네트워크를 최적화하는 과정에서 다음과 같은 contrastive loss function을 계산한다.

$$
    (1-Y)\\frac{1}{2}(D_W)^2 + (Y)\\frac{1}{2} (\\max (0, m-D_W))^2    
$$
여기서 $Y = (0, 1)$은 서로 다른 클래스인지 여부에 대한 **T/F value**가 된다. 만약 두 개의 input이 같은 class라면 $0$을, 다른 class라면 $1$이 된다. 일종의 <U>클래스 종류에 대한 exclusive 'OR'</U>이라고 생각하면 된다. Margin m은 0보다 큰 값으로, Euclidean distance metric $D_W$가 특정 값 이상인 두 샘플에 대해서는 고려하지 않는다. 학습은 similar pair $((x_i,~x_j), 0)$와 dissimilar pair ((x_i,~x_k), 1)을 네트워크에 통과시킨다. Similar pair와 Disimilar pair가 서로 같은 비율일 때가 학습이 가장 안정적이다. 이렇게 획득된 pair에 대해서 loss를 계산하고, 이에 weight를 최적화하는 과정을 거친다. 클래스별 Shot 개수 $E$, class의 개수 $C$에 대해 similar sample과 different sample의 개수를 수식으로 표현하면 다음과 같다.

$$
        N_{same} = \\left(
            \\begin{matrix}
                E \\newline
                2
            \\end{matrix}
        \\right)C 
$$

$$
        N_{diff} = \\left(
            \\begin{matrix}
                EC \\newline
                2
            \\end{matrix}
        \\right)-
        \\left(
            \\begin{matrix}
                E \\newline
                2
            \\end{matrix}
        \\right)C 
$$

다음은 matching network이다. 앞서 언급했던 내용과 유사하게 support set은 $(x_i,~y_i)$가 있고, 각각의 $y$는 class별로 표현된 one-hot label vector가 된다. 그리고 $f$, $g$는 embedding function이라고 하자. Classifier는 weighting factor $a$(softmax)와 query sample $\\hat{x}$에 대해서,

$$
    \\hat{y} = \\sum_{i=1}^k a(\\hat{x}, x_i)y_i = P(\\hat{y} \\vert \\hat{x},~S)    
$$

이처럼 나타내어진다. 여기서 weighting factor를 적용하여 support set에 대해서 query sample의 attention을 계산하는 것이다. Attention kernel은,

$$
    a(\\hat{x},~x_i) = e^{c(f(\\hat{x},~g(x_i)))} / \\sum_{j=1}^k e^{c(f(\\hat{x},~g(x_j)))}    
$$
이처럼 표현되며 이는 contrastive loss에서 표현되는 similarity term과 동일하다. 위의 수식에서 $c$는 cosine similarity로, 두 샘플이 유사한 확률로 간주할 수 있다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209151572-c066ff1a-9b6a-4390-8993-8053fa56669d.png" width="600"/>
</div>

이러한 방식을 통해 계산한 term이 곧 similarity에 기반한 nearest neighbor가 된다. 앞서 언급한 Siamese network에서의 $D_W$는 Euclidean distance였지만 matching network에서는 cosine distance(similarity)라는 점이 차이가 될 수 있겠다. Matching network는 결국 support set $S$에 기반한 query $x$의 output class $y$를 예측하는 classifier network $P$의 log likelihood를 최대화하는 방향이 된다.

$$
    \\theta = \\arg \\max_\\theta \\mathbb{E}_{L \\sim T}\\left(\\mathbb{E}_{S \\sim L,~B \\sim L} \\left(\\sum_{(x,~y) \\in B} \\log P_\\theta (y \\vert x,~S) \\right) \\right)    
$$


# Zero-shot learning
앞서 소개했던 방법들은 샘플의 수가 <U>최소한 1개 이상일 경우</U> 사용할 수 있는 방법이고, 다음에 볼 내용은 zero-shot learning에 대한 부분이다. 

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209151573-accfd4ee-7dcc-469c-9620-6482c65e020e.png" width="600"/>
</div>

사람의 경우, 살면서 얼룩말은 한번도 본적이 없다고 하더라도 말에 대한 지식을 알고 있으며, 얼룩말은 단순히 말과 비슷하게 생겼으나 무늬가 stripe로 구성된 형태라는 말만 듣고서도 충분히 얼룩말을 구분해낼 수 있다. ZSL(Zero-shot learning)은 이렇게 이미 알고 있는(seen) class에 대한 존재를 기반으로 이에 의존하는 inference를 제시하며, 이렇게 사전 학습된 knowledge 혹은 attribute를 통해 어떤 식으로 unseen class에 대응할 수 있는지에 대한 task가 된다. <U>Attribute</U>는 일종의 **category vector**로 표현된다.   
ZSL에서 데이터셋은 다음과 같은 구성을 가진다.

- Seen classes : 학습 과정에서 labeling된 이미지
- Unseen classes : 학습 과정에서 볼 수 없었던 class를 가진 이미지
- Auxiliary information : 각 클래스에 대한 묘사/semantic attributes 혹은 word embedding을 의미. Seen class와 Unseen class 사이에 유의미한 관계를 이어줄 다리 역할을 하게 된다.

## Semantic vectors

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209151577-537bd738-3dc3-4c9c-ac03-58b43ca0016c.png" width="200"/>
</div>

그렇다면 Semantic vector는 어떠한 방식으로 구할까? 이를 테면 위의 그림은 Attribute vector의 한 예시이다. 말 그대로 visual appearance를 각 property에 대해 매핑하는 과정이다. 이렇게 되면 '고양이'라는 class를 한번도 본 적이 없더라도 '꼬리가 있고, 털이 있다' 등의 attribute로 예측이 가능하다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209151580-798e115f-6d65-44b5-b4a9-b24e7dc9bb2c.png" width="500"/>
</div>

또한 다른 방법으로는 word vector가 있다. NLP 기술을 활용해서 attribute에 대한 정보를 자동적으로 학습하는 것이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209151584-c4f48fe5-2164-4fdf-a3be-3bd5649d00f3.png" width="500"/>
</div>

이렇듯 image feature를 실제로 semantic feature에 대입하기 위해서는 deep learning network를 활용해야한다. 이렇게 image feature와 semantic attribute가 같이 공존하는 embedding 공간을 <U>common embedding space</U>라 부르게 된다. 일종의 만남의 광장같은 느낌이다. Common embedding space는 high level vision의 visual space가 될 수도 있고, low level vision의 semantic space가 될 수도 있으며, 혹은 새롭게 학습된 중간 space가 될 수도 있다. 학습은 간단하게도 semantic vectors $v$를 seen data $x$에 대한 supervision으로 삼아서 projection function $f$를 $v = f(x)$와 같이 학습하게 된다. Inference를 진행할 때는 새로운 class dataset $(x^\\ast,~y^\\ast)$에 대해 class specified semantic vector $v^\\ast$를 정의하고, semantic vector space에 mapping된 $f(x^\\ast)$를 기준으로 가까운 neighborhood로 예측을 하게 된다. Attribute에 대해서 제대로 학습이 된 embedding space라면 새로운 class에 대한 attribute도 합리적으로 mapping할 수 있기 때문에, 해당 space에서 구분하면 학습에 사용되지 않았던 class에 대해서도 discrimination이 가능하다.   
Task의 종류에 따라 zero-shot learning 그리고 generalized zero-shot learning으로 구분할 수 있는데, 학습에서 사용된 class에 대해서도 함께 구별하는 task가 generalized zero-shot learning이며 그와는 다르게 test에서만 사용되는 new class에 대해서 discrimination하는 zero-shot learning이 있다.   


# Generative model based methods
그러나 embedding method에 대해서 발생하는 단점이 있는데, 바로 <U>Bias</U>랑 <U>Domain shift</U>이다. Mapping 함수 $f(\\cdot)$는 seen class에 대해서만 학습을 하기 때문에 output을 추출하는데 있어 기존에 학습될 때 본 sample들에 대해서 <U>biasing</U>되는 문제가 생긴다. 그리고 test modality에 대한 domain shift에 의해, $f(\\cdot)$가 unseen class image feature를 semantic vector로 제대로 mapping한다는 보장이 없기 때문에 단점이 존재한다. 풀어서 설명했지만 간단하게 말하자면 test sample에 대해서도 **semantic vector를 잘 뽑아내어야** 위의 방법이 의미가 있는데, 결국 zero-shot이라서 unseen class에 대한 보장이 안되기 때문에 곤란하다는 뜻. 따라서 이런 단점을 보완하고자, zero-shot classifier가 <U>seen/unseen class 모두에 대해서</U> 학습 과정에서 guidance를 받을 수 있는 방법이 필요하다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209151588-e31ecf78-f3a0-4d16-923a-aeac02e3fc01.png" width="500"/>
</div>

그래서 고려를 한 방법이 generative network에 고양이 이미지에 대한 'attribute vector'를 conditional GAN 형태로 넣어주며, fake image가 이러한 out-of-distribution sample의 역할을 대신할 수 있게끔 하는 것이다. 이를 통해 GAN이 unseen class에 대한 semantic feature를 보완해줄 수 있는 효과를 기대하는 것이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209151590-e5754607-9dbe-4676-8fd7-2d72b0bb6c30.png" width="600"/>
</div>

Conditional GAN이 학습되고 나서는 generator의 weight를 고정한 채 unseen class의 attribute를 input으로 전달하여 image feature를 뽑게 한다(이미지 자체를 넣는게 아니라 cGAN이 생성한 feature를 예측에 사용하는 것). 그렇게 되면 GAN이 생성하는 이미지의 퀄리티는 어떨진 모르겠지만, image를 직접적으로 사용하는 방법과 다르게 class agnostic한 학습에 사용될 수 있다는 점이 concept으로 사용되었다고 볼 수 있다.
`,AO=`---
title: "Transformer와 Multimodal에 대하여"
category: "ai papers"
publishedAt: "2022-12-21"
thumbnail: "https://user-images.githubusercontent.com/79881119/209162417-87272d02-5e7d-49fd-bec4-55df5e350680.png"
---


# Convolutional neural network의 발전
기존의 딥러닝에서 사용되던 대부분의 네트워크 구조는 <U>Multilayer perceptron</U>(MLP) 혹은 <U>Convolutional neural network</U>(CNN)이 주를 이루고 있었다. 단순히 modality를 $1 \\times N$ 차원 벡터로 늘려서 연산하는 MLP와는 다르게 Convolutional neural network 구조는 image와 같은 modality에서 성능을 입증받았고, 다양한 연구들이 그 뒤를 이었다. Convolutional neural network가 MLP에 비해 가지는 장점은 많았다. 우선 첫번째로 MLP보다 동일한 hidden layer 개수를 가지는 deep neural network를 구성함에 있어 적은 parameter를 학습시켜도 된다는 점과, 학습 가능한 parameter 수는 더 적으면서도 학습 가능한 representation이 <U>modality에 대해</U> 일반화가 잘된다는 점이다.   
이러한 퍼포먼스 덕분에 NLP(Natural language processing)이나 Audio 관련 딥러닝에서도 CNN 구조를 통해 연구를 시작했으며, 이는 'Inductive bias' 덕분이라고 언급할 수 있다. <U>Inductive bias</U>란, 학습하는 모델이 관측하지 못할 모든 modality에 대해서 추정하기 위해 학습이나 추론 과정에서 주어질 수 있는 모든 '가정'의 집합이다. 이게 무슨 의미냐면 예를 들어 딥러닝 네트워크가 '왼쪽을 보는'라는 고양이에 대한 데이터셋이 없이 고양이에 대한 classification을 학습했음에도, 추론 과정에서 '왼쪽을 보는' 고양이 이미지가 주어졌을 때 해당 객체가 고양이라는 사실을 인지하도록 하기 위한 일종의 constraint라고 보면 된다. 물론 이 예시는 사실 약간 부적절한 내용이지만 **inductive bias**라는 용어가 transformer라는 모델에 대한 개념을 파악하기에 필수적이기 때문에 이를 먼저 간단하게 이해하고 넘어가고 싶었다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152403-2d55f1f9-fb7b-4440-8cd4-e97db1f267a0.png" width="600"/>
</div>

위의 그림을 보면 간단하게 inductive bias에 대해서 설명할 수 있다. Convolution 연산이 진행되는 과정을 보면 필터가 특정 영역(ex. $3 \\times 3$)을 기준으로 필터의 paramter와 feature 값을 모두 multiplication한 뒤 더하는 구조가 된다. 따라서 다음과 같은 두 가지의 가정을 해볼 수 있다.

- 이미지에 속한 object의 경우, 해당 object를 나타내는 픽셀의 상대적 위치는 보장된다(localization).
- 이미지에 속한 object가 움직일 경우, 해당 object에 대한 feature output 또한 이미지 상에서 움직인다(translation equivariance).

첫번째 조건의 경우엔 큰 문제가 없으나, 두번째 조건은 큰 문제가 생긴다. 왜냐하면 translation equivariance는 MLP에서도 처리하지 못했던 문제였고, 만약 같은 object의 <U>위치만 달라짐</U>으로써 얻어지는 <U>feature map의 형태에 변화</U>가 생긴다면, 같은 object에 대해 동일한 예측을 취할 수 있다는 게 보장되지 않기 때문이다. 물론 MLP와는 다르게 CNN에서는 물체의 형태가 달라지지는 않고, 단순히 그 상대적인 위치는 유지된 채로 feature map 상에서의 전반적인 localization만 바뀐다는 것이다.   
그러나 CNN의 구조 상에서 max-pooling과 같이 같은 kernel 내의 feature value를 단일한 값으로 축약하는 filtering module, 그리고 softmax를 통한 확률값 계산을 통해 localization이 보장된 feature map에 대해 동일한 예측값을 낼 수 있다는 사실이 확인되었다. 즉 <U>feature extraction</U> 부분은 translation equivariance와 localization을 기반으로 object에 대해 유의미한 feature 형태를 유지하는 과정이 되며, <U>classifier</U> 부분은 translation invariance를 통해 이렇게 추출된 feature map을 기반으로 일관성 있는 prediction을 하게된다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152405-a2d45124-f1fc-447e-a1e0-9c37e14250fb.png" width="600"/>
</div>


바로 이러한 장점 덕분에 convolutional neural network는 더 적은 parameter 수를 가지고도 같은 class에 대한 feature representation 학습에 유리했고, ImageNet에서의 첫 성공 이후 수많은 연구가 진행되었던 것이다. 무엇보다 sequential data를 처리해야하는 NLP, Audio, Video 등등 temporal information을 추출하는데에도 CNN 구조를 많이 사용하기 시작했다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152408-f033371c-33f4-4970-aa42-6faddd9f7386.png" width="600"/>
</div>


그러면서 자연스럽게 sequential data를 처리하기 위한 <U>RNN</U>(Recurrent neural network)이 등장하게 되었으며, LSTM, GRU와 같은 long-term memory 모듈을 활용하여 input과 output에 대한 global contextual meaning을 학습하려는 노력이 시작되었다.


# 기계 번역에서의 RNN과 한계점

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152411-83f8a105-9fad-4f44-adbb-ef3790ea903d.png" width="600"/>
</div>

예를 들어 '<U>나는 고양이 이름을 에폭이라 짓기로 결정하였다</U>'라는 한국어 문장을 '<U>I decided to name my cat Epoch</U>'라는 영어 문장으로 번역하는 것을 머신러닝/딥러닝으로 해결하려는 기계 번역 task가 있다고 생각해보자. 단순히 이런 task를 RNN의 관점에서 접근하게 되면,   

<div align="center">
    한국어 문장 $\\rightarrow$ $E_{\\theta}$ $\\rightarrow$ 임베딩 $\\rightarrow$ $D_{\\phi}$ $\\rightarrow$ 영어 문장
</div>


위와 같이 표현할 수 있다. 중간에 있는 $E_\\theta, D_\\phi$는 각각 인코더와 디코더를 의미한다. 이러한 <U>Encoder-Decoder 구조</U>를 가지는 RNN 형태로 대표적인 것이 sequence-to-sequence 모델이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152412-1c51e386-83ea-4cd7-b202-b7776a1910a7.png" width="700"/>
</div>


첫번째 가장 큰 문제점은 연산 속도가 된다. 딥러닝이 레이어를 깊게 가져가면서도 연산 속도를 빠르게 할 수 있었던 것은 텐서 연산에 대해 GPU를 통한 병렬 처리가 가능했다는 점이다. 같은 level에서의 feature map value는 동일하기 때문에 convolution 연산을 <U>굳이 순차적으로 진행하지 않고도</U> 모든 연산을 **동시에** 할 수 있기 때문에 구조적 이점을 가져갈 수 있었다. 그러나 RNN의 경우는 그럴 수 없었다. 구조를 보면 알 수 있지만 LSTM(hidden layer로 사용된다고 보면 된다)의 각 계산을 하기 위해서는 이전 LSTM의 결과가 필요하다. 병렬 처리를 통해 LSTM 하나의 연산을 빠르게 한다고 하더라도 문장의 길이 $N$에 대해서는 여전히 <U>bottleneck</U>이 걸리고 있는 것이다. 두번째 문제점은 연산 과정에서 사용되는 context vector의 크기가 고정적이라는 것이다. 이 또한 문장의 길이가 길어질수록 큰 문제가 생기는데, RNN 구조에서 번역 과정에서 참고할 수 있는 feature embedding은 오직 encoder의 final hidden layer의 output이다. 따라서 복잡한 문장이나 번역이 까다로운 경우 문장의 길이가 길어질수록 encoder에서의 문장을 제대로 참고하지 못하는 문제가 발생하였다. 바로 이러한 문제로부터 등장한 것이 attention 메커니즘이고, transformer의 근간이 되는 기술이기도 하다.


# Attention mechanism

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152414-898f80a8-522c-435b-b490-2dbb1a6db297.png" width="500"/>
</div>


앞서 말했듯이 RNN에는 크게 두 가지의 문제점이 있다. 첫번째는 long sentence에 대해 연산 속도가 너무 느리다는 점이고, 두번째는 문장이 길어지면 길어질수록 context vector의 <U>고정적인 길이</U>에 대해 performance 제약을 받는다는 점이다. 이러한 문제를 해결하기 위해서는 RNN을 구성하는 <U>모든 LSTM의 output에 대해</U> reasoning이 필요하게 되었고, 여기서 'attention'이라는 모듈이 제시가 되었다. Attention module은 input에 대해(single input이 될 수도, multiple input이 될 수도 있음) 서로 얼마나 연관도를 가지는지를 weight로 표현한 tensor를 추출한다. 예를 들어 다음과 같은 문장이 제시가 되었다고 생각해보자.

$$
    \\text{I decided to name my cat Epoch}
$$

이를 단어 단위로 tokenize한 뒤에,

$$
    \\text{(I, decided, to, name, my, cat, Epoch)}
$$

각 token을 모두 embedding 조건에 따라서 value로 mapping한 뒤, 단어 '<U>name</U>'에 대해서 같은 문장에 있는 단어들과의 유사성을 구해보려고 한다. 우선 알 수 있는 사실은 'name'과 가장 관련이 있는 단어는 '<U>Epoch</U>'이며, 그 다음으로 중요한 단어는 '<U>cat</U>'이 될 수 있을 것이다.

$$
    \\text{(I, decided, to, name, my, cat, Epoch)} \\rightarrow (0, 0, 0, 0.3, 0, 0.1, 0.6)
$$

극단적인 경우로 가정했지만, 아무튼 이처럼 input의 각 embedding 사이의 유사성을 weight로 매핑하고, 이렇게 추출된 <U>weight의 softmax 확률값</U>을 **attention value**로 삼게 되는 것이다. 이렇게 attention을 사용하게 되면 LSTM와 같은 long term 모듈에 의지하지 않고도 문장 전체에 대한 global correlation 혹은 long range interaction을 획득할 수 있다.   
그러나 여전히 이러한 sequence to sequence를 사용해서도 해결할 수 없는 문제가 있는데, 그것은 바로 decoder에 사용될 feature vector를 추출하기 위한 RNN의 '<U>순차적 연산</U>'을 가속화할 방법이다. 여전히 학습 성능과 추론 시간 사이에 해결할 수 없는 trade-off가 남아있게 된다. 이제 드디어 설명하려는 것이 이러한 bottleneck이 해결될 수 없는 RNN 구조에 <U>의존하지 않고</U> 유의미한 기계 번역을 할 수 있다는 내용의 새로운 패러다임을 제시한 <U>transformer</U>가 되겠다.


# Attention is all you need!

[Transformer](https://arxiv.org/abs/1706.03762)가 바로 이 제목과 함께 <U>거대한 어그로</U>를 끌며 나타났다. 해당 논문의 main idea는, 굳이 RNN과 같은 convolution 구조를 사용해서 global context를 뽑지 않더라도 어차피 attention을 쓰면 즉각적으로 global reasoning이 가능하고, 이러한 attention 연산을 여러번 진행하는 deep neural network 구조가 오히려 기계 번역과 같은 task에 더 적합하지 않겠냐는 것이다. 실제로 기존 RNN에 비해서 연산 속도를 줄이면서도 그 성능을 입증받았고, 이러한 <U>구조적 변화</U>는 NLP 및 여러 연구에 변화를 일으키는 초석이 되었다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152421-69e23550-3f6e-4360-93f2-d6153e4c49f8.png" width="500"/>
</div>

지금부터 transformer 구조를 구성하는 중요 요소 중 하나인 self-attention, multi-head attention 그리고 positional encoding과 masked attention에 대해서 알아보도록 하자.

## Self Attention
RNN에서는 encoder 구조를 통해 feature vector를 추출하였다. 이 feature vector는 모든 input token에 대한 context를 담고 있게끔 학습된다는 것이 RNN 구조에서의 assumption이었고, 여기에 추가적으로 attention value를 통해 이전의 RNN feature vector까지 고려했던게 기존 방식이다. <U>Attention mechanism</U>만을 사용하는 **transformer**는 이러한 의존성을 없애고, 단순히 attention을 같은 문장에 여러번 적용함으로써 이러한 contextual embedding을 추출한다. Attention에서 사용되는 용어에 대해서 먼저 짚고 넘어가도록 하자.

- Query($Q$) : '질문' 혹은 '물음'이라는 의미를 가지고 있다. 문장 내의 각 토큰이 다른 토큰과 어떤 연관을 가지고 있을지 추론해가는 과정에 있어서 '각 토큰'을 의미한다. 예를 들어 위의 예시에서 봤던 것처럼 단어 '<U>name</U>'에 대해서 같은 문장(I decided to name my cat Epoch)에 있는 단어들과의 유사성을 구해보려고 할 때의 'name'과 같다.

- Key($K$) : Query와 pair로 작용하여, query가 특정 key에 대해 value를 물었을 때의 이를 줄 수 있는 역할을 한다. 예를 들어 위의 예시에서 봤던 것처럼 단어 '<U>name</U>'에 대해서 같은 문장(I decided to name my cat Epoch)에 있는 단어들과의 유사성을 구해보려고 할 때 문장 내에 있는 단어들(I, decided, cat 등등)과 같다.

- Value($V$) : Key에 대응되는 value를 의미한다. Query, key, value에 대한 개념은 단일로 이해하는 것보다는 셋의 <U>연관성</U>을 생각해보는 것이 훨씬 간단하다.

입력으로 주어지는 문장 $S$가 있고 이를 embedding 함수 $\\mathcal{E}$를 통해 embedding tensor $X$로 만들었다고 해보자. 이 embedding tensor에 대해 query, key, value는 각각의 weight에 따라 liear projection 된다.

$$
    \\begin{aligned}
        Q =& W_1X \\newline
        K =& W_2X \\newline
        V =& W_3X
    \\end{aligned}    
$$

예를 들어 문장을 총 $n$개의 토큰으로 나눈 뒤, 각 토큰을 $e$의 dimension을 가지는 텐서로 치환했다고 하자. $Q,~K,~V$의 dimension $d$에 대해서,

$$
    \\begin{aligned}
        X \\in & \\mathbb{R}^{n \\times e} \\newline
        K \\in & \\mathbb{R}^{n \\times d},~W_1 \\in \\mathbb{R}^{e \\times d} \\newline
        Q \\in & \\mathbb{R}^{n \\times d},~W_2 \\in \\mathbb{R}^{e \\times d} \\newline
        V \\in & \\mathbb{R}^{n \\times d},~W_3 \\in \\mathbb{R}^{e \\times d}
    \\end{aligned}    
$$

위와 같이 나타낼 수 있다. Attention value를 구하는 여러 방법들 중 transformer에서 사용된 scaled dot product 과정을 소개하면 다음과 같다.   
   

가장 먼저, 서로 다른 input embedding에 대한 score를 계산한다. $S = Q \\cdot K^\\top$. $Q$의 각 row vector는 각각의 token embedding에 대한 값이고, $K^\\top$의 각 column vector는 각각의 token embedding에 대한 key가 된다. 이를 내적하게 되면 $Q,~K$의 row vectors $r^q_i,~r^k_i \\in \\mathbb{R}^d$ $(i = 1,~2,~\\cdots,~n)$ 에 대해 다음과 같이 표현할 수 있다.

$$
    QK^\\top = \\begin{bmatrix}
        r^q_1 \\newline
        r^q_2 \\newline
        \\vdots \\newline
        r^q_n
    \\end{bmatrix}
    \\cdot \\begin{bmatrix}
        {r^k_1}^\\top & {r^k_2}^\\top & \\cdots & {r^k_n}^\\top
    \\end{bmatrix}
$$
이렇게 계산된 값은 각 row vector끼리의 내적으로 구성되며, 이는 row vector의 dimension $d$ 값이 커질수록 커지는 구조가 된다.

$$
    S = QK^\\top = \\begin{bmatrix}
        {r^k_1}^\\top r^q_1 & {r^k_2}^\\top r^q_1 & \\cdots & {r^k_n}^\\top r^q_1 \\newline
        \\vdots & \\vdots & \\ddots & \\vdots \\newline
        {r^k_1}^\\top r^q_n & {r^k_2}^\\top r^q_n & \\cdots & {r^k_n}^\\top r^q_n
    \\end{bmatrix}  
$$

따라서 안정적인 학습을 위해(gradient를 맞춰주기 위해) 위의 score를 dimension의 square root value로 나눠준다.

$$
    S_n = \\frac{QK^\\top}{\\sqrt{d}} = \\begin{bmatrix}
        ({r^k_1}^\\top r^q_1)/\\sqrt{d} & ({r^k_2}^\\top r^q_1)/\\sqrt{d} & \\cdots & ({r^k_n}^\\top r^q_1)/\\sqrt{d} \\newline
        \\vdots & \\vdots & \\ddots & \\vdots \\newline
        ({r^k_1}^\\top r^q_n)/\\sqrt{d} & ({r^k_2}^\\top r^q_n)/\\sqrt{d} & \\cdots & ({r^k_n}^\\top r^q_n)/\\sqrt{d}
    \\end{bmatrix}  
$$

구한 score를 확률값으로 바꿔주기 위해 softmax를 취한다. Softmax function은 exponential 함수를 통해 $0 \\sim 1$ 사이의 값으로 normalization 해주고, 가장 중요한 점은 특정 dimension으로의 합이 $1$이 될 수 있도록 해준다.

$$
    \\begin{aligned}
        \\text{Let }z_{ij} =& ({r^k_j}^\\top r^q_i)/\\sqrt{d}, \\newline
        softmax(z_{ij}) =& \\frac{e^{z_{ij}}}{\\sum_{i=1}^n e^{z_{ij}}} \\newline
        P =& softmax(S_n)
    \\end{aligned}    
$$

이제 여기에 마지막으로 key에 대응되는 value를 곱해주면, 우리가 얻고자 하는 weighted value matrix를 얻을 수 있다.

$$
    Z = V\\cdot softmax(\\frac{Q \\cdot K^\\top}{\\sqrt{d}})    
$$


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152427-711293fb-c92f-4efc-b50c-8208073833cb.png" width="200"/>
</div>


## Masked attention in decoder
Attention 연산은 모두 위에서 설명한 것과 거의 동일하다. 다만 decoder에서 encoder와의 attention을 할 때는 encoder에서의 결과를 토대로 key, value를 상정하고 decoder의 output을 query로 삼는다. 그러나 학습 시에 <U>RNN과는 다르게</U> input으로 tokenized sentence 전체를 넣어주다 보니 causality 문제가 생긴다. 이 문제는 다음과 같다.   
Decoder에서 input 문장에 대해 번역된 결과를 내보낼 때는 recurrent 구조를 가진다. 이는 **RNN based sequence to sequence model**과 **transformer** 모두 동일하다. 그렇기 때문에 학습 과정에서는 '번역 결과'를 알고 있어도 이를 사용하면 안된다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152430-6f3ce74c-b547-4c7f-89b8-6fac89547819.png" width="800"/>
</div>

이 그림을 보도록 하자. 그림에서의 task는 'I want to buy a car'이라는 문장을 다른 언어로 번역하는 과정을 나타낸 것이다. Decoder에서 가장 먼저 <U>BOS</U>(Begin of Sequence를 의미하는 토큰)와 encoder에서의 output을 기반으로 첫 단어('<U>Ich</U>')를 예측한다. 그 다음 단어는 예측된 '<U>Ich</U>'와 encoder에서의 output을 기반으로 두번째 단어('<U>will</U>')을 예측한다. 그 다음 단어는 예측된 '<U>Ich, will</U>'과 encoder에서의 output을 기반으로 세번째 단어('<U>ein</U>')을 예측한다. 이런 식으로 진행된다.   
논문에 나와있지는 않지만 여기서 중요한 실제 학습될 시에는 모델이 예측한 단어가 아닌 실제 ground truth를 사용한다는 것이고, 이를 자연어 연구에서는 **teacher forcing**이라고 부른다. 예컨데 번역 결과를 이미 알고 있다면 $n$번째 단어를 예측할 때 굳이 $n-1$번째까지 예측한 단어를 쓸 필요가 없고 이미 알고있는 $n-1$개의 GT를 사용할 수 있고, 이러한 방식이 encoder 단에서의 attention 학습을 보다 안정적으로 만들어줄 수 있다. 초반에 모델이 수렴되지 않은 상태에서는 decoder가 생뚱맞은 단어를 내뱉게될 것이고, 이를 그대로 decoder 예측의 attention query로 사용하게 되면 학습이 불안정해지는 문제가 발생하기 때문이다.

물론 현재 예측될 단어가 다음에 예측될 단어를 참고하지 못한다는 사실은 'inference'(테스트)에서는 항상 성립한다. 그러나 학습 시에는 이미 번역된 결과를 모두 알고 있고, 이를 supervision으로 삼아서 네트워크를 학습하기 때문에 <U>학습 과정에서의 decoder는 뒷 단어들을 참고할 수 없게</U> 해야한다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152431-a618a50d-b838-47fb-8e90-8333dd5075df.svg" width="800"/>
</div>


연산 과정을 간단하게 표현하면 위와 같다. 맨 윗줄부터가 decoder의 input token에 대한 attention을 나타내고, mask에서 색칠된 부분이 value가 $1$, 그렇지 않은 부분이 value가 $0$으로 $n$번째의 embedding은 $n$보다 작거나 같은 attention weight만 참고할 수 있다.

## Multi-head attention
머신러닝 기법 중 '앙상블'이란 개념이나, 딥러닝 convolutional neural network에서 사용하는 kernel의 channel 수는 모두 비슷한 기능을 가진다. 그것은 바로 같은 feature map에 대해 <U>여러 가지 representation을 학습할 수 있다</U>는 것이다. CNN에서의 구조는 이미 이를 보장할 수 있는 network **width**라는 특성을 가지지만, transformer의 경우에는 attention layer가 여러 채널을 가질 수 없다는 문제가 생긴다. 이를 해결하기 위해 등장한 개념이 바로 <U>Multi-head attention</U>이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152436-7bedef8c-8afe-4456-90a0-956fde4836a9.png" width="300"/>
</div>


개념은 상당히 간단하게도, attention을 할 수 있는 linear layer를 head의 개수 $h$만큼 늘려서 사용하겠다는 것이다. Attention weight를 연산할 수 있는 head를 여러 개 사용하여 계산한 뒤, 결과값을 head의 축으로 concatenate한 뒤 Linear 연산을 통해 원래의 dimension으로 맞춰준다. 예컨데 위에서 사용한 notation 그대로를 사용해보면, head index $h$에 대한 query, key, value linear operator $W_h^Q,~W_h^K,~W_h^V$와 multi-head linear operator $W_o \\in \\mathbb{R}^{hd \\times d}$에 대해서,

$$
    \\begin{aligned}
        Q_h =& W_h^QX,~K_h = W_h^KX,~V_h = W_h^VX, \\newline
        Z_h =& V_h\\cdot softmax(\\frac{Q_h \\cdot K_h^\\top}{\\sqrt{d}}) \\in \\mathbb{R}^{n \\times d}, \\newline
        Z_{concat} =& \\text{Concat}(Z_1;~Z_2;~\\cdots;~Z_h) \\in \\mathbb{R}^{n \\times hd}, \\newline
        Z_{output} =& W_o \\cdot Z_{concat} \\in \\mathbb{R}^{n \\times d}
    \\end{aligned}
$$

다음과 같이 계산되는 구조다.

## Positional encoding
RNN 구조에서 없었던 내용 중에 causality는 여전히 존재한다. 그런데 그것보다 더 문제인 것은 각 임베딩 사이의 거리도 어느 정도 context를 판단할 때 고려가 되는 사항인데, 각 token을 sequential하게 연산하는 RNN은 이를 걱정하지 않아도 되지만 transformer의 경우에는 해당 constraint를 줄 수 있는 방법이 필요하다. 예를 들면 다음과 같은 문장이 있다고 생각해보자,

$$
    \\text{I have a dog named Adam and my friend has a cat named Epoch}    
$$
한글로 번역하면, 나한테는 <U>'아담'이라는 이름을 가진 강아지</U>가 있으며, 내 친구는 <U>'에폭'이라는 이름을 가진 고양이</U>가 있다는 뜻이다. 여기서 중요한 점은 강아지를 가지고 있는 주체와 고양이를 가지고 있는 주체에 대한 문장에서의 거리다. 만약 특정 이름과 동물이 연관된다는 특성에 대해서 token의 위치와 무관하게 학습이 진행되다보면, 다음과 같은 참사가 발생한다.   
   
'I have **a dog** named Adam and my friend has a cat named **Epoch**'   
'I have a dog named **Adam** and my friend has **a cat** named Epoch'   
   
여기서 <U>굵게 표시된 부분끼리 만약 attention value가 높게 책정이 되면</U>, 내가 키우는 강아지와 친구가 키우는 고양이의 이름이 뒤바뀌는 일이 생기는 것이다. 혹은 내 친구가 강아지를 키우고 내가 고양이를 키우는 식으로 번역이 될 수도 있다. 따라서 CNN에서 object가 서로 비슷한 픽셀 위치에 위치한다거나, translation에 불변한다는 특징과 같이 학습하는 과정에서의 inductive bias를 어느 정도 주기 위해서 token의 위치에 따른 임베딩을 추가해주는 방법을 고안하였다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152440-74866c8f-538e-46de-86a9-6e3de9f32142.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/209152441-05269b19-26ac-48e1-8312-1979c03876d9.png" width="400"/>
</div>

Positional encoding으로 사용된 함수는 흔히 알고 있는 sinusoidal 함수를 사용한다. 

$$  
    \\begin{aligned}
        PE_{(pos,~2i)} =& \\sin (pos/10000^{2i/d_{model}}) \\newline
        PE_{(pos,~2i+1)} =& \\cos (pos/10000^{2i/d_{model}})
    \\end{aligned}
$$

여기서 $i$는 embedding dimension을 따라가는 축이고, $pos$가 각 token의 위치를 나타내는 position이다. 이를 실제로 시각화한 그림이 좌측에 있는 얼룩덜룩한 형태가 된다. 식을 보면 알 수 있듯이 $d_{model}$은 앞서 언급했던 embedding의 차원 수 $d$와 같으며, $i$가 <U>증가할수록</U> sinusoidal 함수의 <U>주파수가 점차 감소</U>하는 모양새가 된다. 주기함수를 보면 알 수 있듯이 y축을 따라가면서 함숫값을 보게 되면 **같은 함숫값을 가지는 index**가 reference 위치와는 관계없이 동일한 것을 알 수 있다. 즉 <U>짙은 파란색이랑 또다른 짙은 파란색 사이의 y축 거리</U>(pos 차이)나, <U>짙은 빨강이랑 또다른 짙은 빨강 사이의 y축 거리(pos 차이)</U>가 일정하게 나타난다는 것이다. 그리고 또한 위의 함수는 각 embedding position마다 서로 다른 encoding을 더해줄 수 있다. 함숫값은 주기 함수에 의해 반복이 되지만, 그 어떤 encoded embedding도(x축에 평행한 value들을 의미) 서로 <U>일치하지 않고 unique</U>하다. 즉, 문장이 얼마나 길어지든 상관없이 각 token에 유일한 값으로 매핑할 수 있기 때문에, <U>문장의 길이와 무관하게 모든 token을 구분할 수 있다</U>.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152442-d12a5bfc-aaf1-442e-9733-ea9911e1b1b1.png" width="400"/>
</div>

이처럼 구해준 각 token 별 encoding을 dimension에 맞게 생성해서 모든 embedding에 더해주면 된다.

## Transformer 학습

학습은 간단하게 supervision이 있기 때문에 cross-entropy loss에 대해 최적화가 가능하다. Output으로 나오는 softmax prediction에 대해 가장 최댓값을 가지는 index의 단어를 매핑하고, 제대로 된 단어의 확률값이 1에 가까워질 수 있게 학습된다.

## 그래서 요즘은?
Transformer는 단연코 neural network 중 최근에 가장 활발히 연구된다고 해도 과언이 아닐 정도로 다양한 분야에 접목하기가 좋다. 2017년에 기계 번역 task에 대해서 처음으로 transformer가 제시된 이후, BERT, GPT-3 등등 NLP 관련 모델들이 많이 등장했으며, 특히나 vision 분야에서도 vision transformer가 등장하면서 <U>multimodal approach</U>가 가능해졌다. 모든 modality에 대해 일관적인 구조로 encoding이 가능하다면 이를 토대로 여러 modality를 함께 supervision으로 사용할 수 있다는 관점이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152438-03afe1d5-bd0f-4aa5-8ae5-1f7be17c522f.png" width="600"/>
</div>



# Vision transformer

앞서 기계 번역에서 <U>transformer</U>의 구조 및 학습 방법에 대해 알아보았다. 따로 여기서는 첨부하지는 않았지만 실제로 transformer를 사용했을 때의 성능은 굉장히 좋았고, 이후 다양한 tansformer based approach를 통해 많은 <U>NLP 관련 기술들</U>이 발전할 수 있었다. 이에 비전에서도 NLP에서 사용된 transformer 구조를 사용할 수 없을지에 대한 연구가 'Attention is all you need' 논문 이후로 활발하게 진행되기 시작했다.   
하지만 명확하게 <U>tokenize되기 쉬운</U> natural language나 audio(단어나 음절 단위로 끊는 경우)와는 다르게 image의 경우에는 보다 연속적인 신호로 구성되어 있어서 정확히 어떠한 기준으로 tokenize를 해야할 지 애매하고, 이를 임베딩하는 방법 또한 일반적인 word2vec과 같은 방법을 사용할 수 없다는 점이 문제가 되었다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152451-54daf42a-40fa-49fc-91cf-e6aaf2570e52.png" width="400"/>
</div>

이러한 과정에서 처음으로 vision transformer의 성능을 보여준 논문이 [ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)라는 논문이었고, 해당 논문을 간단하게 살펴보면 다음과 같다. 논문에서 접근한 방식은 상당히 간단하다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209162457-43a1b681-e2d6-4e20-9e85-b8e964efebf5.gif" width="600"/>
</div>

이미지를 총 $n$개의 patch로 나눈다. 논문 제목에서 확인할 수 있겠지만, 이미지 크기가 $256 \\times 256$ 이라면 이를 균등하게 $16 \\times 16$의 patch로 분리한다. 위의 그림에서는 이를 좀 더 간단하게 표현하기 위해 이미지의 $H,~W$를 각각 3등분하여 9개의 샘플을 만들어내는 것을 볼 수 있다. 분리한 patch를 각각 flatten해서 1차원의 텐서로 만든 뒤 linear projection을 통해 embedding space로 보내고, 이 patch sequence 앞쪽에 <U>cls token</U>(이미지의 클래스를 구분할 때, global inference를 하기 위해 기준점이 되는 부분이라고 기억해두자)을 추가해준다. 앞서 transformer에서 했던 것과 같이 마찬가지로 여기서도 positional embedding을 더해주는데, 이미지의 경우 natural language embedding에서 사용했던 것과 같은 sinusoidal embedding의 효과가 나타나지 않기 때문에 여기선 그대신 <U>learnable positional embedding</U>을 사용하게 된다. 모든 attention 연산이 끝난 뒤 encoder output에서 <U>cls token</U>을 받아오고, 여러 번의 attention 연산이 진행되면서 이 cls_token에는 16개의 patch embedding에 대한 <U>global reference</U>가 완료된 상황이다. 이 cls token은 $B \\times D$ 크기를 가지며, 이를 바로 class 예측에 사용하기보다는 attention 연산이 진행되면서 '<U>다른 패치들의 정보를 요약</U>'한다는 느낌으로 접근한다. 따라서 class embedding을 추출한 다음에는 다음과 같은 연산이 진행된다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152455-2fb10959-e2cf-48cb-89f6-7237f5a803b7.png" width="600"/>
</div>


16개의 patch가 있었기 때문에 transformer model의 $Q,~K,~V$ dimension $d$에 대해서 output은 cls token과 함께 $(16+1) \\times d$ 크기의 tensor로 추출된다. 여기에 MLP head로 class 개수만큼의 output을 내보내는 linear 연산($d \\rightarrow C$ where $C$ is the number of classes)을 진행한 뒤, layer normalization 결과를 <U>class 예측에 대한 score map으로 사용</U>한다. 즉, 이를 logit으로 삼아 softmax 연산을 하게 되면 비로소 cls token의 output에 대한 class 예측이 진행되는 것이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152460-d5a17d63-28b3-4860-a9fa-c56412abd7af.png" width="800"/>
</div>


그러나 vision transformer의 경우 데이터셋이 충분하지 않으면 BiT(ResNet 기반 transformer)와 같은 CNN 기반 모델에 비해 성능을 기대하기 힘들었는데, 이 이유는 바로 앞서 설명했던 inductive bias의 부재와 관련된다. CNN based network는 적은 데이터셋으로도 기본적으로 가지고 있는 inductive bias를 통해 빠른 일반화 및 최적화가 가능하지만, 패치 단위로 나누어 각각에 대한 attention을 연산하는 구조인 <U>vision transformer는 이런 의존성이 전혀 없기 때문</U>이다.   
그렇기 때문에 가지고 있는 단점이자 장점은, CNN의 경우 inductive bias 때문에 정해진 연산을 통해 추출할 수 있는 representation map 혹은 feature map이 어느 정도 학습이 진행되고 나면 수렴이 발생하지만, vision transformer는 데이터셋을 대량으로 학습시키면 학습시킬수록 attention 성능을 높일 수 있고, 이는 특정 구조에 따라 수렴하는 형태가 아니라 끊임없이 발전할 수 있다고 해석하였다.   
그렇기 때문에 Vision transformer를 <U>대량의 데이터셋</U> JFT dataset($3 \\times 10^8$ samples)로 사전 학습한 뒤, 원하는 **downstream task**에 맞게 fine-tuning을 하면 더 좋은 성능을 보일 수 있다고 한다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152462-5da11123-b4d4-46f5-b56e-d1c4106d0732.png" width="800"/>
</div>


그리고 <U>convolution</U> 연산에 비해 <U>attention</U> 연산이 가지는 parameter의 개수나 네트워크의 전반적인 규모가 CNN보다 <U>훨씬 크다</U>는 점이 문제가 될 수 있다. VGG-16의 경우 경량화가 많이 부족한 모델임에도 불구하고 175.12M의 parameter 개수를 가지기 때문에 ViT-Large 혹은 ViT-Huge에 비해 훨씬 가벼운 것을 확인할 수 있다. 심지어 ResNet의 경우에 가장 레이어가 깊은 네트워크 중 하나인 <U>ResNet-152</U>의 경우에도 paramter 개수가 <U>60.34M</U>에 그치는 것을 보면, 확실히 transformer 네트워크의 규모가 지나치게 크게 구성된 것을 볼 수 있다. 


# 왜 Vision transformer가 CNN을 이길 수 있었을까?

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152466-28ea9cdf-ab8d-400c-8c10-d6e17abde10c.png" width="800"/>
</div>

가장 간단하게 풀자면 단순히 self-attention 이라는 연산의 mechanism이, 초기의 layer부터 입력된 input 전체에 대한 inference가 가능하기 때문이다. 예를 들어 CNN의 경우에는, $3 \\times 3$의 커널 크기를 통해 $32 \\times 32$ 크기의 receptive field를 가지기 위해서는

$$
    3+2 \\times (n-1) \\ge 32,~n > 14    
$$

14보다 많은 개수의 layer가 필요하다. 이 과정에서의 연산량을 줄이기 위해 추가적으로 max-pooling과 같은 filtering이 들어가면, 이미지의 특정 부분에서는 <U>global한 fine detail을 알아채기도 전</U>에 **image classification**을 해야 하는 문제가 발생하는 것이다. 이를 다르게 말하면 ViT는 모든 레이어에서 균일한 형태의 representation map을 획득할 수 있고, self-attention 연산을 통해 보다 빠르게 global information을 축약해낼 수 있다. 이렇게 빠르게 요약한 global 정보를 여러 레이어를 통해 각 패치마다 공유하면서, 잘 학습된 representation을 전달할 수 있게 되는 것이다. 그러나 어김없이 <U>dataset의 개수가 대량으로 필요하다는 조건</U>(약 10억 개의 샘플)과 그만큼 <U>parameter 수도 많이 필요</U>하다는 것이 제약 조건이 될 수 있다. 추가적으로는 high-resolution image의 경우 attention 연산이 $Q,~K,~V$에 대한 attention score를 연산하는 과정이 되기 때문에 multi-head self attention의 computation이 $(H \\times W)^2$에 비례해서 증가하는 가장 큰 문제가 발생한다. 이제부터 그 각각의 문제점을 해결하기 위한 연구들을 소개해보겠다.


# Dataset을 효율적으로 사용할 수 있는 방법

가장 먼저, 데이터셋을 많이 사용하지 않고 학습시킬 수 있는 방향에 대해서 연구한 논문인 [DeiT: Training data-efficient image transformers & distillation through attention](https://arxiv.org/pdf/2012.12877.pdf)에 대해서 살펴보도록 하자.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152469-4fde8fda-f626-4d1c-999a-6d4d75435c68.png" width="400"/>
</div>

DeiT는 ViT와 동일한 transformer 구조에 대한 학습을 진행하되, JFT와 같은 대용량 데이터셋에 대한 pre-training을 하지 않고도 ImageNet dataset에 대해서만 학습하는 방향을 제시했다. 이전에 작성했던 글들 중에서 Knowledge distillation에 대해 간단하게 소개했던 글이 있는데([참고](https://6unoyunr.github.io/blog/transfer)), 바로 distillation을 통해서 transformer가 지지부진할 때 도움을 많이 줄 수 있는 <U>teacher</U> 역할을 <U>convolutional neural network</U>가 해줄 수 있다는 것이다. Convolutional neural network 구조에 의존하지 않더라도, 단순히 distillation loss를 줄 수 있는 token 하나만 추가하면 된다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152470-4bcf4a9a-85f6-4145-a535-20449cd5d23e.png" width="400"/>
</div>

그런데 여기서 두 가지의 선택권이 있다. 바로 teacher network의 prediction에 대해 soft-label distillation을 loss로 줄 것인지, 아니면 teacher network의 prediction의 최댓값을 one-hot encoding으로 한 hard-label distillation을 loss로 줄 것인지이다.

$$
    \\mathcal{L}_{\\text{global}} = (1-\\lambda) \\mathcal{L}_{\\text{CE}}(\\psi(Z_s), y) + \\lambda \\tau^2 \\text{KL}(\\psi(Z_s/\\tau),~\\psi(Z_t/\\tau))    
$$

이런 식으로 temperature value $\\tau$와 그에 따라 $lambda$ 값을 적절히 조절하여 weighted soft-distillation loss를 더해주는 방법이 있는 한편,

$$
    \\mathcal{L}_{\\text{global}}^\\text{hard Distill} = \\frac{1}{2} \\mathcal{L}_{\\text{CE}}(\\psi(Z_s), y) + \\frac{1}{2} \\mathcal{L}_{\\text{CE}}(\\psi(Z_s),~y_t)   
$$

위와 같이 hard target에 대해서 예측을 하는 hard distillation이 있다. 여기서 표현된 $y_t$는 $\\arg \\max_c Z_t(c)$로 표현한다. 즉 teacher prediction의 prediction 최댓값을 1로, 나머지를 0으로 하는 one-hot encoding 형태로 주어진다. DeiT 논문에서는 아래 방법을 소개하면서 해당 loss term을 최적화하였다. 그러나 이 네트워크의 경우 한계점이 상당히 명확하게 존재하는데, 바로 <U>CNN의 성능에 대해 upper bound가 생긴다</U>는 점이다. **ViT**가 제시되고 나서, <U>JFT와 같은 대량의 데이터셋</U>을 활용하여 학습할 수 있다면 <U>CNN보다 성능이 좋아질 수 있다는 것</U>이 transformer based vision approach의 분석이었는데, 데이터를 효율적으로 쓰려고 하면서 CNN을 teacher network로 사용하다보니 다시 원점으로 돌아오게된 것이다.


# High resolution image에 대한 효율적인 연산
다음으로 언급할 내용으로는 앞서 Vision transformer에서 해결하지 못했던 <U>high resolution image</U>에 대한 연산이다. Vision transformer 연산 과정을 보게 되면, image를 동일한 크기의 patch로 분리하고, 이를 embedding으로 linear projection한 뒤에 일련의 attention 연산을 진행하게 된다. 그러므로 만약 image의 resolution이 2배가 되면, linear projection은 그 값의 제곱인 4배로 늘어나게 되고, attention 연산에 필수적인 $Q,~K,~V$를 통한 attention weight 연산은 그 값의 다시 제곱인 16배로 증가하는 것이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209162456-586c1b3e-7855-426e-a332-73e7542e2863.gif" width="400"/>
</div>

그리고 무엇보다 연산 때문에 token의 개수가 한정적이라면, CNN에서 했던 것과 같이 다양한 scale의 feature를 뽑기가 힘들 것이다. Global information을 빠른 시간에 축적하는 건 장점이 될 수 있는데, 결국 그렇게 축적된 데이터는 하나의 scale에만 국한된다는 점. 즉 vision transformer를 더 발전시켜서 segmentation이나 image restoration과 같은 <U>high-level vision task</U>에도 적용할 수 있어야하는데, 지금의 구조로는 가능성이 보이지 않는다. 바로 여기서 나온 것이 지금 설명할 [Swin-Transformer](https://arxiv.org/pdf/2103.14030.pdf)를 통한 hierarchical feature extraction이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152473-0853ced7-3895-4745-93cd-810b097bf40f.png" width="700"/>
</div>

만약 high-resolution인 이미지에 대해서 <U>정해진 개수</U>의 patch를 뽑는 것이 아니라 <U>단계적으로 patch에 대한 연산</U> 이후 이를 merge하는 과정을 추가하면, '초반에 fine detail을 잡는 과정과 후반에 coarse feature를 잡아내는 CNN과 유사하게 동작하지 않을까?'에 대한 내용을 다룬다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152478-69203891-82b7-4c8b-86e9-8ca45c1610a7.png" width="800"/>
</div>

네트워크 구조는 위와 같다. ViT에서 크게 달라진 점을 찾아보면 patch partition 부분에 대해 가장 처음에는 $\\frac{HW}{4^2}$ 만큼의 patch를(여기서는 갯수를 의미한다), 그리고 점차 patch 크기를 키워서 마지막에는 $\\frac{HW}{32^2}$ 크기의 patch를 사용한다. 크기를 키우는 과정은 인접한 패치끼리 붙이는 과정이다.

1. 가장 처음 패치는 patch partition을 통해 $4 \\times 4 \\times 3$의 크기를 가진다. 참고로 위의 그림에서 보이는 값에 대해서 헷갈릴 수도 있기 때문에 언급하자면, $\\frac{H}{4} \\times \\frac{W}{4}$는 patch 하나의 크기가 되고, 각 패치 내부에는 총 $4 \\times 4$ 단위로 RGB 값을 가진 픽셀들이 들어있다고 생각하면 된다.

2. Stage 1을 통해 Linear embedding으로 patch의 채널 수를 $4\\times4\\times3$에서 $4\\times4\\times C/16$배로 증가시킨다. 각 패치에 대한 임베딩을 뽑는 stage라고 생각하면 된다. 실제 논문에서는 C = 192를 사용했으니, 원래는 RGB 채널 3개로 시작했지만 $192/16 = 12$로 4배 증가시킨 것과 같다.

3. Stage 2 부터는 우리가 알고있던 attention이 진행된다. 다만, 여기서 patch-merging이 일어난다. Patch merging이 일어나게 되면 인접한 4개의 patch가 합쳐지면서 큰 patch가 되고, 그와 동시에 연산량이 줄기 때문에 그만큼을 channel로 증가시켜준다. 즉, $\\frac{H}{4} \\times \\frac{W}{4}$ 크기를 가지고 있던 패치 4개를 붙이면 각 패치의 크기는 $\\frac{H}{8} \\times \\frac{W}{8}$이 되고, 패치 내부의 구획 개수는 일정하기 때문에(이 부분이 제일 중요하다!) **merge 레이어를 지날수록 Attention 연산량이 줄어드는 것**이다. 왜냐? 패치 내부에 attention을 하게 될 구획 개수는 일정한데, 전체 패치 개수는 merge하면서 점차 줄어들기 때문. 그렇기 때문에 channel 수를 2배 증가시켜준다.

4. 참고로 Attention은 각 윈도우 내부에서만 진행된다. 여기서 말하는 윈도우란 곧 패치를 의미하며, 사실상 윈도우라는 개념과 패치라는 개념을 구분해서 써야하지만 이 논문에서는 혼용해서 쓴 것으로 보인다. 아마도 대부분의 리뷰어들이 여기서 혼란을 겪었을 듯 싶다. 각 윈도우 내에서는 구획 개수가 일정하기 때문에(3번에서 언급한 '패치 내부의 구획 개수는 일정하기 때문에'라는 문장과 같은 의미이다! 혼란스럽지 않기 위해 한번 더 언급)

5. 이러한 과정을 attention block을 거치면서 진행한다. 여기에 추가적으로 W-MSA와 SW-MSA라는 개념은 뒤에서 설명하도록 하겠다.

암튼 이렇게 저렇게 패치 사이즈를 조절해가면서 attention을 한다는 과정을 길게 설명했다. 사실 이 부분은 본인이 직접 깨닫기 전까지는 워딩만 보고서는 절대 이해할 수 없다. 왜냐하면 '<U>patch</U>'라는 워딩과 '<U>window</U>'라는 워딩이 paper에서 너무 겹치기 때문이다. 진짜 엄밀하게 따지면 완전 다르진 않고 어느 정도는 겹치는 개념이긴 하지만, shifted-window라는 method를 위해서 이런 식으로 독자(?)들을 혼란스럽게 하다니 조금은 괘씸하다. 아무튼 이제 W-MSA와 SW-MSA에 대해서 보면,   

W-MSA는 윈도우 내부의 패치들에 대한 attention이다. 하나의 윈도우 크기에 아마 논문에서 구현하기로는 $7 \\times 7$의 embedding이 포함될 것이고, 이렇게 하나의 윈도우 내에 포함된 애들끼리의 연산을 의미한다. 그러나 이렇게만 연산을 하게 되면 <U>서로 다른 윈도우에 속한 임베딩은 다른 임베딩의 정보를 얻을 수 없는 경우</U>가 생긴다. 다음과 같은 그림으로 예시를 그려보았다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152481-5933823c-3a1e-494b-9059-f66622435286.png" width="800"/>
</div>

분명 파란색으로 표시된 패치와 붉은색으로 표시된 패치는 위치상 서로 붙어있지만, 가장 마지막 attention layer 전까지는 <U>서로 attention 연산이 불가능</U>하다. 이러한 분단국가 문제는 locality가 중요하게 적용되는 image에 대해서 치명적으로 적용할 수 있을 뿐만 아니라, ViT의 장점 중 하나인 global information을 빠르게 취득하는 것이 불가능해진다. 따라서 다음과 같은 전략을 사용한다.
윈도우 내의 attention을 하는 것은 위의 그림에서 그려진 대로 계산한다. 그러나 shifted window attention은 다음과 같이 window를 이동시키는 전략을 사용한다. 이해가 쉽도록 이미지 전체를 하나의 window로 생각하고, 그 내부를 $4 \\times 4$ 만큼의 패치로 구분해보았다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152489-66f1f113-14bc-4870-a7b2-268ea4441a18.png" width="800"/>
</div>

다음과 같이 윈도우를 일부 이동시키면, 움직인 만큼 원래의 윈도우에 포함되던 부분이 빠져나오게 된다. 이를 채워주는 방법으로는 간단하게 '<U>잘라서 붙이기</U>' 방법을 사용한다. 좀 더 명확하게 말하자면, cyclic shift와 같다. 화학에서 분자 구조(체심 입방 구조 뭐 이런거..)에서의 개수를 구하는 과정에서 물질 전체가 반복되는 구조라고 가정하면 쉽게 풀 수 있었던 방법을 기억하면 된다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152493-93751ee4-db63-4ef8-8009-1b555bd27a11.png" width="800"/>
</div>


이렇게 window를 옮긴 뒤에 attention을 진행하게 되었을 경우에는 다음과 같이 <U>다른 window의 패치와도</U> attention이 연산될 수 있음을 보여준다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152498-b0ec4bc1-d4c5-4974-acc9-ee8c482c4cf6.png" width="800"/>
</div>


그리고 단순히 이전에 사용되던 scaled dot product self-attention 식이 아닌, 이번에는 relative position을 반영한 bias를 score에 더해주게 된다.

$$
    \\text{Attention}(Q,~k,~V) = softmax(QK^\\top / \\sqrt{d}+B)V    
$$

이는 이전의 positional embedding이 절대적인 좌표(sinusoidal embedding)을 더해주었던 접근과는 다르게 각 패치의 위치를 반영하는 attention embedding 방법이다. 이를 간단하게 설명하자면 만약 현재 query로 삼고있는 패치가 좌측 상단이고, attention을 구할 패치가 우측 하단이라면 $x, y$ 모두 증가하는 방향으로 bias를 더해주고, 만약 그 반대 방향이라면 $x, y$ 모두 감소하는 방향으로 bias를 더해주는 것이다. 


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152501-db6b06b0-11df-4d00-a5ad-61d3b9966b4e.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/209152505-289cca9a-ae69-414b-872b-cd81661513d8.png" width="400"/>
</div>


위의 그림을 예시로 삼아보겠다, 만약 Attention을 진행할 하나의 윈도우 내에 총 9개의 patch가 있다면(1~9로 숫자가 붙은 영역들), 이 patch에 대해서 <U>row index의 차이</U>를 나타내는 $x$-axis matrix, 그리고 <U>column index의 차이</U>를 나타내는 $y$-axis matrix를 표현할 수 있다. 

그런 뒤, 여기에 윈도우의 크기를 반영하는 다음과 같은 연산을 거치게 된다.

\`\`\`
# 각 matrix에 (window 크기-1) 만큼 더해주기
y_axis_matrix += window_size - 1
x_axis_matrix += window_size - 1

# 2M-1로 scaling한 뒤 더해주기
x_axis_matrix *= (2 * window_size -1)
relative_position_M = x_axis_matrix + y_axis_matrix
\`\`\`

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152507-ff3cc005-39bb-4e7a-a54c-dee94c41934d.png" width="400"/>
</div>


이렇게 scaling을 해주게 되면 원래 $-M-1 \\sim M-1$ 범위를 가지고 있던 relative matrix들이 $0 \\sim 2M-1$로 scaling된 뒤, 서로 곱해져서 $0 \\sim (2M-1)^2$ 까지의 범위로 표현될 수 있는 것이다.


# CNN과 ViT을 어떻게 하면 잘 조합할 수 있을까?
앞서 살펴본 Shifted window 방법을 사용한 <U>Swin-T</U>의 경우, 훨씬 효율적인 연산이 가능하다는 점과 hierarchical 구조로 인해 CNN이 가지던 다양한 scale에 대한 flexibility를 가질 수 있으며, 그러면서도 연산 속도는 image size에 대해 quadratic하지 않고 <U>Linear</U>하게 유지할 수 있다는 점이 큰 장점이 되었다. 또한 이러한 scale flexibility로 인해 classification 이외에도 다양한 vision task, 예를 들어 detection이나 segmentation 등에도 일반적으로 적용될 수 있는 구조를 제시했다는 점이 되겠다.   
그러나 이러한 Swin-T의 단점은 결국 작은 데이터셋으로 scratch부터 학습하기 힘들다는 점이고, 이런 문제는 ViT에서 제시된 이후로 여전히 해결되지 못했다. 앞서 소개했던 Data effficient transformer의 경우 CNN을 teacher model로 사용하여 knowledge distillation을 진행했지만, 결국 CNN의 성능에 따라 좌우되는 문제가 발생했었다. 그렇다면 과연 CNN이랑 ViT의 장점을 함께 활용하여, inductive bias를 통한 representation의 효율적 학습(<U>data efficiency</U>)과 <U>global information</U> 활용을 함께 할 수 있는 방법은 없을까?   
바로 이러한 질문으로부터 등장했던 연구가 [Convolutional vision Transformer(CvT)](https://arxiv.org/pdf/2103.15808.pdf)이다. CNN의 여러 장점들을 ViT 구조에 결합하고자 했던 것이다. 예를 들어 CNN은 object에 대해 shift, scale 그리고 어느 정도 용인 가능한 distortion이 일어나더라도 이를 <U>invariance하게 처리</U>할 수 있다. 그에 반하여 ViT는 dynamic attention, global information 그리고 보다 나은 generalization이 가능하다.   
CvT에서 바꾼 두 가지의 주된 구조적 형태는 바로 다음과 같다.

1. Transformer의 계층적 구조에 대해, convolutional token embedding이라는 새로운 임베딩 방식을 소개한다.
2. Convolutional transformer block은 연산될 convolutional projection을 생성해낸다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152513-48020390-58f3-4788-8e29-1a6ecab74d94.png" width="600"/>
</div>


결론부터 보자면, 구조를 그대로 사용하면서 더 적은 데이터셋을 활용했던 DeiT는 parameter 수를 효과적으로 줄이거나 성능을 CNN teacher network 이상으로 끌어올리지 못했으나, CvT는 parameter 수를 보다 효과적으로 활용하면서도 성능을 ViT 이상으로 높일 수 있음을 보여주었다. 네트워크의 전반적인 구조를 보면,


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152516-f6734f30-56b8-45dd-8bd9-d22545e46aec.png" width="900"/>
</div>


결국 하고자 하는 것은 input image 혹은 중간의 feature map에 대해서 <U>convolution</U> 연산으로 <U>token embedding</U>을 진행하고, 이 token에 대한 <U>attention</U>을 수행한다. 그런 뒤 output으로 나오게 되는 <U>lower resolution</U>의 feature map에 대해 다시 convolution 연산을 진행, <U>token에 대한 attention을 수행</U>하는 과정이 반복된다.

$l-1$번째 layer에서의 output $x_{l-1}$에 대해 $l$번째 convolution 연산의 output으로 얻을 수 있는 new token map을 $f(x_{l-1})$라고 하자. 여기서의 $f$는 필터링 함수를 의미한다. 실제 official 코드를 참고해보면 convolutional embedding 부분이 다음과 같이 되어있는데,

\`\`\`python
class ConvEmbed(nn.Module):
    """ Image to Conv Embedding
    """

    def __init__(self,
                 patch_size=7,
                 in_chans=3,
                 embed_dim=64,
                 stride=4,
                 padding=2,
                 norm_layer=None):
        super().__init__()
        patch_size = to_2tuple(patch_size)
        self.patch_size = patch_size

        self.proj = nn.Conv2d(
            in_chans, embed_dim,
            kernel_size=patch_size,
            stride=stride,
            padding=padding
        )
        self.norm = norm_layer(embed_dim) if norm_layer else None

    def forward(self, x):
        x = self.proj(x)

        B, C, H, W = x.shape
        x = rearrange(x, 'b c h w -> b (h w) c')
        if self.norm:
            x = self.norm(x)
        x = rearrange(x, 'b (h w) c -> b c h w', h=H, w=W)

        return x

\`\`\`
default setting을 참고하게 되면 \`\`\`patch_size = 7\`\`\`, \`\`\`stride = 4\`\`\`, \`\`\`padding = 2\`\`\`라고 되어있기 때문에 input image의 spatial resolution $H_i \\times W_i$에 대해서 output embedding의 resolution $H_o \\times W_o는$

$$  
    \\begin{aligned}
        H_o = \\left( \\frac{H_i + 2 \\cdot 2 - 7}{4} + 1 \\right) \\newline
        W_o = \\left( \\frac{W_i + 2 \\cdot 2 - 7}{4} + 1 \\right) 
    \\end{aligned}
$$

위와 같다. 이렇게 차원을 축소하면서 embedding을 추출한 뒤, layernorm을 적용하고 난 결과를 추출한다. 이제 이렇게 추출된 <U>token</U>에 대해서 <U>attention 연산</U>을 진행하게 되는데, 기존 방식의 attention과는 다르게 Convolutional projection이 이전의 projection과 다른 점은 다음과 같다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209152520-24aa2a13-3d4d-4560-9a55-eafc4bb0f32d.png" width="900"/>
</div>


가장 왼쪽에 보이는 (a)는 ViT에서 token을 Query, Key 그리고 Value에 mapping할 때 사용했던 linear projection 방식이다. 단순히 weight $W^Q,~W^K,~W^V$를 곱해줌으로써 만들어낼 수 있다. 이와는 다르게 convolutional projection은(중간에 보이는 그림 (b)) token을 reshape 및 padding을 통해 convolution 연산이 가능한 window 구조를 만들어주고, 여기에 convolutional projection을 수행하여 query, key 그리고 value를 만들어낸다. 연산 과정에 대한 official 코드 중 일부를 가져와서 간단하게 설명하면 다음과 같다.

\`\`\`python
def forward_conv(self, x, h, w):
    # Class 토큰을 따로 분리하는 과정/ Convolution 연산은 오직 이미지에 대한 토큰에만 적용하겠다는 의미
    if self.with_cls_token:
        cls_token, x = torch.split(x, [1, h*w], 1)

    # einops의 rearrange를 통해 HW * C로 들어온 input을 C * H * W로 펴주게 된다
    x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)

    # conv_proj_ 는 모두 embedding dimension을 channel input 그리고 output으로 하는 convolution 연산
    # 참고로 convolution은 (kernel=3, stride=1, padding=1)로 사용함으로써 spatial dimension을 그대로 유지한다
    if self.conv_proj_q is not None:
        q = self.conv_proj_q(x)
    else:
        q = rearrange(x, 'b c h w -> b (h w) c')

    if self.conv_proj_k is not None:
        k = self.conv_proj_k(x)
    else:
        k = rearrange(x, 'b c h w -> b (h w) c')

    if self.conv_proj_v is not None:
        v = self.conv_proj_v(x)
    else:
        v = rearrange(x, 'b c h w -> b (h w) c')

    # 모든 연산이 끝나면 다시 rearrange를 통해 원래와 같이 쭉 펴주게 된다.
    if self.with_cls_token:
        q = torch.cat((cls_token, q), dim=1)
        k = torch.cat((cls_token, k), dim=1)
        v = torch.cat((cls_token, v), dim=1)

    # 그리고 아까 떼어냈던 class 토큰을 다시 붙여주게 되면, attention 연산에 필요한 query, key, value를 얻을 수 있다
    return q, k, v
\`\`\`
참고로 위에서 사용되는 convolutional block은 depthwise + pointwise convolution을 사용한다. 

\`\`\`python
def _build_projection(self,
                    dim_in,
                    dim_out,
                    kernel_size,
                    padding,
                    stride,
                    method):
    if method == 'dw_bn':
        proj = nn.Sequential(OrderedDict([
            ('conv', nn.Conv2d(
                dim_in,
                dim_in,
                kernel_size=kernel_size,
                padding=padding,
                stride=stride,
                bias=False,
                groups=dim_in
            )),
            ('bn', nn.BatchNorm2d(dim_in)),
            ('rearrage', Rearrange('b c h w -> b (h w) c')),
        ]))
    elif method == 'avg':
        proj = nn.Sequential(OrderedDict([
            ('avg', nn.AvgPool2d(
                kernel_size=kernel_size,
                padding=padding,
                stride=stride,
                ceil_mode=True
            )),
            ('rearrage', Rearrange('b c h w -> b (h w) c')),
        ]))
    elif method == 'linear':
        proj = None
    else:
        raise ValueError('Unknown method ({})'.format(method))

    return proj
\`\`\`
내부 함수인 projection 생성 함수를 보게 되면 알 수 있다. 실제로 논문에서 설명하기로는

- Depth-wise convolution 2d
- BatchNorm2d
- Point-wise convolution 2d

라고 되어있지만 실제로 구현되어있는 걸 보니까 두번째 단계까지는 맞고, pointwise convolution 대신 <U>linear projection을 통해</U> 각 channel 간의 correlation을 준 것 같다.   
결론적으로 Transformer based model에 비해 CvT는 더 적은 parameter 수와 FLOPS를 가지고도 더 높은 accuracy를 획득하였다. Attention 연산 시에 MLP에 의존하지 않다보니, 같은 projection을 내보내는데 더 적은 수의 parameter를 요구하기 때문이다. 그러면서도 CNN based model과는 다르게 ViT based network의 성능과 같이 높은 수치를 보여주었다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209162407-d9b17c8b-faf9-4458-987b-ee98bb0abab4.png" width="800"/>
</div>



# Multimodal using transformer

지금까지 생각보다 많은 논문들을 리뷰했다. 가장 처음에는 <U>CNN, RNN</U>부터 시작해서 <U>Sequence to sequence</U>. 그리고 이어지는 <U>attention mechanism</U>과 이를 기반으로한 <u>attention only network(transformer)</U>의 발전. 그리고 이러한 NLP에서의 성공이 vision task로 이어질 수 있었던 <U>ViT</U>의 제안 방식과 더불어 여러 한계점을 극복하기 위한 방법들(<U>DeiT, Swin-T, CvT</U>)까지 모두 살펴보았다. 그렇다면 **multimodal**의 지평을 열 수 있게 도와준 transformer가 정확히 어떤 측면에서 다양한 연구에 활용될 수 있는지 짤막하게 소개하며 이번 글을 마무리해볼까 한다.   


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209162417-87272d02-5e7d-49fd-bec4-55df5e350680.png" width="300"/>
</div>


<U>Modality</U>란 내포할 수 있는 양상이 너무 막연하기에 다양한 설명이 될 수 있다. 하지만 지금 살펴보고자 하는 '딥러닝'의 측면에서는 다음과 같이 정의할 수 있다. '<U>Modality</U>'는 vision, audio 그리고 language와 같이 특정 sensor나 관측 방법을 통해 취득할 수 있는 개별적인 <U>communication channel</U>이다. 용어가 많이 생소하겠지만 예를 들어 카메라나 LiDAR(센서) 등등 어떠한 형식의 정보만 수집할 수 있다면 이를 modality라고 표현 가능하다. Thermal 센서를 통해 취득한 열화상 이미지도 또다른 modality고, CT나 MRI 기계를 통해 취득한 의학 영상 이미지 또한 또다른 modality 중 하나가 된다.   
따라서 <U>multimodality</U>, 혹은 <U>멀티 모달</U>이라고 불리는 딥러닝의 task는 vision, text, sound, data 등등 서로 다른 취득 방식으로 획득한 데이터셋을 유의미하게 함께 활용하여 representation learning을 하고자 하는 목적에 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209162421-7299f3f4-06d3-4ad1-9fbb-d16ff22339ba.png" width="700"/>
</div>


간혹 multimodal과 cross-modal 사이에 워딩이 겹치는 문제가 있는데, 각각의 차이점은 다음과 같다. 멀티모달은 딥러닝에서 새롭게 제시된 알고리즘으로, 여러 가지의 modality를 <U>함께 활용</U>하여 학습을 하는 것이다. 예를 들어, 사람은 시각과 청각을 모두 활용해서 사람이나 특정 물체를 판별하는데, 바로 이러한 능력을 computer에 대해서 적용하고자 하는 것이다. 그와는 다르게 cross-modal은 multimodal deep learning으로 접근을 하되, 하나의 <U>modality의 정보</U>가 <U>다른 modality</U>의 <U>성능을 높이는데</U> 사용되는 것이다. 만약 고양이의 이미지를 보았다면, 고양이 울음소리를 들음으로써 '아 이 사진은 고양이겠구나'라고 판단할 수 있는 것이다.   
AI system 중 다양한 modality에 대해 함께 작동할 수 있는 모델을 multimodal이라고 부르며, cross-modal은 서로 다른 task를 활용함으로써 중간에 있는 지식을 활용하는 것이다. **여러 스타트업**이 모여있는 <U>공동 사무실에서</U> 함께 일하는 과정에서 서로 시너지 효과를 내서 모두의 사업이 성공하는 것이 **cross-modal**의 예시가 될 수 있고, 하나의 스타트업 내 <U>여러 부서</U>가 함께 co-work를 해서 각자 맡은 일을 열심히 해서 회사를 키우는게 **multi-modal**의 예시가 될 수 있겠다.   

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209162424-488ac1d5-e7fb-4b7e-aa55-91343d969211.png" width="600"/>
    <img src="https://user-images.githubusercontent.com/79881119/209162425-9ed2e379-a796-43de-b55c-6940358e2a56.png" width="600"/>
</div>

대표적인 text와 vision을 함께 활용하는 multimodal 방식은 위에서 보는 바와 같이 video captioning과 같은 캡셔닝 기술과, video question answering과 같은 reasoning 기술이다. 이외에도 비디오나 미디어의 특정 부분을 text description을 통해 찾아내는 retrieval task도 있으며, text를 적으면 video나 image를 만들어내는 기술이나 audio를 통해 video를 만드는 기술 등등 다양하게 적용될 수 있다.   
바로 이러한 측면에서 'Transformer'는 multimodal에 접근하기 가장 좋은 네트워크 구조다. 


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209162442-15f32068-0ad1-455b-b1ea-70bcaf09d8e8.png" width="700"/>
</div>

Transformer는 단순하게도 모든 형태의 input을 tokenize할 방법만 찾으면, 이를 대표할 만한 representation space로 embedding한 뒤 attention 학습을 진행하면 된다. Embedding 과정이 복잡하지도 않으며, 가장 중요한 점은 '다양한 데이터 형태'에 적용이 된다는 것이다. 그리고 Multimodal transformer에서는 <U>fusion</U> 및 <U>alignment</U>와 같은 cross-modality interation이 attention을 통해서 자동적으로 발생한다. 혹시라도 궁금한 사람은 [survey paper](https://arxiv.org/pdf/2206.06488.pdf)를 보면 잘 정리되어 있어서 좋은 것 같다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209162448-0652bcf8-93e9-4427-aa81-96085c12f147.png" width="900"/>
</div>

보라색과 초록색이 서로 다른 modality embedding이라고 생각하고 각각을 살펴보면 위와 같다. 처음부터 아예 <U>더해서 attention을 진행하는 과정</U>도 있고(a), 더하지 않고 <U>차원 단위로 붙여서</U> 연산하는 과정도 있다(b). 앞선 방법들과는 다르게 transformer layer를 <u>다르게</U> 사용한 뒤, 이후 각각의 output에 대해 <U>하나의 transformer layer로 합치는</U> 과정도 있으며(c), 오히려 처음에는 하나의 layer로 학습한 뒤에 <U>여러 layer로 분리하는 과정</U>도 있다(d). 또한 서로 다른 layer로 학습하되, 각각의 query가 <U>교차되면서</U> 상대방 layer의 attention을 학습하는 방법도 있고(e), 이러한 cross-attention을 진행한 뒤에 결과를 concatenate하는 방식도 존재한다(f). 이 여러 가지 방법들은 모두 conceptually 그럴듯하게 보이며, 각 modality의 연관성이나 문제 해결 방법에 따라 더 분화할 수 있는 구조를 가진다.   
사실 어떤 task가 어떤 방법을 쓰는지에 대해서 구체적으로 알 필요가 있는 것은 아니다. 단지 이 그림에서 시사하는 바는 '<U>Transformer 구조는 여러 modality를 함께 학습하는 과정에서 취할 수 있는 전략이 매우 다양하다</U>'라는 것이다.


# CLIP: Learning Transferable Visual Models From Natural Language Supervision

이러한 multimodal 관점에서 등장한 가장 유명하고, 또 많이 인용되고 있는 논문인 CLIP에 대해서 설명하도록 하겠다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209162453-316f70d1-ae33-4bda-9b11-2cd983ad6d6b.png" width="900"/>
</div>

학습법은 간단하게도, 특정 이미지가 있다면 이를 설명하는 text prompt가 각각 주어지고, 이를 transformer encoder를 통한 embedding으로 각각 바꾼다. image embedding과 text embedding 사이에 positive pair는 가깝게(diagonal 부분), negative pair는 서로 멀게(나머지 부분) 학습하게 되면, 최종적으로는 image에 대한 classification을 text-driven으로 학습이 가능하다는 것이다. 첫번째 contrastive pre-training 부분 다음을 보면 label text로부터 dataset classifier를 만드는게 나오는데, <U>학습 시</U>에 <U>text description</U>을 사용했기 때문에 classification 시에도 <U>비슷한 형태의 description</U>을 주기 위해 '이것은 ○○○의 사진입니다'의 형태로 넣어주게 된다.   
수많은 이미지에 대한 설명과 함께 학습하다보면, 학습 시에 사용되지 않은 classification 이미지에 대해서도 좋은 성능을 보여줄 수 있다는 것이 바로 이 논문이었고, 이후 다양한 형태로 활용되며 현재 multimodal 시장에서 가장 핫한 baseline이라고 볼 수 있다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/209162454-22fc6fb0-c39a-457e-b9a5-c25945eabd37.png" width="400"/>
</div>

녹색으로 표시된 부분이 zero-shot clip이 fully-supervised ResNet보다 더 좋은 성능을 보인 데이터셋이다. 모든 데이터셋에 대해 supervision을 가지고 학습한 ResNet보다 전혀 training sample에 대해 접근하지 못했음에도 높은 정확도를 보이는 것은 정말 혁신적이지 않을 수 없다. [CLIP 논문](https://arxiv.org/pdf/2103.00020.pdf)은 사실 실험적인 부분에서 자세하게 보고 넘어갈 부분이 정말 많아서 이후에 따로 다른 게시글에 논문 리뷰로 다룰 예정이다.
`,LO=`---
title: "About GAN supervsed alignment(GAN-gealing)"
category: "ai papers"
publishedAt: "2023-01-01"
thumbnail: "https://user-images.githubusercontent.com/79881119/210498660-f6d0b978-b4b6-49e3-9569-9ce755c9b37b.png"
---

# 논문 요약 및 소개

이번에 소개할 논문은 여러 생성 모델 분야에 대해서 살펴보다가 흥미있게 봤던 <U>GAN-supervised learning</U>이다. 이전 리뷰들은 여러 논문들을 전반적으로 살펴봤었는데, 이번에는 [이 논문](https://arxiv.org/abs/2112.05143) 하나만 집중해서 볼 예정이다. 사실 이름만 봐서는 단순히 generative network를 학습하는 것처럼 느껴지지만, GAN-supervised learning이라는 의미는 'GAN'이 만들어내는 이미지를 supervision으로 사용함과 동시에 최적화하겠다는 의미가 된다. [Classical congealing](http://vis-www.cs.umass.edu/papers/PAMI_congeal.pdf) method에 대해서 간단히 말하자면 다음과 같다.


# What is congealing?

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/210159138-2413884d-8cae-4102-9f4d-abf4589c980a.png" width="700" />
</div>

자주 봤던 MNIST(숫자 손글씨) 데이터셋이다. 단순하게 이를 0과 1로 구성된 데이터셋이라고 생각해보자. 같은 사람이 동일한 숫자를 쓰더라도, 해당 숫자는 약간의 픽셀 이동이나 기울기, 숫자의 가로 세로 크기 등등 다양한 형태를 가질 수 밖에 없다. 즉 단일 지표를 나타낼 수 있는 데이터셋의 분포 자체가 퍼져있다는 느낌인 것이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/210159184-7d57918a-2cd4-41c7-bd61-4bd1ada9111e.png" width="500" />
</div>

위의 그림은 각각 $j^{th}$ image인 $I^j$가 transformation matrix $U^j$에 대해 변형된 이미지 $I^{j\\prime}$를 의미하고, 해당 이미지에서의 각 pixel 위치에 대한 index $i$에 대한 지표화를 통해 각 이미지 상에서 동일한 위치 $i$를 $x_i^j$로 표현한 것이다. 모든 transformed digit image를 쌓아두고, 같은 위치의 픽셀들에 대해 empirical entropy(entropy estimation)을 진행한다. 

$$
    \\hat{H}(x_i) = -\\left( \\frac{N_0}{N} \\log _2 \\frac{N_0}{N} + \\frac{N_1}{N} \\log _2 \\frac{N_1}{N} \\right)
$$

위의 식에서 notation $N_0$과 $N_1$은 각각 이미지 상에서 같은 픽셀 위치에서의 흰색 부분($1$)과 검은색 부분($0$)의 빈도를 나타낸 것이다. 이를 전체 이미지 개수로 나눠주기 때문에 확률값으로 매핑이 되는 것이다. 최적화하고자 하는 방향은 이 엔트로피를 최소화하여, <U>최대한 모든 이미지에 대한 transformation이 같은 형태의 이미지를 만들 수 있게끔</U> 하는 것이다. 


# 다시 돌아와서, abstract
이러한 framework 및 ideation 과정을 확장하여 만약 unalign된 데이터셋에 대해 <U>균일한 target mode를 생성할 수 있다면</U>, STN(Spatial Transform Network)를 학습할 수 있는 **supervision**으로 GAN의 mode를 사용할 수 있지 않을까?라는 아이디어가 된다.   
실제로 이 방법을 사용하여 기존 SOTA 기준 precise correspondence(aligned된 부분의 일치하는 정도)가 약 $3 \\times$ 증가했으며, 이 방법은 augmented reality나 image editing 혹은 다양한 GAN downstream task에서 학습될 수 있는 이미지 전처리에 활용될 수 있음을 보여주었다.


# Why GAN-gealing?
사실 이 부분은 논문의 introduction 부분을 정리하고자 만든 파트인데, 굳이 제목을 '왜 꼭 GAN-gealing이어야만 했는가?'로 지은 이유는, MNIST에서 사용된 기존 congealing 방식을 적용하기엔 복잡한 RGB 이미지에 대해서는 힘들기 때문이다.   
<U>Visual alignment</U>는 워딩에서 볼 수 있듯이 시각적인 정적 영상/동적 영상에 대해 correspondence(서로 다른 이미지에서 동일한 사물 찾기 혹은 위치 적용하기 등)을 찾거나 registration하는 문제로, 주로 computer vision에서 많이 다루는 <U>optical flow</U>, <U>3D matching</U>(depth estimation과 관련됨), <U>medical imaging</U> 혹은 <U>tracking and augmented reality</U>에 적용될 수 있다.   
물론 최근 연구들로 하여금 pairwise alignment(image A로부터 image B)의 alignment는 쉬워졌지만, 여전히 모든 데이터셋에 대한 alignment는 크게 관심받지 못한 분야인 것은 여전하다. 예를 들어, 모든 데이터셋에 대해 자동으로 annotation을 해준다던지, 모든 이미지에 대해서 keypoint를 찾아낸다던지 등등 일반화된 <U>reference frame</U>이 필요한 경우에 대해서는 해결되지 못한 것이다.   
그리고 FFHQ, AFHQ 그리고 CelebA-HQ와 같이 이미 얼굴 인식이나 crop/transform을 통해 <U>align된 이미지에</U> 대해서 generative model이 학습되었을 경우에 <U>더 높은 quality의 샘플을 만들어낼 수 있다</U>는 것이 여러 연구를 통해 입증되었기 때문에, 보다 global keypoint reasoning이나 alignment가 필요하게 되었다.   
이 논문에서는 binary digit과 같은 단순한 modality에서만 적용될 수 있는 기존 congealing 방식의 아이디어에서 보다 넓은 dataset에 적용될 수 있는 GAN-gealing 방식을 제시한다.   
뒤에서 보다 자세히 설명할 것이지만 간단하게 개요만 잡고 가자면 우선 사전 학습된 <U>GAN</U>의 생성 모델을(학습은 unaligned dataset에 대해 진행) 기반으로 random sample과 target sample을 spatial transformer networks(STN)의 supervision으로 활용, <U>STN의 파라미터</U>와 <U>target sample</U>을 동시에 최적화한다. 즉 STN의 학습 과정은 GAN과는 독립적으로(exclusively) 진행된다.


# Related works

## GAN 모델의 학습
GAN 종류의 generative model은 classification이나 segmentation, 그리고 representation learning이나 3D graphics에 이르기까지 다양한 computer vision 분야에서 활동되고 있다. 이처럼 해당 논문에서는 deep generative networks의 pre-trained된 joint distribution를 활용하고자 하는 것이다. 그러나 기존 GAN의 학습 과정과 같이 GAN 구조에서 먼저 이미지를 생성한 뒤, 이를 기반으로 discriminative network를 학습하는 것이 아니라, GAN-generated image와 discriminative model을 동시에 학습하는 형태가 바로 GAN-gealing이다. 조금 헷갈릴까봐 말하자면, generative network를 학습하는게 아니라 'GAN generated image' 자체를 최적화하게 된다. 단순히 GAN이 생성한 이미지를 supervision으로 사용한 학습 형태가 되며, 그 어떤 픽셀 단위의 augmentation이나 domain knowledge에 따른 생성 데이터의 후처리를 진행하지 않고 사용한다.

## Joint image set alignment
이미지의 <U>평균</U>이라는 개념은 동일한 semantic content를 가지고 있는 image set에서의 통합적인 alignment를 시각화하기 위해 사용되었다. 대표적으로 지금 리뷰하고 있는 논문의 기초가 되는 congealing에서 unsupervised joint alignment(위에서 언급했듯, 단순히 image 간의 엔트로피를 최소화하는 방향으로 최적화를 진행)하는 방식이 이와 같다. 이러한 방법은 binary digit과 같이 형태가 잘 잡혀있는 데이터 구조에 대해서는 잘 작동하지만, 보다 다양한 형태의 데이터에 대해서는 쉽지 않다. 이런 문제들을 해결하기 위해 이후 연구에서는 고차원의 데이터셋을 low-rank의 subspace로 projection하는 연구나, image를 color, appearance 그리고 shape의 feature로 factorize해서 사용하는 등 데이터 단순화를 통한 연구가 진행되었다.   
모든 연구들은 모든 image를 하나의 mode(데이터셋이 projection되는 <U>또다른 subspace</U>라고 생각하면 됨)로 표현 가능하다고 가정한다. 또다른 연구에서는 joint visual alignment 그리고 clustering 기법을 user-driven data를 통해 해결하였다. Unsupervision으로 alignment를 하는게 아니라 bounding box supervision을 활용하고, object 카테고리에 묶이는 여러 mode에 대해 cluster를 진행한다. 이를 자동화한 방식도 있지만, 특정 domain에만 한정되는 문제가 있다. 이렇게 각 이미지의 optimization을 해결하려한 연구 중에서 warping을 network가 예측할 수 있게 했던 연구가 있고, large-scale collection에 대한 clustering과 alignment를 가능케 했지만, 여전히 simple color transformation에 국한된다는 문제가 있다. 이러한 제약은 complex dataset에 대해서 paper가 가지는 assumption을 무력화한다는 단점으로 작용한다.

## Spatial transformer networks

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/210162591-a1184fcc-93ff-47f9-bec4-ca6c21ae6cde.png" width="700" />
</div>

다음으로 소개할 내용은 이 논문에서 GAN supervision으로 학습하고자 하는 주요 프레임워크이자, deep learning based data processing 관련 논문으로 유명한 [STN](https://arxiv.org/pdf/1506.02025.pdf)이다. Deep learning framework를 사용하여 학습 가능한 geometric transformation의 파라미터를 학습하는 과정이다. Warpining에 사용되는 몇 개의 parameter를 딥러닝을 통해 학습하고, sampling grid를 input image에 대해 생성하고 warpining하는 과정을 미분 가능한 연산으로 정의하여 학습이 가능하게끔 하였다. STN framework를 모듈로 활용하여 여러 discriminative task에서 좋은 성능을 확인할 수 있었으며, robust filter learning, view-synthesis 그리고 3D representation learning에서 활용되기도 하였다. 이러한 방법들은 공통적으로 STN을 사용하여 또다른 task를 간접적으로 쉽게 만들고자 작용하는 constraint였지만, 이 논문에서는 STN을 직접 학습시키기 위해 GAN network를 사용한다는 점이 다르다. 


# GAN 기반 supervised learning

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/210162569-06a79464-3c31-48f9-9831-ce2ddb04e6a2.png" width="800" />
</div>

이제 본격적으로 해당 논문의 학습 방법에 대해 자세히 살펴보도록 하자. 설명에 사용할 supervision pair $(x,~y)$는 기존 supervised learning에서 사용했던 바와 같이 source-target 관계에 놓여있다고 생각하면 된다. 우리는 <U>'STN'</U>을 학습시킬거니까, **source image**는 정렬이 안 된 이미지(위의 그림에서 아무렇게나 위치한 고양이)라고 생각하면 되고, **target image**는 정렬이 된 이미지(위의 그림에서 우측과 같이 잘 정렬된 고양이)라고 보면 된다. 이렇게 정렬된 데이터셋을 구성하는 과정에서 GAN generator가 생성한 이미지를 사용하게 되는 것이다.   
먼저, $x$는 미리 학습된 GAN generator로부터 임의로 생성하는 sample이다. 그리고 $y$는 $x$의 latent code에 추가적으로 가해진 latent manipulation를 통해 생성된다.이렇게 pair는 STN network $f_\\theta : x \\rightarrow y$를 학습하는 supervision으로 작용한다.

$$
    \\mathcal{L}(f_\\theta,~y) = l(f_\\theta(x),~y)    
$$

여기서의 $l$은 reconstruction loss이다. 말 그대로 정렬되지 않은 데이터 $x$를 정렬된 데이터 $y$로 만들고자 하는 것. 일반적인 supervised learning에서는 여기서 데이터 $x$와 $y$가 고정이지만, 신기하게도 GAN supervised learning에서는 고정된 데이터를 STN($f_\\theta$) 최적화에 사용하는 것이 아니라 <U>target이 되는 $y$ 또한 end-to-end로 최적화된다</U>. 학습이 끝나고 나면 STN network $f_\\theta$를 사용하여 GAN generated image가 아닌 real input에 대해 테스트하게 된다.

## Dense visual alignment
그렇다면 도대체 어떤 방식으로 congealing을 진행해야 GAN-supervision을 사용할 수 있을지 알아보도록 하자. 이제부터 GANgealing이라고 불리는 알고리즘에 대한 엄밀한 분석이 들어간다.   
GANgealing은 가장 먼저 latent variable generative model $G$를 unaligned input dataset에 대해 학습하는 것으로 시작된다. 이때, input이 되는 latent vector는 $w \\in \\mathbb{R}^{512}$로 표현한다. 왜 $z$가 아니냐고 물어본다면, 나중에 말하겠지만 여기서 사용되는 generator는 styleGAN 기반이기 때문이다. 암튼 이렇게 학습된 $G$가 있다면, 여기서 source가 되는 input sample은 그냥 알고 있는 latent space $w \\sim \\mathcal{W}$에서 추출하고, generator의 생성된 샘플 $x = G(w)$로 정의하면 된다. 이제, 반대로 target image가 되는 $G(c)$를 정의한다. 여기서 $c$는 latend vector로 input에 사용된 $w$와 같은 dimension이 되며, 이를 STN의 target으로 사용하게 된다. 여기서 드는 의문점은 '$w$는 랜덤으로 뽑으면 되는데 $c$는 source가 되는 G(w)에 맞는 latent를 골라야하는데 어떻게 해요?'인데, 결국 앞서 말했던 것과 같이 $c$를 STN network 학습과 더불어 최적화하는 것이 목적이고, generator $G$는 input $c$에 대해 미분 가능한 함수이므로 gradient descent 방식을 사용해 학습 가능하다.

$$
    \\mathcal{L}_\\text{align}(T,~c) = l(T(G(w)), G(c))    
$$

여기서 $T(\\cdot) = f_\\theta(\\cdot)$으로 생각하면 된다. $l$은 두 이미지 사이에 작용되는 distance metric이 된다. 그림 상으로는 <U>perceptual loss</U>를 사용한 듯하다. 아무튼 이걸 latent vector $c$에 대해서 최소하하는 과정은 결국 <U>latent $c$로 하여금</U> 모든 생성된 이미지 데이터 $x$가 <U>spatial transformer($T$)를 통해 가까워질 수 있는 샘플</U>을 찾고자 하는 과정과 같다. 즉 $G(c)$는 모든 이미지 데이터 $x$의 alignment를 생성하고자 하는 목적으로 작용한다는 것. 만약 처음 $c$가 이런 조건을 만족하지 못하고 $T$를 통해 도저히 도달할 수 없는 이미지를 만들어낸다면, 이는 loss function의 작용에 의해 자동으로 최적화될 것이고, 같은 iteration을 계속 반복하다보면 어느샌가 $c$는 평균 이미지를 잘 생성할 수 있게 된다는 것.   
이러한 단순한 접근은 상당히 그럴듯해 보이지만 dataset의 diversity가 적은 경우에만 가능하다는 단점이 있다. 왜냐하면 아무리 학습되더라도 모든 input image $x$는 같은 constant latent $c$에 의해 생성된 이미지 $G(c)$를 타겟으로 학습되기 때문에 최적화에 한계가 있다는 것이다.   
아마 GAN inversion에 대해 실험해본 사람이면 알겠지만, 무작위로 sampling한 latent를 최적화하는 것보다는 <U>input image에 대해 가까운 sample를 기반으로 최적화하는 것</U>이 생성 이미지의 퀄리티를 높이기 좋은 걸 알 수 있을 것이다. 바로 요 아이디어를 적용하였다. 단순히 same target $G(c)$를 모든 randomly sampled image $G(w)$에 사용하는 것이 아니라, pose 및 orientation은 샘플 간에 유지하면서 디테일한 appearance는 $G(w)$를 가져가고 싶었기에, 이를 적용한 latent를 최적화한다. 사실 이 부분은 내가 작성했던 image manipulation글과 StyleGAN based GAN inversion 글을 보면 이해가 빠를텐데, StyleGAN에서 이미지를 만들어내는 방식이 <U>low resolution부터 latent를 constant에 입혀가면서 coarse style부터 점차 fine detail까지 생성하는 과정</U>이다. 따라서 적절한 부분에서 target vector $c$와 input sample을 생성할 때 사용한 random latent $w$를 조합하면, $\\text{mix}(c, w) \\in \\mathbb{R}^{512}$가 곧 $c$의 전반적인 형태를 유지한 채로 $w$가 만드는 이미지의 디테일을 살리고자 하는 것이다.

$$
    \\mathcal{L}_\\text{align}(T,~c) = l(T(G(w)), G(\\text{mix}(c,~w)))
$$

실제 실험에서는 StyleGAN2를 generator로 사용했으며, 이를 사용함에 따라 StyleGAN이 가지고 있는 style-pose disentanglement를 활용할 수 있었다고 한다. 각 샘플마다의 생성되는 target image $G(\\text{mix}(c, w))$를 style mixing을 사용하여 만들게 되고, 여기서 $c$는 synthesis generator의 첫 부분(coarse feature)을 담당하여 sample의 pose를 결정하게 되고, $w$는 later layer에 적용(fine feature)하여 texture를 결정하게 된다. 이러한 mixing point를 결정하는 것 또한 ablation을 진행하였다고 한다. 

## STN parameterization
STN 구조를 다시금 생각해보자. 논문을 읽지 않았다면 유감이지만, STN(spatial transformer network)는 다양한 연구로 뻗어가기 좋은 insight가 되기 때문에 아직 읽지 않았다면 꼭 읽어보기를 추천한다. 아무튼 spatial transformer 함수 $T$는 input으로 image를 받고, input image에 대한 sampling grid $g \\in \\mathbb{R}^{H \\times W \\times 2}$를 추출한다. 이때, 사용하고자 하는 목적에 따라 $g$의 parameter를 주게 된다. Gangealing 논문에서는 회전, scaling, 가로축 및 세로축 이동에 대한 transform을 주는 방식과, unconstrained(자유로운 변형)을 테스트해보았고, 결국 구성한 $T$는 <U>unconstrained STN</U>에 <U>similarity STN</U>(앞서 말했던 회전, scaling 등등)을 준 방식이 된다. 이러한 방식으로 저자들이 구성한 STN은 horizontal flip과 같은 변형도 가능하도록 학습되었다고 한다.   
Unconstrained $T$를 사용함에 있어서, "<U>total variational(TV)</U>" regularizer를 통한 정규화가 효과적이었다고 한다. Total variation은 보통 input에 대한 gradient를 구하고, 이에 대한 합(혹은 평균이 될 수도 있음)의 supremum으로 정의되는데, 그냥 간단히 표현하면 다음과 같다.

$$
    TV(x) = \\sum_n \\vert y_{n+1} - y_n \\vert    
$$

여기서 total variance 거리를 정의하는 방식은 경우마다 다르고, 이 논문에서 regularizer로 sampling grid $g$가 투머치 변형되지 않게끔하기 위해 다음과 같은 loss를 최적화하는 과정에서 사용했다.

$$
    \\mathcal{L}_\\text{TV} (T) = \\mathcal{L}_\\text{Huber} (\\Delta_x g) + \\mathcal{L}_\\text{Huber} (\\Delta_y g)
$$

식에서 알 수 있듯이, $g$의 각 방향($x,~y$)에 대한 partial derivative를 계산한다. 이에 추가로, regularizer $g$가 identity transformation으로부터 크게 벗어나지 않게 하기 위해 파라미터에 대한 norm을 다음과 같이 준다.

$$
    \\mathcal{L}_I (T) = \\parallel g \\parallel^2_2
$$

## Parameterizationn of $c$
앞서 target이 되는 이미지를 만들기 위해 사용되는 latent vector $c$를 언급했었는데, 이 $c$를 단순히 미분 가능한 generator network $G$를 통해 연산된 backpropagation을 진행하는 것이 아니라, 조금 다른 방법을 쓴다. 그대신, $\\mathcal{W}$ space의 top-$N$ principal direction의 linear combination으로 정의하는 형태로 parameterize한다.

$$
    c = \\bar{w} + \\sum_{i=1}^N \\alpha_i d_i    
$$

여기서 principal direction은 우리가 많이 알고 있는 PCA(주성분 분석)을 통해 추출된 축과 같은 의미다. 위의 식에서 $\\bar{w}$는 $w$ vector의 empirical mean이고, $d_i$는 $i$번째 principal direction이다. 그리고 $\\alpha_i$는 학습 가능한 스칼라 coefficient로, 주성분 방향 $N$개에 대해 각각 따로 적용되는 파라미터라고 보면 된다. 이런 방식으로 $c$를 최적화하는 이유는 StyleGAN의 $\\mathcal{W}$ space가 표현력이 뛰어나다는 특징을 가지기 때문이다. 만약 추가로 constraints를 주지 않고 $c$를 최적화하게 되면 <U>target image의 품질이 저하</U>되고, natural image(실제 이미지)가 분포하는 <U>manifold로부터 멀어지는 경향성</U>을 보인다. 주성분 축 $N$의 개수를 줄인다는 것은, 그만큼 $c$의 feasible direction을 줄이는 것이 되고, 이를 다르게 풀어쓰면 보다 $\\mathcal{W}$-mean space에서의 constraint를 주는 것과 같기 때문에 바로 앞서 언급한 manifold 문제를 해결할 수 있다.   
최종으로, 이  논문에서 사용한 GANgealing objective는 다음과 같이 정리할 수 있다.

$$
    \\mathcal{L}(T,~c) = \\mathbb{E}_{w \\sim \\mathcal{W}} (\\mathcal{L}_\\text{align} (T, c) + \\lambda_\\text{TV} \\mathcal{L}_\\text{TV} (T) + \\lambda_I \\mathcal{L}_I (T))
$$

실험에 사용했던 hyperparameter인 람다는 $\\lambda_\\text{TV} = 1000,~2500$이고, loss weight $\\lambda_I$는 1이다.

# Joint alignment and clustering
GANgealing을 지금까지 설명해오면서 묘사했던 것은 LSUM Bicycles나 Cats와 같은 여러 multimodal data에 대해 잘 적용할 수 있다는 것이다. 그러나 LSUN Horses와 같은 몇몇 데이터셋처럼 data에서 <U>single mode로 표현되기 힘들 정도로 다양한 pose를 가진 경우</U>가 있다. 이러한 어려운 상황에 대처하기 위해 GANgealing 기법은 단일 target latent $c$에 대해 학습하는 것이 아닌, clustering algorithm에 적용될 수 있다.   
예를 들어 우리가 학습시키고자 하는 $c$ vector의 개수를 $K$라고 하자. 각 latent constant $c$는 data의 특정 mode를 포착하게 되므로(여기서 mode는 쉽게 이해하자면, 특정 데이터가 가지고 있는 feature를 풀어쓴 것으로 이해하면 된다) 여러 개의 $(c_k)_{k=1}^K$를 학습하는 것이 dataset에 대한 여러 mode를 학습할 수 있는 방법이 된다. 앞서 언급했던 것과 같이 $c$를 최적화하는 방식은 $\\mathcal{W}$ space의 top-$N$ principal direction의 linear combination으로 학습되므로, 각 $c_k$에 대해서 상승되는 $\\alpha$ coefficients가 학습된다. 이와 유사하게 논문에서 사용한 방식은 $K$개의 개별적인 Spatial Transformer $T_k$가 있고, 각 spatial transformer가 각 mode를 학습하는 형태가 된다. 이런 식으로 수정한 GANgealing 구조는 각 cluster에서 모든 이미지 사이의 dense correspondence를 학습할 수 있게 된다. 여기서 중요한 건 그럼 각 $c_k$와 그에 상응하는 $T_k$ pair가 특정 mode에 대해 최적화되게 guidance를 해줘야하는데,

$$
    \\mathcal{L}_\\text{align}^K (T, c) = \\min_k \\mathcal{L}_\\text{align} (T_k, c_k)    
$$

이를 위해 hard-assignment step을 주어 각 mode에 따라 unaligned image를 align할 수 있게 하였다.   
$K = 1$인 케이스는 계속 설명해왔던 unimodal case에 속한다고 생각하면 된다. Test time에는 input fake image $G(w)$를 상응하는 cluster index인 $k^\\ast = \\arg \\min_k \\mathcal{L}_\\text{align} (T_k,~c_k)$를 찾고, 이에 상응하는 spatial transformer $T_{k^\\ast}$로 warping하게 된다. 그러나 문제가 되는 것은 바로 실제 이미지에 대해서 연산을 해야할 경우인데, 왜냐하면 mode assignment을 하는 과정은 앞서 말했던 바와 같이 $\\mathcal{L}_\\text{align}$을 계산해야하고, 이는 <U>input image</U>가 아닌 input image를 생성한 <U>$w$ vector</U>를 알아야하기 때문이다.   
사실 처음에 이 부분이 이해가 잘 안됐는데, 식을 다시 살펴보니 $K$에 대해 clustering을 한 목적 자체는 여러 modality에 대한 최적화를 위함이고, 각 $c$는 결국 input image에 대한 latent $w$와 함께 mixing 및 최적화되는 구조가 되므로 최종적으로는 real image에 대한 연산 과정에서도 latent variable을 알아야한다는 것이다. 이를 해결하는 가장 직관적인 방법으로는 GAN inversion이 있고, 이는 앞서 작성했던 [GAN 관련 게시글](https://6unoyunr.github.io/blog/imagemanipulate)에서도 언급했었지만, image $x$에 가까운 샘플을 만들 수 있는 latent variable $w$를 찾는 것이 목적이다. 하지만 StyleGAN은 FFHQ(얼굴 이미지) 데이터셋으로 학습되었기 때문에 <U>얼굴이 아닌 dataset</U>의 정확한 GAN inversion은 상당히 challenging하고 느린 작업이다.   
따라서 저자들은 조금 다른 방법을 사용했는데, 그것은 바로 input image의 cluster assignment를 classification 결과로 낼 수 있는 네트워크를 학습한 것이다. 간단하게도 input fake image에 대한 target cluster $k^\\ast$를 데이터셋으로 사용 가능하므로 $(G(w),~k^\\ast)$를 하나의 supervision으로 학습하게 되면 굳이 GAN inversion을 통하지 않고서도 임의의 이미지에 대해 assignment가 가능해진다. Classifier는 spatial transformer의 weight를 사용하되, warping head를 classification head로 사용하여 최적화한다. 이러한 방식을 통해 fake sample에 대해 최적화된 sptial transformer, classifier 모두 real image에 좋은 일반화 성능을 보여주었다. 


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/210498660-f6d0b978-b4b6-49e3-9569-9ce755c9b37b.png" width="1000" />
</div>



# 끝으로...

여기까지가 GANgealing에 대한 설명이었고, 실험 내용이나 결과에 대해선 언급하지 않고 끝내도록 하겠다. 사실 최근에 계속 논문을 읽고자하는 방향은 논문에서 구체적으로 어떤 실험을 통해 어떤 결과를 내었는지보다는, 이 논문의 아이디어를 디벨롭하는 과정에서 사용된 insight(related works, preliminary)와 수학적 배경이 조금 더 중요하다고 생각했기 때문이다. 아무튼 결론적으로 말하자면 이 논문은 GAN을 학습한 연구가 아닌 GAN을 이용해 STN을 학습, 이를 통해 효과적으로 unaligned real image를 alignment할 수 있는 방법을 제시했다는 것이 주된 contribution이 될 것 같다. 굉장히 흥미로운 주제였고 광범위하게 사용될 수 있을 것 같다.
`,PO=`---
title: "CLIP(Learning transferable visual models from natural language supervision)에 대하여"
category: "ai papers"
publishedAt: "2023-01-22"
thumbnail: "https://user-images.githubusercontent.com/79881119/213904183-1ff81e1a-0a92-42f2-8742-bc87e5dd69e6.png"
---


# What is modality?
논문 소개에 앞서 <U>multimodal</U>이라는 의미에 대해 짚고 넘어가기 위해 먼저 'modality'의 의미에 대해 확인해보자. 일반적으로 modality가 가지는 의미는 크거나 작게 구분이 가능하다. 만약 확률 분포에 대해서 생각해보면, modality는 <U>하나의 probability density function</U>를 나타내며, 이때 multimodal은 <U>서로 다른 peak</U>(local maxima)를 가지는 두 개 이상의 mode를 의미한다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213900325-9960901d-7940-4231-834b-c9052dc051e7.png" width="400"/>
</div>


하지만 위의 내용은 단순히 확률 분포를 가정한 수학적 접근(해석)에 대한 이야기이고, 실제로 우리가 **딥러닝**에서 이야기하고자 하는 부분은 <U>sensory</U>(감각)과 맞닿아 있는 경우가 많다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213900373-ac1c69a4-5919-4a9d-a020-375e67560f91.png" width="400"/>
</div>


인간에게 있어 감각에는 여러 modality(크게 해석해서, 형태나 모양이라고 보면 된다)인 청각, 촉각, 시각 등등이 있으며 이러한 다양한 감각들이 상호작용하여 맞닿아있는 중간 부분이 사람으로 하여금 <U>추론의 근거</U>로 사용된다. 즉 multimodal이란 소통의 방식이 될 수도 있고, 진화심리학에 따른 <U>특정 행동양식의 기준</U>이 되기도 한다.   
예를 들어, Verbal(언어)에 있어서는 단어 각각의 의미를 서술하는 <U>Lexicon</U>이 있고, Speech의 한 부분을 담당하며 각 단어의 유기적인 관계를 표현하는 <U>Syntax</U>(문법)이 있기도 하며 어떠한 context가 진행되는 사람 간의 관계, 시간과 장소 등 언어의 의미 분석에 대해 다양한 요소를 고려하는 <U>Pragmatics</U>(화용론)도 있다.   
이처럼 언어 이외에도 Visual(시각) 요소에는 <U>gesture</U>, <U>body posture</U>나 <U>proxemics</U>(인간의 사회적이나 문화적인 환경 및 공간과의 관계를 통해 정의된 신체 언어) 등이 있다. 더불어 사람과 사람 간의 eye contact, facial expression도 이러한 **modality**의 한 형태로 볼 수 있다.   
앞서 설명한 내용은 내부분 정보를 수집하는 각각의 신체 기관 및 정보 전달의 수단과 관련된 구성 요소들에 살펴보았고, 이제 조금 더 좁은 의미의 modality에 대해 정의하면 다음과 같다.


# Modality in computational environment

Modality는 무언가가 경험되거나 발생하는 방식 및 수단이다. 따라서 <U>컴퓨팅 환경에서 이해하는</U> modality는 특정 형태의 정보를 의미하기도 하며, 정보가 저장되는 대표적 형태로 이해할 수 있다. 예를 들어 컴퓨터에 어떤 이미지가 저장된다고 했을 때, **'Image' 자체**를 modality로 볼 수도 있고 보다 상세하게 분류해서 'jpeg', 'bmp', 'png' 등 **이미지 저장 및 압축 방식**에 따라 modality를 구분할 수도 있다.   
앞서 본 인간의 감각과 관련된 sensory modality나 수학적 확률 분포에 근거한 probability modality와 그 결을 함께하여, modality(혹은 medium, media)는 어떤 방법이나 기구를 통해 정보를 저장하고 소통하는 것을 의미한다. 즉 <U>통산 및 의사 소통의 시스템</U>으로 이해할 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213900749-965f0ba0-5878-41e5-b53a-c3e5b08230cc.png" width="800"/>
</div>


세상에는 <U>여러 종류의 communities</U>(단체)로 인식된 사회/문화 상의 **플랫폼들**이 잇고, modality는 그 상에서 다양한 방식을 통해 정의되는 정보나 소통 방식에 대한 분류이다. <U>deep learning</U>과 크게 관련이 있는 **Natural
language**(spoken or written), **Visual**(images, videos) 그리고 **Auditory**(voice, sounds and music)가 그러한 분류 중 하나에 속하며, 이때의 multimodal이란 두 개 이상의 modality를 함께 활용하여 어떠한 task를 풀어나가는 것을 의미한다. 단순히 우리가 알고 있는 text와 image를 함께 사용하는(완전히 결이 다른 modality) task뿐만 아니라, <U>modality만 구분된다면</U>(ex. 사람의 목소리 + 악기 연주) 모두 multimodal로 간주할 수 있다.


# Learning Transferable Visual Models From Natural Language Supervision

앞서 multimodal 설명을 굳이 길게 하고 이제야 논문 리뷰를 시작하는 이유는 이 paper의 main contribution 중 하나가 '<U>text와 image 간의 유의미한 관계</U>'를 찾는 것에 집중했기 때문이다. ~~무려 48쪽이나 되는 분량을 잡아먹는~~ 이 논문은 워낙 유명하기 때문에 다양한 블로그에서 리뷰를 했고, 유튜브를 찾아봐도 강의 영상이 정말 많은 것을 알 수 있다. 본인이 이 글에서 집중하고자 하는 것은 [이 논문](https://arxiv.org/abs/2103.00020)이 가지는 <U>paper</U>나 <U>방법론</U>에서의 장단점, 그리고 contribution이다. 


# NLP task의 발전과 CV task의 한계점
**Computer vision task**와 **Natural language processing(NLP)** deep learning은 발전 방향에 있어 차이가 있었다. 처음에 그 시작은 CNN(Convolutional Neural Network)를 baseline으로 recurrent neural network이 개발되었지만, attention mechanism 및 transformer가 소개되며 기계 번역과 관련된 task의 성능이 크게 향상되었고, <U>transformer의 encoder 및 decoder 구조를 활용</U>한 GPT나 BERT model이 발전하며 **많은 downstream task**의 성능이 크게 향상될 수 있었다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213901161-30db60a8-bb44-46c8-9f42-ea2e45de38fe.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/213901191-b4994362-af6a-4685-bcf0-9a9924ec3b1a.png" width="400"/>
</div>


컴퓨터 비전과는 다르게 이러한 대규모 언어 모델이 발전할 수 있었던 가장 큰 이유 중 하나는 <U>데이터셋 수집의 용이성</U>이었다. 보통 영상(image)과 관련된 task를 해결하려면 이에 맞는 domain을 정의하고, 각 domain에 맞는 distribution을 한정하여 이에 맞는 데이터를 수집 및 라벨링하는 과정을 거치게 된다. 예를 들어 강아지를 분류하는 task를 수행하고자 한다면, 분류하고자 하는 강아지의 사진과 각 사진에 대해 강아지 품종을 매칭하는 과정을 거친다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213901272-c17978e2-d803-4e0f-b7d8-4b80fa4ca7cb.png" width="800"/>
</div>


하지만 여기서 **image dataset** 수집의 한계가 드러난다. 만약 분류하고자 하는 강아지 품종이 수천/수만 가지가 된다면 이에 맞는 index labeling이 되어야하고, 무엇보다 만약 사진에 강아지보다 background 비중이 높다거나(사진마다 object의 크기가 다르기 때문), 강아지 여러 마리가 포함된 사진 등등 <U>데이터셋 분포의 일관성 및 퀄리티</U>를 보장할 수 없기 때문이다. 또한 위의 그림처럼 열심히 데이터를 모아서 distribution을 맞춘 dataset을 구성했는데, 추가적으로 '시바견'을 구분해야한다면 해당 class에 대한 <U>데이터셋 구축을 처음부터</U> 해야하기 때문이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213901370-3d8a3d22-f504-4aaf-bcc4-74cfa9f3cf82.png" width="800"/>
</div>


이는 단순히 웹 상에서 **다량의 데이터셋**을 획득하고, 획득한 다양한 <U>text prompt</U>를 <U>token 단위로 supervision에 활용</U>할 수 있는 NLP 모델과 큰 차이가 있다. 이렇듯 computer vision에서 퀄리티, 분포를 모두 고려한 유의미한 데이터셋 구축 과정을 '<U>gold labeling</U>'이라 부르며, 이러한 gold label 없이는 **computer vision task**에서 deep learning의 좋은 성능을 기대하기 힘들다.   
또한 NLP와 CV에서 차이가 발생하는 것이 new task에 대한 zero-shot 및 few-shot 성능에 대한 부분이다. 앞서 설명했던 것과 같이 자연어 모델의 경우 웹 상에서 다량의 데이터셋을 구축 가능하며 분포가 무한에 가까운(continous signal) 이미지와는 다르게 언어는 인간이 사용하는 vocabulary나 문장 내에서 대부분의 형태나 variation이 있기(discrete signal) 때문에 <U>대용량의 데이터를 통해 학습된 대용량 네트워크</U>는 다른 **task**에도 쉽게 적응이 가능하다.


# Restricted form of supervision
이러한 SOTA computer vision system의 한계점(미리 정의한 object category에 따른 데이터만을 학습할 수 있고, 실제로 데이터 수집 단계에서 이러한 방법 이외에는 사용할 수 없다는 것)을 이 논문에서는 해결해야할 문제로 제시하였다. 이를 해결하지 못한다면 <U>NLP 모델</U>이 가지는 **generality**(일반화 성능), **usability**(다양한 downstream task에 활용할 수 있음)을 CV에서는 영원히 가져갈 수 없다는 것이다.   
따라서 CLIP 논문에서는 <U>image 학습에 raw text를 함께 활용</U>하여, image supervision 자체를 학습하는 것보다는 text prompt와의 관계성을 통해 **text representation**과 **image representation**을 연결하는 방식을 사용했다.   
이는 데이터 수집 단계에서도 한계가 있었던 기존 computer vision 접근 방식에도 적용될 수 있는데, 웹에서 이미지를 수집하고 이에 맞는 <U>class categorize</U>를 진행하는 것보다는, 웹 상에서 자주 검색되거나 반복되는 **text prompt**를 기반으로 이를 묘사하는 이미지를 검색하게 되면 <U>굳이 domain distribution을 고려한 dataset 구축 없이도</U> 학습이 가능하기 때문이다. 예를 들어 위키피디아 상에서 많이 언급된 단어들 중 하나가 '<U>귀여운 고양이</U>'라면, 해당 text를 기반으로 크롤링한 image를 '귀여운 고양이'라는 text와 연관짓는 학습만 진행하면 되기 때문이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213902189-7c92b1f4-776c-4334-a980-0a461eedbb02.png" width="800"/>
</div>


기존 방식과 비교한 형태는 위의 그림과 같다. 데이터셋 구축 단계에서 task를 정의하는 부분이 빠지고, <U>image/text의 유기적 학습</U>을 위한 **text prompt**를 설정하고 이를 토대로 데이터를 수집하는 과정이 추가되었다. **Text prompt**에 대한 데이터를 수집했기 때문에 기존의 <U>task based dataset 수집</U>과는 다르게 라벨링에 **추가 과정**이 들어가지 않는다.


## Task-robust representation learning with CLIP
사실 이미지와 텍스트 사이의 유기적인 관계를 학습하고자 한 연구는 CLIP 이전에도 존재했었다. <U>머신러닝 기반</U>으로 접근했던 방식으로는 **content based image retrieval**(특정 문서와 함께 주어진 이미지를 기반으로 nouns, adjectives를 예측하는 task), **caption prediction with manifold learning** 그리고 **multimodal deep Boltzmann machine**을 활용한 low level image와 text tag feature의 학습이 있다. 이후에도 CNN based approach나 Transformer based approach로 접근한 <U>deep learning 기반</U>의 방법들이 제시되었지만, 여전히 '<U>image representation learning</U>'이라는 관점에서 natural language 방식들은 <U>zero-shot learning</U> 성능을 끌어올리지 못하고 있었다. 이렇듯 CLIP 이전의 deep learning 방식들은 대부분 JFT-300M dataset과 같이 대용량 데이터셋을 활용하는 방식이라던지, 관련된 prompt를 instagram hashtag로 사용하여 대용량의 dataset으로 학습하는 등 앞서 설명했던 기존 computer vision 방식의 **gold-label**과 **unlimited amounts of text prompt** 기반으로 수집한 데이터셋 중간 과정에 놓인 연구들이다. 여전히 한정된 supervision을 가지고 있으며, 고전적인 softmax classifier에 의존한 학습 때문에 representation의 유연한 학습과 zero-shot 성능을 제한하는 요소가 되었다.   
이러한 weakly-supervised method와는 다르게 [VirTex](https://arxiv.org/abs/2006.06666), [ICMLM](https://arxiv.org/abs/2008.01392) 그리고 CLIP 논문이 가장 큰 insight를 얻었던 [ConVIRT](https://arxiv.org/abs/2010.00747)의 경우에는 이 논문의 방향성과 비슷하게 language 정보를 사용하여 image representation을 학습한다. 하지만 이러한 연구들에서는 <U>대용량의 데이터를 사용해보지 않았다</U>는 점에서 CLIP 저자들이 <U>ConVIRT</U>(참고로 ConVIRT는 medical diagnosis와 관련된 task라 데이터셋이 한정적이다)의 컨셉을 그대로 이용하되, 대량의 WebImageText dataset을 활용하여($400M$) scratch부터 contrastive representation learning을 진행한 것이다.   
학습 과정에서 저자들은 가장 효율적인 학습이 가능한 방법론을 탐색했으며, 결론적으로는 자연어 모델인 <U>GPT 시리즈</U>와 같이 <U>다양한 task에 적용 가능한</U> 네트워크를 학습시킬 수 있었다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213903087-38687ef4-47e8-43ef-ad4c-0d5b8c209277.png" width="600"/>
</div>


Image-captioning baseline인 transformer language 모델을 학습하는 것보다 <U>bag of words</U> prediction(BoW)을 학습하는 것이 더 효율적이고, bag of words prediction과 CLIP(constrastive learning)을 함께 활용한 것이 **zero-shot** 성능을 높이는 데에 있어 약 $12$배의 학습 효율이 나타났다고 한다.


# Several approaches

앞서 길게 설명하고 넘어오긴 했지만, 해당 논문에서 **approach**를 분류하여 설명한 부분이 사실상 <U>main contribution</U>을 알아보기에 가장 적합한 구간이라고 생각되어 각 요소마다 간단하게 요약하고 넘어가도록 하겠다.

## NLP supervision
기존의 <U>text와 image pair를 학습했던 연구</U>들은 대부분 unsupervised, self-supervised, weakly-supervised 등의 방법을 토대로 <U>natural language supervision</U>을 사용했기 때문에 image representation 학습에 text 정보를 직접 연관지을 순 있었으나 NLP가 가지는 정보를 제대로 활용하지 못했다. 하지만 NLP는 training 과정에서 $1$ to $N$ mapping이 필요없으며, categorized된 labeling에 비해 활용할 수 있는 정보가 많다는 장점이 있다. 따라서 이 논문에서 집중하고자 한 내용은 NLP를 supervision으로 활용해서 image를 통해 text를 예측하는 형태로 image/text multimodal learning을 진행하자는 기존 concept에서 벗어나, **NLP representation**에 **image representation**을 연결만 시켜주자는 concept을 사용하였다.

## Dataset 수집
MS-COCO, Visual Genome 그리고 YFCC100M과 같은 데이터셋은 수가 현저히 적거나, 대용량의 데이터셋이라 하더라도 image의 metadata(ex. image filename)을 이용한 정보 추출이 어려웠기 때문에 대용량 데이터셋 구축이 힘들었다.   
따라서 NLP supervision에서 수집하는 web based 방식에서 motivation을 얻어, 인터넷 상에서 많이 사용되는 query(text prompt)를 augmentation하여 $500,000$ 만큼 준비했으며, 각 query에 맞는 (image, text) pair를 최대 $20,000$ 씩 구축하였다. 결국 목표로 했던 **GPT-2 training dataset**인 '<U>WebText</U>'의 총 갯수에 필적하는 대용량 데이터셋인 '<U>WebImageText</U>'를 구할 수 있었다.

## Selecting an efficient pre-training method

그러나 데이터셋 크기가 커진만큼 학습에 걸리는 시간 또한 길어졌기 때문에, 빠른 수렴을 위한 효율적인 pre-training method를 찾는 것이 중요했다. 저자들은 이 부분에서 시행착오를 겪은 여러 방법들에 대해 언급한다.

#### Jointly trained an image CNN and text transformer from scratch
VirTex 논문에서와 같이 접근한 경우가 된다. 하지만 이 방법을 사용하게 되면 위에서 봤던 그래프(transformer based learning)와 같이 간단한 baseline인 **ResNet과 BoW encoder를 사용**했을 경우보다 수렴 성능이 낮은 것(약 $3$배)을 확인할 수 있다.

#### Bag or Words with simpler baseline
하지만 BoW 방식과 transformer 방식의 차이는 구조(parameter 개수, CNN and transformer)에 있을 뿐 결론적으로 두 방식 모두 공통적으로 각 이미지에 대해 exact words를 찾고자 하는 <U>NLP supervision concept</U>에서 벗어나지 못했다.   
이는 단순히 이미지를 표현하는 방식을 단일의 description에 한정지었기 때문인데, 예를 들어 다음과 같은 이미지가 있다고 생각해보자.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213903991-7ee64c92-1253-4f0e-b06d-0119cade64cb.png" width="500"/>
</div>

이 사진을 묘사하는 text prompt는 단순히 '고양이 두마리가 있는 이미지'만 있는 것이 아니라, '서로 좋아하는 두 고양이', '초원에 서 있는 두 고양이' 등 <U>관련된 text embedding space</U>에서 **유사한 영역에 놓일 수 있는** 모든 representation의 총체가 될 수 있다. 결국 contrastive learning(관련이 있는 text embedding과의 유사성을 올리고, 관련이 없는 text embedding과의 유사성을 낮추는) 방식이 더 효율적인 학습 효과를 불러왔으며 바로 이 방법이 CLIP에서 사용한 학습법이다.

## Contrastive learning
만약 $N$개의 (image, text) pair의 batch가 있다고 해보자. CLIP은 $N \\times N$의 가능한 (image, text) pair에 대한 **prediction**을 진행한다. 정답이 되는 text prompt는 batch 내에 image당 하나씩 총 $N$개 있기 때문에, $N \\times N$ prediction에서 $N$은 positive pair가 되고 나머지 $N^2-N$은 negative pair가 된다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213904183-1ff81e1a-0a92-42f2-8742-bc87e5dd69e6.png" width="500"/>
</div>


위의 그림과 같이 각 sample embedding에 대한 <U>symmetric cosine similarity</U>를 matrix로 표현 가능한데, 이를 normalize한 cross-entropy loss를 최적화한다고 생각하면 된다. Batch 단위에서 similarity를 기반으로 cross-entropy loss를 사용하는 형태는 multi-class $N$-pair loss, InfoNCE loss 등의 연구로부터 소개되었고, 이 논문에서도 **같은 메커니즘**을 사용하였다.   

## Training from scratch
데이터셋 개수가 $400M$이므로 overfitting을 걱정할 필요가 없다. 따라서 네트워크를 pre-training하는 과정에서 따로 representation과 관련된 initializing 없이 바로 scratch부터 학습을 진행하였으며 crop 이외에는 data augmentation을 진행하지 않았다. 또한 multi-modal embedding space로 mapping하는 과정에서 non-linear projection을 진행하지 않았는데, 이는 self-supervised learning에서 사용되던 방식과는 다르게 여기에서는 <U>해당 training efficiency</U>와 관련된 method를 사용할 필요가 없었기 때문에 연산량을 줄인 것으로 보인다. 그리고 entropy 조절에 사용되는 temperature normalizing hyperparameter $\\tau$는 직접 설정하게 되면 ablation 진행이 어려웠기 때문에 학습 가능한 parameter로 설정하였다.

## Network selection
BoW baseline으로 사용된 ResNet-50을 CLIP에서도 사용했으며, 이에 [ResNet-D](https://arxiv.org/pdf/1812.01187v2.pdf) 논문을 통해 제시된 다양한 improvement 또한 encoder로 사용하였다. 또한 image embedding의 dimension을 맞추기 위해 feature extraction의 global average pooling을 attention pooling layer로 대체하였다.   
ResNet 구조 이외에는 ViT(Vision Transformer)에서 조합된 patch와 position embedding에 추가 layer normalization을 더한 것 이외에는 동일한 구조를 사용하였다.   
Text encoder로는 [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)를 사용하였고, base size로는 $12$개의 layers의 $512$의 channel dimension width를 가지는 네트워크를 사용하였다(attention head는 $8$개).


# Results
사실 이 논문이 방대하게 길어진 이유 3가지는 다음과 같다.

1. Related works가 너무 많다.
2. 저자들이 주장하는 contribution이 방대하다.
3. 컨셉 하나에 대해 설명하는 과정(빌드업)이 너무 크다.

그러다보니 실험 결과도 자연스럽게 길어지게 되었고, 논문을 모두 읽으면서 파악하기에는 다소 무리가 있지 않나 싶다. 따라서 비교적 <U>주요한 내용들만 위주로</U> figure를 참고해서 작성해보았다.

## Zero-shot with prompt engineering

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213904974-bec5109c-1bf0-4a67-8347-91b0c299e048.png" width="400"/>
</div>

우선 CLIP이 주로 다룬 내용은, **자연어 모델**과 같이 **이미지 모델** 또한 충분한 데이터셋을 기반으로 representation learning이 진행된다면, 이에 따라 <U>새로운 task에 대한 performance</U>도 **zero-shot**으로 진행될 수 있음을 증명하는 것이었다. 새로운 task(classification)에 대해 downstream task를 진행하는 과정은 다음과 같다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213905079-04fba334-510c-4708-ab1e-a83b8c1bc098.png" width="500"/>
</div>


여기서 저자들은 단순히 class 이름을 prompt로 사용하는 것이 아닌, '<U>A photo of a {object}</U>' 식의 prompt engineering을 각 dataset 특성에 맞게 진행하여 성능을 올릴 수 있었다고 한다. 학습에 사용되는 (image, text) pair 자체가 object를 표현하는 단어로 구성된 것이 아니라, <U>image scene을 설명하는 description</U>이기 때문에 학습된 **domain distribution**과 **class name distribution**에 차이가 있다는 것이 주된 분석이었다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213905043-21a4ccaa-184a-4d46-8655-c98ac5ffd531.png" width="500"/>
</div>


$N$개의 class에 대한 text prompt $N$개 와의 similarity를 구할 수 있고, 이 중 가장 높은 유사도의 index가 해당 image의 prediction이 되는 구조다. 실제로 **prompt engineering**을 통해 <U>classification 성능</U>(average)이 올라가는 것을 확인할 수 있다.

## Zero-shot vs fully-supervised baseline


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213905261-bb97911c-d308-46b4-b911-242a256b5bcd.png" width="350"/>
</div>


ResNet-50을 baseline study에 사용했기 때문에 linear probe와의 성능 비교가 가능한데, 위의 표에 있는 총 $27$개의 dataset을 fully-supervision에 사용한 결과보다 $16$개의 dataset에서 더 좋은 성능(positive $\\Delta$)를 보여주는 것을 확인할 수 있다.

## Zero-shot vs Few-shot


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213905359-aff9e1f2-6790-4283-9644-f744974df527.png" width="500"/>
</div>


마찬가지로 여러 few-shot methods에 대해 <U>일정 수의 샘플을 제공</U>했을 때 중에서 가장 최고의 성능을 보인 그래프끼리 비교하면 zero-shot CLIP이 4-shot method에 필적하는 결과를 보여주고, 심지어 <U>BiT-M, SimCLRv2</U> 등 기존 방식은 **16-shot**을 진행했음에도 **Zero-shot CLIP이 획득한 정확도**를 넘기기 어려운 것을 확인할 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213905487-ccc29be3-832e-4a68-9fff-c82e28244278.png" width="500"/>
</div>


<U>CLIP의 zero-shot</U> 성능에 필적할 만한 성능을 보이기 위한 few-shot sample의 수를 나타낸 표를 보면 대부분의 dataset에 대해서 평균 $20.8$의 sample이 필요한 것을 알 수 있으며, 이를 토대로 CLIP method가 가진 **data efficiency** 및 **task robustness**에 대해 확인할 수 있다.

## Zero shot performance & Linear probe


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213905589-43e5a2ec-1605-45b9-b596-fc0278564a4d.png" width="500"/>
</div>


또한 논문에서는 단순히 CLIP이 '텍스트와 이미지만 대충 유사하게 connection'한 것이 아니라, classifier 기반의 linear probe system에서 학습 가능한 image representation을 image encoder가 동일하게 학습이 가능하다는 점을 위의 그래프를 통해 보여준다. <U>Zero-shot 성능이 좋지 않다면</U> 마찬가지로 <U>Linear probe 성능도 좋지 않다</U>는 경향성을 보여줌으로써, classification 학습이 어려운 task에 대해 **CLIP도 성능이 낮아지는 결과**를 보여주었다.

## Zero shot performance with network scale


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213905665-6b1e381e-eae6-4b74-8742-6596d334146f.png" width="500"/>
</div>

또한 **기존 방식들**(ex. EfficientNet)에서 주장했던 경향성과 동일하게 image encoder의 representation power가 늘어날수록(network의 scale이 커질수록) 이에 따른 학습된 <U>CLIP의 zero-shot 성능이 높아지는 것</U>을 알 수 있다.

## CLIP is also a good representation learner

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213905803-ed18c7b2-8dfb-4017-8290-d026b85e3ae2.png" width="1000"/>
</div>


학습 과정에서의 **GFLOPS/Image**에 따른 **Average score** 비교는 위의 그래프와 같다. 위의 결과 그래프는 zero-shot을 비교한 것은 아니고 실제로 CLIP이 pre-trained 과정에서 학습한 representation이 <U>Linear probe</U> 성능에 얼마나 도움이 되는지 확인한 절차이다. 실제로 그래프상 가장 좋은 성능을 보인 CLIP network와, 함께 비교한 network 중 가장 높은 representation learning 성능을 보인 <U>EfficientNet-NoisyStudent</U>와 여러 데이터셋에 대해 성능 비교를 하였다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213905957-a00dd72b-9d84-44c5-a202-b33e76587155.png" width="350"/>
</div>


물론 <U>ViT(L/14) 기반으로 학습된 CLIP</U>이기 때문에 당연히 더 좋겠다고 예상하긴 했지만 아무튼 $27$의 dataset 중에서 $21$ 만큼의 데이터셋에 대해 더 좋은 성능(positive $\\Delta$)를 보여주었다.

## Robustness to natural domain shift
물론 supervision을 활용한 deep learning network들의 성능이 충분히 좋아지긴 했으나, 여전히 distribution shift와 같은 문제에 취약한 면이 있다. 예를 들어 단순히 <U>ImageNet dataset</U>에 대해 학습된 네트워크는 각 class의 object의 특징에 대해 **유의미한 feature**를 학습 및 인식하기보다는, <U>training dataset</U>의 **in-distribution**을 neural network에 fitting하는 형태로 학습이 진행된다는 것이다. ImageNet dataset에 대해 $7$가지의 natural distribution shift가 일어난 dataset인 [ImageNetV2](https://arxiv.org/abs/1902.10811), [ImageNet Sketch](https://arxiv.org/pdf/1905.13549v2.pdf), [Youtube-BB
and ImageNet-Vid](https://arxiv.org/pdf/1906.02168v3.pdf),  [ObjectNet](https://papers.nips.cc/paper/2019/hash/97af07a14cacba681feacf3012730892-Abstract.html), [ImageNet Adversarial](https://arxiv.org/abs/1907.07174) 그리고 [ImageNet Rendition](https://arxiv.org/pdf/2006.16241v3.pdf)에 대해 validation을 진행하였다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213913187-9e5293a6-e6fd-4442-86da-29541a8c0ce3.png" width="700"/>
</div>


사실 이 figure에 대한 설명이 논문에 없는 부분이 잘 이해가 가질 않지만 일단 이 부분에 있기 때문에 해석하는 바로는 CLIP의 zero-shot 모델들이 linear probe를 통해 domain transfer를 진행했을때, 보다 ImageNet의 성능을 잘 유지하는 것으로 보아 domain shift 현상에 잘 대처한다고 볼 수 있다. 하지만 위의 figure가 main은 아니고, 다음 figure를 보면 왜 <U>CLIP model</U>이 **zero-shot** 및 **domain shift** task에서 game changer로 등장할 수 있었는지 실험적 증거를 확인할 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213913448-cd6058d5-26bc-47f1-927c-4bb18ae42be9.png" width="1000"/>
</div>


이상적인 상황에서는 domain shift가 일어나더라도 ImageNet dataset에 대한 성능과 domain shifted dataset에서의 성능이 동일해야한다($y = x$). 단순히 ImageNet에 대해서 학습한 경우 그 성능이 $2 \\sim 30\\%$ 감소하는 것을 확인할 수 있지만, zero-shot CLIP 모델의 경우 $7$개의 natural domain shifted dataset의 성능 평균이 <U>최대 약 $75\\%$나 상승할 수 있음</U>을 보여주었다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213913670-3f082bde-b940-4d35-ba77-4029d3b233f5.png" width="1000"/>
</div>


여기서 의문이 들 수도 있는 점은, 단순히 zero-shot을 기반으로 성능을 측정하는 것과 dataset에 <U>specific</U>하게 **fine-tuning**한 <U>logistic regression classifier</U>에 유의미한 차이가 생길 수 있지 않을까에 대한 부분이다. 결과를 보게 되면 ImageNet에 대해 fine-tuning한 경우 original dataset에 대해서는 $9.2\\%$의 괄목할만한 정확도의 향상이, ImageNet의 데이터셋 수집 방식과 유사했던 ImageNetV2에 대해서는 그에 비해 절반 정도의 정확도의 향상이 이루어졌고, Video dataset인 Youtube-BB와 ImageNet-Vid를 제외한 데이터셋에 대해서는 성능이 떨어지는 결과가 나타났다.   
사실 그냥 text encoder를 포함한 CLIP model로 <U>zero-shot classification</U>을 진행했다면, 위에서 열심히 실험했던 내용에 기반하여 'text가 guidance를 주기 때문에 domain에 robust한 예측을 할 수 있다'라고 결론을 내릴 수 있지만, 놀라운 점은 classifier를 통해 downstream task를 fine-tuning했을 때도 domain shift에 대한 정확도가 유지될 수 있다는 점이다. **이 부분에 대해서는 저자들이 명확하게 그 근거를 설명하지 못한 채** 넘어갔다.   
저자들이 추가로 확인한 실험 중 하나는 transfer dataset의 class들은 항상 $1000$-way classifier가 적용되는 ImageNet과 일치하지 않는다는 점 때문에 문제가 생긴 점이었는데, 이전 과정에서는 이를 ImageNet class의 하위 항목에 해당되는 prediction에 대한 max-pooling으로 진행하였다. 이러한 방식은 정확하지 않았으며, 저자는 이러한 방식 대신 CLIP에서 사용할 수 있는 방법인 'class name을 prompt화 시킨 text embedding'을 사용하였다. 그 결과 Video based dataset인 Youtube-BB, ImageNet-Vid의 성능 향상이 있었으며 ObjectNet의 약간의 성능 향상만이 있었다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213914563-51f46a4c-8b7e-4d08-86e4-b79b67b7e6ec.png" width="400"/>
</div>


위의 그림을 통해 Zero-shot CLIP에 Few-shot 만큼의 supervision이 더해질수록, fully-supervised learning에 비해 가지던 robustness가 줄어드는 것을 확인할 수 있다. 즉 학습에 사용되는 dataset sample이 많아질수록 <U>in-distribution</U>에 fitting되어 기존의 zero-shot CLIP model에 비해 같은 ImageNet performance에 대한 robustness가 감소한다고 할 수 있다.

## Comparison to Human Performance

사람으로 하여금 zero-shot 성능을 측정하기란 굉장히 어렵다. 하지만 이런 상황에서도 zero-shot 혹은 few-shot task performance를 비교할 수 있는 방법이 있는데, 바로 Oxford-IIT Pets dataset($37$개의 강아지 혹은 고양이 종류 맞추기)에 대한 것이다. 아무리 사람이라도 강아지나 고양이의 모든 종에 대해 알고 있지는 않을 것이다.
특정 이미지를 보여주고, 해당 종에 대해서 알고 있다면 종류를 대답하면 되고 그렇지 않다면 'I don't know'를 대답하면 된다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213914909-19070974-15ed-476d-b066-2acbf47ac437.png" width="600"/>
</div>


어느 정도 직관으로 이해할 수 있지만, 사람은 보통 $1$개의 이미지가 주어지면(one-shot) 해당 종에 대한 분류 정확도가 올라가지만, 추가로 $n$개의 이미지를 보여주더라도 해당 종에 대한 분류 정확도가 올라가지는 않는다. 이를 표현하는 문장으로는 'humans know what they don't know'인데, 본인이 모르는 것에 대해서는 명확하기 때문에 한 번 해당 내용을 학습하는 것으로 충분하다는 뜻이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213914869-d1f0cf60-7da0-4929-a566-938483f2fb9c.png" width="600"/>
</div>


따라서 <U>딥러닝 네트워크로 하여금</U> 사람과 같이 **few-shot** 성능이 prior knowledge를 활용하여 유의미한 결과를 이끌어내지 못하는 점이 유사하고, 이를 찾아내는 것이 **CLIP의 중요한 발전**이 될 것이라고 언급한다. 또한 위의 그래프에서 볼 수 있듯, 각 종에 대한 zero-shot 성능을 인간과 비교했을 때, 서로 어렵게 느끼는 breed에 대한 경향성이 어느 정도 유사한 것을 확인할 수 있다.

## Data overlap analysis
앞서 WebImageText 데이터셋을 구축하는 과정에서 <U>대량의 웹 이미지</U>를 가져와 학습한다고 했는데, 여기서 발생할 수 있는 문제는 validation에서 사용할 일부 dataset 또한 web 상에서 획득할 수 있는 sample에 해당되기 때문에 이러한 validation set이 pre-training dataset에 들어가게 된다면(<U>leak into</U>) 문제가 될 수 있다. 이를 방지하는 한 가지 방법은 네트워크를 학습시키기 전에 사전에 duplicated image(validation set과 동일한 샘플)을 모두 제거하는 작업이 필요하지만, 결국 <U>모든 dataset에 대해 검증을 거쳐야한다</U>는 점이 time-consuming하다.   
이러한 brute-force 방식 대신, 저자들은 overlap이 발생하는 방식을 정의하고 다음과 같은 과정을 통해 overlap을 확인하였다.

1. 각 validation dataset에 대해 duplicate detector(논문의 appendix에 있다고 한다)를 각 example에 대해 수행한다. 그런 뒤 nearest neighbor를 직접 조사한 뒤, 특정 threshold(sample간 거리 metric)를 설정하여 Overlap(threshold보다 큰 유사성을 가지는 샘플)과 Clean(threshold보다 낮은 유사성을 가지는 샘플)로 subset을 구성한다. Data의 clean한 정도(겹치는 데이터셋이 없음을 의미한다)를 확인하기 위해 All(Overlap + Clean)에 대한 Overlap의 비율을 구한다.

2. 그런 뒤 All, Overlap, Clean에 대해 CLIP RN50x64에 대한 zero-shot accuracy를 계산한다. (All accuracy) - (Clean accuracy)가 의미하는 바가 contamination에 의한 accuracy 차이가 되므로, 이 값이 양수라면 얼마나 overlapping data에 의해 over-fitting 되었는지에 대한 정보를 반영한다.

3. Overlap의 양이 적은 경우가 있기 때문에 binomial significance test를 추가로 진행한다. Clean accuracy를 null hypothesis로 잡고 Overlap subset에 대한 one-tailed $p$-value(greater)를 얻는다. 추가로 $99.5\\%$의 Clopper-Pearson confidence intervals 계산을 Dirty subset에 진행한다.

#### Binomial distribution test
실험 결과를 확인해보기 전에 Binomial distribution test에 대해서 수식을 통해 이해하면 다음과 같다. 우선 $H_0$라는 test hypothesis가 있고, 이를 null hypothesis라고 부르도록 하자. 우리가 진행할 test에서 null hypothesis는 clean accuracy이므로, 적은 양의 <U>overlapping dataset을 제외하고 진행한</U> zero-shot validation 성능을 의미한다.   

$$
    H_0 = \\pi = \\pi_0    
$$

정확도는 $0 \\sim 1$의 값을 가지게 되므로, 이를 CLIP model이 overlapping되지 않은 데이터셋을 기준으로 특정 샘플에 대해 '정확하게 분류할 확률'이라고 하자.   
Overlap dataset이 $n$개 있고, 그 중 샘플 $k$만큼이 CLIP model에 대해 정확하게 분류될 확률이라면 다음과 같이 표현할 수 있다.

$$
    \\text{Pr}(X = k) = \\begin{pmatrix}
        n \\newline k
    \\end{pmatrix}   p^k (1-p)^{n-k} 
$$

우리는 Overlap dataset 내부의 확률 $p$로 하여금 이 값보다 커질 $p$-value를 구하고 싶다. 왜냐하면 그래야만 <U>overfitting을 가정할 수 있기 때문</U>이다.

$$
    p = \\sum_{i=0}^{k} \\text{Pr}(X = i) = \\sum_{i = 0}^k \\begin{pmatrix}
        n \\newline i
    \\end{pmatrix}   \\pi_0^i (1-\\pi_0)^{n-i}
$$


해당 $p$를 구하는 one-tail test는 위와 같다. Sucess의 개수가 $n\\pi_0$가 되는 것이 기준점이 되기 때문이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213917253-bea7c88d-647d-408d-b0c8-a059d9744ae1.png" width="1000"/>
</div>


결과 그래프는 위와 같다. 총 $35$개의 dataset에 대해 실험을 진행한 결과, Overlapping보다 Clean subset의 accuracy가 높은 $9$개의 dataset에 대해서는 overlap이 없다고 결론을 내렸다. 결과적으로는 대부분의 데이터셋에 대해서 overlap이 심하지 않았고, overall accuracy가 크게 shifted된 부분은 $0.1\\%$의 threhold 기준으로 $7$개의 dataset이 있었다. 이 중 Bonferroni correction을 기준으로 $2$개의 dataset이 가장 큰 것으로 판별되었다(Birdsnap, Country211). 


# 결론
논문에서는 뒤에 추가로 논문이 가지는 한계점이나 appendix를 통해 설명하지 못한 디테일한 구현 방법에 대해서 소개한다.
이 논문이 가지는 contribution 중 가장 메인이 되는 부분을 정리해보면 다음과 같다.

1. Text model에서 사용되던 대용량 dataset을 구축하여 image model을 효율적으로 학습하였다.
2. 유의미한 text to image 관계를 학습할 수 있었다. 즉, multimodal을 가능케 했다.
3. Zero-shot transfer learning의 성능을 비약적으로 향상시켰다.

그리고 실험 내용을 기반으로 확인한 추가 contribution은 다음과 같다. 이 부분은 사실 main contribution이 아닌 sub contribution에 해당된다.

1. Linear probe나 logistic regression시 성능을 보아, representation learning이 어느 정도 잘 진행됨을 알 수 있다.
2. Domain shift에 robust한 모델을 학습하였다.
3. 단순히 무작정 dataset을 모은 것이 아니라, dataset의 overlapping 문제도 분석하였고, 이를 통해 성능 향상이 overfitting에 의한 결과가 아님을 증명하였다.

`,IO=`---
title: "MaskCLIP 논문 리뷰"
category: "ai papers"
publishedAt: "2023-01-24"
thumbnail: "https://user-images.githubusercontent.com/79881119/214461848-424e4ad3-6f46-4ccf-b7e2-15559865bc6b.png"
---


# 들어가며...

사실 리뷰할 MaskCLIP은 ECCV paper와 CVPR paper 두 종류가 있다. 논문 제목은 다르지만 main idea의 제목이 같다보니 조금 혼란스러운 감이 없지 않아 있었다. 처음에 읽고 싶었던 논문은 '[Extract Free Dense Labels from CLIP](https://arxiv.org/abs/2112.01071)'이며, 그 다음에 추가로 CVPR에 올라온 논문 제목은 '[MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining](https://arxiv.org/abs/2208.12262)'으로, 아직 학회에 제출을 하진 않은 것 같지만 느낌상 NeurIPS 2023에 올라올 것 같은 내용이다. ECCV 2022에 올라갔던 논문인 Dense label extraction은 segmentation task에 댇해 CLIP의 zero-shot representation을 어떻게 하면 가장 잘 활용할 수 있을까에 대한 실험을 진행했던 연구였고, 아카이브에 올라온 논문인 self-distillation은 CLIP의 image representation 학습에 self-supervised learning 방식 중 하나인 generative approach(masked autoencoder 방식)을 적용한 연구다. 


# CLIP의 장점?
VL(Visual-language) contrastive learning를 통해 vision의 modality인 image와 text modality와의 관계성을 찾은 것이 CLIP 논문이었고, 해당 논문을 기반으로 다양한 dataset의 downstream task(classification, object detection 그리고 segmentation 등등)을 zero-shot 혹은 few-shot 으로 해결할 수 있는 길이 열리게 되었다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214461848-424e4ad3-6f46-4ccf-b7e2-15559865bc6b.png" width="500"/>
</div>

사실상 GPT와 같은 대용량 언어 모델 이외에 computer vision에서 웹 상에서 획득할 수 있는 text prompt 기반 대용량 데이터셋을 학습에 사용한 것은 CLIP이 처음이었으며, 단순히 downstream task를 잘 해결할 뿐만 아니라 language와 vision의 관계를 찾은 것에서 image captioning, view synthesis와 같이 prompt 기반 DL engineering이 가능해졌다는 점에서 <U>멀티모달의 새 시대를 연 장본인</U>이라고 볼 수 있다.


# Segmentation에서는 활용할 수 없을까?
ECCV 2022 논문인 'Extract Free Dense Labels from CLIP'는 바로 이러한 CLIP의 장점에 집중하여, 어떻게 하면 CLIP의 zero-shot performance를 segmentation에서도 활용할 수 있을까에 대해 연구를 진행하였다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214462980-32bef7a2-dce7-4c90-bba2-3d18798696f9.png" width="500"/>
</div>

이는 굉장히 큰 의미가 있었던 것이 만약 우리가 위와 같은 그림을 segmentation해야 한다고 가정했을 때, 조커와 배트맨을 독립적으로 instance labeling 해야하는 문제가 발생한다. 물론 단순하게 생각해서 각 사람 이미지를 독립적으로 구분하는 작업을 하나씩 라벨링하고 이를 supervision으로 활용해서 학습하게 된다면 얼추 segmentation 성능이 기대하는 것만큼 나오겟지만, 그렇다고 해서 모든 영화의 모든 인물들에 대한 프레임별 labeling을 수작업으로 진행하고 각각을 새로운 label에 매칭하는 것은 <U>학습 과정에서나 데이터셋 구축에서도</U> 시간과 노력이 많이 드는 과정이다.   
기존의 SOTA segmentation 방식은 ImageNet에 대한 pre-trained weight으로 초기화한 representation을 활용하여 segmentation task에 적용하는 한편, 이 논문에서는 CLIP이 가지는 <U>global image representation</U>(Web image scale에 대한 representation)을 활용하고자 한다. CLIP이 ImageNet base로 학습된 representation에 비해 가지는 장점은 다음과 같다.

1. 각 feature에 local image semantic(각각의 feature dimension은 이미지의 일부에 대한 정보를 담는다)을 학습 가능하다. 사실 이 부분은 ImageNet에 pre-trained된 baseline weight의 효과와 거의 유사하다고 볼 수 있다.
2. Open-vocabulary의 concept을 학습할 수 있다. 앞서 설명했던 것처럼 구체적인 labeling 없이도 원하는 object에 대한 segmentation이 가능하다.
3. 물체들 간의 상호작용, 관계 그리고 spatial location에 대한 풍부한 문맥상의 정보를 학습 가능하다.

이러한 CLIP의 장점이 모여 segmentation task에서 CLIP representation의 supervision을 활용할 수 있다면, <U>효율적인 task performance</U>를 기대할 수 있으리라는 것이다.


# Failure and success
사실 처음부터 저자들이 실험에 성공한 것은 아니었다. 가장 쉽게 생각할 수 있는 방법은, CLIP 모델 자체도 image encoder로 ResNet 구조를 활용하기 때문에 pre-trained weight을 가져와서 초기화시킨뒤, CLIP image encoder의 weight를 <U>segmentation task에 맞게 fine-tuning</U> 시키면 성능이 좋아질 수 있겠다는 생각을 하게 된다. 예를 들어 segmentation task에는 DeepLab 모델들이 SOTA로 사용되었는데, DeepLab의 weight를 CLIP image encoder의 weight로 초기화시킨 뒤에 backbone을 segmentation에 맞게 학습을 시키는 것이다. 이러한 과정이 CLIP의 text embedding을 굳이 사용하지 않아도 된다는 장점이 있었지만, 이러한 접근법은 CLIP의 장점 중 하나인 <U>unseen class에 대한 segmentation을 진행할 수 있는 능력</U>을 완전히 배제한 것이다.   
따라서 단순히 weight를 초기화하고 fine-tuning을 진행하는 기존 방식에서 벗어나, MaskCLIP이라고 불리는 접근법은 CLIP의 image encoder로부터 추출된 patch-level feature를 활용하여, 기존 CLIP의 마지막 layer였던 attention pooling을 진행하지 않고 CLIP의 text encoder로부터 얻을 수 있는 $1 \\times 1$ convolution weight를 기반으로 dense prediction을 진행한다. 사실 여기에서는 convolution 연산이 가지는 의미 자체를 하나의 trick으로 사용한 것인데, 원래라면 기존의 text embedding과 image embedding 사이의 similarity를 통해 classification을 진행하는 task라고 해보면, 사실 이 과정은 특정 prompt에 대한 text embedding과 그에 맞는 image embedding 간의 correlation을 계산하는 것이고, 결국 $1 \\times 1$ convolution 연산이 가지는 의미가 각 <U>dimension</U>에 대한 <U>inner production</U>이기 때문이다.   
또한 CLIP baseline 구조를 그대로 사용했기 때문에 MaskCLIP은 좋은 zero-shot 성능을 보였으며, ResNet과 ViT 구조 모두 segmentation에 대해 적용할 수 있었다(이는 <U>attention pooling</U> 연산 구조 자체가 ViT의 <U>image class token</U>이 self-attention하는 과정과 완전히 동일하기 때문). 추가로 segmentation의 성능 향상을 위해 학습할 필요가 없는 **key smoothing**과 **prompt denoising** 과정을 추가하였다. 두 방법에 대해서는 뒤에서 보다 자세히 보고 넘어가도록 하겠다.


# MaskCLIP approach

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214468226-ea549c5c-8af7-4828-8b52-3f01f5fa6f24.png" width="500"/>
</div>


MaskCLIP+로 표시된 부분은 이후에 따로 설명하기로 하고, 우선 MaskCLIP으로 표시된 부분(짙은 회색)을 먼저 확인하면 위와 같다. 참고로 MaskCLIP에서는 학습 과정이 전혀 필요 없고, 단순히 특정 이미지에 대해 얻고자 하는 class label에 대한 text prompt를 만든 뒤, 이렇게 만든 각 class별 text prompt를 encoding한 결과를 $1 \\times 1$ convolution의 weight으로 사용하여 <U>image embedding</U>을 <U>dense prediction</U>으로 확장시킨다. 기존 attention pooling이 image feature의 average를 class에 대한 token으로 간주하여 모든 pixel 정보에 대한 attention을 구했던 것과 달리, MaskCLIP에서는 **attention pooling** 이전의 모든 픽셀 정보들을 활용하는 형태가 된다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214469422-31e57d8d-8ba7-4791-93ef-444e269fb30a.png" width="400"/>
</div>


하지만 단순히 이렇게 했을 경우에는 prediction 성능이 크게 좋지는 않았는데, 이유는 아무래도 text prompt와의 similarity를 기준으로 prediction이 진행되는 구조이다 보니, 기존 segmentation task에서와 같이 locality(비슷한 위치의 pixel은 같은 class일 확률이 높다)에 대한 정보가 담기기 힘들고, 무엇보다 feature 단위의 <U>pixel prediction</U>은 noisy한 결과를 보여줄 수 있기 때문이다. 이러한 문제들을 해결하고자 제시한 방법이 바로 앞서 설명을 넘겼던 **key smoothing**과 **prompt denoising** 과정이다.

## Key smoothing

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214470022-12174ae9-f4d6-4873-a3b0-76114b4e7bf1.png" width="400"/>
</div>


굳이 attention pooling layer가 필요없다면, 결국 key, value값이 더 이상 학습에 관여하지 않게 된다는 의미이다. 그러나 결국 attention layer도 CLIP 학습에 있어 prediction에 영향을 주었을 것이고, segmentation task를 위해 이를 아예 제거하고 본다는 것은 attention layer가 CLIP image encoder의 representation 학습에 끼친 영향력을 아예 무시한다는 것과 같다. 이러면 안되는 것이, attention layer로 하여금 한 픽셀과 다른 픽셀의 연관성을 key value로 학습했을 것이고, 이러한 학습 결과가 각 local semantic에 대해 '<U>어떤 부분들을 참고해야하는지</U>'에 대한 정보를 담았을 것이라고 볼 수 있다. 예컨데 위의 그림과 같이 $K_1,~K_2$ 그리고 $K_3$에 해당되는 feature 영역이 있다고 생각해보자. 서로 다른 물체 영역에 속하는 $K_1$(잔디밭)과 $K_2$(고양이)는 유사도가 낮지만, $K_2$(고양이)와 $K_3$(고양이)는 유사도가 높을 것이다. 

$$
    \\text{pred}_i = \\sum_j \\cos \\left( \\frac{k_i}{\\parallel k_i \\parallel_2},~\\frac{k_j}{\\parallel k_j \\parallel_2} \\right) \\text{pred}_j   
$$

바로 이러한 key value의 특성을 고려한 prediction이 key smoothing이며, $i$번째 patch에 대한 prediction을 진행할 때, <U>다른 위치의 patch에 대한 key 값과의 유사도</U>를 고려하겠다는 의미가 된다. 참고로 논문에서는 similarity와 곱해지는 $\\text{pred}$ 부분의 index가 $i$로 표기되어 있는데, 의미하는 바를 읽어보면 $j$가 맞는 듯하여 위와 같이 식을 수정해보았다.

## Prmopt denoising

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214471903-e7364181-c82e-4c0d-b6a5-3dc60b7571e1.png" width="400"/>
</div>

또다른 문제는 image에 포함된 class가 너무 많을 경우 발생하는 문제다. 만약 위와 같은 그림에서 'cat', 'grass' 그리고 'tree stump'를 구별하는 segmentation task라고 한다면, 'tree stump'의 경우 이미지의 극히 일부에만 포함되어있어 오히려 key smoothing 과정에서 <U>다른 class에 대한 confidence를 방해</U>하는 역할을 한다. 따라서 본 논문에서는 만약 특정 class에 대한 key similarity가 모든 feature pixel에 대해 $0.5$보다 작을 경우, 해당 class를 prediction 과정에서 아예 배제시키는 방법을 사용하였고, 이를 통해 더 좋은 성능을 얻을 수 있었다고 한다.


# Mask CLIP to Mask CLIP+

여기서 멈추지 않고 논문에서는 Mask CLIP 대신 <U>Mask CLIP+</U>라는 방법을 추가로 제시한다. Mask CLIP의 장점은 CLIP base로 zero-shot segmentation이 가능하다는 것이지만, 달리 표현한다면 CLIP architecture 대신 segmentation에 좋은 성능을 보이는 기존 baseline인 <U>DeepLab</U>이나 <U>PSPNet</U> 등등을 사용할 수 없다는 단점이 있다. 특히 [ASPP](https://arxiv.org/abs/1606.00915)와 같은 방식을 사용할 수 없다는 점에서 segmentation task에 특화된 네트워크 설계가 CLIP based method에서는 한정적일 수밖에 없다.   


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214472605-12257674-fbfc-45e4-a386-82f0dbc8444b.png" width="600"/>
</div>


따라서 해당 논문에서는 MaskCLIP+에 대해 위와 같은 framework를 제안한다. 기존 CLIP network는 학습하지 않고, 단순히 MaskCLIP+에 사용될 network가 학습할 수 있는 pseudo-label을 만들어준다. MaskCLIP+에서 사용되는 backbone은 <U>기존 fine-tuning 방식과 동일</U>하게 ImageNet에 대해 pre-trained된 네트워크를 사용한다. 그러나 기존 방식과 다른 점은 fine-tuning 과정에서 사용하는 classifier는 학습하지 않고 CLIP의 text-prompt를 기반으로 생성한 $1 \\times 1$ convolution을 사용한다는 것이다. 이를 통해 CLIP이 아닌 다른 network 베이스에도 <U>CLIP space에 대한 knowledge distillation</U>이 가능하다는 접근을 보여주었다.   
다만, MaskCLIP+에서 pseudo label을 계속해서 supervision으로 사용하게 되면 segmentation backbone(dilated backbone)의 성능 upper bound가 CLIP 구조에 한정될 수 밖에 없기 때문에 학습 schedule 상 약 $1/10$ 까지는 CLIP의 pseudo label을 활용하되, 이후에는 성능 수렴이 발생하여 이를 제거하고 <U>self-training</U> 과정을 거쳤다고 한다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214473245-c8549814-b913-4dfd-9b8b-de184aab570f.png" width="600"/>
</div>


그 결과 놀라울 정도로 좋은 segmentation 성능을 보여주었고, 단순히 fine-tuning하는 기존 방식에서 벗어나 CLIP guidance learning 기반의 text prompt의 representation이 <U>supervised based segmentation</U>에 특화된 네트워크 구조에서도 <U>zero-shot segmentation</U>이 가능하게끔 할 수 있는 방법론으로 제시되었다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214473452-a071fb41-b647-4398-867a-68a657778a56.png" width="600"/>
</div>


Fully supervised mIoU가 사실상 <U>zero-shot이 가질 수 있는 최대 한계치</U>라고 볼 수 있는데, MaskCLIP+가 달성한 수치는 기존 SOTA를 뛰어넘어 더 이상 발전이 힘든 수준까지 올라간 것을 볼 수 있다.


# CLIP의 문제점
이렇듯 완벽할줄만 알았던 CLIP도 사실은 문제점이 있었다. 사실 원래 모든 연구는 크나 작으나 어느 정도 약점을 가지고 있는데, CLIP은 그에 비해 contribution이 너무 크기 때문에 사실상 흠을 잡기가 애매했다. 이러한 상황에서 ECCV에서 나온 MaskCLIP과 지금부터 리뷰할 CVPR의 MaskCLIP은 방향성이 다른데, ECCV는 segmentation에서 CLIP을 사용할 수 있을 정도로 CLIP이 학습한 text prompt based representation이 풍부한 contextual meaning을 담을 수 있음에 집중했고, CVPR에 올라온 MaskCLIP은 <U>CLIP이 제시한 학습법</U>으로는 <U>image representation</U>의 충분한 학습이 힘들다는 점에 집중했다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214480622-c8502045-89dd-4799-ad3a-7bd188ad9cf5.png" width="600"/>
</div>


예컨데 만약 좌측과 같이 고양이가 있고, 해당 이미지에 대해 "Two cats are on the grass"라는 문장이 있다면, 사실 text prompt가 줄 수 있는 contextual information은 <U>단순히 object에 대한 정보와 둘 사이의 관계</U>에 대한 내용이지, 실질적으로 이미지의 background나 object를 제외한 디테일한 사물/texture 형태에 대한 어떠한 정보도 줄 수 없다는 것을 알 수 있다.   
따라서 단순히 language information guidance를 토대로 CLIP을 학습하게 되면 충분한 데이터셋을 기반으로 image to text relationship 성립이 가능하지만, 정말로 학습된 후에 특정 text prompt를 기준으로 네트워크가 어디에 집중하는지 확인해보면, image description을 기준으로 학습된 부분은 representation에 잘 포함되지만 그렇지 않은 영역은 representation에 잘 포함되지 않는다는 것이다.


# Learn image representation with self-supervised learning 


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214481546-868cabe5-9c33-46b3-84a8-c27b75abc2cb.png" width="1000"/>
</div>


따라서 논문에서 제시하고자 한 해결책은 CLIP도 결국엔 이미지 자체의 supervision을 사용할 수는 없기 때문에 SimCLR, Moco 등등 contrastive한 방법이나 MAE, BEiT 혹은 DINO와 같이 generative한 approach로 SSL을 CLIP 학습에 함께 사용하고, 이를 통해 <U>image representation</U>의 효율적인 학습을 이끌어내고자 한 것이다. 물론 기존에 self-supervised learning 방법을 CLIP에 적용하고자 했던 SLIP과 같은 방법들이 있었지만, 각 방법들은 contrastive learning 방법을 적용한다던가, salient object에 집중하게 하여 이 논문이 제시한 CLIP의 문제점 중 하나인 surrounding object에 대한 contextual information이 부족한 점을 채우지 못했다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214482252-0b21e9f1-d436-4713-ae8f-367f9ec41219.png" width="1000"/>
</div>


또한 BEiT, MAE 방식의 경우 token이나 image patch에 대해 self-supervision을 가지고 image representation을 학습하는 형태가 되기 때문에, high level feature인 image embedding과 text embedding 간의 유사도를 기준으로 학습하는 CLIP 방식에서 generative learning 방법을 적용하기엔 <U>서로 학습 목적으로 삼는 representation의 level</U>이 다르다는 문제가 발생한다.   
따라서 해당 논문에서는 pixel 단위의 supervision이 아닌 feature 단위의 supervision과, patch 단위가 아닌 global 단위로 representation을 학습하는 CLIP 구조 자체를 self-teacher로 삼아 patch 단위로 <U>knowledge distillation</U>을 진행하는 **self-distillation** 방식을 사용한다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214482836-8d69ec8c-0256-4f86-9bbb-c9af20290ded.png" width="1000"/>
</div>


모든 ablation에 대한 저자들의 framework 제안은 위와 같다. 가장 우측에 있는 MaskCLIP 학습 형태가 최종적으로 제안하는 학습 형태가 된다. 이 논문의 main contribution은 크게 세 개로 나눌 수 있다.

1. VL contrastive에 새로운 학습 framework를 제안함으로써, masked self-distillation을 통해 VL model의 transfer 성능을 올릴 수 있다.
2. 단순히 SSL 방법론을 적용하여 visual model의 성능을 높이고자 한 것이 아니라, 여러 MaskCLIP 형태에 대해 ablation을 진행하였다.
3. 제안한 MaskCLIP(self-distillating) 구조가 zero-shot, linear-probing 그리고 finetuning과 같은 setting에서 모두 좋은 성능을 보였다.


# Results

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214483478-fa3cfe96-c0af-47e7-8903-49facd9bcdfb.png" width="500"/>
    <img src="https://user-images.githubusercontent.com/79881119/214483520-8f688413-14af-4c02-a9bd-7fe94f51bb04.png" width="500"/>
</div>


결과적으로는 MaskCLIP이 기존 CLIP의 성능에 비해 월등히 좋아졌으며, 논문에서 주장하는 것은 image representation에 대한 추가 loss term을 통해 <U>수렴 속도가 빨라졌다는 점</U>에 집중을 한다. 아마 이 부분은 image representation에 supervision을 추가로 주면서 단순한 embedding인 text space보다 복잡한 signal인 image space가 text에 대한 representation보다 학습 속도가 느리기 때문에 발생하는 수렴 차이를 어느 정도 좁혀줄 수 있는 방법이기 때문이지 않을까 생각해본다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/214483978-01d2e1ac-129d-4f60-8d5b-03984dcaa0a6.png" width="700"/>
</div>


실제로 논문이 주장한 바와 같이 특정 text prompt에 대해서 네트워크가 집중하는 부분이 보다 reasonable해진 것을 알 수 있다.
`,UO=`---
title: "(StyleCLIP, StyleGAN-NADA) CLIP based image manipulation에 대하여"
category: "ai papers"
publishedAt: "2023-01-29"
thumbnail: "https://user-images.githubusercontent.com/79881119/215320268-1ddc118f-2e02-4fe1-be7a-919196a3d324.png"
---


# StyleGAN
[StyleGAN](https://arxiv.org/abs/1812.04948)의 등장은 사실상 생성 모델을 style based approach로 해석한 첫번째 논문이라고 볼 수 있다. StyleGAN 논문을 이 글에서 자세히 설명하지는 않겠지만 가볍게 요약하자면, constant vector로 표현된 하나의 feature map 도화지가 있고($4 \\times 4 \\times 512$), 이 도화지에 latent vector로부터 추출된 style 정보들(평균값인 $\\mu$와 표준편차인 $\\sigma$)를 affine layer를 통해 얻어내어 이전의 feature map을 modulation하면서 점차 사이즈를 키워나가는 구조다. 점차 feature map의 spatial resolution을 키운다는 점에서 타겟팅이 된 논문 구조인 [PGGAN](https://arxiv.org/abs/1710.10196)도 같이 읽어보면 좋다.

# Image manipulation

아무톤 styleGAN의 디테일한 방법론에 대해서 논하고자 하는 것은 아니고, styleGAN이 가져온 이후 연구들의 동향에 대해서 살펴볼 수 있다. 무작위로 뽑아낸 latent vector $z$ 혹은 $w$로부터 '<U>style 정보</U>'를 얻어낼 수 있다는 것은 반대로 생각해서 특정 이미지를 주었을 때, 해당 이미지를 만들어낼 수 있는 latent vector $z$ 혹은 $w$를 추출할 수 있다는 말과 동일하다.   
이러한 GAN inversion 개념은 image manipulation에 큰 동향을 불러왔으며, 특히나 styleGAN의 경우 high-resolution image synthesis가 가능케 한 논문이었기 때문에 <U>고퀄리티의 이미지 조작이 가능하다는 점</U>이 main contribution이 되었다. 이에 여러 이미지 조작 논문들이 나왔으며, 해당 내용에 대해 궁금한 사람들은 본인이 작성한 [image manipulation 포스트](https://6unoyunr.github.io/blog/imagemanipulate)를 참고하면 좀 더 좋을 것 같다.   
StyleCLIP도 결론부터 말하자면 사전 학습된 StyleGAN을 활용한 Image manipulation이라는 task에 대해서 다룬 내용이고, 이 논문에서는 기존 방식과는 다르게 VL contrastive 논문인 CLIP을 활용한 <U>text guidance</U>가 보다 image manipulation에 조작 편의성을 가져다줄 수 있다는 점에 집중하였다.


# 기존 방법들의 문제점은?

앞서 언급하기도 했지만 <U>image manipulation task</U>에 대한 연구 동향은 대부분 사전 학습된 styleGAN latent space에서 유의미한 semantical information을 찾고, 이를 조작하는 방식으로 진행된다. 그러나 직접 latent space를 searching하는 과정 자체는 구체적으로 이미지 상에서 어떤 스타일을 변화시킬지도 모르고, 무엇보다 사용자가 원하는 이미지 조작이 이루어지기 힘들다는 것이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/215320268-1ddc118f-2e02-4fe1-be7a-919196a3d324.png" width="900"/>
</div>


예를 들어 위와 같은 이미지를 보면, 평범한 표정의 여자 이미지를 만들어내는 latent vector $w_1$이 있고, 이와는 다르게 놀라는 표정의 여자 이미지를 만들어내는 latent vector $w_2$가 있다고 해보자. 단순히 latent space를 searching하는 과정에서는 '놀라는 표정'을 만들어낼 수 있는 latent manipulation 방향을 알 수 없기 때문에 random하게 찾는 과정을 거칠 수 밖에 없다. Latent vector는 $512$ 크기의 차원 수를 가지기 때문에 supervision이 없다면 인간이 직접 찾아야하는 번거로움을 피할 수 없다. 이러한 방법을 사용한 것이 [GANspace](https://arxiv.org/pdf/2004.02546.pdf), [Semantic face editing](https://arxiv.org/pdf/1907.10786.pdf) 그리고 [StyleSpace 분석](https://openaccess.thecvf.com/content/CVPR2021/papers/Wu_StyleSpace_Analysis_Disentangled_Controls_for_StyleGAN_Image_Generation_CVPR_2021_paper.pdf) 논문들에 해당된다. 이러한 문제들을 해결하기 위한 방식이 attribute에 대한 classification을 guidance로 삼는 [InterfaceGAN](https://arxiv.org/pdf/2005.09635.pdf)이나 [StyleFlow](https://arxiv.org/pdf/2005.09635.pdf)가 제안되었다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/215321251-44e2194d-1ae1-4fc8-857e-ac61137e92e4.png" width="900"/>
</div>


만약 latent space에서 특정 element를 바꿨을 때의 attribute 변화가 '얼굴 표정'임을 원한다면 해당 attribute의 변화를 최대화하는 쪽으로 latent manipulation을 진행하게 되는 것이다. 이외에도 parametric model인 3DMM을 활용하여 3D face mesh에 consistent한 sampling을 진행하는 [StyleRig](https://arxiv.org/pdf/2004.00121.pdf) 논문 등등이 소개되었다.   
어찌되었든 기존 방식들은 정해진 semantic direction을 찾아가는 과정을 바꾸지는 못했고, 이는 사용자의 image manipulation 방식에 제한을 둘 수밖에 없었다(latent space를 searching하거나, classifier에 의한 guidance). 기존 방식에서 벗어나 latent space에 mapping되지 않은 direction에 대해서 searching하는 과정을 고려하면(StyleRig), <U>manual effort</U>와 충분한 갯수의 <U>annotated data</U>가 필요한 것을 알 수 있다.


# CLIP을 활용한 image manipulation
따라서 저자들은 방대한 text-image web dataset으로 prompt 기반 image representation을 학습한 VL contrastive model의 성능에 집중하였고, 해당 네트워크가 <U>prompt based zero-shot image classification</U>에서도 성능을 입증했던 바와 같이 마찬가지로 image manipulation 과정에서도 어느 정도 자유도를 보장할 수 있다고 생각하였다. CLIP 네트워크와 기존 framework인 styleGAN을 혼용하는 방식은 여러 가지가 있을 수 있지만, 저자들은 이 논문에서 총 세 가지의 방법들을 언급하였다.

1. 각 이미지 별로 text-guided latent optimization을 진행하는 방법. 단순히 CLIP model을 loss network(latent vector를 trainable parameter로 gradient를 보내고, clip encoder의 결과와 text encoder의 결과 사이의 similarity를 loss로 주게 되면 latent 최적화가 진행되는 방식)
2. Latent residual mapper를 학습하는 방법. 이 과정에서는 latent vector를 직접 최적화하는 것이 아니라 latent mapper를 학습하여, $\\mathcal{W}$ space의 특정 latenr를 CLIP representation에 맞는 또다른 latent vector로 mapping한다.
3. 이미지에 상관 없이 text prompt에 맞는 style space를 학습하는 방법. 이 방법은 styleGAN이 가지고 있는 $\\mathcal{W}$ space의 disentanglement 특징을 그대로 유지하면서 style mapping을 하고자 하는 것이 주된 목적이다.


# Related works

## Vision-Language tasks
가장 먼저 언급할 수 있는 related works로는 <U>vision + language</U> representation에 대한 내용이 될 것이다. Vision과 Language가 융합된 연구에는 language based image retrieval(원하는 부분 찾아내기), image captioning(이미지를 묘사하는 캡션 달기) 혹은 visual question answering(시각적인 정보를 토대로 질문에 대한 답변하기) 등등 여러 가지 task가 있다. BERT 모델이 여러 language task에 대해 좋은 성능을 보였던 바를 토대로, 최근 VL 방법들은 대부분 joint representation(이미지와 텍스트 간의 유의미한 관계)를 학습하기 위해 <U>transformer backbone</U>을 활용하기 시작했다. CLIP도 마찬가지로 <U>transformer backbone</U> 기반으로 학습되었다고 생각하면 된다. 물론 image encoder는 ResNet50 구조를 차용한 구조의 경우 완전한 transformer라고 볼 수는 없지만, attention pooling을 통해 어느 정도 embedding을 추출하는데 있어 transformer와 유사하게 동작한다고 할 수 있다. 

## Text-guided image generation and manipulation
물론 image generation에 대한 연구도 multimodal로 진행되었던 적이 있다. 예를 들어 conditional GAN 구조를 사용하면서, 이때 condition vector를 text embedding을 활용하여 학습하는 형태가 될 수 있다. 가장 초창기 연구([논문링크](https://arxiv.org/abs/1605.05396))의 경우 아직 transformer 아키텍쳐가 발전되기 전이었기 때문에 단순한 RNN 구조를 차용하여 text embedding을 추출, 이를 generator와 discriminator 학습에 있어서 conditional 요소로 사용하게 된다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/215374864-a3774c59-1894-4e93-925b-bbba30f72354.png" width="900"/>
</div>


이후에는 multi-scale GAN 구조를 활용하여 image quality를 올리거나, attention mechanism을 text와 image feature 간에 활용하는 형태의 연구도 진행되었다([논문링크](https://arxiv.org/pdf/1711.10485.pdf)).


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/215375227-be754bf6-97c0-4b3a-9ca0-dadbcd63ae91.png" width="900"/>
</div>


앞서 설명한 간단한 내용들은 image generation에 해당되고, 우리가 지금 포커싱하는 것은 특정 image에 대해서 text가 이미지 수정에 대한 supervision을 줄 수 있는 image manipulation과 관련된 이야기를 해볼까 한다. 사실 image manipulation이라는 task는 generation보다 까다롭기 때문에 위에서 언급한 것과 같이 <U>딥러닝 초창기</U>부터 연구가 진행되어오던 것은 아니다. 그럼에도 GAN 구조를 활용한 초창기 연구들이 있었다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/215375676-9fc38293-eff9-41d8-bb78-8b525cf9322c.png" width="900"/>
</div>

GAN encoder/decoder structure를 활용한 초반 여러 방법들의 경우 image와 text description 각각의 semantic을 disentangle하는데 초점을 맞추었다. 비교적 가장 최근의 image manipulation 논문 중 하나라고 볼 수 있는 [ManiGAN](https://arxiv.org/pdf/1912.06203.pdf)은 ACM이라는 text-image <U>affine combination module</U>을 제안하여 manipulated된 이미지의 퀄리티를 높일 수 있었다. 구조를 보게 되면 styleGAN에서 사용되는 affine을 기반으로 한 style modulation 과정과 상당히 닮아있는 것을 확인할 수 있다. 이러한 앞선 연구들의 특징은 styleGAN base로 접근하지 않고 대부분 basic GAN approach를 사용했지만, StyleCLIP 논문에서는 StyleGAN approach를 사용했다는 점이 차이점이 될 것 같다.   
물론 StyleGAN을 사용한 approach는 StyleCLIP이 처음은 아니다. [TediGAN](https://arxiv.org/pdf/2012.03308.pdf)도 마찬가지로 StyleGAN의 approach를 사용했는데, StyleCLIP 논문에서의 방법과 차이가 있다면 text를 StyleGAN의 latent space로 mapping하는 encoder를 학습하고, 이를 통해 text 기반으로 찾은 style space와 image의 style space를 융합하고자 한 것이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/215376914-5c2dedbc-189c-49b9-b9c0-8054b37e78d8.png" width="600"/>
</div>


어찌보면 TediGAN의 approach는 StyleGAN에서 8개의 MLP로 생성한 $\\mathcal{W}$-space 구조가 normal distribution $\\mathcal{Z} \\sim \\mathcal{N}(0,~I)$에 기반하기 때문에, image modality와 text modality의 차이가 있다고 하더라도 <U>encoder에 의한 implicit functional mapping이 가능</U>하다고 생각했던 것 같다. 사실 이러한 방법이 조금은 더 text와 image 간의 유의미한 representation을 찾는 방법이라고 생각되지만, CLIP의 성능 덕분인지 TediGAN보다는 StyleCLIP의 성능이 더 좋았다고 한다. 이외에도 DALL-E 방식도 있지만 GPU가 어마무시하게 사용된다는 점에서 text to image manipulation의 용이성이 떨어진다는 점이 있다. StyleCLIP처럼 논문 형태로 나온 것은 아니고 단순히 온라인 포스팅으로 StyleGAN과 CLIP을 image manipulation에 사용한 방식이 있다([참고 링크](https://towardsdatascience.com/generating-images-from-prompts-using-clip-and-stylegan-1f9ed495ddda)).


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/215377921-95be986c-42e7-436a-811b-640742392199.png" width="500"/>
</div>


위의 그림을 잘 보게되면 결국 StyleCLIP에서 image manipulation approach로 생각한 latent optimization 과정이 거의 똑같은 것을 알 수 있다. 하지만 위의 방식은 이미지를 처음부터 생성하는 관점에서의 latent optimization이고, 이 논문은 특정 이미지를 만들어내는 style space의 latent vector에 대해서 optimization을 진행, <U>image manipulation</U>에 보다 초점을 맞췄다는 점이 차이가 될 수 있겠다.

## Latent space image manipulation
Image manipulation이라는 task에서 pre-trained styleGAN generator를 사용하는 방법은 이미 다양한 논문들이 존재한다. StyleGAN의 intermediate style space인 $\\mathcal{W}$는 disentanglement 등 image manipulation 관점에서 도움될 특징이 많기 때문이다. 보통 특정 이미지를 latent space로 매핑한 뒤에, manipulated image의 representation으로 guidance를 주는 방법이 일반적인데, 여기서 image annotation을 supervision으로 사용하는 approach와 직접 meaningful direction을 찾는 approach로 나눌 수 있다. 대부분의 styleGAN을 base로 한 manipulation 연구들에서는 $512$ 차원의 $\\mathcal{W}$-space vector를 사용하거나, 이를 확장시켜서 각 feature level에 따라 style를 넣는 $\\mathcal{W}+$ vector를 사용한다. 그러나 [stylespace 분석 논문](https://arxiv.org/pdf/2011.12799.pdf)에서 stylespace $\\mathcal{S}$를 사용하는 것이 더 좋다는 주장을 하였고, 기존 space보다 disentanglement에 효과적임을 보여주었다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/215379873-de5b104f-831d-4240-bf46-97d1a6705691.png" width="700"/>
</div>

앞서 말했던 바와 같이 StyleCLIP 논문에서는 총 세가지의 optimization 방법을 제시했는데, 다시 언급해보면

1. 각 이미지 별로 text-guided latent optimization을 진행하는 방법. 단순히 CLIP model을 loss network(latent vector를 trainable parameter로 gradient를 보내고, clip encoder의 결과와 text encoder의 결과 사이의 similarity를 loss로 주게 되면 latent 최적화가 진행되는 방식)
2. Latent residual mapper를 학습하는 방법. 이 과정에서는 latent vector를 직접 최적화하는 것이 아니라 latent mapper를 학습하여, $\\mathcal{W}$ space의 특정 latenr를 CLIP representation에 맞는 또다른 latent vector로 mapping한다.
3. 이미지에 상관 없이 text prompt에 맞는 style space를 학습하는 방법. 이 방법은 styleGAN이 가지고 있는 $\\mathcal{W}$ space의 disentanglement 특징을 그대로 유지하면서 style mapping을 하고자 하는 것이 주된 목적이다.

위와 같다. 따라서 latent optimization을 직접 진행하게 되는 1번과 2번 방법은 기존 StyleGAN space인 $\\mathcal{W}$, 그리고 $\\mathcal{W}+$를 그대로 사용하게 되고, 3번의 경우 $\\mathcal{S}$ space를 새롭게 학습하는 과정이 된다(input agnostic).


# StyleCLIP text-driven image(latent) manipulation

논문에서는 위에서 설명한 세 가지 방법에 대해 따로 실험을 진행하였다. 가장 간단한 방법은 각 source image와 원하는 style에 대해 묘사하는 text prompt를 기반으로 최적화하는 것이다. 다만 이러한 방법은 hyperparameter에 대해 불안정한 학습이 진행될 수 있기 때문에 각 샘플 별로 mapping network를 학습시켜서 latent space를 mapping하는 방법을 사용할 수도 있다. 이를 <U>local mapper</U>라고 부르는데, mapper의 목적은 latent space에서 이미지를 생성하는 각 latent vector를 최적화하는 step을 inference하여 한 번에 mapping할 수 있게끔 학습하고자 하는 것이다. 각 mapper의 학습에는 단일 text prompt에 대해 각 image sample의 latent starting point를 기준으로 학습하게 된다.   
하지만 저자들이 실험한 결과를 바탕으로는 이러한 각 manipulation에 대한 mapper 학습을 할 경우 mapper의 step이 유사해진다는 것(manipulation 다양성이 떨어짐)과 disentanglement를 제대로 활용할 수 없다는 점이 문제가 되었다. 따라서 input agnostic(샘플의 starting point에 상관없이) mapping network를 학습하는 방법을 사용했으며, 이때의 global manipulation direction은 style space $\\mathcal{S}$에서 계산되었다고 한다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/215386009-5771a369-9e87-4e98-8bea-427fe25502e4.png" width="900"/>
</div>


## Latent optimization
Latent optimization은 상당히 간단한 식으로 표현된다. 딱히 길게 풀어쓸 내용은 아니기 때문에 수식을 먼저 보고 해석하는 식으로 마무리하겠다.

$$
    \\begin{aligned}
    &\\arg \\min _{w \\in \\mathcal{W}+} D_\\text{CLIP} (G(w), t) + \\lambda_\\text{L2} \\parallel w - w_s \\parallel_2 + \\lambda_\\text{ID} \\mathcal{L}_\\text{ID} (w) \\newline
    &\\mathcal{L}_\\text{ID} (w) = 1- \\left< R(G(w_s)), R(G(w)) \\right>
    \\end{aligned}
$$

위에서 $R$은 ArcFace network(얼굴 인식)의 pretrained된 feature extractor로 보면 되고, $\\left< \\cdot, \\cdot \\right>$ 연산은 곧 두 생성된 이미지에 대한 feature embedding 사이의 cosine similarity를 계산한 것과 같다. Input image는 [e4e](https://arxiv.org/abs/2102.02766)를 기반으로 $w_s$로 mapping된다고 생각하면 된다. $D_{\\text{CLIP}}$은 보이는 바와 같이 생성된 이미지에 대한 <U>CLIP image embedding</U>과 <U>text embedding</U> 사이의 코사인 거리를 좁혀주는 역할을 하게 되고, 나머지 term은 <U>원본 이미지를 얼만큼 유지할 지</U>(contents 유지력)에 대한 지표가 된다. 해당 유지력에 대한 weight는 $\\lambda_\\text{L2},~\\lambda_\\text{ID}$로 조절할 수 있다.

## Latent mapper
그러나 위에서 보이는 latent optimization 과정은 각 input image와 text prompt에 대해 변동성이 크기 때문에 각 process마다 적절한 hyperparamer 조정 과정이 필요하다. 그렇기 때문에 다음으로 생각해볼 수 있는 방법은 latent starting point $w$에 상관없이(input image를 고정하지 않고), text prompt $t$에 대한 latent mapper $M_t$를 학습하여 $\\mathcal{W}+$로의 manipulation step을 학습하고자 하는 것이다. 즉, pretrained styleGAN 기반으로 latent를 최적화하던 과정을 <U>하나의 함수로 구현</U>한 것이 mapper라는 개념이다.   

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/215380004-e5e82b34-8b57-44fe-9211-6dc1013b3178.png" width="900"/>
</div>

StyleGAN의 각 layer level에 따라 조절하는 image의 detail이 달라지기 때문에 coarse, medium 그리고 fine에 대한 mapper를 다르게 설정하였다. 모든 mapper는 StyleGAN의 mapper layer 수의 절반인 4개의 layer를 가지는 fully-connected networks를 사용했다. 보다 디테일하게 살펴보면, latent mapper는 latent $w_s$를 다이렉트로 최적의 latent vector $w$로 매핑하는 형태가 아니라, <U>변화량을 예측</U>하는 네트워크가 된다.

$$
    \\mathcal{L}_\\text{CLIP}(w) = D_\\text{CLIP}(G(w + M_t (w)), t)    
$$

이외에는 이전에 사용했던 loss term과 모두 동일하다. 이때, $M_t(w) = w - w_s$와 같다고 생각하면 된다. 논문에서는 $\\lambda_\\text{L2} = 0.8$ 그리고 $\\mathcal{L}_\\text{ID} = 0.1$을 사용했다.

$$
    \\mathcal{L}(w) = \\mathcal{L}_\\text{CLIP}(w) + \\lambda_\\text{L2} \\parallel M_t(w) \\parallel_2 + \\lambda_\\text{ID} \\mathcal{L}_\\text{ID} (w)
$$

그런데 고정으로 사용한 것은 아니고 <U>샘플마다 다른 parameter를 사용한 것</U>도 있다고 한다. 사실 여기서 이미 <U>논문의 limitation</U>이 보인다고 할 수 있는게, text based approach를 사용했으면 어느 정도 **latent searching** 과정에서의 편의성은 보장되었지만 **hyperparameter searching** 과정에서의 편의성은 보장되지 않았기 때문에 결국 거기서 거기라고 생각했다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/215389280-7f136251-9a53-4692-ab5a-1c4265161981.png" width="700"/>
</div>



## Global direction
Latent mapper는 optimization 과정에 비해 빠른 inference time을 보장했지만, fine-grained disentangled manipulation(디테일한 부분들을 서로 구분지어 manipulation)하는 과정이 기존 방식에서는 어렵다는 것을 알게 되었고, 이는 결국 $\\mathcal{W}+$ spcae 자체가 가지고 있는 한계점으로 분석되었다. 따라서 논문에서는 앞서 소개했던 style space $\\mathcal{S}$에서 global direction을 찾고자 하였다. 만약 $s \\in \\mathcal{S}$가 style space에서의 code라고 해보자. 그리고 $G(s)$는 해당 style code를 기반으로 생성된 image라고 생각해볼 수 있다. Manipulation을 원하는 attribute에 대한 text prompt가 있다고 생각하고, 여기서 style code $s$의 manipulation direction인 $\\Delta s$를 찾고자 한다. 즉, 우리가 원하는 attribute를 최대화하는 방향으로 $G(s + \\alpha \\Delta s)$라는 이미지를 만들고 싶다는 것이다. 여기서 style direction $\\Delta s$는 <U>manipulation을 원하는 attribute를 제외하고는 다른 attribute를 변화시키지 않아야한다</U>. 변화의 정도를 step size $\\alpha$가 결정하게 된다.   
먼저 CLIP text encoder를 사용하여 joint language-image embedding으로부터 $\\Delta t$를 구하고, 이를 다시 manipulation direction $\\Delta s$로 mapping하는 것이다. Text에 대한 direction $\\Delta t$는 natural language를 text prompt engineering하여 구하고, 이에 해당되는 $\\Delta s$는 각 style channel이 target attribute에 끼치는 영향을 고려하여 설정된다. 이를 보다 자세히 풀어쓰면 다음과 같다. 예를 들어 image embedding에 대한 manifold $\\mathcal{I}$와 text embedding에 대한 manifold인 $\\mathcal{T}$가 있다고 했을 때, image에서의 semantic changes를 이끌어내는 방향 벡터와 text에서의 semantic changes를 이끌어내는 방향 벡터는 서로 어느 정도 collinear(large cosine similarity)를 가질 것으로 예상되고, 특히 각각을 normalization을 거친다면 거의 동일할 것으로 예상된다.   
따라서 원본 style의 이미지 $G(s)$와 변화된 style에 대한 이미지 $G(s + \\alpha \\Delta s)$에 대해 각각의 $\\mathcal{I}$ manifold에서의 embedding을 $i$, $i+\\Delta i$로 표현할 수 있다. 두 이미지의 CLIP 상에서의 차이를 $\\Delta i$로 표현할 수 있기 때문에, 스타일 변화에 대한 text embedding 벡터인 $\\Delta t$가 있다면 둘 사이의 유사도를 통해 global direction을 찾을 수 있게 된다.

##### Natural language to $\\Delta t$

사실 이 부분은 대부분의 CLIP based approach에서 모두 사용하는 코드/방법이기 때문에 알아두는 것이 좋은데, 그 이유는 특정 text prompt에 대한 특징을 단일 image 하나에 mapping하는 것은 사실상 이미지와 텍스트 간의 modality 차이 때문에 바람직하지 않다는 점 때문이다. 만약 '안경을 쓴 사람'이라는 image description 하나로는 정확하게 어떤 머리를, 어떤 얼굴을 그리고 어떤 성별의 사람이 안경을 썼는지 표현할 수 없기 때문에 흔히 image와 text 사이에는 one to one mapping function이 불가능하고, 결국 학습된 CLIP space 또한 이러한 manifold 간의 차이가 있다고 말할 수 있다.   
이런 inconsistency를 줄이기 위해서 제시된 방법 중 하나가 바로 ImageNet zero-shot classification에서 사용된 template이고, 물론 지금 task는 해당 dataset 기반이 아니지만 여전히 text prompt augmentation 관점에서는 유용하기 때문에 사용하였다. 사용하는 방법은 다음과 같다.


\`\`\`python
imagenet_templates = [
    'a bad photo of a {}.',
    'a photo of many {}.',
    'a sculpture of a {}.',
    'a photo of the hard to see {}.',
    'a low resolution photo of the {}.',
    'a rendering of a {}.',
    'graffiti of a {}.',
    'a bad photo of the {}.',
    ...
    ...
]
\`\`\`

위와 같이 중괄호 안에 들어갈 text prompt 부분을 남겨둔 채로 약 $80$개의 prompt engineering template를 사용한다. 일종의 앙상블 혹은 정규화라고 보면 되는데, 각 embedding을 평균내는 것으로 embedding space 상에서의 한 점을 mapping하는 것이 아닌, probability distribution을 mapping하는 효과를 줄 수 있다.

##### Channelwise relevance

그리고 사실 앞서 말했던 사항은 $\\Delta i$와 $\\Delta t$의 관계이고 실상 우리가 접근해야하는 style code $s$를 어떤 식으로 manipulation해야하는지 언급하지 않았는데, 보통 style code를 channel 별로 구분해서 원하는 attribute를 변화하고자 할 때 다음과 같은 메커니즘을 사용할 수 있다. 만약 각 <U>style code의 coordinate</U> $c$를 증가시키거나 감소하여 생성한 이미지에 대해 <U>CLIP image manifold</U> $\\mathcal{I}$에서의 변화를 $\\Delta i_c$라고 하자. 만약 이미지가 coordinate 상관없이 $\\Delta i$라는 방향으로 변화했을 때, <U>style coordinate $c$에 대한 relevance</U>($R_c$)는 $\\Delta i_c$를 $\\Delta i$에 projection한 것과 같다.

$$
    R_c (\\Delta i) = \\mathbb{E}_{s \\in \\mathcal{S}} \\left( \\Delta i_c \\cdot \\Delta i \\right)    
$$

실제로는 100개의 image pair를 사용하여 평균 변화량에 대해 예측하도록 하였다. 우선 각 image pair는 $G(s \\pm \\alpha \\Delta s_c)$로 설정되었으며, 여기서 $s_c$는 $c$ coordinate를 제외하고는 모두 $0$인 벡터이고, $c$ coordinate은 해당 channel의 standard deviation으로 설정한다. 각 채널에 따른 relevance $R_c$를 계산했을 때 만약 relevance가 threshold인 $\\beta$보다 작아진다면 해당 채널을 무시한다. Threshold parameter는 <U>entanglement를 얼마나 허용할 지</U>에 대한 지표가 된다.

$$
    \\Delta s = \\begin{cases}
        \\Delta i_c \\cdot \\Delta i, & \\text{if }\\vert \\Delta i_c \\cdot \\Delta i \\vert \\ge \\beta \\newline
        0, & \\text{otherwise}
    \\end{cases}    
$$


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/215396601-781f1bdf-d647-4ed8-bc39-532e271cb8e0.png" width="1200"/>
</div>



# 결론

이 논문의 main contribution이라고 하면 stylespace를 적절하게 사용하여 CLIP embedding을 어떤 방식으로 image manipulation에 사용할 지 다양한 방법을 적용했다는 점과 text embedding을 직접 활용하는 것이 아니라 CLIP space에서의 유사도를 활용하여 style transfer를 진행했다는 점이 될 수 있다. 그러나 결국 <U>hyperparameter에 취약</U>하여 각 sample마다 clip에 대한 <U>attribute change가 안정적으로 일어나지 않을 수 있다는 점</U>과, 서로 유의미한 관계에 있는 object가 아니라면(호랑이와 사자) <U>image manipulation이 어렵다는 점</U>을 한계점으로 생각해볼 수 있다.   
특히 StyleGAN이 학습된 baseline인 얼굴 이미지와 비슷한 형태의 modality에는 style mixing이 자유로운데, 그에 반해 CLIP space는 보다 broad하고 diversity가 보장된 embedding representation을 활용할 수 있다. 그냥 본인 생각을 소신있게 풀어보자면, StyleGAN baseline이 style mapping이라는 관점에서 image manipulation 연구에 최적화가 되어있지만, 어찌보면 그 때문에 다양한 연구나 많은 paper가 나오지 못하는 원인일 수도 있겠다고 생각해본다.


# StyleGAN-NADA

StyleCLIP과 접근법은 비슷하지만, latent manipulation으로 접근했던 것과 다르게 layer finetuning으로 접근한 [styleGAN-NADA](https://arxiv.org/pdf/2108.00946.pdf) 방식도 있다. 결국 이 논문에서 해결하고자 하는 task도 image generator를 이미지 manifold 상의 특정 target domain으로 mapping하는 과정에서 <U>text prompt만 가지고 guidance(supervision)</U>을 줄 수 있다면, 굳이 style에 대한 image를 supervision으로 사용하지 않고도 domain에 한정된 image manipulation에서 벗어나 <U>다양한 형태의 스타일링이 가능</U>하다는 것이다. 아래는 실제로 StyleGAN-NADA 방법을 통해 다양한 스타일로 변환된 결과를 보여준다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/215648852-17c8a3ef-4706-4bd5-ac3b-63b845322d30.png" width="500"/>
    <img src="https://user-images.githubusercontent.com/79881119/215648859-836d6a4d-0823-4d12-8d9e-7e1778baf484.png" width="500"/>
</div>


**StyleCLIP**이 가졌던 단점 중 하나는 $\\mathcal{W}+$ space, $\\mathcal{S}$ space 모두 결국 사전 학습된 StyleGAN의 domain 내에서 image manipulation이 진행되기 때문에 in-domain이라는 문제가 해결될 수 없다는 점이었다. 그러나 해당 논문이 접근했던 방식과 더불어 CLIP의 text guiding이 쉽게 적용된 것은 아니었다. 자칫 잘못된 방법으로 최적화를 진행하게 되면 adversarial solution(실제로 현실적인 image manifold에서 벗어나 artifact가 많이 생기는 현상)으로 **target domain이 학습될 수 있기 때문**이다. 뒤에서도 추가로 계속 설명하겠지만 최적화에 사용된 loss 형태는 StyleCLIP에서 사용한 방식과 유사하게 CLIP embedding space에서 **text direction**과 **image direction**을 맞춰주는 것이다(나란히).


# Related works

그동안 text guided image synthesis 관점에서 다양한 연구들이 진행되어왔다. 사실 CLIP이라는 논문은 학습 과정에서 image와 text의 관계에 대해 초점을 맞춘 것 뿐이지만, 이 연구는 text prompt를 활용한 image synthesis나 manipulation task에도 다양하게 활용될 수 있었다. StyleCLIP 방법에서도 확인할 수 있지만, CLIP을 사용하여 StyleGAN과 같은 사전 학습 모델에 대한 최적화 관점으로 접근하는 방식이 대부분 latent optimization 방법이고, <U>특정 이미지를 생성하는 latent code를 찾고자 하는 것</U>이 주된 목적으로 작용했다. 하지만 StyleGAN-NADA에서는 이런 방법들 대신 <U>image generator 자체를 text prompt guidance를 통해 최적화</U>하여, 한정된 domain에서 벗어난 image manipulation이 가능하게 하였다.

그리고 무엇보다 text-guided synthesis와 관련되어 찾아볼 task는 한정된 데이터를 기반으로 generator를 학습시키는 연구들이다.  일종의 few-shot learning을 generator에 적용하고자 했던 연구들은 적은 수의 데이터가 generator를 overfitting시키거나 mode-collapse(샘플의 다양성이 떨어지는 현상)를 일으킬 수 있는 문제가 있었기 때문에 augmentation 방법을 사용하거나 auxiliary task를 사용하여 representation을 보다 풍부하게 학습하고자 하였다. Style-NADA에서는 이러한 데이터의 부족함 때문에 생기는 overfitting이나 mode-collapse를 걱정할 필요 없이, 어떠한 데이터도 사용하지 않고 단순히 CLIP based text prompt만 guidance로 사용하게 된다.

추가로 이 논문을 이해하기 위해 StyleGAN, StyleCLIP에 대한 이해가 필요하지만 이 부분은 이미 작성한 내용이므로 넘어가도록 하겠다. StyleCLIP 관련 내용은 위에서 확인해볼 수 있고, StyleGAN 관련 내용은 게시글로 포스팅하였다([참고 링크](https://6unoyunr.github.io/blog/gan2)).


# CLIP based guidance


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/215648862-2005c3e6-3ee6-4692-b814-eee2b7aff28b.png" width="500"/>
</div>


결론적으로 논문에서 사용한 방식을 간단한 그림으로 나타낸 것이 위의 figure이다. Source domain에 대해 사전 학습된 generator $G$를 copy한 뒤, 하나는 <U>frozen</U>하여 계속 source domain의 이미지를 만들게 하고 다른 하나는 <U>fine tuning</U>하여 target domain을 만들게 하되, 이때의 guidance를 CLIP loss로 주는 방식이다.

물론 저자들은 StyleCLIP에서 겪었던 시행착오와 동일하게 Global loss부터 시작하여 왜 directional CLIP loss가 중요한지 설명하였다. 사실 해당 부분이 **CLIP based style transfer, image manipulation 논문**에서 가장 중요한 점이라고 생각되어 디테일하게 짚고 넘어가고 싶었다.

### Global loss

가장 쉽게 생각할 수 있는 것은 generator가 생성한 이미지와 target이 되는 text prompt 사이의 CLIP loss를 최적화하는 방식이다.

$$
    \\mathcal{L}_\\text{global} = D_\\text{CLIP} (G(w), t_\\text{target})
$$

Latent code $w$가 주어졌을 때, image generator $G$에 의해 생성된 이미지를 image encoder $E_I$에 통과시켜 나온 임베딩과, target text prompt를 text encoder $E_T$에 통과시켜 나온 임베딩 사이의 코사인 유사도를 계산한다. StyleCLIP과 다른 점은 위의 loss에서 최적화하고자 하는 parameter가 $w$가 아닌 $G$의 parameter라는 점이다. 아무튼 가장 간단하게 생각할 수 있는 방법이지만, 위의 방법을 사용하게 되면 <U>adversarial solution</U>으로 최적화가 되는 문제가 발생한다. Generator parameter가 고정되지 않고 학습이 되는 상황에서 Wassertein loss나 adversarial loss와 같이 real image manifold를 유지해줄 수 있는 방법이 없기 때문에 이러한 문제가 발생한다. 또다른 문제는 이러한 loss term은 <U>mode collapse</U>를 일으킨다는 것인데, 바로 다음과 같은 그림을 보면 이해가 쉽다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/215648868-84893a52-5139-413f-8a42-45c5777f728f.png" width="500"/>
</div>


예를 들어 **고양이 이미지**가 target domain에 해당되고 **강아지 이미지**가 source domain에 해당된다고 하자. 둘을 각각 CLIP space(image embedding) 상에 올려놓은 것이 붉은색 점들과 푸른색 점들이다. 그리고 **‘고양이’라는 text prompt**에 대한 CLIP space(text embedding)상의 점과 **‘강아지’라는 text prompt**에 대한 CLIP space(text embedding)상의 점을 각각 표시한 것이 청록색/보라색 점에 해당된다.

단순히 생성된 이미지가 text prompt에 부합하고자 한다면 학습은 (b)에서 보는 바와 같이 진행된다. 모든 강아지 이미지가 단순히 text prompt에 가장 가까운 image embedding을 만들게끔 하는 것이 minimizer가 되기 때문에, 붉은색 점들이 그리는 분포와 같이 <U>다양한 고양이 이미지를 생성하지 못하고</U> 녹색 분포와 같이 협소한 공간에서 이미지를 생성하게 된다. 이를 mode-collapse라고 부른다.

### Directional CLIP loss

위와 같은 문제가 발생할 수 있기 때문에 StyleCLIP에서 사용한 방법과 같이 global direction approach를 사용하게 된다. Global direction approach란, 이미지를 target text prompt에 가까워지도록 움직이는 것이 아니라 source text에서 target text로의 방향만 알려주고, 이를 source image 벡터에 더하게 되면 방향에 대한 정보만 더해줄 수 있기 때문에 샘플의 다양성은 유지할 수 있다는 것이다. 

$$
    \\begin{aligned}
        \\Delta T =& E_T(t_\\text{target}) - E_T (t_\\text{source}) \\newline
        \\Delta I =& E_I (G_\\text{train}(w)) - E_I (G_\\text{frozen}(w)) \\newline
        \\mathcal{L}_\\text{direction} =& 1-\\frac{\\Delta I \\cdot \\Delta T}{\\vert \\Delta I \\vert \\vert \\Delta T \\vert}
    \\end{aligned}
$$


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/215648873-6a890147-2e22-4e6a-a66e-629cb218cb7f.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/215648877-d6a9ac55-27b5-4fa5-ab6d-2cf4085ff31a.png" width="500"/>
</div>


이 방법에 대해 잘 나타낸 것이 위에 보이는 그림이다. 이전과는 다르게 방향을 일치시켜주는 것만으로도 충분히 target domain에 유사한 이미지를 생성할 수 있는 것을 확인할 수 있다.


# Layer freezing

Domain shift가 texture를 바꾸는 방식(예를 들어, 실제 사진을 그림처럼 바꾸는 과정)의 경우 같은 training scheme을 적용하더라도 mode collapse가 발생하거나 overfitting이 발생했다고 한다. 기존의 few-shot domain adaptation 방법들에서 network weights의 일부를 제한하는 형태를 통해 synthesized result의 퀄리티를 높였는데, 저자들은 이 방법이 zero-shot인 이 task에서도 적용될 수 있을 것이라 생각했다. <U>파라미터 수를 줄이고 최적화하는 방법</U>을 통해 간접적으로 학습되는 network의 크기를 줄일 수 있고, 적은 데이터셋에 대해 overfitting을 방지할 수 있는 <U>정규화 작업</U>이 된다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/215648878-55b6b464-864e-46b3-bde7-8dbda2792575.png" width="500"/>
</div>


그렇다면 학습할 레이어는 어떻게 고를 수 있을까? 

이를 설명하기 위해서 저자들은 **본인들이 아이디어를 빌드업한 과정**을 차례대로 설명해준다. StyleGAN을 읽어봤다면 알겠지만 각 위치에 들어가는 style code가 서로 다른 semantic attribute에 영향을 끼친다. 예를 들어 위의 그림에서 $w_1$, $w_2$는 상대적으로 coarse feature를 담당하는 latent code가 될 것이고, 그와 반대로 $w_l$에 가까워질수록 fine feature를 담당하는 latent code가 될 것이다. Image manipulation 논문에서 좋은 효과를 보였던 $\\mathcal{W}+$도 어찌보면 각 layer 층에서 style에 영향을 미치는 style code를 서로 다르게 사용하였기 때문이라고 할 수 있다. 이처럼 특정 스타일의 이미지 혹은 도메인의 이미지를 만들기 위해서는 여러 개의 $w_i \\in \\mathcal{W}$ 중 가장 도메인 변화에 큰 영향을 끼치는 layer를 위주로 fine-tuning하는 것이 효과적이고, high quality 이미지를 만들 수 있는 방법이다.

우선 $k$개의 layer를 골라야하기 때문에 랜덤한 갯수 $N$개의 latent code를 $M(z) = w$의 형태로 추출해내고, 이를 $\\mathcal{W}+$로 각 레이어에 대한 style code로서 replicate하게 된다. 이 내용은 본인 게시글 중 image manipulation 관련 글을 보면 보다 이해가 쉬울 것이다([참고 링크](https://6unoyunr.github.io/blog/imagemanipulate)). 그런 뒤 $i$번의 iteration을 통해 StlyeCLIP의 latent-code optimization을 진행하고(여기서는 학습되는 parameter가 generator가 아니라 latent code라고 보면 된다), 각 latent code의 변화를 측정한다. 그렇게 가장 많이 변화한 $w$ code를 기준으로 $k$개의 layer를 고르고, 이를 학습에 사용한다.


# Latent-Mapper mining

앞서 말했던 방법들을 통해 생성되는 이미지의 정규화를 효과적으로 진행했음을 알 수 있다. 그러나 이러한 방법들은 generator로 하여금 완전히 target domain을 생성하게끔 학습하지 못하게 했는데, 예를 들어 dog를 cat으로 바꾸는 task에서, fine-tuning을 거친 네트워크가 강아지와 고양이 모두 생성하는 네트워크가 되거나, 혹은 강아지랑 고양이 그 사이의 애매한 이미지를 만들게 된다는 것이다. 이러한 문제들을 피하기 위해 StyleCLIP의 latent-mapping 방식을 같이 사용하여, latent code를 cat과 관련된 region으로 옮기는 작업을 추가했다고 한다.


# 결론


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/215648880-5fcf3a4d-9426-46d1-ab6b-9fc4d76902c0.png" width="1000"/>
</div>


최적화 과정에 있어서 해당 논문은 <U>image embedding, text embedding</U> 모두 guidance로 사용할 수 있고, 같은 최적화 방법이 두 guidance 모두에서 잘 작용했다고 한다. 이 논문이 가지는 contribution은 StyleCLIP과는 다르게 StyleGAN의 generator 부분의 parameter를 효과적으로 fine-tuning하는 방식을 사용했고, 이를 통해 기존에 할 수 없었던 out of domain style 적용이 효과적으로 이루어질 수 있음을 보여주었다. 다만 최적화 과정에서 latent code를 배제했을 때 domain이 섞이는 현상을 해결하기 위해 latent mapping 방식을 같이 사용한다던지, 학습할 layer를 mining하는 과정에 시간이 걸린다는 점이 문제가 될 수 있다. 그럼에도 불구하고 **CLIP을 사용하여 기존 style mixing 방식과는 다르게 접근**했다는 점이 인상깊었다.
`,DO=`---
title: "Zero-shot Text-Guided Object Generation with Dream Fields에 대하여"
category: "ai papers"
publishedAt: "2023-02-07"
thumbnail: "https://user-images.githubusercontent.com/79881119/217228342-09cbf4d6-be6e-4086-98dc-e34b2a8a5fe2.png"
---


# 들어가며 …

이 논문에서 **가장 핵심이 되는 키워드**만 따로 생각해보면, text representation을 통해 학습된 multi-modal image 관계를 사용하여 3D object를 렌더링하는 것이다. 3D generation 혹은 rendering의 경우 획득이 비교적 간단한 데이터셋인 이미지와는 다르게 captioning된 3D dataset이 필요하다. 이는 3D generation network로 하여금 한정된 갯수(pool)의 category만 생성할 수 있다는 한계와 문제점을 가진다(ex. [ShapeNet](https://arxiv.org/abs/1512.03012)).


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/217228269-31afad43-1d31-4c0a-9976-eeff42540e1f.png" width="400"/>
</div>


그렇기 때문에 한정된 구조나 texture를 가지는 기존 방식에서 벗어나, 웹 상에서 대량으로 수집된(대략 4억 장의 text prompt-image pair) [WebImageText dataset](https://github.com/google-research-datasets/wit)에 학습된 [CLIP 네트워크](https://arxiv.org/abs/2103.00020)를 사용하여 generation에 guidance를 주는 방법을 선택하였다. 구체적인 학습 과정은 뒤에서 마저 설명하겠지만 간단하게 컨셉만 보면, 여러 방향에서 수집된 camera view에 대해 최적화된 Neural Radiance Field(NeRF)가 target caption과의 유사성이 높게끔 학습시킨다. 이때 유사성에 대한 guidance로 사용하는 것이 바로 **CLIP network**이며, 단순히 CLIP network를 통한 loss를 주게 되면  3D 구조가 무너지거나 fidelity가 악화되는 문제가 발생하였기 때문에 여기에 추가로 간단한 geometric prior를 주었다. **Geometric prior**에는 sparsity를 유발하는 transmittance에 regularization,  scene bound, 그리고 새로운 MLP 구조가 포함된다. 


# Why zero-shot is important?

논문을 읽기 시작하면서 **가장 근본적인 질문**이 문득 떠올랐다. 사실 충분히 Neural Field에 대한 연구는 진행되었고, 주어진 dataset만 있다면 high fidelity의 3D representation을 렌더링하거나 새로운 각도나 방향에서의 image를 생성하는 것은 그리 어렵지 않을 것이다. 그리고 zero-shot에 대해서 SOTA 연구들을 살펴보면, 그 성능이 fully supervised learning을 진행한 연구에 비해 지나치게 떨어지는 경우도 있다. 그렇기 때문에 zero-shot 논문들을 보게 되면 **연구의 필요성**에 대한 설명이 필수적이라는 것을 알 수 있다. 3D object model은 흔히 Game이나 Virtual reality를 쉽게 접할 수 있는 환경에서 많이 사용된다. Unity나 언리얼 엔진같은 프로그램을 다뤄봤다면 알 수 있겠지만 우리가 가상 현실이나 게임에서 볼 수 있는 모든 형태의 3D object, 하물며 2D object 조차도 digital software(Blender나 Maya 등등)에 의존해서 생성되고 여기에 texture(무늬)를 입히는 과정도 디자인 작업이 필요하다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/217228280-adf8cbe4-089f-4d9c-8520-10ae5c91075e.png" width="400"/>
</div>



사실상 NeRF와 같은 연구에서 사용되는 모든 형태의 object도 이에 기반한 작업물이고, 앞서 설명했던 WebImageText dataset과는 다르게 인터넷 상에서 무료 에셋으로 열려있는 3D 오브젝트는 퀄리티의 일관성이나 다양성 측면에서 쉽게 **수집하기 어렵다**는 문제가 있다. 딥러닝을 하기 위해서는 데이터셋이 필수적인데, 데이터셋을 모으고자 그래픽 디자이너만 몇 만을 고용해서 오브젝트를 만들고 있을 수도 없고, 디자이너들 각각의 스타일도 모두 다르기 때문에 균일한 distribution을 구성할 수 없다는 현실에 부딪힌다.

그렇기 때문에 데이터셋 수집이 어려운 환경에서 **접목시킬 수 있는 여러 방법론**(meta learning, low-shot learning, domain generalization, unsupervised-domain adaptation 등등) 중 이 논문에서는 zero-shot learning을 적용하고자 한 것이고, 위에서 설명한 내용이 사실상 이 논문에서 task를 정의하기 전에 설정한 기존 연구들의 problem 혹은 limitation이라고 볼 수 있다.


# Challenging in multimedia application

NeRF와 같은 연구 목적이라면 단순히 3D dataset을 구성하는 과정에서 몇 가지의 category만 구성하고, 각 카테고리에 맞는 오브젝트의 형태만 구성하면 된다. 하지만 만약 오브젝트를 생성하는 목적이 실제 멀티미디어 환경에서 소비자를 만족시킬 목적이라면 의자와 같은 단순한 사물도 목적에 따라(소파, 벤치, 휠체어 등등) 혹은 재료에 따라 texture가 달라지고 이를 구성해야하는 어려움이 따른다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/217228284-122e98ee-493f-4cec-b8ac-78d00445e54e.png" width="300"/>
    <img src="https://user-images.githubusercontent.com/79881119/217228288-b8e3512a-1b8f-413b-bad8-a12e5f886c0a.png" width="900"/>
</div>


기존 approach들이 3D dataset을 단순히 **point cloud**나 **voxel grid**, 혹은 **triangle mesh**로 형태만 표현하던 것과는 다르게 시각적인 방향에 대한 3D geometry와 texture를 고려해야한다는 어려움이 생긴 것이다.


# Automatically generate open-set 3D models

따라서 저자들이 연구하고자 한 dream fields 연구는 이러한 기존 방식들의 제약으로부터 벗어나기 위해 open-set의 natural language prompt로부터 image representation을 학습하고, 이를 실제로 다양한 zero-shot task에 적용했을때 좋은 성능을 보였던 CLIP을 사용하였다. Dream fields는 NeRF를 학습하는 과정에서 scene의 geometry와 color 모두 perceptual metric을 최대화하는 방향으로 학습되며, 논문 제목에서 알 수 있듯이 학습에 3D training dataset이 아예 사용되지 않는다. NeRF 연구에서는 다양한 방향에서 획득한 RGB photo를 ground truth로 학습하여 새로운 방향에서의 image reconstruction을 진행한다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/217228289-d2db45e9-7a88-411d-a44d-f53a51e0bd67.png" width="700"/>
</div>



따라서 위에서 보는 바와 같이 학습된 $F_\\theta$가 implicit 3D space를 학습하고, 딥러닝이 내포하는 특정 object에 대한 representation을 viewing point, direction을 통해 샘플링하여 새로운 각도에서의 이미지를 생성할 수 있다는 것이다. NeRF의 장점은 interpolation이 부드럽고 색 변화에도 자연스럽게 대처할 수 있다는 점이다. NeRF의 퀄리티를 높이기 위해 이후 여러 연구들이 진행되었지만, 모든 연구들의 공통점은 description으로부터 novel image를 생성하는 것이 불가능하다는 것이다. Input image들이 존재하고, 이에 기반하여 새로운 각도의 이미지를 렌더링할 수는 있으나 **text description**을 3D semantic feature로 사용할 수 없다. CLIP의 image-text representation과 NeRF의 volumetric rendering 과정 모두 미분 가능하기 때문에  이른바 ‘dream fields’는 zero-shot으로도 충분히 학습이 가능하다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/217228291-99eb8691-35bd-4c4c-a80b-42bdfe2f116f.png" width="500"/>
</div>



그러나 아무런 regularization 없이 CLIP representation(text prompt에 의한 supervision)만 활용하여 학습하다보니 다양한 **artifact**가 발생하였고, 이를 해결하기 위해 **geometric constraints**를 추가했다고 한다.


# Contribution

이 논문이 가지는 contribution은 비교적 명확하기도 하고, 실제로 저자들이 introduction에 작성하였다. 해당 내용을 간단하게 요약해보면,

- Image/Text pair로 학습된 CLIP 모델을 통해 3D shape이나 multi-view dataset 없이도 NeRF를 최적화할 수 있었다.
- Zero-shot description에 대해 다양한 3D object generation이 가능하다.
- Geometric priors를 적용했을 때 fidelity가 상승하였다.

이 중 마지막 **geometric prior**과 관련된 내용은 사실 실험을 진행하면서 제안된 sub-contribution으로 보는 것이 더 적절할 것 같고, 위의 두 내용이 이 논문의 가장 중요한 포인트라고 생각한다.


# Related works

해당 연구는 [DeepDream](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)에서 가장 큰 영감을 받았다고 한다. 사실 본인은 이 부분은 살짝 too much하다고 보았는데, 아무래도 NeRF를 사용해서 완전히 새로운 object를 만들어내고자 하는 것은 비교적 최근 연구인 CLIP을 기반으로 하지 않았더라면 아이디어 빌드업이 불가능할 정도로 어려운 연구라고 생각했기 때문이다.  Neural network를 통해 원하는 이미지를 생성하고자 하는 연구들은 많이 진행되었다. 대표적으로 pre-trained된 generative network를 사용하여 추가 학습 없이 image를 생성하는 것은 GAN inversion이나 latent optimization 등등 많은 연구가 진행되어왔다. 이 논문에서 CLIP을 제외하고는 가장 유의미한 관계를 가지는 연구가 style transfer의  관점에서 진행한 differentiable image parameterization라고 한다.

솔직히 이 부분 보면서 어이가 없었던 것은 논문을 읽다보면 알 수 있겠지만 CLIP을 사용한 것 자체에서 기존 style transfer 연구들과는 차별성을 두어야하고 오히려 본인들이 작성했던 paper보다 해당 paper에 insight를 줄 수 있는 논문이 훨씬 많아 보이는데도 불구하고 인용수를 늘리기 위해서 작성한 느낌이 강하게 들었다. 고의가 아니라면 미안하지만 본인은 related works의 초반부가 대체 왜 들어갔는지 이해가 안된다. 아무튼 계속하자면, 기존의 style이나 content 기반의 loss를 image-text loss로 대체하여 text prompt에 기반한 style 및 content에 대한 generation의 controllability를 늘릴 수 있었다. 왜냐하면 style transfer의 관점으로 접근했을 때는 target이 되는 style image feature가 필요하고, 실제로 style이 적용되었을 때 quality 또한 안정적이지 않기 때문에 문제였지만 CLIP에 기반한 style transfer는 따로 style image가 필요 없고 단순히 description만 사용하기 때문이다. [기존 방식](https://distill.pub/2018/differentiable-parameterizations/)의 경우 한정된 geometry(예를 들면 토끼 object)나 optimized된 texture에만 한정된 transfer를 사용했지만 dream fields 연구를 통해 open-ended text guided generation이 가능하게 되었다.

물론 해당 paper 이외에도 CLIP을 사용한 3D generation 연구들 중  [CLIP-Forge](https://arxiv.org/abs/2110.02624)도 있지만, 해당 연구에서는 geometry만 생성하는 decoder를 사용했고 ShapeNet category에 대해서만 guidance를 주었다는 점에서 out of domain generation이 불가능하다는 문제가 있다. [Text2Shape](http://text2shape.stanford.edu/)연구는 아래에 보이는 것과 같이 text-conditional WGAN 학습을 통해 voxelized objects를 생성하는 task를 진행했지만, 마찬가지로 각 ShapeNet 카테고리를 생성하는 task에서 벗어나지 못한 점과 NeRF와는 다르게 voxel 특성상 한정된 resolution을 가지는 문제가 있었다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/217228296-61ab9531-1744-4afd-afed-18d554b37d55.png" width="1200"/>
</div>



실제로 voxel 기반의 shape generation을 보면 그다지 성능이 좋지 않은 것을 확인할 수 있다.

Related works에 재밌는 연구도 있는데, MIT 연구실에서 한 사람이 혼자 진행한 연구 중  ‘[Evolving evocative 2D views](https://arxiv.org/pdf/2111.04839.pdf)’는 약 4쪽짜리 페이퍼로 아카이브에 올라가있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/217228302-dadb4e0a-b689-4857-ad05-5ad936f86c04.png" width="800"/>
</div>



이 연구에서는 3D supershapes(3D 공간에서의 실제 형태)를 하나의 viewpoint에 대해 projection한 2D view를 최적화하는 loop 알고리즘을 통해 CLIP score를 맞추고, manually coloring하는 과정을 거친다. 이외에도 CLIP을 사용한 다양한 연구들이 소개되는데, 예를 들어 human SMPL model의 vertices나 textures를 수정하여 stylize하는 [ClipMatrix](https://arxiv.org/abs/2109.12922)연구나, signed-distance fields를 editing할 수 있는 인터페이스를 구축한 연구도 있다. 이미지의 경우에는 더 다양한 연구들이 진행되었는데, SIREN network의 weight를 CLIP을 통해 학습함으로써 image generation에 적용하거나, VQGAN-CLIP, StyleGAN-CLIP 등등 image generation에서 사용될 수 있는 다양한 네트워크를 CLIP과 함께 사용한 실험들이 있다. 물론 2D image에 대한 generative model를 NeRF와 함께 사용하는 3d-aware GAN과 같은 연구들도 진행되었지만, 대부분의 연구는 open-set text에 대한 generation에 대한 능력이 배제된 것을 알 수 있다.


# Backgrounds

### NeRf

NeRF는 사실 앞서 논문 리뷰로 따로 다루기도 했고, 하도 유명한 논문이다보니 사전 지식에 대해서 아는 사람들이 많을 것 같다. NeRF는 scene의 density나 color를 MLP를 통해 학습하게 되고, 이때 network에 query로 들어가는 것이 특정 3D point의 좌표인 $(x,~y,~z)$와 viewing direction $(\\theta,~\\phi)$이다. Notation을 보고 어느 정도 짐작은 가겠지만 네트워크를 학습하는 과정은 canonical space와 spherical space의 좌표계를 동시에 사용하며, 이때 canonical space를 implicit하게 학습하는 MLP는 해당 위치의 density 혹은 transmittance를 연산하는데 집중하게 되고 spherical space는 해당 위치의 color(RGB value)를 연산하는데 집중하게 된다. 이렇게 학습된 파라미터를 통해 어떤 각도에서나 물체가 보이는 모습을 2D image로 렌더링할 수 있게 된다.

보다 단순화된 형태로, 이 논문에서는 MLP가 3D position인 $x$를 input으로 받은 뒤 각 위치에 대한 density $\\sigma_\\theta(x)$와 color $c_\\theta (x)$를 output으로 내보내게 된다. 이렇게 MLP를 통해 추출된 정보를 기반으로, 특정 viewpoint에서의 image를 다음과 같은 식을 토대로 렌더링하게 된다. 각 픽셀은 다음과 같이 ray $r(t)$에 따라 volume rendering equation 결과값을 기반으로 색이 정해진다.

$$
    C(r, \\theta) = \\int_{t_n}^{t_f} T(r, t) \\sigma_\\theta (r(t)) c_\\theta (r(t)) dt  
$$


위의 식에서 $T(r, t)$는 transmittance로, $t_n$에서 출발한 ray가 $t$위치에 도달할 때까지 absorb(object에 의해 decaying)되지 않을 확률을 의미한다. 즉, scene boundary $t_n$ 부터 $t$ 까지 물체가 존재하지 않을 확률값으로 해석하면 된다.

$$
T(r, t) = \\exp\\left( -\\int_{t_n}^t \\sigma_\\theta (r(s))ds \\right)
$$

그러나 실제로는 이와 같이 ray를 따르는 모든 point를 기준으로 적분할 수 없기 때문에 다음과 같이 ray를 smaller segments $(t_{i-1} \\leq r_i < t_i)$로 분리하여, 각 segment 내에서는 $\\sigma$와 $c$가 어느 정도 constant하게 유지된다고 가정을 한다. 만약 point가 아주 세밀하게 나눠진다면 실제로 이 가정을 따라갈 수 있게 된다.

$$
C(r, \\theta) \\approx \\sum_i T_i (1 - \\exp (-\\sigma_\\theta (r(t_i))\\delta_i))c_\\theta (r(t_i))
$$

$$
T_i = \\exp \\left( -\\sum_{j < i} \\sigma_\\theta (r(t_j))\\delta_j \\right),~\\delta_i = t_i - t_{i-1}
$$

MLP parameter $\\theta$와 pose $p$에 대한 setting만 있다면, 각 pixel에 대한 ray를 샘플링하여 color 및 density를 계산할 수 있고 이를 모두 rendering하면 특정 픽셀에서의 color $C(r, \\theta)$를 계산할 수 있고 image $I(\\theta, p)$를 구할 수 있게 된다.

또한 MLP를 사용했을 때, 3D representation에 대한 implicit한 학습은 어렵기 때문에 feature의 representation을 고차원으로 올려주기 위해 positional encoding을 더해준다.

$$
\\gamma (x) = (\\cos (2^l x),~\\sin (2^l x))^{L-1}_{l=0}
$$

$L$은 positional encoding의 level을 의미한다. 실제 구현 단계에서는 [mip-NeRF](https://arxiv.org/abs/2103.13415)에서 제시된 integrated positional encoding(IPE)를 사용했다고 한다.

### IPE(Integrated Positional Encoding)


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/217228304-e1060464-073b-4717-8264-dacccf47592a.png" width="700"/>
</div>



IPE의 컨셉은 위와 같은 사진에서 확인해볼 수 있다. NeRF의 경우에는 모든 frequency를 동일하게 encoding하는데, 이렇게 되면 좌측 이미지에서 보이는 것과 같이 고차원의 encoding value는 aliasing이 발생하게된다(encoding frequency보다 sampling rate이 더 작아지는 경우 발생). 그렇기 때문에 우측 이미지와 같이 샘플링 영역에 대해 gaussian과 같이 적용시켜, 샘플링하는 region이 동일하게 취급당할 수 있게끔 해준다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/217228305-fc2e71d1-590a-4195-b89e-f7bc210faca6.png" width="300"/>
</div>



카메라의 center of projection $o$로부터 pixel이 존재하는 평면에 수직인 방향(normal vector)를 축으로 뻗어나가는 원뿔을 생각해보자. 원뿔의 꼭짓점은 점 $o$이며 $d$벡터가 원뿔의 높이를 대변한다고 생각할 수 있다(아래 그림 참고).


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/217228310-064b35c9-b705-44a5-9959-3d4156c97254.png" width="700"/>
</div>



그렇다면 픽셀이 존재하는 평면과 평행하면서 center of projection $o$로부터 일정 거리 떨어진 곳($o+d$)에 하나의 image plane을 생각해볼 수 있고, 이때 해당 pixel의 반지름(혹은 width)을 $\\dot{r}$이라는 parameter로 정의할 수 있다. 이때 위의 그림에서 보이는  $t_0$와 $t_1$ 사이의 position $x$들은 다음과 같이 정의된다.

$$
F(x, o, d, \\dot{r}, t_0, t_1) = \\left(t_0 < \\frac{d^\\top(x-o)}{\\vert \\vert d \\vert \\vert_2^2} < t_1 \\right) \\& \\left( \\frac{d^\\top (x-o)}{\\vert\\vert d \\vert\\vert_2 \\vert\\vert x-o \\vert\\vert_2} > \\frac{1}{\\sqrt{1+(\\dot{r}/\\vert\\vert d \\vert\\vert _2)^2}}  \\right) 
$$

위의 내용은 사실 모든 $x$에 대해 샘플이 $n$번째 원뿔 요소에 포함되는지 여부고, 이를 실제로 positional encoding에 적용할 수 있어야한다. 가장 간단한 방법은 앞서 소개한 positional encoding $\\gamma (x)$를 모든 point에 대해 구한 후, 특정 영역 내의 point를 평균낼 수 있다.

$$
\\gamma^* (o, d, \\dot{r}, t_0, t_1) = \\frac{\\int \\gamma (x) F(x, o, d, \\dot{r}, t_0, t_1) dx}{\\int F(x, o, d, \\dot{r}, t_0, t_1) dx}
$$

하지만 공식에서 볼 수 있듯이 결국 모든 샘플에 대해 평균내는 과정도 intractable하기 때문에 결국 평균 연산 또한 효율적으로 계산해서 approximation해야하는 문제가 발생한다. 그렇기 때문에 가장 우측에 보이는 것과 같이 $\\gamma(x)$의 expectation을 따르는 multivariate gaussian을 가정하는 integrated positional encoding을 사용하였다.

### Image-text models

CLIP과 같은 image-text를 함께 활용한 모델은 다음과 같이 간단한 공식으로 표현할 수 있다. 각 모델은 이미지 인코더인 $g$와 텍스트 인코더인 $h$로 구성된다. 각각의 인코더는 modality에 따른 data를 embedding space로 mapping하는 역할을 한다. 만약 이미지 $I$에 대응되는 text prompt $T$가 있다면, image embedding $g(I)$와 text embedding $h(T)$는 서로 유사한 방향 벡터를 가지게 된다. 여기서 방향 벡터라는 의미는 CLIP space를 상정했을때, 텍스트와 이미지 각각 특정 벡터로 치환되는데, 그와 동시에 L2 norm이 $1$이 되게끔 normalize를 진행한다. 실제 CLIP에서 classification을 진행할 때 코드는 다음과 같다.

\`\`\`python
# Prepare the inputs
image, class_id = cifar100[3637]
image_input = preprocess(image).unsqueeze(0).to(device)
text_inputs = torch.cat([clip.tokenize(f"a photo of a {c}") for c in cifar100.classes]).to(device)

# Calculate features
with torch.no_grad():
    image_features = model.encode_image(image_input)
    text_features = model.encode_text(text_inputs)

# Pick the top 5 most similar labels for the image
image_features /= image_features.norm(dim=-1, keepdim=True)
text_features /= text_features.norm(dim=-1, keepdim=True)
similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
values, indices = similarity[0].topk(5)

# Print the result
print("
Top predictions:
")
for value, index in zip(values, indices):
    print(f"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%")
\`\`\`

중간에 ‘top 5 most similar labels for the image’ 부분에서 확인할 수 있듯이, \`.encode_image()\` 메소드와 \`.encode_text()\` 메소드를 통해 인코딩된 embedding을 normalize하여 비교하는 것을 볼 수 있다. 만약 크기를 정규화하지 않는다면 실제 cosine similarity(distance)를 제대로 반영하지 못할 수 있기 때문이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/217228312-43dc415c-c74d-43ef-b957-85011bccdb8e.png" width="500"/>
</div>



$N$개의 text-image pair가 주어지게 되면 서로 대응되는 $N$개의 쌍에 대해서는 positive pair로, 나머지 $N^2-N$개의 쌍에 대해서는 negative pair로 간주하여 contrastive learning을 진행한다. InfoNCE를 대칭 matrix 형태로 연산하게 되므로 마치 text embedding에 대한 information과 image embedding에 대한 information 사이의 관계(mutual information)을 극대화하는 문제로 치환된다.


# Methods

### Object representation

NeRF의 scene representation 학습 방법을 토대로 설계되면서 dream fields는 MLP의 parameter set $\\theta$를 통해 neural network의 query인 position $x$에 대한 output인 $\\sigma_\\theta (x)$와 $c_\\theta(x)$를 추출하게 된다. 따라서 NeRF space는 MLP 내부에 implicit하게 내장되며, 이는 3D space의 각 부분에서의 density와 color로 대표되는 가상의 공간을 만든 것과 같다. 앞서 설명했던 바와 같이 원래 NeRF 논문과는 다르게 simplified된 버전으로 camera의 viewing direction과는 무관한 네트워크를 가정하였으며, 이는 viewing direction을 사용했을 때의 장점이 없었기 때문이라고 한다.  카메라의 pose $p$를 알게 되면 해당 카메라가 존재하는 평면 상의 모든 pixel에 대한 ray r$(t)$를 가정할 수 있고, 각 ray에 대한 sampling equation

$$
T_i = \\exp \\left( -\\sum_{j < i} \\sigma_\\theta (r(t_j))\\delta_j \\right),~\\delta_i = t_i - t_{i-1}
$$

을 통해 이미지 $I(\\theta, p)$를 렌더링할 수 있다. 샘플링 갯수는 곧 fidelity와 관련이 있고, 논문에서는 $192$를 고정값으로 사용했다고 한다.

### Objective function

실질적으로 dream fields가 최적화될 수 있는 object function은 의외로 간단하다. NeRF space는 MLP parameter $\\theta$에 대해 미분 가능하고 마찬가지로 CLIP encoder $g, h$ 또한 freeze된 상태로 학습에 관여하지만 여전히 backpropagation을 통해 gradient 연산이 가능하다. 따라서 CLIP loss를 기반으로 end-to-end 학습이 진행되는 과정에서 parameter $\\theta$에 적용되는 유의미한 loss는

$$
\\mathcal{L}_\\text{CLIP}(\\theta, \\text{pose } p, \\text{caption } y) = -g(I(\\theta, p))^\\top h(y)
$$

위와 같다. Camera pose $p$에 대해 rendering된 이미지 $I$를 이미지 인코더에 통과시킨 임베딩이, 원하는 text prompt와 유사한 관계를 갖게끔 학습시키게 된다(cosine similarity를 최대화). 사용하는 image encoder, text encoder는 CLIP 원본 논문의 ViT 기반을 사용하기도 했으며, [LiT baseline](https://arxiv.org/abs/2111.07991)도 사용했다고 한다.  Few-shot NeRF 학습에 대해서 다룬 논문인 [DietNeRF](https://arxiv.org/abs/2104.00677)에서는 rendering된 이미지와 real image의 유사도를 계산하는 방법을 사용했는데, 이 논문에서는 real image가 아닌 단순히 caption과의 비교를 통해 zero-shot(object photo가 아예 없는 상황) task도 가능케한 것이다. 물론 DietNeRF 그림을 보면 알겠지만 CLIP image encoder를 활용한 consistency loss를 사용한 점에서는 CLIP embedding space를 low-shot learning에 활용했다는 유사점을 볼 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/217228314-1ec75432-97c3-4f06-8e04-25734551295e.png" width="400"/>
</div>



### Challenges with CLIP guidance

하지만 CLIP은 단순히 이미지와 텍스트 묘사가 얼마나 유사한 지에 대한 정보만 줄 뿐이지, text 자체가 image의 디테일에 대한 semantic information을 내장하고 있지는 않다. 이런 문제는 NeRF를 최적화하는 과정에 있어 더 두드러지는데, multiple direction에서 보여지는 이미지가 주어졌을 경우에는 유의미한 3D representation을 학습할 수 있었던 네트워크는 많은 supervision에 대해서 학습할 수 있기 때문에 다음 그림과 같이 spurious density 문제(novel view에 대해서 제대로 된 representation이 학습되지 못하고 흩어지는 현상)을 최소화할 수 있게 된다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/217228317-b95e1d77-e74e-4352-b935-1344dfa2706b.png" width="400"/>
</div>



하지만 zero-shot task로 상정한 지금, 오로지 네트워크가 학습에서 의존할 수 있는 supervision은 text prompt 뿐이므로 NeRF 학습 상황이 굉장히 unconstrained한 문제가 생긴다. 최적화 이론에서 optimization을 진행할 함수나 집합이 너무 방대하고 복잡할 경우, PCA와 같이 주성분 방향만 찾거나 penalty(regularization)를 주어 한정된 feasibility를 가지도록 하여 searching space를 최소화하는 방법을 사용한다. NeRF에서 다량의 이미지를 학습에 사용하는 것, 딥러닝에서 large dataset을 통해 네트워크를 학습하는 과정도 결국 비슷한 맥락이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/217228320-f2c77e3a-52dd-49b5-8228-a9dc4a1e958e.png" width="500"/>
</div>



서로 다른 각도에서 촬영한/획득한 이미지가 많으면 많을수록 네트워크로 하여금 각도에 따른 viewing image에 대한 constraints가 증가하는 것이다. 실질적으로 $N$개가 있을 때 네트워크가 상호작용하는 과정을 여러 번 거친다고 가정했을 때, $1$개의 이미지만 추가되는 과정은 신경망 연산에는 $N^l$ 이상의 학습 효과를 줄 수 있기 때문이다. 여기서 $l$은 레이어 수를 의미한다. 따라서 zero-shot learning에 대해 단순히 text prompt만 적용하는 것은 아래 그림과 같이 좋은 결과를 보여주지 못했다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/217228322-b8424c53-3796-4677-81bf-25e400e2eb57.png" width="500"/>
</div>



가장 눈에 띄는 문제로는 density가 붕 뜨는 region(비어있는 부분)이 생긴다던가, object 부분에 density가 집중되어야 선명한 fidelity의 object가 생성되는데 그냥 camera space 전반에 산재한다는 문제가 발생하였다.

### Pose sampling

Random crop과 같이 image dataset에 흔히 적용하는 augmentation 기법은 이 논문이 참고한 style transfer 연구였던 deep dream 등의 연구에서 image generation 성능을 높이는 방법으로 사용되었다. Image augmentation은 그러나 in-plane의 2D transformation에서만 적용될 수 있다는 문제가 있다. Dream fields는 3D data augmentation을 사용하기 위해 각 training iteration마다 서로 다른 camera pose extrinsics를 샘플링하는 방법을 사용하였다. Scene을 중심으로 $360^\\circ$ 의 azimuth에서 uniformly sample하였고, 각 training iteration은 object을 서로 다른 방향에서 본 모습을 샘플링하게 된다. MLP는 서로 같은 scene representation을 공유하고 있기 때문에, 단순히 camera sampling을 하는 것만으로도 object geometry 성능에 큰 향상을 줄 수 있다.

단순히 카메라의 회전각 이외에도 camera elevation(focal length 조절, object와의 거리) 또한 augmentation에 사용될 수는 있었으나 굳이 사용하지는 않았다고 한다.

### Encouraging coherent objects through sparsity

Near field artifacts(field 경계에 artifact가 생기는 것)이나 spurious density(듬성듬성하게 샘플링되는 것) 문제를 해결하기 위해서 dream fields rendering 과정에서 opacity regularization을 진행했다고 한다. Opacity는 앞서 설명했던 transmittance와 연관되는 내용인데, transmittance는 ray $r$이 $t$부터 plane 근처 $t_n$ 사이를 이동하는 과정에서 물체에 흡수되지 않을 확률이다(밀도가 높은 점이 있으면 흡수율이 높다). 저자들이 가정한 것은 total transmittance를 $N$개의 sampling segment를 통과하는 동안의 joint transmittance라고 가정하였다. 식이 동일하니까 해석하는 과정에서는 큰 문제가 없다.

$$
\\begin{aligned}
T_i =& \\exp \\left( -\\sum_{j < i} \\sigma_\\theta (r(t_j))\\delta_j \\right) \\newline
=& \\prod_{j=1}^{i} \\exp \\left( -\\sigma_\\theta(r(t_j))(t_j - t_{j-1}) \\right)
\\end{aligned}
$$

그런 뒤, transmittance loss를 다음과 같이 정의하였다.

$$
    \\begin{aligned}
        \\mathcal{L}_T =& -\\min (\\tau, \\text{mean}(T(\\theta, p))) \\newline
        \\mathcal{L}_\\text{total} =& \\mathcal{L}_\\text{CLIP} + \\lambda \\mathcal{L}_T
    \\end{aligned}
$$

여기서 $\\tau$는 저자들이 설정한 target transparency가 된다. 만약 joint probability가 높아진다면 그만큼 spurious density가 적다는 의미이고, 반대로 낮아진다면 ray 전반에 걸쳐 density가 산재한다는 뜻이기 때문에 해당 loss term을 최적화하는 것이 object에 대한 정규화 방법이 될 수 있다. 따라서 초반 안정적인 학습을 위해 $\\tau$를 $88\\%$로 두고 학습시킨 뒤, $500$ iteration 이후에는 $40\\%$로 감소시켜 completely transparent(object가 아예 안생겨버리는 현상)을 방지하였다. Focal length에 대해서 $\\tau$를 scaling하는 것이 서로 다른 object distance에 대해서 잘 적용되었다고 한다. 그리고 렌더링 과정에서 단순히 white 혹은 black background에 대해 진행할 경우 transmittance가 수렴하더라도 scene이 background에 치우치는 문제가 발생하였고, 이를 줄이기 위해서 background를 랜덤한 이미지로 설정하는 것이 object 학습에 도움이 되었다고 한다. Dream fields는 gaussian noise, checkerboard 그리고 random fourier texture를 background로 사용하고 이를 random gaussian noise로 smoothing하여 사용했다고 한다. 


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/217228324-8d62abcd-d50f-4404-b5eb-ec212b29e11e.png" width="600"/>
</div>



사용된 prompt는 “An illustration of a pumpkin on the vine”이다. 실제로 렌더링된 모습을 보면 regularization이 적용될 경우 sparse한 image 경향성이 줄어드는 것을 확인할 수 있고, 추가적으로 white background에서 augmented background로 수정했을 때 sharper object가 생성되는 것을 볼 수 있다.

### Localizing objects and bounding scene

Neural Radiance Fields를 image reconstruction에 학습시킬 때, scene content는 NeRF에서 주로 사용되는 데이터셋과 같이 중앙 부분에 align되는 것이 일반적이다. Dream fields는 굳이 center에 놓이지 않더라도 3D object의 중앙 부분을 예측할 수 있으며, ray shifting도 그에 맞게 유지할 수 있음을 확인하였다. Origin을 찾는 과정은 rendering된 object의 무게중심을 exponentially moving하는 방법을 사용했고, object가 지나치게 치우치는 것을 방지하기 위해 density를 masking함으로써 일정 크기의 cube 내에 object를 위치시켰다.

### Neural scene representation architecture

NeRF는 8개의 layer를 가진 MLP를 사용했고, 모두 같은 width(channel 수)를 가지고 있으면서 RGB value를 내보내기 위한 두 개의 추가 layer가 있었다(아래 그림 구조 참고).


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/217228327-5dba54b4-f3af-45c7-9b7a-1d8d4a93eacf.png" width="700"/>
</div>



해당 논문에서는 위의 구조 대신 residual MLP structure를 사용했고, residual connection은 two dense layer마다 반복해서 사용했다고 한다. 모델 구조가 논문에 그림으로 첨부가 되어있지 않아서 official 코드를 찾아보니 아래와 같이 나와있었다. Google이다 보니 \`pytorch\`가 아니라 \`flax, jax\` 모듈을 사용하였다.

\`\`\`python
class MipMLPLate(nn.Module):
  """MLP architecture."""
  activation: str, features_early: Sequence[int], features_residual: Sequence[Sequence[int]]
  features_late: Sequence[int], fourfeat: bool, max_deg: int, use_cov: bool, dropout_rate: float

  @nn.compact
  def __call__(self, mean, cov=None, x_late=None, decayscale=1., *, deterministic):
    """Run MLP."""
    # Integrate the positional encoding over a region centered at mean.
    if not self.fourfeat:
      # Axis-aligned positional encoding.
      feat = 2**np.arange(self.max_deg)[:, None, None] * np.eye(3)
      feat = feat.reshape(-1, 3)
    else:
      # Random Fourier Feature positional encoding. Fix the PRNGKey used for the
      # fourier feature basis so the encoding does not change over iterations.
      fourfeat_key = random.PRNGKey(124124)
      dirs = random.normal(fourfeat_key, (3, 128))
      dirs = dirs / np.linalg.norm(dirs, axis=-1, keepdims=True)
      rads = 2 ** (self.max_deg * random.uniform(fourfeat_key, (128,)))
      feats = (rads * dirs).astype(np.int32)
      feats = np.concatenate([np.eye(3), feats], 1).astype(np.float32)
      feat = feats.T

    mean_proj = (mean[Ellipsis, None] * feat.T).sum(-2)
    if self.use_cov:
      cov_diag_proj = ((cov[Ellipsis, None] * feat.T).sum(-2) * feat.T).sum(-2)
      decay = np.exp(-.5 * cov_diag_proj * decayscale**2)
    else:
      # Disable IPE
      decay = 1.
    x = np.concatenate([decay * np.cos(mean_proj),
                        decay * np.sin(mean_proj)], -1)

    # Network
    activation = nn.__getattribute__(self.activation)
    for feat in self.features_early:
      x = activation(nn.Dense(feat)(x))
      x = nn.Dropout(self.dropout_rate)(
          x, deterministic=deterministic)

    for feat_block in self.features_residual:
      h = nn.LayerNorm()(x)
      for l, feat in enumerate(feat_block):
        h = nn.Dense(feat)(h)
        h = nn.Dropout(self.dropout_rate)(
            h, deterministic=deterministic)
        if l < len(feat_block) - 1:  # don't activate right before the residual
          h = activation(h)
      x = x + h

    if x_late is not None:
      x = np.concatenate([x, x_late], axis=-1)
    for feat in self.features_late[:-1]:
      x = activation(nn.Dense(feat)(x))
      x = nn.Dropout(self.dropout_rate)(
          x, deterministic=deterministic)
    x = nn.Dense(self.features_late[-1])(x)  # don't activate output
    return x
\`\`\`

\`jax\` 모듈에서의 \`np\`는 \`numpy\`와 동일하고 마찬가지로  \`flax.linen\` 의 \`nn\`은 \`pytorch\`의 \`torch.nn\` 과 유사하게 동작한다고 보면 된다. Residual layer는 2 layers마다 적용하였고, layer normalization을 각 residual layer 앞쪽에 적용해주는 것이 효과적이었다고 한다. 또한 API에 구체적으로 나타나있지는 않지만 bottleneck 구조와 같이 중간 channel을 expansion하는 과정이 있다고 생각하면 된다.

또한 vanishing gradient 문제가 highly transparent scenes(대부분의 rendering이 0가 될 경우)에서 발생할 수 있게 되므로 ReLU activation function을 적용하는 대신 Swish를 사용하였으며 density $\\sigma_\\theta$를 softplus function을 통해 rectify했다고 한다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/217228329-5722b141-5cec-4e42-8a78-5c926ea4e878.png" width="400"/>
</div>



$$
f(x) = x \\cdot \\sigma(x),~\\sigma(x) = \\frac{1}{1+e^{-x}}
$$


# Results

### Quantitative results on geometric priors


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/217228331-f17970bf-ae50-4883-9765-f6f54d2e8791.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/217228340-29b945ee-0cb9-4b02-9a32-8d2b3fc017fa.png" width="900"/>
</div>



이 논문에서 실제로 사용했던 geometric prior들이 COCO caption에 대해 생성된 이미지를 retrieval할 수 있는지  측정하였다. 논문에서 사용한 metric들이 적용이 될 때마다 생성한 샘플에 대해 retrieval 정확도가 올라가는 모습을 볼 수 있다. 또한 regularization이 사용될 때마다(논문에서 제시한 transparency에 대한 정규화) retrieval 성능이 올라가는 것을 볼 수 있다.

### Compositional generation


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/217228342-09cbf4d6-be6e-4086-98dc-e34b2a8a5fe2.png" width="900"/>
</div>



해당 그림에서는 cherry picking하지 않은 dream fields의 샘플들을 보여준다. Shape과 material에 대해 독립적으로 다르게 구성한 object를 생성하고 결과를 확인해보았다. DALL-E와 같은 네트워크 또한 caption에 대해 충분히 좋은 이미지를 생성할 수 있지만, 이 논문과 같이 3D object를 직접 생성해내지는 못한다는 한계가 있다.

### Regularization strength


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/217228352-e6281836-ba10-4ed6-bc3f-808d6c190523.png" width="800"/>
</div>



그리고 transparency에 대한 strength를 크게 주면 줄수록 object가 컴팩트한 사이즈를 가지는 것을 확인할 수 있다. 위의 object를 생성한 description은 ‘A cake toped with white frosting flowers with chocolate centers’이다.


# Discussion and Limitations

Dream fields는 zero-shot을 처음으로 NeRF에 효과적으로 적용했던 논문이라고 볼 수 있지만, 여러 한계점이 존재한다. 첫번째는 generation 과정이 iterative optimization을 필요로 하기 때문에, 샘플링을 하는 시간이 오래 걸린다는 점이다. 논문 저자들은 meta-learning이나 amortization 방법을 통해 추후에 이를 가속화할 수 있다고 제시한다.

두번째로는 모든 perspective에 대해 동일한 prompt를 사용한다. 이는 object의 서로 다른 면에 비슷한 무늬가 반복되게하는 문제를 만들었다고 한다(일종의 texture collapse 문제 같은 것들이 발견된 듯하다). 카메라 위치에 따라 충분히 캡션이 달라질 수 있는 상황을 가정하지 못한 점이 문제라고 한다. 예를 들어 같은 물체라도 모든 면에서 보았을 때 동일한 형태가 되는 texture가 아니라면 앞면에서 봤을 때와 뒷면에서 봤을 때 묘사할 수 있는 appearance가 달라질 수 있기 때문이다.

마지막으로는 CLIP 네트워크 자체가 가지는 문제다. 아무리 CLIP이 image to text representation을 잘 학습했더라도, ground truth training image의 일부에 대해서도 잘못된 score rendering을 하는 경우가 생긴다. CLIP 네트워크의 성능이 generation에 영향을 미칠 수 있다는 점이다. Pre-trained model에 의존하는 것은 네트워크가 내포하고 있을지도 모르는 bias를 감수하겠다는 것이다.
`,MO=`---
title: "Bootstrapping Language-Image Pre-training(BLIP, BLIP2) 논문 리뷰"
category: "ai papers"
publishedAt: "2023-02-14"
thumbnail: "https://user-images.githubusercontent.com/79881119/218607874-ab1d0447-78ca-431f-b45f-9d74f7afdbbd.png"
---


# BLIP을 제시한 이유

기존에 많이 살펴보았던 <U>CLIP</U>과 같이 vision과 language의 multimodality를 활용한 학습을 <U>Vision-Language Pre-training</U>(VLP)이라고 부른다. 그러나 기존 VLP의 학습 형태는 understanding based task(Encoding을 통해 downstream task를 해결하는 구조) 혹은 generation based task(Decoding을 통해 특정 modality를 생성하는 구조) 둘 중 하나만 잘 해결하는 경향이 있다. 게다가 VLP의 성능을 높이기 위해서 CLIP과 같은 연구에서 사용한 방법은 web으로부터 얻은 noisy한 image-text dataset을 활용하는 것이었는데(여기서 noisy란 뜻은 잘 정제된 데이터가 아니라, 무작정 웹으로부터 많이 수집하려다 보니 불필요한 데이터셋, 예를 들어 어떤 text prompt와 크게 관련없는 image 데이터가 섞여있는 경우를 의미함),  사실 이런 문제는 supervision을 보다 많이 활용할 수 있는 방법에 대한 차선책이지, 정말로 **이상적인 방법은 아니라는 것**이다. 가장 좋은 supervision을 줄 수 있는 방법은 당연하게도 web에서 얻은 데이터셋 크기와 같이 billion 단위의 pair를 구성하고, 각 pair가 완벽한 prompt matching이 되는 경우일 것이다. 하지만 이는 현실적으로 불가능하다.

BLIP paper에서 제시하고자 하는 것은 새로운 VLP framework를 제시하여 앞서 말했던 understanding task와 generation task 함께 학습할 수 있게끔 하는 것이다. 함께 성능을 높일 수 없었던 서로 다른 두 task를 통합함으로써  얻을 수 있는 장점은 multitask learning을 통해 보다 넓은 범위의 vision-language task(VL task) 를 수행할 수 있고, 각각의 성능을 향상시킬 수 있다는 점이다. 물론 단순히 여러 task에 대해 동시에 학습해서 성능 향상이 이루어진 것은 아니고 저자들이 제시한 것은 generation task에서 사용될 수 있는 **decoder**와 understanding task에서 사용될 수 있는 **encoder**를 일종의 caption synthesizer와 caption filter로 사용한 것이다. 이는 web에서 획득한 텍스트 기반 이미지 dataset의 noise를 줄일 수 있는 방법이 되었고, 결국 여러 task에 대해 성능을 올릴 수 있었다고 한다. BLIP 논문의 링크는 다음과 같다([논문 링크](https://arxiv.org/abs/2201.12086)).


# 기존 VLP의 한계점들

위에서도 짧게 언급했지만 기존 VLP 연구의 한계점은 크게 model의 관점에서와 data의 관점에서 설명해볼 수 있다.

### Model perspective

대부분의 VLP 구조는 encoder-based 아키텍쳐를 가지고 있거나([CLIP](https://arxiv.org/abs/2103.00020), [VILLA](https://arxiv.org/abs/2006.06195)) 보통의 transformer와 같이 encoder-decoder 아키텍쳐를 가진다([Unifying Vision and Language](https://arxiv.org/abs/2102.02779), [Deep Learning Library](https://arxiv.org/abs/1912.01703)). 그러나 encoder만 사용한 구조는 text generation task에 직접적으로 적용되기 힘든 구조를 가지고(실제로 representation 자체가 transfer되기 어렵다), encoder-decoder는 text 생성에만 집중하다보니 image-text retrieval같이 상호 modality간의 understanding이 필요한 task의 성능이 그리 좋지 않다는 것을 확인하였다.

### Data perspective

CLIP, [ALBEF](https://arxiv.org/abs/2107.07651)(about alignment) 그리고 [SimVLM](https://arxiv.org/abs/2108.10904)(about weakly supervised learning)과 같은 대부분의 SOTA 모델들은 Web으로부터 수집 가능한 대량의 image-text pair로 학습된다. 물론 dataset을 증가시킴으로써 얻을 수 있는 장점은 performance가 증가한다는 것이지만, 이 논문에서 밝힌 것은 단순히 vision-language learning에서 데이터셋 크기만 키우는 것이 정공법은 아니고, noisy web text(이미지와 상관 없는 텍스트)를 활용하는 것은 그리 좋지 않을 수 있다는 사실이다. 논문에서 이러한 한계를 뛰어넘을 수 있는 BLIP을 제시하였고, 모델 구조의 관점에서나 데이터 관점에서 모두 VLP framework에서 학습에 효율적으로 적용될 수 있는 방법을 제시하였다. 


# Contribution

논문에서 밝힌 data와 model 관점에서의 contribution은 다음과 같다.

### Multimodal mixture of Encoder-Decoder(MED)

보다 <U>유기적으로 transfer learning</U>을 진행할 수 있는 model architecture를 제시한다. MED는 단순히 encoder based나 encoder-decoder structure가 아닌 unimodal encoder, image-grounded text encoder 그리고 image-grounded text decoder를 아우른다. Model은 multitask learning에서 진행하는 것과 같이 각 구조별로 $3$개의 objective function으로 학습되는데, **unimodal encoder**에 대해서는 image embedding과 text embedding 간의 <U>contrastive learning</U>, **image-grounded text encoder**에서는 <U>image-text matching</U>을 최적화하게 되고 마지막으로 **decoder**에서는 <U>image-conditioned language modeling</U>을 최적화하게 된다.

### Captioning and Filtering(CapFilt)

Noisy한 image-text pair로부터 dataset을 bootstrapping할 수 있는 방법으로 제시된 것이 <U>synthetic model</U>(decoder)인 **captioner**와 <U>understanding model</U>(encoder)인 **filter**를 사용하여 original <U>web caption</U>과 <U>synthetic caption</U> 중 **noisy한 샘플을 없애는 작업**이다. 뒤에서 더 디테일하게 설명하겠지만 captioner와 filter는 서로 독립적으로 활용될 수 있으며, 단순히 synthetic caption을 만들지 않고 filtering만 진행하거나(filter only) captioning만 진행하는 경우(captioner only) 둘 다 사용하는 것보다 성능이 좋지 않은 것을 확인하였고, 이를 통해 성공적으로 caption을 bootstrapping할 수 있음을 확인하였다. 그리고 더 다양한 caption을 만들어내는 것이 augmentation 역할을 함으로써 성능 향상에 큰 기여를 했다고 한다.

위의 두 방법을 사용하여 만든 BLIP 모델은 image-text retrieval, image captioning 그리고 VQA(video question answering) 등등 다양한 task의 성능을 높일 수 있었다. 그리고 해당 모델을 단순히 video-language task에 transfer했을 때 zero-shot performance가 SOTA 네트워크를 뛰어넘을 정도로 representation 학습이 잘될 수 있음을 확인하였다.


# Related works

### Vision-language pre-training

VLP task에서 vision model과 language 모델을 학습함에 있어서 초점을 맞춘 것은 성능이 뛰어난 <U>human-annotated text를 구하기 힘든 이유</U> 때문에 web 상에서 크롤링한 image/text pair를 사용했다는 점이다. 그러나 대부분의 VLP task에서 사용한 web based dataset의 경우 filtering(유의미한 데이터셋을 잘 정제해내는 작업)을 간단하게만 진행하기 때문에 web text에 대한 <U>noise는 제거하지 못한 채</U> 사용할 수 밖에 없다. 물론 noise에 의한 drawback은 존재하지만, 그보다 <U>데이터셋을 증가시킴에 따른 성능 향상이나 장점</U>이 더 크기 때문에 이를 간과하고 넘어가게 된다. 이 논문에서는 기존 VLP 방식에서 짚고 넘어가지 않은 noisy web text를 다루면서, noisy web text를 데이터셋으로 사용할 수 밖에 없는 상황은 차선책인 것임을 보이며 CapFilt(Captioner and Filter)를 제안하였다.

기존에는 vision task와 language task를 통합하기 위한 다양한 방법론이나 연구들(image captioning, VQA 등등)이 제안되었지만 가장 해결하기 어려웠던 점은 <U>understanding task</U>와 <U>generative task</U> 성능에 모두 최적화가 가능한 구조를 찾기 힘들었던 것이다. 그간의 encoder-decoder model이나 encoder only 혹은 decoder only 구조와는 다르게 구조들을 modality의 관점으로 섞어서 만들게 된 BLIP 구조는 다양한 downstream task를 처리할 수 있으면서 동시에 pre-training 과정도 간단하게 구성한 연구라고 주장한다.

### Knowledge distillation

<U>Knowledge distillation</U>(KD)는 모델의 경량화와 함께 제안된 방법 중 하나이다. 가장 일반적인 knowledge distillation은 단일 task에 대해 좋은 성능을 보이는 teacher network와, 해당 teacher network의 representation을 배우고자 하는 student network를 포함하게 된다. 만약 classification task라면 student network가 모방하는 것은 teacher network가 예측한 각 class의 probability가 되며, 이때 최적화하는 것은 예측 확률에 대한 KL-divergence가 된다. 추가로 확장되어 나온 개념인 <U>self-distillation</U>이란 teacher network와 student network가 서로 같은 구조를 가진 경우를 의미한다. KD와 같은 방법론은 classification이나 VLP task에서 효과적이라는 연구들이 진행되어왔다. BLIP 논문에서도 마찬가지로 KD의 개념이 포함되는데, 기존 KD 연구에서와 같이 teacher network의 prediction을 student network가 완벽히 모방하는 형태가 아닌, <U>captioner</U>가 semantic-rich synthetic caption(이미지 정보를 풍부하게 담아낸 합성된 캡션)의 knowledge를 distill하게 되면, <U>filter</U>가 noisy caption을 걸러내면서 distillation하는 과정을 거치게 된다. 비유를 하자면 distillation 과정에서 편집이 진행되는, 즉 라이브 강의가 아닌 일종의 <U>잘 정제된 인터넷 강의</U>라고 볼 수 있으려나 싶다.

### Data augmentation

Computer vision에서의 data augmentation은 <U>dataset regularization</U>으로, overfitting을 방지하는 방법으로 널리 연구되어왔다.  그러나 language task에 대한 data augmentation은 domain specific(문법을 고려하거나, 언어 특성상 문맥이나 상황에 따라 묘사하는 범위가 크게 달라질 수 있음)하기 때문에 불명확한 점이 없지 않아 있었다. 최근에 들어와서는 language model 중 generative model을 토대로 synthesize하는 방법을 통해 augmentation을 진행하게도 했다. 기존 NLP에서 generative model을 사용하는 등의 연구들은 대부분 low-resource(언어 모델의 학습)과 관련된 task에 주로 적용되었지만, 이 논문에서는 <U>synthetic caption dataset</U>을 VLP에 사용했다는 점이 차이가 될 수 있다.


# Method

저자들이 제시한 BLIP의 구조는 VLP(Vision-Language Pre-training) 과정에 있어서 noisy-image pair로부터 학습할 수 있는 효율적인 방법을 제시하고, 그와 더불어 CapFilt라는 방식을 통해 논문의 가장 main contribution이라고 할 수 있는 <U>dataset의 bootstrapping</U>를 이끌어낸다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607874-ab1d0447-78ca-431f-b45f-9d74f7afdbbd.png" width="1000"/>
</div>


### Model architecture

**아키텍쳐**는 위의 그림을 보면 직관적으로 파악이 가능한데, 결론부터 말하자면 BLIP이 제시하는 trainining 구조는 <U>MED(Multimodal mixture of encoder-decoder)</U> 구조를 가지며, 총 3개의 역할을 달리하는 encoder 및 decoder 구조가 융합된 형태를 지닌다.

우선 CLIP과 같은 VLP 네트워크에서 학습하는 image와 text embedding 사이의 <U>contrastive learning</U>을 위해 unimodal encoder를 image domain과 text domain에 대해서 사용한다. **Image domain**에서의 encoder는 Vision transformer를 사용하고 **text encoder**는 BERT를 사용하게 된다. 이에 저자들은 특정 image를 기반으로 유의미한 caption을 필터링하기 위한 understanding module로 image-grounded text encoder를 사용하였다. Image-grounded text encoder는 위의 그림에서 중간에 cross-attention layer를 통해 image embedding 정보를 text encoder에서 활용함으로써 이미지와 텍스트의 matching 정도를 임베딩으로 내놓게 된다.

그리고 마지막으로는 image를 기반으로 synthesized caption을 augmentation(논문에서 사용한 용어를 토대로 정정하자면, dataset bootstrapping) 방법으로 사용하기 위해 설계한 text decoder가 존재하게 된다. 학습 framework에 대한 description은 아래에서 보다 자세히 다루도록 하겠지만, 우선 위의 그림에서 보이는 구조를 크게 파악하고 넘어가는 것이 논문을 이해하는데 큰 도움이 되는 것 같다.

### Pre-training objectives

**아키텍쳐 설명**과 더불어 figure에는 각 functionality를 담당하는 구조가 최적화할 objective function을 함께 명시해놓았다. 우선 가장 좌측부터 보면 <U>ITC(Image-text contrastive loss)</U>가 있는데, 이는 visual transformer와 text transformer가 서로 positive pair(image와 text description이 일치하는 경우)라면 가깝게, negative pair(일치하지 않는 모든 경우)라면 멀게 encoding하게끔 unimodal encoder를 학습하게 한다. Vision과 text에 대한 <U>understanding을 improving</U>할 수 있는 방법으로, 앞서 설명했던 바와 같이 대부분의 VLP에서 진행하는 contrastive learning 과정이라고 이해하면 된다.

두번째로 보이는 <U>ITM(Image-text matching loss)</U>는 image grounded text encoder를 학습하는 과정으로, binary classification task를 수행하게 된다. Encoder가 추출하는 prediction은 unimodal vision encoder에서 획득할 수 있는 semantic 정보를 text embedding과의 attention을 통해 주어진 text가 image에 positive(match)인지 negative(unmatch)인지 구분하게 된다. 그러나 단순히 BCE task로 접근하게 되면 contrastive loss와는 다르게 <U>negative sample의 어려움 정도</U>에 따라 loss optimization이 진행될 수 없기 때문에 <U>hard negative mining strategy</U>를 통해 분류가 어려운 negative sample이 학습에 더 많은 관여를 할 수 있게끔 해주었다.

마지막으로 <U>LM(Language modeling loss)</U>는 image grounded text decoder를 사용하게 되고, autoregressive한 방법으로 주어진 text input에 대해 cross-entropy loss를 최적화하게 된다. VLP에서 사용되는 MLM loss와는 다르게 LM loss는 visual information을 기반으로 유의미한 caption을 생성할 수 있게끔 학습 가능하다.

구조를 보게 되면 학습 시 필요한 네트워크 구조가 총 4개이므로 parameter 수가 급증하게 된다. 이를 방지하기 위해서 저자들은 self-attention layer를 제외하고는 text encoder와 text decoder의 모든 parameter를 공유하는 형태의 multi-task learning으로 접근했다. Self-attention layer를 제외한 이유는 encoding과 decoding task가 input에 있어 집중하는 부분이 서로  다르기 때문인데, 예를 들어 encoder는 bi-directional attention을 사용하지만 decoder는 causality가 보장되어야하기 때문(학습 과정에서는 현재 예측하는 토큰 이후의 context는 attention에 관여할 수 없음)이다. 이를 제외하고 image grounding 역할을 하는 cross-attention layer나 FFN network는 큰 차이가 없었기 때문에 두 네트워크의 parameter를 공유하는 방법을 통해 학습 효율을 높였다고 한다.

### CapFilt


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607877-6bb08a32-dd2b-48dd-826b-f5814d00f125.png" width="1000"/>
</div>


**Annotation cost** 때문에 **image-text pair**를 만드는 과정에서 web-crawled dataset$(I_w, T_w)$로는 high-quality pair$(I_h, T_h)$를 만들기 어렵다는 문제가 있었다. 대량의 데이터셋을 구축하는 과정에서 alt-text가 이미지를 제대로 묘사하지 못할 경우 <U>최적의 distribution과는 멀어질 가능성</U>이 있다. 이러한 noise 문제를 기존에는 <U>데이터셋의 크기에 의한 장점</U> 때문에 무시하고 있었으나, BLIP 논문에서는 위와 같은 CapFilt 구조를 통해 해결해보고자 하였다. 

데이터셋은 human annotated(high quality dataset)인 $(I_h, T_h)$ 그리고 $(I_w, T_w)$를 같이 사용한다. <U>High quality dataset</U>은 <U>web based dataset</U>과는 다르게 noisy alt-text가 없기 때문에 이를 filter와 captioner의 finetune 과정에 사용한다. Fine-tuned text decoder는 synthetic text $T_s$를 각 web image $I_w$에 대응하여 생성하게 되고 fine-tuned text encoder는 대응되는 이미지 $I_w$를 grounding으로 간주하여 $T_w$와 $T_s$를 필터링하게 된다. 만약 web dataset 중 $I_w$를 제대로 묘사하는 text prompt가 아닌 경우가 있다면 이를 제외시키고, text decoder에 의한 caption 또한 마찬가지의 과정을 거친다. 최종적으로 CapFilt를 통해 추출된 dataset은 필터링이 불필요한 high-quality dataset $(I_h, T_h)$, filtering된 web dataset $(I_w, T_w)$ 그리고 $(I_w, T_s)$이다. <U>이런 과정을 통해 bootstrapping된 dataset</U>을 이용하여 새로운 network를 pre-training하는데 사용했다고 한다. 이를 반복할수록 **더 높은 quality**의 dataset이 학습에 사용되고, 그로 인해 **네트워크의 성능**을 높일 수 있었다.


# Limitations and appearance of BLIP-2

위의 학습 과정을 보면 대략 알 수 있겠지만 <U>여전히 한계점이 존재</U>한다. 우선 pre-training 과정에서 parameter를 공유하는 방법을 통해 전체적으로 학습이 되는(fine-tuning) 네트워크를 <U>단순화했다는 점에서는 어느 정도 장점</U>이 있지만, 그럼에도 불구하고 여전히 end-to-end로 네트워크를 학습해야하는 방법론에서는 벗어나지 못했다. ‘Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation’이라는 제목에서 알 수 있듯이 이 논문에서 보여주고자 하는 주된 장점은 성능이 좋은 image encoder와 text encoder, 그리고 text decoder 전반을 융합하여 다양한 task에 높은 performance로 적용 가능한 구조를 만들고자 한 것이다. 그렇기 때문에 여전히 modality gap을 줄이기 위해 수행해야하는 학습 과정이 복잡하다.

BLIP 2번째 논문에서는 이러한 <U>기존 방식들의 한계점을 문제로 삼으며</U> VLP 문제에 접근한다.  


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607879-5e02144a-fbcb-4675-b4a4-b7ef6b84d617.png" width="800"/>
</div>


논문의 main figure는 위와 같다. 결국 BLIP 2의 framework는 위 그림으로 모두 설명이 가능할 정도로 간단하다는 것을 볼 수 있다. BLIP 논문이 총 4개의 transformer block 구조를 각각 optimize하는 형태라면, 이 process는 단순히 2 step의 pre-training 과정을 통해 vision과 language의 alignment가 가능하다고 주장한다. 자세한 방법은 논문의 개요를 간단하게 짚은 뒤 설명하도록 하겠다.


# VLP research의 방향성?

CLIP과 더불어 vision과 language를 함께 학습하고자 하는 task들이 무수히 많이 소개됨에 따라 여러 연구가 진행되었다. 그러나 대부분의 방법론의 경우 학습 과정에서 cost가 너무 많이 드는 과정을 제시한다는 점이 큰 문제가 되었다. Vision-language 연구는 vision과 language 사이에 있기 때문에 자연스럽게 각각의 unimodal 연구들(vision은 computer vision에서의 SOTA, language는 NLP에서의 SOTA)을 잘 융합하는 것에 초점을 맞추게 된다. 이 논문에서는 vision model과 language model 의 generic하고 compute-efficient한 방법을 제시하여, pre-trained vision model과 language model을 직접 건드리지 않고(학습 과정에서 guidance에는 사용되지만, frozen된 상태로 사용된다) bootstrapping할 수 있는 연산 효율적인 방법들에 집중하였다. LLM(Large language models)는 일종의 GPT3나 BERT같은 대형 언어 모델을 지칭하는데, 보통 language 네트워크는 generation이나 zero-shot transfer 성능이 상당히 높은 것으로 알려져있지만, vision task와 같이 다른 task에 적용될 때 학습된 representation을 잊는다던가(catastrophic forgetting)하는 문제가 발생하지 않도록 pre-trained representation을 잘 보존하는 것이 관건이다.

VLP에서 이러한 unimodal LLM의 representation을 잘 활용하기 위해서는 cross-modal alignment가 잘 이루어지는 것(vision representation과 language representation 사이의 적절한 매칭 관계)이 중요하다. Text embedding space를 하나의 위상 공간 $T$라 정의하고 마찬가지로 이에 대응되는 image embedding space $I$를 생각해보았을 때, 각 manifold가 $1$대 $1$ mapping이 되는 것이 가장 이상적이고 바람직한 형태이며, 더 나아가서 유사한 이미지끼리의 위상 관계가 유사한 텍스트끼리의 위상 관계에서도 유지되는 것이 가장 중요하다. 이렇게 서로 다른 modality를 가지는 데이터들의 embedding space간의 관계성을 찾고자 하는 것이 multimodal 연구의 alignment 항목이며, VLP에서도 이를 찾기 위한 연구가 진행되었다.

하지만 BERT나 GPT3와 같은 대용량 언어 모델은 image modality를 supervision으로 가지지 않기 때문에 representation을 유지하기 위해 단순히 freezing하여 사용하는 것은 최적화하기 어렵다는 문제가 생긴다. 결국 BLIP-2 이전에 제시된 논문인 [Frozen](https://proceedings.neurips.cc/paper/2021/hash/01b7575c38dac42f3cfb7d500438b875-Abstract.html)이나  [Flamingo](https://arxiv.org/abs/2204.14198)의 경우 image to text generation loss에 의존하였고, 이러한 방식은 modality gap인 alignment를 충분히 최적화하기엔 부족한 loss로 판단되었다.

저자들은 이러한 기존 방식들이 가져던 한계점을 극복하고자 frozen unimodal models인 image encoder, LLM을 활용할 수 있는 방법으로 modality 사이에 Q-former(Querying Transformer)를 제안하였으며, 이 구조는 image encoder에서 나온 embedding에서 LLM에 유용하게 사용될 수 있는 embedding을 추출해내고, 이를 통해 LLM 성능을 높이고자 한 것이다. 처음에는 방법론만 단순하게 보고는 하나의 위상에서 다른 위상으로 옮겨가는 과정을 보고 **복합적인 형태의 STN**이나 **transformer encoder**라고 보였다. 그러나 이런 insight보다는 VLP 연구에서 사용되었던 방법 중 cross-attention layer가 있는데, 이걸 **단일 module로 구현**하여 **scalability를 높일 수 있는 연구**로 방향을 잡았다고 보는게 더 적합하다고 생각된다.

Q-former는 앞의 image encoder나 뒤의 text decoder를 굳이 학습할 필요가 없다는 점에서 module과 관련된 모든 연구들의 장점(효율적, lightweight)을 가지고 있으며 그와 동시에 vision과 language 사이에 유의미한 modality relationship을 간단한 학습 구조로 찾을 수 있다는 것을 밝혀내었다.


# Contributions of BLIP-2

원본 논문인 BLIP과 살짝 다른 제목으로 BLIP-2는 본인들의 contribution에 대해 잘 보여준다고 생각한다. BLIP-2의 풀네임은 ‘Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models’ 이다.

- BLIP-2는 frozen pre-trained image and text network를 **효율적으로 사용**하면서 Q-former를 통해 **modality gap을 줄일 수 있는 방법**을 제시하였다. 학습 방법은 representation learning stage와 text generative learning stage로 구분된다. BLIP-2는 **기존 방식들에 비해 간단한 구조**를 토대로 다양한 VL task에서 SOTA를 기록하였다.
- OPT, FlanT5와 같은 성능 좋은 LLMs을 기반으로 BLIP-2는 zero-shot image to text generation을 진행한다. 보다 좋은 LLM을 사용하여 좋은 visual knowledge reasoning, visual conversation이 진행될 수 있다는 것은 앞으로도 BLIP-2가 제안한 모듈을 통해 확장성 있는 접근이 가능하고, 이를 통해 vision language task의 성능을 높일 수 있다는 가능성이 있다. **즉, scalable한 approach**이다.
- Frozen unimodal models를 사용하고 lightweight Q-former를 학습함으로써 BLIP-2는 compute-efficient하게 성능을 높일 수 있었다. 예를 들어 BLIP-2는 앞서 소개했던 Flamingo보다 zero-shot VQA 성능이 $8.7\\%$가 높았으며, 심지어 **54배 적은 parameter 수로 이를 충족**할 수 있었다.


# Related work

### End-to-end VLP(Vision-Language Pre-training)


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607881-56fa1d22-c69e-40ca-8c96-c9b906771fee.png" width="800"/>
</div>


위의 그림은 multimodal learning with transformers란 survey 논문([참고 링크](https://arxiv.org/abs/2206.06488))에서 밝힌 최근 multimodal learning research architecture의 구조들을 보여준다. Survey 논문에서도 밝히듯이, 각 task에 따라 가장 효율적인 아키텍쳐는 다르다. 즉 구조가 따로 정해져있지 않다는 것이 어쩌면 연구의 자유도를 높일 수 있는 장치이면서도 많은 ablation을 요구하는 challenging task에 해당된다. 네트워크 구조와 더불어 여러 pre-training objective function들도 함께 제시되었지만, 최근에는 대부분 image-text contrastive learning, image-text matching 혹은 masked language modeling과 같은 몇 개의 loss term이 주로 사용되었다.

여러 방법을 막론하고 대부분의 VLP 방법들은 **large scale(web based) image/text pair** dataset을 활용하여 end-to-end 학습을 진행한다. 최근 대용량 언어 모델이나 transformer based 아키텍쳐는 대부분 기존 CNN based architecture에 비해 성능 향상을 얻은 대신 그만큼의 trade-off로 parameter의 손실을 감수해야했고, 이에 따라 VLP 연구에서도 같은 bottleneck이 적용될 수밖에 없다. 게다가 이미 잘 학습된 representation을 가지는 LLMs과 같은 모델들의 representation을 효율적으로 leverage할 수 있는 방법을 찾기도 어려운 마당이다.

### Modular Vision-Language Pre-training(VLP)

앞서 언급한 방법들의 경우 pre-trained model을 frozen하는 방법을 사용하지 않고 학습 과정에서 이용하고자 하는 목적이 강했는데, 저자들은 이러한 방식을 on-the shelf라고 언급하면서 본인들은 SOTA 성능을 보이는 다양한 network들을 직접적으로 사용하는 것에서 벗어나 <U>off-the-shelf method</U>를 고안하기 시작했다고 한다. 그래서 그런지 앞서 언급한 related works보다는 <U>pre-trained network를 frozen</U>하는 방식이 이 논문의 ideation에 도움이 되었다고 할 수 있다.

초기 작업에서는 frozen object detector를 사용하여 visual feature를 뽑는 방법을 사용했으며 최근 [LiT](https://arxiv.org/abs/2111.07991)에서는 pre-trained image encoder for CLIP을 frozen 상태로 사용하였다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607884-2515bfe6-eb75-4b77-94dc-954117489268.png" width="700"/>
</div>


해당 논문에서 제안한 방법론에서는 <U>image encoder를 frozen</U> 상태로, <U>text encoder를 fine-tuning</U>하는 과정이 가장 성능이 좋게 나왔다고 한다. 아무튼 말하고자 하는 것은 CLIP representation을 유지하는 방향으로 학습 방법론을 제시하는 것이 도움이 된다는 연구 결과가 있었다는 점이다. LiT가 image encoder를 lock(frozen) 상태로 이용했다면 반대로 language model(LLM)을 freeze해서 사용하는 vision to language task도 소개되었다. Frozen LLM을 사용할 때 가장 어려운 점이 바로 text와 image semantic information 사이의 alignment인데, 이를 해결하기 위해 Frozen 논문에서는 LLM의 모델에 입력으로 들어가는 text prompt에 image encoding 결과를 soft prompt로 사용하였다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607886-a2c36b2f-baaa-4cb2-895e-f7ac345907a7.png" width="700"/>
</div>


LLM은 학습이 안되는 상태로, <U>vision encoder만 LLM의 도움을 받아</U> alignment를 진행하는 쪽으로 fine-tuning을 할 수 있다는 형태의 연구가 되었다. Flamingo는 Frozen 논문과는 다르게 input에서 alignment를 진행하지 않고 <U>feature map에서 alignment</U>를 진행했는데, <U>cross-attention layers</U>를 LLM에 추가함으로써 visual feature와 함께 text prompt가 attention을 진행할 수 있게 하였는데, 이때 새롭게 추가된 layer만 pre-training 시키는 방법을 사용한다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607888-cfc642a7-a09d-47e9-aaac-46363220b585.png" width="800"/>
</div>


두 방법 모두 공통점이라면 LLM 네트워크를 freeze한 상태로 사용한다는 점이고, 차이점은 **Frozen**은 image encoder를 fine-tuning하는 형태로 input alignment를 학습 목적으로 삼았고 **Flamingo**는 image encoder도 freeze한 상태로 cross-attention이 진행되는 일부 layer만 scratch부터 학습하였다는 점이다.

이러한 기존 방식들은 모두 image encoder나 LLM의 구조를 직접적으로 바꾸지는 않지만 <U>fine tuning하거나 레이어를 추가하는 식으로 alignment를 해결</U>하고자 하였고, 학습 과정에서 computational cost가 어느 정도 크다는 단점이 있다. 저자들은 이러한 문제들을 해결하기 위해 <U>BLIP-2 아키텍쳐를 제시</U>하였고, image encoder와 LLM의 representation을 <U>효율적으로 leverage</U>할 수 있는 방법을 제시하였다.


# Method

앞서 본 BLIP 논문에 비해 BLIP-2가 가지는 장점 중 하나는 메소드가 간단하다는 것이다. BLIP 논문에서는 학습을 위해 크게 $3$개의 부분으로 나뉘는 파트(unimodal representation learning, image-grounded matching and image-grounded text generation)가 각각 최적화되어야 했지만 이번 논문에서는 모듈을 제시하여 bootstrapping하는 방법을 사용하였다.

Vision modality와 language modality 사이의 modality gap을 bridge하는 모듈의 이름을 <U>Q-former(Querrying Transformer)</U>로 정의하였으며 Q-former에 의한 학습은 image encoder를 활용한 <U>vision-language representation learning stage</U>와 뒤이어 진행되는 LLM 기반 <U>generative learning stage</U>로 나뉜다. 그나마 limitation이라고 하면 단계별로 나뉜 training step이라고 할 수 있겠다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607891-a876e26d-6fa0-4f2f-b95c-e0060e53cc42.png" width="1200"/>
</div>


모델의 framework를 보여주는 figure는 위와 같다. **Q-former**는 <U>학습이 가능한 querry</U>를 가지고 있으며, 이 querry들은 이미지에서 특정 text가 주어졌을때 alignement에 활용될 수 있는 feature를 추출해주는 역할을 한다. Q-former가 없었던 BLIP 논문에서는 image-text matching, image-text contrastive learning 그리고 image-grounded text generation 모두 서로 다른 loss term으로 서로 다른 network(물론 text encoder와 decoder가 같은 weight으로 학습되었지만)를 최적화하는 방식이었지만, BLIP-2에서는 Q-Former의 encoder-decoder 구조에 대해서 <U>causality만 조절</U>해줌으로써 encoder에서와 decoder에서 각각의 task에 맞는 optimization이 진행될 수 있다는 장점이 있다. 실선으로 표시된 부분이 image and text matching(bi-directional)과 image-grounded text generation(uni-direction)을 보여주며 점선과 ‘x’ 표시로 이어진 부분이 각 uni-modal(encoder와 decoder) 부분의 contrastive learning이다. 우측 그림을 보면 알겠지만 image and text matching은 모든 semantic 정보를 참고하여 alignment를 진행할 수 있고, image-grounded text generation은 autoregressive한 생성 과정 때문에 causality가 학습 과정에 적용되는 것을 확인할 수 있다. 마찬가지로 image-text contrastive learning에서는 각 query token에 대한 positive sample(하얗게 표시된 부분)과 negative sample(회색으로 표시된 부분)에 대해 학습되는 것을 시각화하였다.

### Bootstrap vision-language representation learning from a frozen image encoder

Representation learning stage에서는 Q-former를 frozen image encoder와 연결하여 image-text pair를 통해 학습하게 된다. 처음 학습 단계에서 얻고자 하는 것은 Q-former가 가지는 학습 가능한 parameter인 querry를 잘 학습하여 visual representation으로부터 text prompt와 유의미한 관계를 가지는 특징을 추출하고자 하는 것이다. BLIP에서 얻은 아이디어를 바탕으로 세 개의 서로 다른 objective를 최적화하였다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607893-960183d0-a794-4292-9710-dcbade846a95.png" width="600"/>
</div>


Loss term에 대한 추가 ablation이 없었다는 것은 아쉽지만 우선 위의 표를 보게되면 ITG(generation) loss term이 미약하게나마 성능을 높인 것을 확인할 수 있다. 사실상 training 과정에서 첫번째 단계의 가장 중요한 점이 frozen image encoder가 보다 text prompt와 유의미한 관계를 가지는 feature를 뽑는 과정이기 때문에 **text generation loss(ITG)**가 성능에 주된 영향을 끼칠 것이라고 생각했는데 의외로(?) ITC+ITM loss가 성능 부분에 있어서 높은 결과를 보여주었다.

그리고 앞서 BLIP 논문을 설명할 때 빠진 부분이 있는데, negative sampling을 진행할 때 momentum queue를 사용했었는데 BLIP-2 paper에서는 batch 내부에서 positive pair와 negative pair를 구분짓는 방식을 사용했다고 한다. momentum queue에 대한 내용은 moco 논문들([참고 링크](https://arxiv.org/abs/1911.05722))과 SimCLR 논문([참고 링크](https://arxiv.org/abs/2002.05709))을 보면 보다 이해가 쉬울 것이다.

### Bootstrap vision to language generative learning from a frozen LLM

앞선 학습 과정을 통해 frozen image encoder와 Q-former 사이의 representation alignment이 마무리가 되었기 때문에 다음 작업으로 Q-former와 frozen LLM 사이의 alignment를 진행하게 된다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607895-2ea24f75-a584-4f40-b676-2b903d921e8c.png" width="1000"/>
</div>


앞서 학습 과정을 보면 Q-former 자체도 하나의 lightweight transformer이기 때문에 visual 정보에 대한 embedding을 output으로 내보내게 되는데, 이를 어쩐 일인지 그대로 LLM의 input으로 사용하지 않고 FC layer를 통해 projection을 해서 학습에 사용하였다.  아무래도 text prompt에 prepend(앞쪽에 붙여서) 학습한 것을 보니 input에 대해 alignment를 진행하는 frozen 방식과 유사하되 이때의 image encoder는 Q-former로 대신 생각하면 될 것 같다. ~~근데 fully connect layer를 곁들인..~~ 혹시나 해서 Frozen 논문을 찾아봤는데 해당 논문에서는 input에 prepend를 하는 과정에서 FC layer를 사용하지 않았다.

그리고 위의 그림을 보면 알 수 있지만 LLM을 사용할 때 decoder base랑 encoder-decoder base 모두 실험해보았다고 했다. Decoder base(OPT)에서는 FC layer로 나온 soft visual prompt를 그대로 output text 추출에 대한 input으로 사용하고, encoder-decoder base(FlanT5)에서는 FC layer로 나온 soft visual prompt를 원래의 prefix text에 붙여서 사용하게 된다.


# Results


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607897-2809d23c-ebdc-4767-b33b-972f384e0ed7.png" width="1000"/>
</div>

성능은 open-source가 가능한 BLIP과 비교했을 때도 상당히 높은 수치를 보여주었고 비교적 최근 SOTA인 Flamingo나 BEIT-3과 같은 구조에 대해서도 최대 $+8$의 정량적 평가 향상을 보여주며 그 성능을 보여주었다. Q-former의 학습은 representation alignment 과정이 주요하게 작용하는데, 이를 보여주는 것이 바로 다음과 같이 학습 stage를 구분했을 때와 아닐 때의 학습 차이를 보여주는 그래프이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607902-cc4d0603-2f7b-4f50-b83b-4cc506b75180.png" width="800"/>
</div>


그리고 학습 두번째 stage에서 학습에 쓸 수 있는 LLM baseline으로는 decoder based/encoder and decoder based 모두 가능하기 때문에 FlanT5와 같은 encoder-decoder 기반의 구조에서는 아래와 같이 image question and answering이 가능한 것을 볼 수 있다. 사실 아래 task는 VQA와 같은 question answering이라 불리지 않고 instructed zero-shot image-to-text generation이라고 부르는데 큰 차이는 없는 것 같다(아닌가?).


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607906-4e28961b-3d6b-4fdb-8aa3-bba81aa05179.png" width="800"/>
    <img src="https://user-images.githubusercontent.com/79881119/218607912-7c0b577d-aace-499c-90f0-9244361d1c6d.png" width="500"/>
</div>


그리고 우측 그림은 VQA fine-tuning에서 사용한 구조라고 한다.


# Conclusion and discussion

VLP task에서 보다 효율적으로 vision model과 language model을 함께 사용할 수 있는 방법을 연구한 BLIP과 후속 논문인 BLIP-2에 대해서 리뷰해보았다. 아카이브에 올라온 지 얼마 되지 않은 따끈따끈한 작품이지만 굉장히 흥미롭게 볼 수 있었고 이해를 완벽하게 하진 못한 부분들도 있었지만 insight를 얻어가기 괜찮은 논문이라고 생각했다.

하지만 의문이 들었던 점은 BLIP-2의 경우 FC layer를 사용했던 이유라던가, ITG loss가 직접적으로 performance에 그리 큰 영향을 끼치지 않는 것으로 보이는 부분에 대한 분석이었고, 그 이외에는 논문에서 자체적으로 저자들이 제시한 limitation인 few-shot에 대한 in-context learning 방법을 찾지 못했다는 점과 LLM의 up-to-date information의 부재로 인한 image to text generation의 failure case가 존재한다는 점이다.

Web을 기반으로 학습된 좋은 성능의 모델을 그대로 사용할 수 있다는 점은 장점이 되지만 LLM 네트워크를 학습할 때 존재하지 않았던 정보에 대한 inference를 image가 포함한다면 성능이 좋지 않을 수 있음을 보았다.
`,RO=`---
title: "Prompt learning in Vision-Language(CoOp, CoCoOp) 논문 리뷰"
category: "ai papers"
publishedAt: "2023-02-19"
thumbnail: "https://user-images.githubusercontent.com/79881119/219934909-7b186c22-5b2c-4431-abb6-059bf1d07357.png"
---


# 들어가며…

대용량의 <U>pre-trained VL model</U>(CLIP, ALIGN 등등)은 여러 downstream task에서 <U>효과적으로 representation을 transfer</U>할 수 있음을 보여주었다. 특히 zero-shot이나 linear probing에서 보여준 성능은 vision과 language를 함께 활용하는 것이 보다 open world set에 대해 적합한 학습 구조라는 것을 증명하였다. 대부분 one-hot encoding과 같은 <U>discretized label</U>(class를 label로 mapping하는 형태)를 사용하는 것이 traditional representation learning의 트렌드였다면, VLP(Vision language pre-training) task에서는 image와 text를 같은 feature space에 어떻게 하면 유의미한 관계를 가지게끔 위치시킬 수 있을지 연구하게 되었다. 이러한 연구를 토대로 각 downstream task를 해결하기 위한 <U>prompting</U>을 적용하였으며, 여기서 말하는 prompting이란 일종의 ‘<U>텍스트를 구성하는 방법론</U>’이라는 의미가 된다. 이번 포스팅에서 다룰 주제가 prompt learning인 만큼 prompt에 대한 개념이 계속 사용되기 때문에 대략적인 개념만 짚고 넘어가도록 하자.

자세한 내용은 뒤에서 더 디테일하게 설명할 것이지만 이 논문에서 저자들은 <U>prompt engineering</U>이 challenging하다는 점을 문제로 삼는다. 보통의 딥러닝에서 학습 성능을 높이기 위한 방법으로 hyperparameter를 조정하는 과정을 거치곤 하는데, 이처럼 VLP task에서 prompt를 어떠한 방식으로 만들어내는지에 따라 task별 성능의 등락폭이 너무나도 커져서 무시할 수 없는 전처리 과정으로 간주되었다. 하지만 단순히 단일 task의 성능을 높이기 위해 사실상 <U>무한에 가까운 경우의 수</U>를 가지는 prompt를 모두 테스트하는 것도 굉장히 시간이 오래 걸리는데, 이들 중 가장 좋은 성능을 보이는 prompt를 기준으로 학습을 진행했더라도 다른 task에서는 <U>또다시 prompt engineering 과정을 처음부터 시작</U>해야하는 것이다. 그리고 특정 언어에 대한 representation을 학습하기 위해 prompt를 만드는 과정이 <U>domain expertise</U>를 요구한다는 점이 문제가 된다.

따라서 이 논문에서는 NLP task에서 제시한 prompt learning 방법론을 사용하여, prompt 자체를 최적화하는 <U>Context Optimization</U>(CoOp)을 제시하였고, CoOp은 prompt의 context word를 학습 가능한 벡터로 간주하여 VL task의 model parameter는 frozen 시킨 채로 각 downstream task에 최적의 prompt를 찾기 위한 학습 과정을 거친다. CoOp을 통해 더 적은 shot(학습 샘플)으로도 hand-crafted prompt의 성능을 따라잡을 수 있었으며 학습이 충분히 진행된 후에는 downstream task의 성능이 대략 $15\\%$ 상승한 것으로 나타났다.


# Limitations of existing methods and apperance of VLP

CLIP 논문 리뷰에서도 언급했지만 CLIP이 문제시했던 내용은 충분히 많은 representation을 학습할 수 있는 <U>Web 기반 이미지 데이터셋</U>에 대한 부분이었고, 구체적으로는 <U>왜 기존 image modality에 대한 학습법</U>이 좋은 representation learning 방법이 아니었는지 언급하지 않고 넘어왔었다. ResNet과 Vision Transformer와 같은 연구들에서 확인할 수 있듯이 classification task의 경우 정해진 갯수의 object category가 있고, 각 카테고리에 대한 description은 indexing을 거쳐 discrete label로 간주하게 된다. 예를 들어 <U>CIFAR10</U>과 같은 경우에는,


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934924-6bf23756-4bec-4c2d-ab25-90539e24cdf9.png" width="600">
</div>


위와 같이 총 $10$개의 클래스를 가진 데이터셋을 사용하는데, 각각의 카테고리 정보(airplane, cat, frog 등등)은 학습 과정에서 단어의 뜻으로 supervision을 주는 것이 아닌 one-hot encoding 형태로 cross-entropy loss를 최적화하는데 사용되었다. ImageNet dataset은 약 1000개의 클래스로 구분되는데, 다음과 같은 클래스들이 포함된다.

\`\`\`python
281: 'tabby, tabby cat',
282: 'tiger cat',
283: 'Persian cat',
284: 'Siamese cat, Siamese',
285: 'Egyptian cat',
\`\`\`

이는 완전히 무관한 카테고리인 ‘orange’라던지 ‘mushroom’과의 언어적/맥락적 유사도를 전혀 고려하지 않고 <U>단순히 서로 다른 카테고리일 경우에는 다른 인덱스를 부여</U>하는 형태의 supervision이 될 수 밖에 없다. 이러한 <U>discretized label</U>이 가지는 문제는 다음과 같이 묘사할 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934884-25d0d7bd-acf1-4b77-a5e4-eb580320e411.png" width="400">
    <img src="https://user-images.githubusercontent.com/79881119/219934885-578f5f29-c458-4aa6-94bd-88a2eaeff2e3.png" width="480">
</div>


만약 **siamese cat**과 **tiger cat**이 있다면 분명 <U>두 사진은 다른 카테고리</U>이지만, 어쨌든 카테고리의 이름에서 볼 수 있듯이 ‘고양이’라는 공통점을 가지고 있고, 무엇보다 두 이미지에서 확인할 수 있는 object의 attribute(동물, 털이 있음, 수염이 있음 등등)가 유사하다고 판단할 수 있다. 그와는 반대로 아래에서 볼 수 있는 **사과 이미지**는 위의 두 사진과 사실상 겹치는 attribute가 없다고 볼 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934886-e477a8f9-9163-48af-afa8-17812c16b808.png" width="400">
</div>


하지만 기존 classification task에서 사용하는 카테고리별 label은 <U>text가 가지는 연관성</U>을 이미지 특성과 전혀 연관짓지 못한다는 것이다. 그렇기 때문에 실제로 open world set에 대해 적용할 수 있는 다양한 object를 판별하기 위해서는 continous signal인 image를 discrete signal인 one-hot encoding으로 단순하게 mapping을 하는 것이 옳지 않다. 또한 카테고리를 묘사하는 text가 더 <U>자세하면 자세할수록</U> network가 분류해야할 <U>category의 개수는 증가</U>하게 되고 그렇게 되면 정답인 label 이외의 <U>다른 object들에 대한 노드가 방해 요소</U>로 작용할 수 밖에 없다. 기존 computer vision 관련 딥러닝에서 다루던 방법은 visual recognition system을 closed set으로 간주한다는 점이 한계가 되었고, 데이터가 추가되면 기존에 학습했던 데이터에 추가하여 <U>새로운 classifier를 학습해야하는</U> 문제점이 생기게 되었다(보다 관심이 생긴다면 continual learning, domain generalization 관련 논문을 찾아보면 좋다).

따라서 비교적 최근 pre-training을 discretized label이 아닌 text embedding에 대해 진행하는 VLP model인 CLIP과 ALIGN이 등장하기 시작했고, vision representation learning에서 기본 방식에 비해 zero-shot transfer 성능을 눈에 띄게 높일 수 있었다. 학습 아이디어는 간단한데, 바로 image encoder과 text encoder의 output을 align하는 방식을 사용한 것이다. CLIP과 ALIGN 논문에서는 모두 <U>contrastive loss</U>를 objective function으로 사용했는데, 만약 image를 묘사하는 text랑 서로 **positive pair**(image-text pair를 학습 데이터로 사용했다)라면 **embedding space** 상에서 <U>가까워지도록</U>, **negative pair**라면 <U>멀어지도록</U> 학습하게 된다. 이와 같은 방법으로 CLIP과 ALIGN은 대용량의 데이터셋에 대해 pre-trained된 네트워크를 사용하여 <U>다양한 task</U>에 맞는 <U>prompting</U>을 거쳐 knowledge transfer을 진행하였다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934887-21a4e1bc-569d-4078-870f-45422742032a.png" width="700">
    <img src="https://user-images.githubusercontent.com/79881119/219934888-e0ad3742-7bac-42f8-ad7f-c722a75f515c.png" width="600">
</div>


**Task specific prompting**과정은 위의 그림들 중 좌측(<U>CLIP framework</U>)에서 확인할 수 있는데, classification을 진행할 class의 object name을 사용하여 “A photo of (Class)” 등의 형태(task마다 prompt 형식이 달라짐)로 <U>formatting을 진행</U>한 뒤, text encoder를 거쳐 나온 **텍스트 임베딩**을 <U>classifier weight으로 사용</U>하게 된다.


# Limitations of prompt engineering in VLP

사전 학습된 network를 사용하는 VL task에서 성능에 주된 영향을 보여준 것은 적절한 prompt engineering이었다. 그러나 적절한 prompt를 정의하는 것은 쉽지 않았기 때문에 이를 tuning하는 과정이 굉장히 오래 걸리는 작업이었고, 약간의 prompt 변화로도 <U>성능이 크게 좌우</U>되는 task도 존재했다. 예를 들어 Caltech101 dataset과 같은 경우, “a photo of (class)” 대신 “a photo of a (class)”를 썼을 때 무려 성능이 $5\\%$ 증가할 정도로 <U>변동이 큰 것</U>을 확인할 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934889-04937a61-abee-4e74-b265-93c782e1ce81.png" width="600">
</div>


게다가 prompt engineering은 task에 대한 prior knowledge도 있어야하며 language model이 작동하는 방식에 대한 domain knowledge가 필요하다. 


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934891-8f949757-edd8-4e9c-8726-41c50737d8bf.png" width="1200">
</div>


‘texture’라던지, “centered satellite photo”와 같은 specific한 description은 해당 dataset이나 분야에 대해 제대로 알고 있지 않다면 <U>prompt engineering에 적용하기 힘들기 때문</U>이다.  그리고 가장 큰 문제는 앞서 말했던 것처럼 이와 같은 prompt engineering이 실질적으로 <U>optimal한지 확신할 수 없기 때문</U>에 일정 수준의 성능 향상으로만 만족할 수 밖에 없는 상황이다.


# Prompt engineering in VLP task

위에서 볼 수 있는 여러 문제점들을 해결하기 위해 NLP task에서 비교적 최근 등장한 <U>prompt learning research</U>의 아이디어를 기반으로 CoOp에서는 <U>downstream task에 최적화</U>된 prompt를 찾고자 하였다. 기존의 prompt를 찾는 방식은 최적의 hyperparameter를 찾기 위해 tuning 및 검증하는 단계와 같았지만 CoOp 논문에서 제시하는 방법은 적절한 prompt를 찾는 과정을 자동화하는 것이다. 즉 prompt의 context words를 학습 가능한 vector로 취급한다는 것이다. 다양한 task를 다루기 위해 논문에서는 <U>두 가지 방식을 제시</U>하였다. 첫번째는 <U>unified context</U>로 모든 class에 대해 동일한 prompt를 학습시키는 방법이고 두번째는 <U>각 class마다의 prompt를 학습</U>시키는 방법이다. 대부분 첫번째 방법이 좋은 성능을 보였으나, 몇몇 fine-grained category를 가진 task에서는 두번째 방법이 더 효과적이었다고 한다.


# Contributions

논문에서 선택한 방법인 <U>prompt learning</U>은 기존의 prompt engineering에서 사용한 방식처럼 text prompt를 discretized explanation(category)로 생각하지 않고 continous signal로 취급한 것과 같다. CoOp의 <U>prompt optimization의 효과</U>를 확인해보기 위해 $11$개의 벤치마크 데이터셋을 사용, 여러 object나 scene 그리고 action 등등 다양한 형태의 category로 분류된 task에서 evaluation을 진행하였다. 결과적으로 논문에서 제시한 본인들의 contribution은 다음과 같다.

- 직접 VLP 네트워크에 대해 다루지는 않고, 이를 활용한 downstream application을 연구한 시기적절한 연구라고 생각한다. 그리고 기존의 VLP 방식에서의 비효율적인 문제점을 제시하였다(prompt engineering).
- 사전 학습된 VL model을 통해 prompt engineering이 자동화될 수 있게 하기 위해, continous prompt learning 방식을 기반으로 접근하였고 unified/class specific 방식 두 가지를 제시함으로써 보다 다양한 recognition task에 적용될 수 있는 방법을 제시하였다.
- Hand crafted prompt 방식과 linear probe model 방식을 downstream task에 적용했을 때 VLP 네트워크의 representation을 transfer learning이 진행되는 것보다 더 효율적으로 최적화가 가능하며 성능 측면에서도 기존 방법들을 뛰어넘었다.  그리고 VL model에 있어서 domain shift에 보다 robust하다는 특징이 있다.


# Related works

### Vision Language models

결국 <U>prompt learning</U>을 적용한 것은 VLP task의 <U>pre-trained representation</U>을 <U>효과적으로 transfer</U>해서 사용하고 싶기 때문인데, 비교적 최근 연구인 **CLIP**과 **ALIGN**과 같이 text와 image encoder의 결과를 <U>contrastive하게 학습한 형태</U>가 image/text multimodal의 기본 아키텍쳐로 사용된다. 두 연구 모두 web 기반의 대용량 데이터셋을 사용했다는 점과 large minibatch를 기반으로 contrastive learning을 진행했다는 점을 공통점으로 삼을 수 있다. 물론 CLIP과 ALIGN 이전에도 text와 image를 같은 embedding space에 올리고자 했던 연구는 있었지만, text embedding을 추출하는 방식(Word2Vec, TF-IDF 등등)이나 matching하는 방식(metric learning, multi-label classification, n-gram language learning 등등)이 현재 SOTA인 contrastive representation learning based와는 차이가 있다.

그러나 이 논문에서 밝히는 바는 본인들의 연구는 이러한 기존의 vision-language model의 연구와 방향성이 다르다고 한다. 이는 기존의 VLP task는 <U>image와 text를 동일 embedding space 상에 alignment하는 방법</U>에 대해 초점이 맞춰져 있었다면 이 논문에서는 이미 학습된 <U>pre-trained knowledge를 transfer하는 방식</U>에 대해 초점을 맞춘 것이다. 논문에서는 hand-crafted 방식으로 prompt engineering을 하는 과정을 prompt learning으로 바꾸는 것이 효과적일 것이라고 밝혔다.

### What is prompt learning?

**Large pre-trained language model**(LLM)의 knowledge probing을 하는 방식으로는 ‘<U>빈칸 채우기</U>’ 방식의 cloze texts 방법이 제시되었고, 이는 NLP task에서 <U>prompt learning</U> 연구가 진행될 수 있는 발판이 되었다.

**Knowledge probing**이 빈칸 채우기 방식이라고 했는데, 간단하게도 <U>probing의 기본 컨셉</U>은 주어진 cloze-style의 prompt에 대해 <U>정답을 생성하게끔</U> 하는 것이다. ‘[How can we know what language models know?](https://arxiv.org/pdf/1911.12543.pdf)’ 논문에서는 text mining을 통해 여러 prompt를 후보군으로 생성하고, training accuracy를 보이는 prompt를 optimal prompt로 사용하는 방식을 제안하였다. [AutoPrompt](https://arxiv.org/pdf/2010.15980.pdf)에서 제시한 방법은 아래 그림에서 볼 수 있듯이 label likelihood에 대해 가장 큰 gradient 변화를 주는 token을 searching한 뒤(<U>gradient based search</U>라고 부른다) 이를 prompt generation에 사용하였다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934893-cec8afe9-afe0-49c9-b10b-bd8b0b98e477.png" width="800">
</div>


이 논문에서는 <U>continous prompt</U> 방식을 사용하게 되는데, 이 방법의 문제점이라면 기존 discrete token에 대해 text embedding space에서 찾는 것보다 학습된 word가 <U>정확히 어떤 prompt를 나타내는지</U> 시각화할 수 없다는 점이다. 그럼에도 불구하고 저자들이 continous prompt 방식을 사용한 것은 VLP task의 주된 목적은 명확한 prompt embedding을 추출하는 것이 아니라, VL model을 <U>downstream task</U>에 사용했을 때 좋은 성능을 보이는 prompt를 <U>tuning하는 과정을 자동화</U>하기 위함이다. 


# Method


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934894-0538a799-821f-4650-9ad6-9139a0b6255d.png" width="800">
</div>


방법론은 굉장히 심플하다. 단순히 기존의 prompt engineering 부분을 학습 하능한 context vector로 설정하고 이를 최적화하는 과정을 사용한다. 물론 VLP 모델 자체를 학습하는 것과는 orthogonal하다는 저자들의 말과 같이 학습 과정은 CLIP baseline framework에서 step (2)에서 (3) 과정과 같다. 각 <U>downstream task</U>에 대해서는 <U>supervision이 있기 때문</U>에 context를 최적화할 수 있는 것이다.

### CLIP baseline

네트워크 구조는 CLIP을 사용하였는데, 구조를 간단히 확인해보자면 <U>vision encoder</U>와 <U>language encoder</U> 각각 있는데 vision encoder는 ResNet50과 같은 **CNN baseline**과 ViT와 같은 **transformer baseline**을 사용하였으며 language encoder로는 **transformer**를 사용하엿다. CLIP의 텍스트 인코딩 방식은 BPE를 사용한다. 학습 과정은 related work에서 간단하게 설명했었지만 다시 한번 설명하면 다음과 같다. Batch 단위의 image-text pair를 가지고 matched pair(contrastive learning에서의 positive pair)에 대해서는 cosine similarity를 최대화하고 unmatched pair(contrastive learning에서의 negative pair)에 대해서는 cosine similarity를 최소화하는 방향으로 학습한다. 다양한 image/text representation에 대해서 학습시키기 위해 CLIP은 web 기반 대용량($400M$)의 paired dataset을 학습에 사용한다.

CLIP의 주된 <U>contribution</U> 중 하나가 바로 <U>zero-shot inference</U> 성능이 높다는 것인데, CLIP은 웹 기반으로 <U>다양한 text prompt</U>에 대해 학습이 되어있기 때문에 다양한 category를 가지는 classification dataset에 대해 **downstream task**를 수행할 수 있다. 예컨데 $f$가 image encoder에 image $x$를 통과시켜 얻은 feature이며, $(w_i)_{i=1}^K$를 각 class에 대한 description $t_i$를 text encoder에 통과시켜 얻은 weight라고 생각하면 된다. $K$는 downstream task의 클래스 개수를 의미하고 prompt는 ‘a photo of a (class).’와 같이 설정하여, (class)라 명시된 부분에 ‘cat’, ‘dog’와 같이 class 이름이 들어가게 된다. 그런 뒤 prediction은 cosine similarity를 기반으로 softmax probability를 사용하게 된다. cosine similarity를 일종의 score라고 보면 된다(<U>유사도</U>가 높을수록, 해당 class일 <U>확률이 증가</U>하는 구조). 

$$
    p(y = i \\vert x) = \\frac{\\exp (\\cos (w_i, f) / \\tau)}{\\sum_{j=1}^K \\exp(\\cos (w_j, f)/\\tau)}
$$

$\\tau$는 <U>temperature parameter</U>로 CLIP pre-train 과정에서 학습되는 parameter이다. 기존 classifier가 <U>closed-set visual concepts</U>(정해진 class를 구분하는 task로서 학습됨)에 대해서만 discrete label을 학습하는 구조였다면, CLIP은 <U>open-set visual concept</U>를 유기적으로 학습할 수 있다는 것이 <U>high-capacity network</U>를 구성할 수 있는 방법이 되었다. 

### Context Optimization

그러나 위의 방법에서 볼 수 있듯이 ‘a photo of a (class)’와 같은 prompt는 사람이 직접 각 task에 대해 <U>좋은 성능을 보이는 prompt를 찾는</U> tuning 과정이 필요하다. CoOp 논문에서는 이를 자동화할 수 있는 방법으로 다음과 같이 두 방법들을 제시한다.

**Unified Context**

모든 class에 대해서 같은 context를 공유하는 방식이다. Text prompt $g(\\cdot)$에 주어지는 prompt는 다음과 같은 형태로 정의해볼 수 있다.

$$
    t = (V)_1(V)_2 \\cdots (V)_M (\\text{CLASS})
$$

$(V)_m$으로 표시된 부분이 각각 특정 word의 embedding이라고 가정하면 된다(CLIP에서는 $512$의 dimension을 가진다). $M$은 <U>사용할 word embedding의 갯수</U>가 되며, 이는 hyperparameter로 정해주게 된다. Prompt $t$를 text encoder에 통과하면 각 class에 대한 <U>classification weight vector</U>를 구할 수 있고, <U>prediction probability</U>는 위에서 봤던 식과 동일하게 구할 수 있다.

$$
    p(y = i \\vert x) = \\frac{\\exp (\\cos (g(t_i), f) / \\tau)}{\\sum_{j=1}^K \\exp(\\cos (g(t_j), f)/\\tau)}
$$

그런데 사실 <U>최적의 context 구조</U>가 “a photo of (class)” 형태일 수도 있지만, “a photo of (class), a type of object”일 수도 있기 때문에 <U>학습 가능한 prompt</U>를 다음과 같이 지정해줄 수도 있다.

$$
    t = (V)_1 \\cdots (V)_{\\frac{M}{2}}(\\text{CLASS})(V)_{\\frac{M}{2}+1} \\cdots (V)_M
$$

이처럼 prompt는 **latter cell**을 채울 수도 있고, **termination signal**을 통해 더이상 채우지 않을 수도 있게끔 학습할 수 있다.

**Class-specific context**

앞서 언급한 방법은 모든 class에 대해 <U>동일한 context</U>를 학습하게 되고, 다른 방법으로는 각 <U>class 마다의 context를 학습</U>하는 방법이 있다. 서로 다른 class index $i$와 $j$에 대해,

$$
    (V)_1^i(V)_2^i \\cdots (V)_M^i \\neq (V)_1^j(V)_2^j \\cdots (V)_M^j
$$

위와 같이 <U>서로 다른 context를 학습</U>하는 것이다. 이 방법은 일반적인 task와는 다르게 <U>fine-grained classification</U>이 필요할 때 효과적이었다고 한다.


# Experiments

저자들이 실험한 데이터셋은 총 $11$개로, ImageNet, Caltech101, Oxford-Pets, StanfordCars, Flowers102, Food101, FGVCAircraft, SUN397, DTD, EuroSAT 그리고 UCF101을 사용했다고 한다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934896-5b60da1e-4aad-4333-aa57-133fc7aaaf76.png" width="800">
</div>


<U>사용된 dataset의 statistics</U>는 위와 같았다. Hand-crafted prompt로 사용한 prompt는 연구에서 ablation을 통해 <U>가장 좋은 성능을 보이는 prompt</U>를 적용했을때다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934897-83e27c9f-e872-4f31-8354-e88cb744e457.png" width="600">
</div>


$11$개의 dataset에 대해 <U>CoOp 방식을 사용한 평균 결과</U>는 위와 같이 나와있다. ‘end’라고 표시된 것이 CLASS description을 마지막에 넣은 context를 최적화했을 때가 되고 ‘mid’라고 표시된 것이 CLASS description을 중간에 넣은 context를 최적화했을 때가 된다. CSC는 class specific하게 학습했을 경우인데, 대체로 경향성을 보게 되면 <U>unified prompt</U>를 적용했을 때가 성능이 좋은 것을 볼 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934899-7721601c-223b-4336-b857-76a4d4035639.png" width="400">
    <img src="https://user-images.githubusercontent.com/79881119/219934901-c0484884-3fa8-4620-84e3-ea55dc8b34be.png" width="400">
    <img src="https://user-images.githubusercontent.com/79881119/219934902-11a25846-78f8-4266-b61d-e9a5ebcd20f8.png" width="400">
</div>


물론 모든 경우에 **unified prompt**가 좋은 성능을 보이지는 않았고 일부 dataset에 대해서는 sample 수가 증가할수록 <U>오히려 CSC가 더 좋은 성능</U>을 보인 경우도 있었다. 이 논문의 가장 main contribution이라고 생각하면, zero-shot CLIP에 대해 여러 dataset의 fine-tuning(Linear probing) 과정에서4-shot 이전까지는 few-shot 성능이 zero-shot 성능 이상으로 보장되지 않았던 것이 limitation으로 제시되었는데, prompt learning을 통해 <U>few-shot 성능</U>이 얼추 <U>zero-shot 성능 이상으로 올라가는</U> 경향성을 확인할 수 있다는 것이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934903-f53b86ca-f618-4fef-8be9-829627e7c7fc.png" width="600">
</div>


위의 그래프를 보게 되면 <U>장점이 더 명확</U>해지는데, Zero-shot CLIP과 비교했을 때 CoOp 방식에 <U>$16$-shot을 적용한 few-shot network</U>가 적게는 $1.24\\%$, 많게는 $45.97\\%$의 성능 향상을 보이는 것을 알 수 있다. 물론 오히려 성능이 하락한 경우(Food101)도 있지만, $11$개의 dataset 중 $10$개의 dataset에 대해 <U>성능 향상을 보이는 것</U>을 확인할 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934905-2383c97b-1547-4d97-a356-93514afbabe7.png" width="700">
</div>


그리고 <U>CoOp을 통한 학습</U>이 위와 같은 <U>domain shifting 상황</U>에 대해서 보다 robustness를 가지는 것을 확인할 수 있다. 특히 ImageNet-R, ImageNet-Sketch 등의 dataset에 대해서는 단순히 source에 대한 성능 향상에 대한 경향성보다는 더 큰 폭으로 성능이 좋아질 수 있는 것을 볼 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934906-b68c65df-b638-40cd-b97f-b6b3a5304b34.png" width="700">
</div>


그리고 실험 결과를 보면 context length를 $16$만큼 사용했는데, 이에 대한 **ablation**과 <U>vision backbone</U>에 따른 <U>CoOp의 경향성</U> 또한 확인하였다. 기존 zero-shot CLIP이 보였던 성능 추이에 비슷하게 나오는 것을 확인할 수 있으며, backbone 구조에 무관하게 <U>CoOp 방식이 성능 향상에 잘 적용될 수 있다</U>는 점을 보여주었다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934908-7e1ff3ff-e917-4602-ba88-00ff341c9812.png" width="800">
</div>


위는 appendix에 각 dataset에 따라 학습된 context와 가장 유사한 vector를 나타낸 것이다. Continous prompt learning을 사용했기 때문에 학습된 vector가 특정 단어를 나타낼 수는 없지만, 위를 통해 간접적으로나마 학습된 prompt와 유사한 word를 찾을 수 있다. 사실 이걸 보면서 느낀 점은 prompt를 continous하게 학습하게 되면 기존의 text 구조 체계가 무너질 수 밖에 없기 때문에, 성능을 높이는 방법으로는 CoOp이 적절할 수는 있으나 <U>image/text와의 관계성을 보여주기엔 한계쩜이 많다는 것</U>이다.  저자들이 related works를 작성하는 과정에서 본인들의 연구가 <U>VLP와는 동떨어진 연구라고 주장했던 이유</U>가 바로 이 때문이 아닐까 조심스럽게 추측해본다.


# Limitation in CoOp and appearance of CoCoOp

CoCoOp에는 치명적인 문제가 존재한다. 이는 바로 학습 과정에서 context가 <U>downstream task에 overfitting</U>이 되다보니, in-domain class에 대해서는 좋은 성능을 보이지만 <U>비슷한 distribution</U>을 가지는 <U>out-of domain class</U>에 대해서는 낮은 성능을 보인다는 점이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934909-7b186c22-5b2c-4431-abb6-059bf1d07357.png" width="900">
</div>


예를 들어 SUN397 dataset에 존재하는 class category인 ‘Arrival gate’나 ‘Cathedral’과 같이 존재하는 class에 대한 accuracy는 zero-shot에 비해 학습된 prompt가 더 좋은 성능을 보이지만,


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934910-961725f8-8edb-43f1-aa47-9649558a6220.png" width="900">
</div>


‘Wind farm’이나 ‘Trail railway’와 같이 비슷한 distribution(<U>scene understanding</U>이라는 관점에서) hand crafted prompt를 사용하는 zero-shot baseline에 비해 오히려 성능이 나빠지는 것을 확인할 수 있다. 즉 최적화된 text prompt가 특정 dataset을 기준으로 <U>seen class</U>에 대해서만 <U>overfitting</U>되는 문제가 발생한다.

사실 이러한 문제는 어느 정도 당연히 제시될 수 밖에 없는 것이 앞서 CoOp 논문 experiment 과정에서 마지막에 언급한 Appendix 표에도 잘 드러나 있는데, 학습된 prompt와의 nearest neighbor을 시각화했을 때 합리적인 word나 문장이 전혀 생성되지 않고 사실상 <U>이미지의 문맥과는 거의 동떨어진 description</U>이 나오는 것을 확인할 수 있었다. 애초에 논문에서 <U>유연한 context</U>를 실험해보기 위해 CLASS prompt를 문맥 중간에 넣거나 뒤쪽에 넣는 식으로 variation을 주었지만,  이러한 과정이 text domain에 대한 intuition에 아무런 영향도 끼치지 못했다는 것처럼 보인다.


# Conditional Context Optimization

논문에서는 이러한 <U>weak-generalization 문제</U>를 해결하기 위해 image captioning과 비슷한 방법을 사용한다. Input image가 ‘<U>학습되는 prompt</U>’에 guidance를 줄 수 있게끔 meta-network를 구성하고, 이를 통해 prompt가 class에 overfitting되는 문제를 정규화하는 것이다. Image captioning에서도 instance에 따른 optimization이 class shift 문제에 대해 보다 robust한 학습이 가능하다고 밝혔다. Prompt가 최적화하는 과정에서 text encoder의 영향만 받다 보니 image representation의 transfer이 약해진다고 판단한 듯하다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934912-418fd3ed-99a5-4f2d-b677-77a68e085ba5.png" width="900">
</div>


따라서 image encoder(ViT, ResNet)의 class token을 기반으로 meta network를 학습하여, 각 image sample instance에 대해 <U>conditioning된 meta-token</U>을 생성하는 방법을 사용하였다. Context tokens($v_1, v_2, \\cdots v_M$)이 오롯이 특정 dataset에 대한 prompt 최적화가 되면 overfitting이 될 수 있기 때문에, context token으로 하게끔 일반화 가능한 정도의 prompt만 학습하고, 나머지 각 image에 대한 정보는 $\\pi$가 줄 수 있게 <U>token</U>을 <U>lightweight meta network</U>로 conditioning한다는 것이다. 컨셉을 보고 생각했던 것은 context token을 학습하는 과정은 <U>잘 짜여진 도화지를 만드는 작업</U>이고, $\\pi$를 학습하는 과정은 좋은 representation을 <U>그릴 수 있는 팔레트를 구성</U>하는 작업이라는 것이다. 기존의 CoOp이 맨땅에서 그럴듯한 prompt를 만들어내고자 하다보니 overfitting될 수 밖에 없었고, CoCoOp은 conditioning을 통해 text encoder와 image encoder의 <U>역할을 분리</U>함으로써 정규화가 가능해진다고 해석되었다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934914-24332975-a9da-4efe-99f1-10a30fa67162.png" width="1100">
</div>



<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934915-aaf4b8b2-224c-480b-b973-263cc98ca375.png" width="1100">
</div>


두 예시에 대해 각각의 <U>CoOp vs. CoCoOp</U> 결과는 위와 같다. 위의 결과는 class specific prompt에 대한 성능이 CoOp와 CoCoOp이 얼추 비슷하게 나온다는 것을 보여주고, 아래의 결과는 unseen class prompt에 대한 성능이 CoCoOp이 훨씬 좋다는 것을 보여준다.  CoOp에서는 zero-shot 성능보다 $10\\%$나 떨어지는 경우에 대해서도 성능이 오히려 올라가는 결과를 보여주었다.


# Conditional Context Optimization(CoCoOp)

사실 논문을 보면 알 수 있듯이 <U>related works</U>나 <U>method</U>가 원래 본인들 연구였던 CoOp paper에서 살짝만 바꾼 수준이다. 뭐 본인들 paper을 그대로 reference로 작성하였기 때문에 굳이 tackle을 걸 필요는 없지만 나도 나중에 논문을 쓰면 저렇게 쓰고 싶다는 생각을 해보았다. 사실 원래 연구에 그냥 meta-network만 추가한 것과 같아서 experiment setting도 쉬워보였기 때문이다.

아무튼 그렇기 때문에 나머지 수식은 모두 동일하고 meta network를 통한 meta token $\\pi$에 대한 식만 살펴보면 다음과 같다. 저자들이 가장 단순하게 먼저 생각했던 것은 $M$개의 context token에 대해 <U>따로 학습되는</U> $M$개의 neural networks를 설계하는 것이었다. 그러나 $M\\times$의 neural network를 보두 학습하는 것은 CoOp 기준으로 $16$개의 network를 학습하는 과정이 되기 때문에 <U>너무 heavy</U>하고, <U>parameter를 적게 사용할 수 없는 방법</U>이 되기 때문에 $M$개의 context vector에 모두 공통적으로 더해질 수 있는 token을 생성하는 <U>meta network를 구성</U>하였다.

파라미터 $\\theta$를 학습 가능한 파라미터로 가지는 Meta-Net($h_\\theta(\\cdot)$)에 대해 input image embedding $f = E_I(x)$에 대한 context vector는 다음과 같이 구성할 수 있다. 우선 앞서 CoOp 논문에서 사용했던 수식에 추가로 설명하면,

$$
    t = (V)_1(V)_2 \\cdots (V)_M (\\text{CLASS})
$$

정해진 갯수의 학습 가능한 prompt vector를 다음과 같이 설계하고, meta token $\\pi = h_\\theta(f)$를 각 vector에 더한 conditioned vector $V_m(f) = (V)_m + \\pi$를 prompt로 대체할 수 있다.

$$
    t(f) = (V_1(f),~V_2(f), \\cdots ,~V_M(f), (\\text{CLASS})
$$

그렇게 되면 기존의 prediction probability는 다음과 같이 수정된다.

$$
    p(y = i \\vert x) = \\frac{\\exp (\\cos (g(t_i(f)), f) / \\tau)}{\\sum_{j=1}^K \\exp(\\cos (g(t_j(f)), f)/\\tau)}
$$

학습 과정에서는 meta network의 <U>parameter $\\theta$</U>와 <U>context vector</U> $V_m$이 함께 gradient based로 update된다.

Meta network는 정말 심플하게 2개의 layer를 가지는 structure로, Linear-ReLU-Linear의 MLP로 구성했다고 한다. 보다 복잡한 구조는 future work로 남기겠다고 했는데, 여기에 더 복잡한 구조를 써서 유의미한 결과를 얻어내는 것 자체는 큰 contribution이 안될 것 같다(~~논문 주제 하나 또 잃었네~~).


# Experiments

실험에 사용한 dataset은 기존 CoOp 연구에서의 $11$개의 dataset을 그대로 사용하였다. 실험 결과 확인한 accuracy 평균은 다음과 같다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934917-5e774c08-85e1-45ab-be82-293f41344704.png" width="600">
</div>


CoCoOp을 사용한 것이 New(unseen class), H(Base + New) 모두 CoOp보다 좋은 결과를 보여주었다. 물론 CLIP이 unseen class에 대한 zero-shot 성능은 제일 좋았으나, base dataset에 대한 성능이 $11\\%$ 차이가 난다는 점에서 CoCoOp 방법이 seen class와 unseen class <U>모두에 적용될 수 있는 방법</U>이라는 것을 보여준다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934919-f8061919-1e32-4526-96b4-9ab6a3803c51.png" width="800">
</div>


각 dataset에 대한 unseen class와 base class 성능 비교는 위와 같다. Base class에 대해서는 CoOp 성능이 더 높은 경우가 많았는데, 이는 <U>overfitting</U> 덕분이라는 분석이 있기 때문에 유의미하지 않았고 <U>unseen class</U>에 대해서는 CoCoOp이 <U>기존 방법을 모두 뛰어넘었다</U>는 점이 주목할만한 점이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934920-76be2e2f-152d-4995-a844-7116db4e7d85.png" width="800">
</div>



<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/219934923-1506df2d-ae29-4475-9cd1-5e9c582481a9.png" width="800">
</div>


Generalization 성능에 대해서 언급한 만큼 domain generalization에 대한 실험도 빠질 수 없는데, 실제로 CoCoOp이 source domain과 target domain의 차이가 커짐에도 CoOp보다 robust한 성능을 보였다고 한다.


# 결론

CoOp 논문, CoCoOp 논문 둘 다 <U>prompt learning을 기반</U>으로 <U>CLIP downstream task</U>의 성능을 높이고자 한 방법론을 다룬다. CoOp에서는 NLP task의 여러 prompt learning 기법 중에서 최적화 과정에 적용할 수 있는 <U>continous prompt learning</U> 방법을 사용, <U>downstream task의 성능을 높이는데 집중</U>했으며 CoCoOp에서는 CoOp에서 성능을 높이면서 놓친 <U>seen class에 대한 overfitting 문제</U>를 다룸으로써 unseen class에 대한 <U>generalization 방법</U>도 meta-network를 통해 제시하였다.

NLP task에서 적용되던 domain generalization 방법을 VL network에 적용하면서 VLP 연구와 orthogonal하게 독자적인 논문들을 작성했다는 점이 contribution이 될 것 같으며, 사실 CoOp이나 CoCoOp은 성능에서도 충분히 좋은 논문이지만 ‘이렇게 써야 논문을 쓸 수 있겠구나’라는 생각을 하게 된 paper라고 생각된다.
`,OO=`---
title: "CoCa - Contrastive Captioners are Image-Text Foundation Models 논문 리뷰"
category: "ai papers"
publishedAt: "2023-03-01"
thumbnail: "https://user-images.githubusercontent.com/79881119/222339744-76d29d47-10f1-4dbb-bd7d-503221bf6f6e.png"
---


# 들어가며…

여러 task에서 대용량의 데이터셋으로 <U>large-scale pretrained network</U>를 학습하는 이유는 다양한 downstream tasks에 학습된 <U>representation을 빠르게 적용</U>하기 위함이고, 이를 딥러닝에서는 **representation transfer** 혹은 **knowledge transfer** 관점에서 접근한다. 리뷰할 [CoCa 논문](https://arxiv.org/pdf/2205.01917.pdf)도 결론적으로 말하자면 image/text pair를 활용하여 보다 다양한 task에 robust하게 활용될 수 있는 <U>large network를 학습</U>하고자 하는 방법론을 제시한다. CoCa는 CLIP이나 ALIGN에서 하는 encoder 단위의 contrastive learning(understanding task)와 SimVLM과 같은 encoder-decoder 단위에서의 generative learning(generation task) <U>구조를 통합</U>하고자 한다.

기존의 트랜스포머 기반 encoder-decoder 구조에서는 encoder의 output에 대해 decoder의 attention layer가 cross-attention 구조를 가지지만, CoCa에서는 이를 분리(절반은 unimodal encoder로 사용하고, 나머지 절반을 multimodal decoder로 사용함)하면서 embedding에 대한 <U>contrastive 학습과 captioning</U>을 **동시에 학습**할 수 있는 구조를 제시하였다. 이 논문의 제목인 CoCa(Contrastive + Captioner)가 바로 이러한 맥락에서 나왔다고 보면 된다.

학습 구조는 뒤에서 보다 디테일하게 설명할 예정이지만, 단순히 디코더 구조를 분리함으로써 loss term 두 개를 사용할 수 있는 것이 중요한게 아니라 하나의 computational graph로 end-to-end 학습이 가능하다는 점이다. 사실 contrastive learning과 generative learning을 같이 활용할 수 있는 방법에 대한 연구는 CoCa가 유일하진 않지만, 보다 간단한 구조로 <U>scratch부터 학습하면서</U> VLP의 가장 큰 두 기둥이라고 볼 수 있는 contraative learning과 generative learning을 <U>융합시켰다는 점</U>이 중요한 포인트가 된다.


# Why new architecture is needed?

사실 VLP task에 대한 논문을 읽다보면 introduction이 다 비슷하게 시작되는 것을 볼 수 있다. 그럼에도 불구하고 이 논문의 연구 과정에서 <U>문제로 삼고 넘어간 부분</U>에 대해 제대로 이해하기 위해서 정리하도록 하겠다.

Vision 문제들은 딥러닝 연구가 시작되며 모두 CNN(Convolutional Neural Network)와 같은 single-encoder 구조를 가지고, 각 task에 맞게 학습된다. 이때 주로 활용되는 네트워크가 우리가 많이 알고 있는 ResNet, GoogleNet, EfficientNet 등등의 pretrained backbone이고 image understanding(classification)이나 video understanding(action recognition, grounding 등등)을 해결하는 과정에서 특정 head에 대한 fine-tuning을 진행하는 등의 <U>representation transfer를 사용</U>한다. 그러다가 최근에 나온 CLIP과 ALIGN 계열의 연구로부터 VLP(Vision-Language Pretraining)이 제안되었고, 학습법은 web에서 획득할 수 있는 대용량의 image-text pair를 기반으로 배치 단위의 contrastive learning을 진행하는 것이었다. Image와 Text pair가 서로 매칭된다면 <U>positive sample로서</U> 학습이 되고, 서로 매칭되지 않는다면 <U>negative sample로서</U> 학습이 되는 구조이다. Contrastive learning을 목적으로 하는 VL task에서는 image encoder와 text encoder가 각각 unimodal로 학습되는 dual-encoder 구조가 되며, 각각 담당하는 modality를 embedding space로 매핑하면 <U>contrastive loss</U>로 하여금 **cross-modal alignment가 진행**되는 구조이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/222339732-fa0d3d4e-d8e6-4087-bf5b-4cc564f92fb8.png" width="600">
</div>


**CLIP**과 **ALIGN**이 contrastive 관점에서의 image와 text를 align하는 문제로 접근했다면, **SimVLM**과 같이 generative 문제의 관점으로 접근한 연구들도 있다.  차이점이라면 서로 다른 modality인 image와 text 간의 <U>alignment</U>를 어떻게 진행하는가에 대한 부분인데, dual-encoder로 처리한 contrastive learning은 직접 각 encoder가 implicit하게 modality를 이해하는 중간 과정에 <U>explicit supervision</U>을 주지 않는 입장이라면 이와는 반대로 generative learning은 이미지 혹은 이미지 + prompt에 대한 정보와의 attention을 통해 이미지를 묘사하는 text를 생성하는 학습 구조를 가지기 때문에 <U>decoder 중간에서</U> alignment가 진행된다고 볼 수 있다. 물론 encoder-decoder 구조의 encoder의 output 또한 query에 대한 key/value가 되기 때문에 간접적으로 alignment가 이루어진다고 볼 수 있다. 


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/222339735-8948bcb0-f6e3-4ddf-8ed6-472afcd5036f.png" width="600">
</div>


결국 **두 방법 모두** <U>학습의 주된 목적</U>은 <U>이미지와 텍스트를 잘 연관짓는 것</U>인데, 각각의 task는 구조 특성상 task agnostic하지 않다는 것이 드러났다. 이를테면 contrastive learning의 경우에는 zero-shot classification과 같은 understanding 기반 downstream task의 성능이 매우 좋으나, 이미지의 각 부분에 대한 captioning/reasoning 성능이 떨어지는 문제가 발생하기도 하고, generative learning의 경우에는 encoder-decoder 구조를 가지기 때문에 image 없이는 text-only representation을 생성할 수 없는 문제(text modality에 대한 unimodal encoder가 부재하기 때문)가 있거나, image encoder의 output을 사용한 <U>understanding task</U>의 probing 과정에서도 <U>representation의 성능이 떨어진다는 점</U> 등의 문제가 발생한다.

그렇기에 이 논문에서 single-encoder/dual-encoder/encoder-encoder 연구들의 모든 패러다임을 융합할 수 있는 형태를 제안하고자 한 것이고, 학습 과정도 단일화하여 image/text 관계성을 기반으로 end-to-end 최적화가 가능한 알고리즘을 만들고자 한 것이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/222339738-b00793f2-afba-4d96-959d-a080a6aa642e.png" width="600">
</div>


위의 그림을 보면 우측에 나와있는 세 가지의 큰 틀의 패러다임이 존재한다. 가장 좌측에 보이는 것이 multimodal을 제외하고 볼 수 있는 vision recognition(understanding) task가 되고, 중간이 CLIP/ALIGN과 같은 VL contrastive learning 접근법이고, 마지막으로 우측이 generative learning에서 사용하는 encoder-decoder 구조가 된다. 개인적으로는 우측 그림에서 <U>multimodal decoder 하나만 있는 것</U>이 논문에서 말하고자 하는 흐름과 더 맞을 것 같다는 생각을 했다.


# Is it first time?

..라고 한다면 그렇지는 않다. ALBEF와 같은 논문들이 이미 image/text와 관련된 여러 task를 하나의 large network로 해결하고자 학습법을 제시하였다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/222339740-e32b3821-62da-4dd4-8fa4-bd1b5598dc36.png" width="600">
</div>


그러나 그림에서 볼 수 있듯이 학습 과정에서 <U>단일 batch</U>에 대해 single process로 학습되지 못한다는 점, encoder 구조를 사용했기 때문에 generative task에서는 <U>추가로 decoder head가 필요</U>하다는 점 그리고 BERT와 같이 pre-trained model의 representation에 의존하여 학습되기 때문에 <U>scratch부터 학습에 사용할 수 있는 방법론은 아니라는 점</U>이 차이가 있다(+momentum module을 통해 distillation하는 과정도 필요).


# Approach

앞에서도 언급했지만 <U>논문에서 목적으로 삼은 것</U>은 single encoder/dual encoder/encoder-decoder에 대한 모든 supervision을 natural language로부터 획득하는 것이다. 보통 단일 encoder로 classification하는 경우 cross entropy loss를 통해 human annotated class vector와의 KL divergence를 <U>최소화하는 방향</U>으로 score network가 학습된다.

$$
\\mathcal{L}_\\text{Cls} = -p(y) \\log q_\\theta (x)
$$

$p(y)$ 는 one-hot encoding된 class vector이며 $q_\\theta(x)$는 parameter set $\\theta$로 구성된 score network의 prediction을 의미한다. 하지만 단순히 class vector에 대해 학습하는 것은 label 정보를 natural language로 간주하지 않고 discretized label로 간주하기 때문에 language와의 <U>유기적인 학습이 불가능</U>하게 된다. 이를 해결하기 위해 제시된 방법 중 contrastive learning은 image encoder와 text encoder $\\mathcal{I}$와 $\\mathcal{T}$에 대해 인코딩된 image embedding $x$, text embedding $y$를 contrastive하게 모델링한다.

$$
\\mathcal{L}_\\text{Con} = -\\frac{1}{N} \\left( \\sum_i^N \\log \\frac{\\exp (x_i^\\top y_i / \\sigma)}{\\sum_{j=1}^N \\exp(x_i^\\top y_j/\\sigma)} +  \\sum_i^N \\log \\frac{\\exp (y_i^\\top x_i / \\sigma)}{\\sum_{j=1}^N \\exp(y_i^\\top x_j/\\sigma)} \\right)
$$

Captioning은 비교적 간단한데, 보통 autoregressive한 language decoder를 학습할 때 특정 시점을 기준으로 causality를 만족하는($t$ 이전의 prompt만 참고하는 형태) conditional likelihood에 대한 negative log likelihood를 모두 더해서 사용한다. 따라서 captioning loss는 각 generation이 독립적이라는 가정 하에 모든 caption이 제대로 생성될 수 있게끔 조건부 확률을 최적화한다.

$$
\\mathcal{L}_\\text{Cap} = -\\sum_{t=1}^T \\log P_\\theta (y_t \\vert y_{< t}, x)
$$

<U>sequence to sequence</U> 구조에서는 학습 효율성을 위해 teacher-forcing을 사용한다(prediction이 조건부 기준이 되지 않고 ground truth를 조건부의 기준이 되는 것).

소개한 개념 중에서 contrastive loss와 captioning loss를 더해주기만 하면 masked decoder가 추가된 encoder-decoder 구조(decoder가 물론 두 파트로 나뉨)에서 end-to-end 학습의 objective function으로 사용할 수 있다.

$$
\\mathcal{L}_\\text{CoCa} = \\lambda_\\text{Con} \\cdot \\mathcal{L}_\\text{Con} + \\lambda_\\text{Cap} \\cdot \\mathcal{L}_\\text{Cap}
$$


# Decoupled text decoder and CoCa architecture

앞서 언급한 loss를 기반으로 학습하기 위해서는 <U>captioning</U>과 <U>contrastive learning</U>이 동시에 진행되어야한다. 식을 보면 알 수 있지만 captioning은 <U>conditional likelihood</U>를 최적화하게 되고, contrastive는 <U>unconditional likelihood</U>를 최적화한다. 보다 정확하게 말하자면 각각의 modality에 대한 encoder에 대한 output가 <U>서로 독립이라고 가정했을 때</U>의 joint distribution에 대한 negative log likelihood를 최적화하는 것과 같다.

만약 단일 구조의 decoder를 사용하게 되면 하나의 모델에서 두 가지의 objective를 학습할 수 없으므로(conditional likelihood와 unconditional likelihood), 저자들이 제시한 방법은 앞서 설명했던 것과 같이 decoder를 두 부분으로 decouple하여, unimodal과 multimodal을 담당하도록 분리하는 것이다. Unimodal decoder layer는 input text를 latent vector로 인코딩하고, 이때 input은 일반적인 transformer decoder에서 연산하는 것과 같이 causality에 따라 masking한다(하단의 $n_\\text{uni}$ layers). 그리고 이렇게 연산된 text encoder에서 나온 output을 기반으로 image encoding 결과와 cross-attention을 진행하는 상단의 $n_\\text{multi}$ 레이어가 존재하게 된다. Decoder layer는 정확히 절반으로 나누었다고 한다($n_\\text{uni} = n_\\text{multi}$).


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/222339744-76d29d47-10f1-4dbb-bd7d-503221bf6f6e.png" width="500">
</div>


Transformer 기반 vision task encoder에서는 ‘cls’ 토큰과 같이 이미지 전체에 대한 정보를 인코딩하는 부분이 존재하고, 이때 encoder로부터 나오는 토큰은 각각 $n$차원의 벡터로 생각하면 된다. 마찬가지로 contrastive learning을 진행하기 위해서 저자는 image encoder 외에 unimodal text decoder의 문장 끝에도 ‘cls’ 토큰을  붙여서 문장 전체에 대한 representation을 통해 contrastive loss를 계산한다.


# Attention pooling

<U>Contrastive loss</U>와 같이 <U>understanding을 베이스</U>로 하는 경우에는 단일 token에 각 modality 전체에 대해 요약하는 형태가 보통 일반적이고 잘 이용되지만(single pooled image embedding), 이와는 다른 downstream tasks(논문에서 말하고자 하는 것은 generation task)의 경우 단순히 이미지 전체/텍스트 전체를 요약하고자 하는 것이 아니기 때문에 보다 <U>많은 visual token이 사용</U>되는 것이 좋은 성능을 보장할 수 있다.

따라서 위의 그림에서 보는 바와 같이 contrastive learning에 사용될 attention pooler와 multimodal text decoder에서 cross attention에 사용될 attention pooler를 정의하였고, 여기서 attention pooler는 <U>image encoder의 output</U>을 key/value로 가지면서 $n$개의 <U>query를 파라미터로 최적화</U>한다고 볼 수 있다. Generative loss를 위한 query는 $256$, contrastive loss를 위한 query는 $1$을 사용하였다.


# CoCa for downstream tasks

학습된 CoCa는 image/text에 대해 다양한 zero-shot transfer가 가능하다. CLIP/ALIGN과 같은 기존 방식들이 가능한 classification부터, image and text retrieval, video and text retrieval 등등이 있다.

이외에도 zero-shot이 아닌 task에 대해 frozen encoder를 사용할 수도 있다. 앞서 설명했던 attention pooler가 각 task마다 적절한 query를 학습하게 되므로, <U>task 마다의 attention pooler를 학습</U>하는 과정이 결국 encoder가 학습한 <U>representation을 다양하게 쓸 수 있는 방법</U>이 된다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/222339746-9c293ab0-018c-4433-8647-019d12d76fc6.png" width="500">
</div>


예를 들어 video action recognition의 경우, 위와 같이 각 프레임을 encoder에 통과한 embedding을 토대로 attention pooling을 진행, 비디오에서 흔히 얻고자 하는 spatial/temporal information에 대한 query를 정의한 뒤에 해당 query에 attention된 정보를 통해 action recognition task에 맞는 softmax cross-entropy를 계산하게 된다. Pooler는 query를 하나만 가지므로(spatial 혹은 temporal) 연산이 크게 어렵지 않다.

Video-text retrieval에서는 더 간단하게 $16$ 프레임의 embedding 평균을 사용하게 되고, 이때 target embedding으로는 각 <U>video의 캡션을 인코딩</U>해서 사용한다. 


# Experiments


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/222339747-275fc33a-a582-4321-b764-71ae043efd81.png" width="500">
</div>


CoCa의 장점이라고 할 수 있는 것은 모든 task에 대해 쉽게 적용 가능한 학습법 및 네트워크 구조를 고안했다는 것이다. 실제로 각 task의 기존 SOTA 성능을 노란색으로 표현했을 때, CoCa가 전반적으로 성능들을 뛰어넘는 것을 확인할 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/222339752-f5c93277-10cf-4e0d-bbcb-5fa01b94835f.png" width="600">
</div>


위의 표를 보면 알 수 있듯이 CoCa는 encoder가 frozen 된 상태에서도 충분히 기존 네트워크에 필적하는 성능을 보여주며, task에 따라 fine-tuning하게 되면 SOTA 성능을 모두 뛰어넘는 것을 볼 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/222339754-ff548bd1-a5e2-4c75-83a5-a26f466199bc.png" width="700">
</div>


CoCa는 image encoder에 대해 ViT와 동일한 구조를 가지기 때문에 표에서 볼 수 있는 것과 같이 표현된다. 가장 parameter 수를 적게 가지는 네트워크부터 시작하여 가장 parameter 수를 크게 가지는 네트워크까지 classification에 대해 fine-tuned 및 zero-shot 성능을 확인해볼 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/222339756-6cc09827-9dbc-4a7d-b2f1-4d786ca8b090.png" width="600">
</div>


가장 큰 네트워크 기준 Flickr와 MSCOCO에 대한 image-text retrieval 성능은 위와 같다. Image to text retrieval이란 이미지를 기반으로 매칭되는 텍스트를 찾는 task이고 text to image retrieval은 텍스트를 기반으로 매칭되는 이미지를 찾는 task가 된다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/222339759-8da1be12-d92c-4af5-8455-1df24f7dcfb0.png" width="600">
</div>


또한 domain generalization에 관련된 dataset인 ImageNet-어쩌구 종류들에 대해서도 zero-shot 성능이 매우 좋은 것을 볼 수 있다. SOTA인 BASIC 기준으로 모든 dataset에 대해 성능이 좋고, 상대적으로 domain shift가 빡센 dataset에 대한 경향성도 유사하게 나온다. 이외에도 video/text retrieval, captioning 등 여러 task에 대한 결과가 있지만 결국 성능이 잘나왔다는 것을 보여주는 것이기 때문에 넘어가도록 하겠다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/222339761-b836d1ce-81fe-4e60-9b44-323b263031f1.png" width="700">
</div>



# Ablations


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/222339766-ba9ea371-a6f7-408e-8cea-3989c313f1b1.png" width="650">
</div>


저자가 한 모든 ablation은 위와 같다. 참고로 표에 나와있는 LE는 linear evaluation, ZS는 zero-shot, AE는 attention evaluation 그리고 FT는 fine-tuning을 의미한다. Ablation은 사실 리뷰어들 방어용이라 논문 이해에는 큰 무리가 없으니 넘어가도록 하겠다.

`,zO=`---
title: "DDIM(Denoising Diffusion Implicit Models) 이해하기"
category: "ai papers"
publishedAt: "2023-04-20"
thumbnail: "https://user-images.githubusercontent.com/79881119/233402059-ca6cb328-11d6-4039-bf0c-7248553516be.png"
---


# 들어가며 …

<U>DDPM</U>은 adversarial training과 같이 직접 latent prior를 지정해줄 수 없는 GAN 모델과는 다르게 collapse가 발생하지 않고 안정적인 학습이 가능하다는 장점을 통해 <U>generative model의 새로운 기대주</U>로 떠오를 수 있었다.

그러나 DDPM의 경우 학습 시 많은 iteration($T = 1000$)을 거치면서 아주 작은 가우시안 노이즈를 픽셀 별로 더하고, 각 step에 대한 noised input($x_t$)을 기반으로 샘플링을 진행하기 때문에 Markov process를 벗어날 수 없었다.

Markov process가 안정적인 prior $p(z)$를 만들어줄 수 있는 방법이지만, **세상 모든 것에는 득이 있다면 실이 있듯**이 DDPM은 샘플링 속도가 현저하게 느리다는 단점이 있었다. 왜냐하면 Markov process는 이전 state에 대해 다음 state가 의존하는 cascade 구조를 가지는데, 이는 GPU와 같은 hardward 가속기로 병렬 처리가 불가능하기 때문이다. 실제로 같은 resolution을 가지는 이미지를 생성하는데 있어서 GAN과 같은 implicit model에 비해 **천 배 많은 연산량이 필요**한 것을 볼 수 있다(예컨데 $32 \\times 32$ 크기의 이미지를 $50k$ 만큼 만들기 위해서는 GAN으로는 1분 미만으로 생성할 수 있지만 DDPM으로는 20시간이 걸린다).  수식 전개 및 디퓨전과 관련된 자세한 내용은 DDPM에 대한 소개글과 DDPM 수식 조지기 글을 참고하면 좋다.

아무튼 이 논문에서는 DDPM의 **샘플링 과정이 너무 느리다는 점**을 지적하여, DDPM과 objective는 동일하게 가져갈 수는 있되 샘플링 과정에서 non-Markovian process를 보장할 수 있는 새로운 방법을 제시한다. 이때 DDPM의 완전히 stochastic한 샘플링 과정과는 대비되는 deterministic(원래 이미지인 $x_0$에 대한 조건부가 적용됨)하다는 점에서 신경망의 decoder 구조와 같은 $1$ to $1$ mapping trajectory를 생성하였고, 이를 이른바 <U>DDIM(Denoising diffusion implicit model)</U>이라고 불렀다. 


# 개요

DDPM의 아이디어는 Markov process인 forward process를 tractable하게 바꾼 posterior $q(x_{t-1} \\vert x_t, x_0)$를 reverse process가 따라가게끔 학습하는 것이었다. DDIM은 non-Markov process를 상정하여 deterministic한 샘플링을 가능하게 하므로 다음과 같이 time $t$에 대한 **noised input**이 $x_0$로부터 온 녀석임을 명시해줌으로써 이전 state에만 의존했던 방법에서 벗어날 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/233402059-ca6cb328-11d6-4039-bf0c-7248553516be.png" width=""/>
</div>



즉 원래의 $p_\\theta(x_{t-1} \\vert x_{t})$이 따라가는 분포가 <U>Markovian이 아닌 non-Markovian임</U>을 보장할 수 있게 되므로 sampling 과정에서 복구해야할 이미지 $x_0$에 대한 정보를 디퓨전 프로세스가 implicit하게 인지할 수 있다는 것이다.

그리고 가장 중요한 점은 이런 식으로 바꿈으로 인한 <U>objective function의 변화가 없었기 때문</U>에, 굳이 다른 objective function으로 다시 학습할 필요는 없고 그냥 위의 그림을 통해 전개한 샘플링 방법만 사용하면 pre-trained DDPM을 그대로 사용할 수 있다는 장점이 있다. 또다른 장점이라고 한다면 $x_0$를 인지하고 있는 deterministic sampling이기 때문에 굳이 noised sample을 만들기 위해 거친 forward process의 $T = 1000$를 전부 reverse process를 통해 거치지 않더라도 그의 부분집합인 sub-sequence $T_\\text{sub} = [\\tau_n,~\\tau_{n-1}, \\cdots,~\\tau_0]$를 거치더라도 <U>큰 quality 손상 없이 샘플링이 가능</U>하다는 장점이 있다.

마지막으로 샘플링 과정에서 $x_{t(t < T)}$인 noised image가 모두 $x_0$에 조건화된 샘플링이기 때문에 **latent space에서의 interpolation**이 유의미한 <U>semantic interpolation</U>으로 이어질 수 있다는 것이다.

예컨데 GAN inversion처럼 GAN 모델에서 image editing이 보다 간편했던 이유는 latent $z$가 mapping되는 과정이 단순하게 style mapper $F$와 image synthesizer $G$에 의해 implicit하게 생성된 $x = F(G(z))$가 $z$의 공간상의 정보와 $x$의 공간상의 정보를 non-linear warping할 수 있었기 때문이다. 따라서 정도의 차이는 있지만 $x_1$과 $x_2$를 만드는 두 latent $z_1$, $z_2$를 interpolation하는 $\\alpha \\cdot z_1 + (1-\\alpha) \\cdot z_2$가 image 공간에서도 $\\beta \\cdot x_1 + (1-\\beta)x_2$와 같은 효과를 보여줄 수 있다는 것이었다. 물론 non-linearity이기 때문에 Affine이 수식 상으로는 정확하게 맞아떨어지지는 않지만 semantic하게 금발의 여자와 흑발의 여자 중간의 갈색 머리 여자가 나온다는 직관적인 연산이 가능한 것이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/233407069-92d7f333-5069-41c3-9f3b-3ec90976b4b7.png" width="1000"/>
</div>


이러한 직관적 sampling이 diffusion process에서는 stochasticity 때문에 불가능하다는 한계가 있었는데, 이를 극복할 수 있는 방법 중 하나가 바로 **DDIM**이 될 수 있다는 것이다.


# 수식 증명

DDIM에서는 DDPM과 다른 notation을 사용하기 때문에 조금 혼란스러울 수도 있다. 본인은 DDPM의 notation을 선호하는 편이라 **DDIM에 있는 수식 증명을 죄다 DDPM notation으로 대체할 생각**이다.

$$
q(x_{1:T} \\vert x_0) := \\prod_{t=1}^T q(x_t \\vert x_{t-1}) , \\text{ where }q(x_t \\vert x_{t-1}):= \\mathcal{N}(\\sqrt{\\alpha_t}x_{t-1}, ~(1-\\alpha_t)I)
$$

위에서 볼 수 있는 $\\alpha_t$는 각 time step $t$에 대해 scheduling된 variance인 $\\beta_t$를 $1$에서 뺀 값으로, 수식 증명 게시글에서도 밝힌 바 있듯이 위와 같이 forward process를 통해 noise를 만들어가게 되면 variance를 $1$로 유지하면서 가우시안 분포를 만들어낼 수 있다. 위와 같은 방식으로 noise를 쌓아가게 되면 $q(x_t \\vert x_0)$에 대해 다음과 같이 정리할 수 있다. 아래의 식에서 $\\bar{\\alpha}_t = \\prod_{\\tau=1}^t \\alpha_\\tau$가 된다.

$$
q(x_t \\vert x_0) := \\int q(x_{1:t} \\vert x_0)dx_{1:(t-1)} = \\mathcal{N}(x_t;~\\sqrt{\\bar{\\alpha}}_t x_0, (1-\\bar{\\alpha}_t)I)
$$

그리고 실제로는 위와 같은 stochastic process(분포에서 랜덤하게 뽑는 과정)이 아니라 reparameterization metric을 사용하는 것은 DDPM에서와 동일하다.

$$
x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon, \\text{ where }\\epsilon \\sim \\mathcal{N}(0, I)
$$

놀랍게도 요 식에서 바로 DDIM 식을 유도할 수 있다. 해당 내용에 대한 증명은 Bishop의 pattern recognition 책의 2절 115번 공식을 토대로 한다(가우시안 분포, 조건부 확률에서 특정 변수에 대한 marginal을 구하는 방법). 간단하게 해당 수식을 보이자면 다음과 같다.

$$
\\begin{aligned}
p(x) =& \\mathcal{N}(x \\vert \\mu,~\\Lambda^{-1}) \\newline
p(y \\vert x) =& \\mathcal{N}(y \\vert Ax+b,~\\mathrm{L}^{-1})
\\end{aligned}
$$

위와 같은 분포가 있을 때, $x$에 대해 projection한 $y$ 분포는 다음과 같다.

$$
p(y) = \\mathcal{N}(y \\vert A\\mu+b,~\\mathrm{L}^{-1}+ A \\Lambda^{-1}A^\\top)
$$

조건부의 평균이 $x$에 대해 변수화가 되었는데 이게 prior의 평균으로 수렴하고, 분산은 꽁다리에 붙는 형태가 되었다. 다시 DDIM으로 돌아와서, 우리는 귀납적으로 다음과 같은 constraint를 만족하는 non-Markovian process의 ‘posterior’를 학습하고자 한다. 

$$
q_\\sigma(x_{t-1} \\vert x_t,~x_0)
$$

이거 어디서 봤는데? 싶다면 바로 DDPM 식에서는 이를 원래 forward였던 $q(x_{t-1} \\vert x_t)$를 $t>1$인 모든 time step에 대해 $x_0$에 조건화시키면서 Markov process를 유지한 상태로 Baye’s rule을 전개하는데 사용했었다(아래와 같음).

$$
q(x_{t-1} \\vert x_t,x_0) = q(x_t \\vert x_{t-1},x_0) \\frac{q(x_{t-1} \\vert x_0)}{q(x_t \\vert x_0)}
$$

그런데 조금 다른 점이라면 이번에는 모든 time step $t$에 대해 같은marginal인 $q(x_{t} \\vert x_0)$를 가지는 새로운 non-Markovian 분포 $q_{\\sigma}(x_{t} \\vert x_{t-1}, x_0)$을 찾고자 하는 것이다. 이걸 찾을 때 귀납적으로 증명하기 위해서는 다음과 같은 process를 거치게 된다.

1. $t = 1$일 때, $q(x_{1} \\vert x_0) = q_{\\sigma}(x_1 \\vert x_0)$ 임은 자명하다.
2. $t = \\tau$일 때, $q(x_{\\tau-1} \\vert x_0) = \\int q_{\\sigma}(x_{\\tau-1} \\vert x_{\\tau}, x_0) dx_\\tau$를 만족한다고 생각해보자($\\tau \\ge 1$).
3. 그렇다면 $t = \\tau + 1$에 대해서도 $q(x_{\\tau} \\vert x_0) = \\int q_{\\sigma}(x_{\\tau} \\vert x_{\\tau+1}, x_0) dx_{\\tau+1}$를 만족시킬 수만 있다면, 모든 $t \\ge 1$에 대해 DDIM의 모든 forward process의 marginal을 DDPM과 맞출 수 있다.

앞서 Bishop 책에서 본 공식을 머리 속에 박아넣은 채로 우리가 통일시켜야할 marginal을 살펴보면 다음과 같다.

$$
q(x_{t-1} \\vert x_0) = \\mathcal{N}(x_{t-1};~\\sqrt{\\bar{\\alpha}}_{t-1} x_0, (1-\\bar{\\alpha}_{t-1})I)
$$

이 상태에서 $x_{t}$를 조건부로 박아넣게 되면 다음과 같이 정리된다. 참고로 $q(x_t \\vert x_0)$의 확률과 같이 나타내면 다음과 같이 표현 가능하다.

$$
\\begin{aligned}
q(x_t \\vert x_0) =& \\mathcal{N}(x_{t};~\\sqrt{\\bar{\\alpha}_t}x_0,(1-\\bar{\\alpha}_{t})I) \\newline
q_\\sigma(x_{t-1} \\vert x_t, x_0) =& \\mathcal{N}(x_{t-1};~\\sqrt{\\bar{\\alpha}_{t-1}}x_0 + C\\cdot(x_t - \\sqrt{\\bar{\\alpha}_t}x_0),\\sigma_t^2 I)
\\end{aligned}
$$

위의 식에서 $\\sigma_t$는 같은 marginal을 만족하는 모든 가우시안 분포의 임의의 standard deviation이라고 생각하면 된다. 평균 부분에 $C$라는 constant를 붙여서 위와 같이 식을 쓴 이유는 다음과 같다. Bishop 책 2절의 115번 공식에 따르면 위와 같은 식을 만족하는 두 분포에 대해,

$$
\\mathbb{E}\\left(q_\\sigma(x_{t-1} \\vert x_0)\\right) = \\mathbb{E}\\left(q_\\sigma(x_{t-1} \\vert x_t, x_0)\\right) \\rvert_{x_t = \\mathbb{E}(q(x_t \\vert x_0))}
$$

위와 같기 때문에 $\\sqrt{\\bar{\\alpha}_{t-1}}x_0 + C\\cdot(x_t - \\sqrt{\\bar{\\alpha}_t}x_0)$ 식에서 $x_t$ 대신  $\\sqrt{\\bar{\\alpha}_t}x_0$를 대입했을 때 $\\sqrt{\\bar{\\alpha}_{t-1}}x_0$가 나올 수 있드록 의도적으로 만들어준 것이다. $C$라는 constant를 우리가 임의로 지정했기 때문에 이를 구하고자 variance에 대한 식을 마찬가지로 적용하면,

$$
\\mathrm{Var}(q_\\sigma(x_{t-1} \\vert x_0) = \\mathrm{Var}(q_\\sigma(x_{t-1} \\vert x_t, x_0))+C^2 \\cdot \\mathrm{Var}(q(x_t \\vert x_0))
$$

이고, 우리가 미리 알고 있는 marginal 분포의 variance를 사용하여 위의 식을 다시 나타내면,

$$
(1-\\bar{\\alpha}_{t-1})I = \\sigma_t^2I + C^2 \\cdot (1-\\bar{\\alpha}_t)I
$$

Identity matrix($I$) 부분을 제외하고 $C$를 coefficient에 대해서 다시 표현하면,

$$
C = \\frac{\\sqrt{1-\\bar{\\alpha}_{t-1}-\\sigma_t^2}}{\\sqrt{(1-\\bar{\\alpha}_t)}}
$$

위와 같이 나타낼 수 있다. 따라서 임의의 variance $\\sigma_t^2$에 대한 non-Markovian posterior를 다음과 같이 정리할 수 있다.

$$
q_\\sigma(x_{t-1} \\vert x_t, x_0) = \\mathcal{N}(x_{t-1};~\\sqrt{\\bar{\\alpha}_{t-1}}x_0 + \\frac{\\sqrt{1-\\bar{\\alpha}_{t-1}-\\sigma_t^2}}{\\sqrt{(1-\\bar{\\alpha}_t)}} \\cdot(x_t - \\sqrt{\\bar{\\alpha}_t}x_0),\\sigma_t^2 I)
$$

다시 한번 언급하자면 위의 식은 DDPM에서 유도했던 posterior와 전혀 다른 의미를 가진다. 이번에는 반대로 위의 non-Markov process posterior를 likelihood로 바꿔주게 되면,

$$
q_\\sigma(x_{t} \\vert x_{t-1}, x_0) = q_\\sigma(x_{t-1} \\vert x_t,x_0) \\times \\frac{q_\\sigma(x_t \\vert x_0)}{ q_\\sigma(x_{t-1} \\vert x_0)}
$$

위와 같다. 결국 논문에서 하고자 했던 Markov forward process $q$와 동일한 Margin을 가지는 non-Markovian forward process $q_\\sigma$를 구해낼 수 있게 되었다.


# Non Markovian posterior to DDPM loss

이제 실제로 왜 위와 같은 $q$에 대한 최적화가 DDPM loss와 동일한지 확인해보자. 참고로 DDPM loss를 기준으로 바뀌는 부분은 $q$ 부분만 해당되므로 다음과 같이 표현할 수 있다. DDPM의 lower bound를 non-Markovian forward process로 대체한 것이다.

$$
\\mathbb{E}(-\\log p_\\theta (x_0)) \\leq \\mathbb{E}_q \\left( -\\log p_\\theta(x_T) - \\sum_{t \\ge 1}\\log \\frac{p_\\theta(x_{t-1} \\vert x_t)}{q_\\sigma(x_t \\vert x_{t-1}, x_0)} \\right)
$$

이걸 posterior로 바꾸고 지지고 볶는 과정은 DDPM에서 이미 다 했기 때문에 굳이 증명할 필요는 없고 결론부터 보자면 variational inference loss는 다음과 같이 나온다.

$$
D_{KL}(q(x_T \\vert x_0) \\vert\\vert p_\\theta(x_T)) -\\sum_{t > 1} D_{KL} (q_\\sigma(x_{t-1} \\vert x_t, x_0) \\vert\\vert p_\\theta(x_{t-1} \\vert x_t)) -\\mathbb{E}_q\\left(\\log p_{\\theta}(x_0 \\vert x_1) \\right)
$$

결국 바뀐 것은 forward process를 나타내는 분포 $q$에 $\\sigma$라는 non-Markovian posterior의 standard deviation만 붙은 것이다. DDPM에서 했던 것처럼 첫번째 term은 날려버리고 나머지 term에 대한 디퓨전 모델의 학습을 reverse process $p_\\theta$에 대해 다음과 같이 표현 가능함. 

$$
p_\\theta^{(t)}(x_{t-1} \\vert x_t) = \\begin{cases} \\mathcal{N}(x_0(x_t, \\epsilon_\\theta), \\sigma_1^2I), &\\text{if }t = 1\\newline q_\\sigma (x_{t-1} \\vert x_t,~x_0(x_t,\\epsilon_\\theta)),& \\text{otherwise}\\end{cases}
$$

참고로 위의 식에서 $x_0(x_t, \\epsilon_\\theta)$는 네트워크가 각 $x_t$에 대해 더해졌을 것이라고 예측한 noise인 $\\epsilon_\\theta^{(t)}$를 통해 역으로 추적한  $x_0 = (x_t - \\sqrt{1-\\bar{\\alpha}_t} \\cdot \\epsilon_\\theta^{(t)}(x_t)) / \\sqrt{\\bar{\\alpha}_t}$를 의미한다. 이를 통해 $t>1$인 부분에 대해서만 확인해보면 가우시안 분포에 대한 KL divergence는 exponential을 벗겨버리므로 간단하게 표현된다.

$$
\\begin{aligned}
&D_{KL} (q_\\sigma(x_{t-1} \\vert x_t, x_0) \\vert\\vert p_\\theta^{(t)}(x_{t-1} \\vert x_t)) \\newline
&=D_{KL} (q_\\sigma(x_{t-1} \\vert x_t, x_0) \\vert\\vert q_\\sigma(x_{t-1} \\vert x_t,~x_0(x_t,~\\epsilon_\\theta))) \\newline
&=\\mathbb{E}_q\\left(\\frac{\\parallel x_0 - x_0(x_t,\\epsilon_\\theta) \\parallel_2^2}{2\\sigma_t^2} \\right)
\\end{aligned}
$$

Reparameterization으로 식을 $\\epsilon$(실제로 더해진 noise)을 통해 다시 정리하면,

$$
=\\mathbb{E}_q\\left( \\frac{\\parallel (x_t-\\sqrt{1-\\bar{\\alpha}_t}\\epsilon)/\\sqrt{\\bar{\\alpha}_t} - (x_t-\\sqrt{1-\\bar{\\alpha}_t}\\epsilon_\\theta^{(t)}(x_t))/\\sqrt{\\bar{\\alpha}_t} \\parallel_2^2}{2\\sigma_t^2} \\right)
$$

슬슬 어디서 많이 본 애가 나타나기 시작했다. 계수로 붙는 애들을 정리하고 나면 simplified version인 익숙한 친구가 눈에 보인다.

$$
=\\mathbb{E}_q \\left( (1-\\bar{\\alpha}_t)\\cdot\\frac{\\parallel \\epsilon - \\epsilon_\\theta^{(t)}(x_t) \\parallel^2}{2\\sigma_t^2\\bar{\\alpha}_t} \\right) = \\mathbb{E}_q\\left( \\frac{1-\\bar{\\alpha}_t}{2\\sigma_t^2\\bar{\\alpha}_t} \\cdot \\parallel \\epsilon - \\epsilon_\\theta^{(t)}(x_t) \\parallel^2  \\right)
$$

결국 DDIM sampling을 하기 위해 설정한 non-Markovian posterior가 실제로 DDPM loss로 수렴한다는 사실이 입증된다.


# Sampling with DDIM

앞서 상정한 non-Markovian forward process에 predicted $\\epsilon_\\theta$를 써서 샘플링 과정을 보면 다음과 같다.

$$
x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}\\underset{\\text{predicted }x_0}{\\left( \\frac{x_t - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_\\theta^{(t)}(x_t)}{\\sqrt{\\bar{\\alpha}_t}} \\right)} + \\underset{\\text{direction pointing to }x_t}{\\sqrt{1-\\bar{\\alpha}_{t-1} - \\sigma_t^2} \\cdot \\epsilon_\\theta^{(t)}(x_t)} + \\underset{\\text{random noise}}{\\sigma_t z},~z \\sim \\mathcal{N}(0, I)
$$


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/233402043-3f3fc8fe-24cd-4eb2-949e-3f8ffc9d7af8.png" width=""/>
</div>



식을 보게 되면 $x_0$를 예측하고 $x_t$를 다시 복구하는 형태를 가지는데, 이때 굳이 $t-1$번째 샘플을 예측할 것이 아니라 $t-2$번째 샘플을 예측하더라도 큰 무리가 없는 것을 볼 수 있다. 이처럼 DDIM 샘플링은 $T$의 time step을 전부 소모하지 않고도 일부 $t$만 샘플링 용도로 사용할 수 있다. 예컨데 만약 길이가 $T$인 전체 time sequence $[1,~2,~\\cdots,~T]$의 sub-sequence인 $[x_{\\tau_1},~\\cdots,~x_{\\tau_S}] (S < T)$가 있을 때, non-Markov process인 forward process를 새롭게 $q(x_{\\tau_i} \\vert x_0) = \\mathcal{N}(\\sqrt{\\bar{\\alpha}_{\\tau_i}}x_0,~(1-\\bar{\\alpha}_{\\tau_i})I)$로 정의할 수 있으며, 더이상 이전 state에만 의존하지 않기 때문에 연속 sequence에 대한 의존성도 없어진다.


# Experiment

Sampling 식을 보게 되면 $\\sigma$라는 새로운 hyperparameter가 등장한 것을 볼 수 있다.

$$
x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}\\underset{\\text{predicted }x_0}{\\left( \\frac{x_t - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_\\theta^{(t)}(x_t)}{\\sqrt{\\bar{\\alpha}_t}} \\right)} + \\underset{\\text{direction pointing to }x_t}{\\sqrt{1-\\bar{\\alpha}_{t-1} - \\sigma_t^2} \\cdot \\epsilon_\\theta^{(t)}(x_t)} + \\underset{\\text{random noise}}{\\sigma_t z},~z \\sim \\mathcal{N}(0, I)
$$

해당 식에서 variance를 따로 계산해보면 다음과 같이 엡실론에 붙은 애들을 제곱해서 더한 결과와 같다.

$$
\\mathrm{Var}(x_{t-1}) = \\frac{\\bar{\\alpha}_{t-1} \\cdot (1-\\bar{\\alpha}_t)}{\\bar{\\alpha}_t} + (1-\\bar{\\alpha}_{t-1} - \\sigma_t^2) + \\sigma_t^2 = \\frac{(1+\\alpha_t-2\\bar{\\alpha}_t)}{\\alpha_t}
$$

즉 해당 식이 시사하는 바는 $\\sigma$는 DDPM과 DDIM의 stochastic와 deterministic을 조절해주는 역할을 수행하고, 실질적으로 sampling variance에는 변화를 주지 않는 것을 확인할 수 있다. DDPM의 샘플링 식을 recap 해보면 다음과 같다.

$$
x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\left( x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}}_t} \\epsilon_\\theta(x_t, t)\\right)+\\sigma_tz
$$

그리고 이 식이 나오게 되면 Markovian forward process에 대한 tractable probability distribution $q(x_{t-1} \\vert x_t, x_0)$의 표준편차는 다음과 같았다.

$$
\\tilde{\\sigma}_t^2 = \\beta_t \\times \\left( \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t} \\right)
$$

놀랍게도 DDIM의 $\\sigma_t$가 위의 형태가 되면 정확하게 DDPM의 샘플링 식과 동일해진다. 그래서 다음과 같이 $\\eta$를 hyperparameter로 정의하게 되면 DDPM과 DDIM 샘플링을 조절할 수 있게 된다.

$$
\\sigma_{t} = \\eta\\sqrt{(1-\\alpha_t) \\times \\left( \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t} \\right)}
$$

$\\eta = 1$이면 완전한 DDPM이 되고 $0$이면 완전한 DDIM이 된다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/233402047-07361544-87f3-4d51-8d6f-718fab06dcfc.png" width=""/>
</div>


DDPM($\\eta = 1$)과 DDIM($\\eta = 0$)은 그리고 그 중간에 대해서 $T < 1000$에 대해서는 적은 step만큼 학습시킨 결과를 FID로 나타내었고 DDPM이 급격하게 성능이 안좋아지는 모습을 볼 수 있지만 DDIM은 퀄리티를 유지하는 것을 볼 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/233402052-326292dc-ad90-42b9-a89d-aa7147de3bfb.png" width=""/>
</div>


당연하겠지만 step 수를 늘릴수록 샘플링 성능은 더 좋음 근데 샘플링 시간이 오바라서….

대략 $20 \\sim 100$ step에서 $1000$ step에 필적할 생성 성능이 보이는데, 단순 계산으로 10배에서 50배 속도 향상과 같음.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/233402053-53e4c7da-a2e3-4602-9be9-fe526824eef8.png" width=""/>
</div>


그리고 가장 좋은 점인 latent interpolation이 생성된 이미지의 semantic과 유의미한 관계를 가지게 되었다. 그와중에 Shutterstock 조금씩 드러나는게 개웃김 ㅋㅋㅋㅋㅋ(나만 그런가 ㅠ)


# Neural ODE with DDIM

놀랍게도 DDIM과 DDPM은 diffusion인데 diffusion을 discrete하게 사용하기 때문에 미분 방정식이라고 보긴 힘들다. 그런데 DDIM은 오일러 근사(추정)이 가능한 샘플링의 형태를 보여준다. 예컨데 샘플링 식을 다시 끌고와보면,

$$
x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}\\underset{\\text{predicted }x_0}{\\left( \\frac{x_t - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_\\theta^{(t)}(x_t)}{\\sqrt{\\bar{\\alpha}_t}} \\right)} + \\underset{\\text{direction pointing to }x_t}{\\sqrt{1-\\bar{\\alpha}_{t-1} - \\sigma_t^2} \\cdot \\epsilon_\\theta^{(t)}(x_t)} + \\underset{\\text{random noise}}{\\sigma_t z},~z \\sim \\mathcal{N}(0, I)
$$

오 보인다 보여 DDIM은 앞서 말했던 것처럼 $\\sigma = 0$으로 두면 된다.

$$
\\frac{x_{t-1}}{\\sqrt{\\bar{\\alpha}_{t-1}}} = \\frac{x_t - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_\\theta^{(t)}(x_t)}{\\sqrt{\\bar{\\alpha}_t}} + \\frac{\\sqrt{1-\\bar{\\alpha}_{t-1}} \\cdot \\epsilon_\\theta^{(t)}(x_t)}{\\sqrt{\\bar{\\alpha}_{t-1}}}
$$

애초에 discrete한 샘플링에 대한 식이었기 때문에 시간 간격을 죄다 $\\Delta$로 바꿔주면 differential equation 형태로 바꿔볼 수 있다.

$$
\\frac{x_{t-\\Delta}}{\\sqrt{\\bar{\\alpha}_{t-\\Delta}}} = \\frac{x_t }{\\sqrt{\\bar{\\alpha}_t}} + \\left(\\frac{\\sqrt{1-\\bar{\\alpha}_{t-\\Delta}} }{\\sqrt{\\bar{\\alpha}_{t-\\Delta}}} - \\frac{\\sqrt{1-\\bar{\\alpha}_t}}{\\sqrt{\\bar{\\alpha}_t}} \\right)\\cdot \\epsilon_\\theta^{(t)}(x_t)
$$

$x_t/\\sqrt{\\bar{\\alpha}_t} = \\bar{x}_t,~\\frac{\\sqrt{1-\\bar{\\alpha}_t}}{\\sqrt{\\bar{\\alpha}_t}} = \\sigma$라고 식을 바꾸게 되면,

$$
d\\bar{x}(t) = \\epsilon_\\theta^{(t)}\\left(\\frac{\\bar{x}(t)}{\\sqrt{\\sigma^2+1}}\\right)d\\sigma(t)
$$

위와 같은 미분 방정식이 된다. SDE를 통한 continous diffusion process 논문을 보게 되면 DDPM과 차이가 있는 점이 바로 Neural ODE의 solution으로써 $x(T)$를 제시하기 때문에 one to one mapping(deterministic)이 성립할 수 있다는 것이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/233402056-f89e08b1-e534-43ec-bdb8-b21473f591bb.png" width=""/>
</div>


아무래도 ODE가 성립하다보니 $x_0$를 $x_T$로 바로 인코딩하거나 $x_0$로 리컨이 가능한데, 결국 trajectory도 $\\Delta$에 따라 오차가 정해지기 때문에 $S$가 클수록(시간 간격이 짧을수록) 실제 미분 방정식의 솔루션에 가까워지는 것을 볼 수 있다. 위는 CIFAR-10에 대한 리컨 error를 의미한다.
`,BO=`---
title: "Score-based generative modeling through stochastic differential equations 이해하기"
category: "ai papers"
publishedAt: "2023-04-22"
thumbnail: "https://user-images.githubusercontent.com/79881119/233843040-036f2660-0202-4772-8e70-070b16379e88.png"
---


# 들어가며 …

오늘 리뷰할 논문은 <U>Score based generative modeling</U>을 continous variable SDE로 풀어낸 논문이며, diffusion based approach 중 가장 유명한 DDPM과 더불어 **디퓨전 기초 논문**이라고 불리는 녀석이다(근데 난이도로 봐서는 기초는 아닌 것 같음). DDPM과 결을 달리하는 부분은 sampling 방식에서 <U>numerical하게 미분 방정식을 푼다</U>는 점이고 단계적 예측에 의존하는 DDPM이나 DDIM과는 다르다고 볼 수 있다. 이 논문의 장점은 **score based approch**를 사용하는 다양한 방법들을 일반화하고 다양하게 변화시켜 보다 효과적인 generative sampling에 대해 디스커션한다는 것이다.

DDPM보다는 수식이 훨씬 어렵기도 하고 확률 미분 방정식은 풀어내는 과정에서 constraint에 따라 해결할 수 있는 범위가 크게 바뀌기 때문에 조금 더 복잡하게 느껴진다. 그래도 해당 논문을 이해해야만 <U>단계적 샘플링</U>에 국한된 DDPM, DDIM과 같은 방식에서 벗어날 수 있는 사고가 가능하다고 느꼈다.

# Score based generative process

예를 들어 어떠한 데이터셋 $x \\sim \\mathcal{X}$에 대해 marginal 확률 분포인 $p(\\mathcal{\\mathcal{X}})$이 있다고 생각해보자. 샘플링하기 쉬운 임의의 분포 $z \\sim \\mathcal{Z}$로부터 시작하여 실제 데이터와 유사한 데이터 $\\hat{x} \\sim q(\\mathcal{\\hat{X} \\vert \\mathcal{Z}})$를 만들기 위해서는 두 분포 사이의 연관성을 찾아야하고, 이러한 task를 딥러닝으로 해결하고자 하는 것이 deep generative modeling의 개요이다.

대표적인 방법으로는 likelihood의 <U>maximum lower bound</U>(ELBO)를 사용하여 intractable한 확률 분포를 간접적으로 alignment하는 방법이 있으며(Variational Autoencoder), 


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/233843039-c0351a11-e6bb-4f20-b737-195bb27afc76.png" width="1000">
</div>


학습 단계에서부터 임의로 샘플링한 노이즈 $z$에 대해 거짓 이미지(생성된 이미지)를 만드는 디코더와 실제 이미지를 구분하는 인코더의 적대적 학습을 통해 sampling quality를 올리는 <U>GAN</U>(Generative Adversarial Network)이 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/233843050-649389c2-9cb9-4fcc-bdad-11684cc004bd.png" width="900">
</div>


이외에도 <U>역연산이 가능한 분포 간의 관계</U>를 flow로 정의하고, 이를 통해 latent와 data 분포 사이의 관계를 찾는 flow based modeling도 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/233843037-b7cb4339-09b1-4ac4-8086-40d40993c48e.png" width="900">
</div>


그런 모델과는 다르게 score based model은 어떤 노이즈가 주어졌을때, 이를 기준으로 실제 데이터셋을 만들어낼 수 있는 방향(gradient)를 학습하여, 노이즈로부터 점진적으로 데이터 샘플을 만드는 process를 정의하게 된다.


# What is score?

Score에 대한 설명은 DDPM 게시글에서도 잠깐 다뤘었지만 다시 언급하자면 확률 밀도 함수에서의 기울기이다. 예컨데 원래 데이터의 연속 변수에 대한 확률 분포 $p(x)$가 있다면, 
$t$번째 step을 기준으로 하는 noised sample $x_t$이 그럴 듯한 샘플을 생성하게끔 움직여야하는 방향은 $p(x)$의 gradient에 해당될 것이고, 이를 energy based approach와 연관짓게 된다면 log likelihood의 gradient를 score로 정의할 수 있게 된다.

$$
\\nabla_x \\log p(x)
$$

바로 요 방법들을 사용한 생성 모델 중 혜성처럼 등장한 애들이 SMLD(Score matching with Langevin Dynamics)와 DDPM(Denoising diffusion probablistic Model)이다. 사실상 두 방법들 모두 일반화시키게 되면 같은 미분 방정식에 계수만 달라지는 꼴이 되므로 같은 문제를 푼다고 할 수 있다(이 부분에 대해서는 수식 증명에서 밝힌다).


# Score estimation = Solving SDE


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/233843040-036f2660-0202-4772-8e70-070b16379e88.png" width="900">
</div>


그렇다면 대체 디퓨젼이라는 모델의 메커니즘과 확률 미분 방정식을 어떻게 연관지어 생각할 수 있을까? 예컨데 위와 같은 그림을 생각해보자. DDPM이나 SMLD에서 데이터로부터 노이즈를 만드는 과정은 간단하게도 아주 작은 diffusion을 더해줌으로써 성립한다. 이를 discretized($x_0,~x_1,~\\cdots,~x_T$)해서 노이즈를 더했던 것과 다르게 만약 distribution이 점차 노이즈로 변해가는 과정이 일련의 연속 시간에 대해 정의될 수 있다면, 이는 **Ito SDE의 솔루션**으로 모델링될 수 있다.

사실 이 부분에 대한 내용은 뒤에서 DDPM 및 SMLD의 process와 연관지어 증명하면서 같이 이해해야하기 때문에 앞부분에서는 그냥 그렇다 생각하고 넘어갔다.

$$
dx = f(x,~t)dt + g(t)dw
$$

<U>확률 미분 방정식</U>도 미분 방정식이 가지는 여러 형태 중 하나이므로, 변화율($dx, dt, dw$)에 대한 관계로 문제가 정의된다. 미분 방정식의 해는 $x(t)$에 대해 해당 변화율이 그리는 <U>trajectory(궤도)</U>로 이해할 수 있다. 물론 확률 미분 방정식은 정해진 solution $x(t)$에 추가로 <U>stochastic term</U> $dw$가 붙는다는 점에서 차이가 있다(일단 얘는 무시하고 넘어가자). 


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/233843036-45330187-3a23-4343-95fe-36dbdaff1608.png" width="700">
</div>


$dw$는 시간에만 의존하는($x$의 실제 trajectory와는 무관) diffusion term이고 $dt$는 $x$의 **실제 trajectory를 따라가는 방향**을 정의하게 된다. 결국 solution $x(t)$는 특정 시점 $t$가 주어졌을 때, 해당 위치에서의 $x(t)$를 찾을 수 있는 함수의 꼴로 정의되고, 이를 위의 검은색 실선과 같이 <U>함수의 형태로 표현</U>할 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/233843042-3bb40e10-aae5-40d4-87c6-8c9274cf4ea0.png" width="700">
</div>


하지만 이를 반대로 생각하면 solution을 정의하는 trajectory는 방향에 따라 달라질 수 있기 때문에 이번에는 역으로 $x_T$에서 $x_0$을 따라가면서 $x(t)$를 예측하는 새로운 문제를 정의해볼 수 있다.

$$
dx = \\left(f(x,~t) -g^2(t)\\mathbf{\\nabla_x \\log p_t(x)}\\right) dt + g(t) d\\bar{w}
$$

식을 잘 보게되면 **diffusion term**인 $g(t)dw$는 forward SDE와 동일하고 **drift term**인 $dt$가 적용되는 궤도의 방향을 forward에서 reverse로 바꿔주는 과정에 score term이 들어가는 것을 볼 수 있다(수식에서 진하게 표시된 부분).

결론부터 말하자면 <U>forward SDE</U>와 <U>reverse SDE</U>는 출발점과 도착점이 다르지만 서로 같은 궤도를 그려야하며, 만약 우리가 forward SDE(궤도의 형태)를 미리 알고 있다면 reverse SDE에게 **솔루션을 제공**해줄 수 있다는 것이다. 즉 score를 예측하는 것이 곧 reverse SDE를 푸는 과정이 되고, 따라서 SDE solver 형태의 모든 샘플러가 생성에 관여할 수 있다는 일반화로 이어진다. 이 논문에서는 PC(Predictor-Corrector) sampler와 deterministic sampler 두 가지를 소개한다. 이름에서도 볼 수 있듯이PC sampler의 경우 score based model을 활용하여 보다 퀄리티 좋은 샘플링을 목적으로 하며, deterministic sampler는 랜덤한 부분들을 모두 제거했기 때문에 빠른 샘플링이나 latent manipulation을 목적으로 한다는 점에서 차이가 있다.


# Training and sampling with SMLD/DDPM

SMLD이 뭔지 제대로 짚지 않고 넘어왔는데 이게 뭐냐면 NCSN(Noise Conditional Score Network)이다. **NCSN**이랑 **DDPM** 모두 <U>score based generative model</U>이면서 각각 SDE problem을 방법론으로 서로 다르게 발전시킨 논문들이다. 이에 대해 살펴보기 위해 **SMLD**와 **DDPM**에서 어떤 식으로 score estimator를 학습했고, 어떻게 샘플링했는지 살펴보도록 하자.

### Denoising score matching with Langevin Dynamics(SMLD)

$x$에 대해 $\\sigma^2$의 variance를 가지는 perturbation kernel이 더해졌다고 생각해보자. 해당 kernel은 noise 분포인 $p_\\sigma(\\tilde{x} \\vert x) := \\mathcal{N}(\\tilde{x}; x, \\sigma^2I)$를 만족한다. <U>점차적으로 커지는 noise scale</U>인 $\\sigma_{\\min} = \\sigma_1 < \\sigma_2 < \\cdots < \\sigma_N = \\sigma_{\\max}$ 가 있다고 생각해보자. 이때 가장 작은 noise인 $\\sigma_{\\min}$에 대해서는 $p_{\\sigma_{\\min}}(x) \\approx p_\\text{data}(x)$를 만족할 수 있을 정도라고 보면 되고, 가장 큰 noise인 $\\sigma_{\\max}$에 대해서는 $p_{\\sigma_{\\max}}(x) \\approx \\mathcal{N}(x;0,\\sigma_{\\max}^2I)$를 만족할 수 있을 정도라고 생각하면 된다.

Denoising score matching의 개요는 직접 $\\nabla_x \\log p_\\text{data}(x)$를 예측할 수 없기 때문에(실제 분포는 intractable) 이에 근접하게끔 아주 작은 노이즈가 더해진 noised sample $\\tilde{x}$의 conditioned score를 예측하게 된다($\\nabla_{\\tilde{x}} \\log p_{\\sigma_i}(\\tilde{x} \\vert x)$). 가장 심한 노이즈부터 시작해서 원래의 데이터와 거의 유사한 수준의 데이터까지 <U>다양한 노이즈 단계</U>에 대해 score estimator $s_\\theta(x, \\sigma)$는 다음과 같은 objective를 가진다.

$$
\\theta^\\ast = \\underset{\\theta}{\\arg\\min} \\sum_{i=1}^N \\sigma_i^2 \\mathbb{E}_{p_\\text{data}(x)} \\mathbb{E}_{p_{\\sigma_i}(\\tilde{x} \\vert x)} \\left( \\parallel s_\\theta(\\tilde{x}, \\sigma_i) - \\nabla_{\\tilde{x}} \\log p_{\\sigma_i}(\\tilde{x} \\vert x) \\parallel_2^2 \\right)
$$

충분한 데이터셋과 모델 capacity에 대해 학습된 score-based model은 샘플링 과정에 필요한 score를 잘 예측하게끔 학습된다. 샘플링은 조금 복잡하지만 Annealed Langevin dynamics를 사용했기에 이중 loop라고 생각하면 된다. 각 노이즈 단계 $\\sigma_i$에 대해 $N$부터 $1$까지 점차 노이즈의 농도를 낮춰가며 각 noise kernel에 대해 총 $M$번의 Langevin MCMC를 진행한다. 즉 $N \\times M$ 만큼의 process라고 생각하면 된다. 예컨데 $\\sigma_i$ 만큼의 노이즈가 더해진 분포에 대해서,

$$
x_i^m = x_i^{m-1} + \\epsilon_i s_{\\theta^\\ast}(x_i^{m-1}, \\sigma_i) + \\sqrt{2\\epsilon_i}z_i^m
$$

gradient ascent의 learning rate라 볼 수 있는 step size $\\epsilon_i > 0$와 랜덤한 standard normal distribution vector $z_i^m \\sim \\mathcal{N}(0, I)$에 대해 위의 프로세스를 총 $M$번 진행한다. $M$번 만큼의 sampling이 끝나게 되면 농도가 낮춰진 노이즈 단계 $\\sigma_{i-1}$에 대해 다시 $M$번의 MCMC를 진행한다($x_{i-1}^0 = x_i^M$).

$$
x_{i-1}^m = x_{i-1}^{m-1} + \\epsilon_{i-1} s_{\\theta^\\ast}(x_{i-1}^{m-1}, \\sigma_{i-1}) + \\sqrt{2\\epsilon_{i-1}}z_{i-1}^m
$$

\`\`\`python
for sigma in sigma_N, ..., sigma_1:
	for step in 1, ..., M:
		noise = N(0, I) # Random noise
		x = x + epsilon * model(x, sigma) + sqrt(2*epsilon)*noise
\`\`\`

충분히 큰 $M$(sampling 수)과 아주 작은 $\\epsilon$(step size)를 통해 생성한 샘플인 $x_1^M$은 다시 원래 데이터의 분포를 따라가게 된다.

$$
p_{\\sigma_{\\min}}(x) \\approx p_{\\text{data}}(x)
$$

### Denoising diffusion probabilistic models(DDPM)

DDPM에서는 noise scale을 단계적으로 증가시킨 variance schedule(sequence)인 $\\beta_i$를 정의한다.

$$
0 < \\beta_1,~\\beta_2,~\\cdots,~\\beta_N < 1
$$

각 training data point(이미지) $x_0 \\sim p_\\text{data}(x)$에 대해, 앞서 정의한 variance schedule에 대해 perturbation kernel를 다음과 같이 Markov Chain에 따라 다음과 같이 정의하게 된다.

$$
p(x_i \\vert x_{i-1}) = \\mathcal{N}(x_i;~\\sqrt{1-\\beta_i}x_{i-1},~\\beta_i I)
$$

따라서 이를 모든 sequence에 대해 적용하면 다음과 같은 marginal을 획득할 수 있다.

$$
p(x_i \\vert x_0) = \\mathcal{N}(x_i; \\sqrt{\\bar{\\alpha}_i}x_0,~(1-\\bar{\\alpha}_i)I),~\\bar{\\alpha}_i := \\prod_{j=1}^i (1-\\beta_j)
$$

또한 해당 표현법은 앞서 SMLD의 예시와 같이perturbed data distribution으로 표현할 수 있다 ($p_{\\bar{\\alpha}_i}(\\bar{x} \\vert x)$가 위에서 획득한 marginal을 의미함).

$$
p_{\\bar{\\alpha}_t}(\\tilde{x}) = \\int p_\\text{data}(x)p_{\\bar{\\alpha}_i}(\\tilde{x} \\vert x) dx
$$

또한 DDPM의 경우에는 노이즈가 더해지는 각 process에서 변수의 variance가 $1$로 고정되기 때문에 $x_N \\sim \\mathcal{N}(0, I)$를 만족하게 된다. DDPM에서는 reverse process를 다른 형태로 표현했지만 SMLD와 같이 score matching 형태로 reverse process의 variational Markov chain을 표현하면 다음과 같다.

$$
p_\\theta (x_{i-1} \\vert x_i) = \\mathcal{N}(x_{i-1};~\\frac{1}{\\sqrt{1-\\beta_i}}(x_i + \\beta_i s_\\theta(x_i,~i)), \\beta_iI)
$$

그리고 이에 따라 evidence lower bound도 다른 weighted summation을 가지게 된다. 아래의 식을 보게되면 SMLD에서 전개했던 식과 상당히 닮아있는 것을 볼 수 있는데, weighted summation의 계수로 작용하는 $1-\\bar{\\alpha}_i$가 $i$번째 perturbation kernel인 $p_{\\bar{\\alpha}_i}(\\tilde{x} \\vert x)$의 variace인 것을 알 수 있다. 앞서 SMLD에서도 계수가 perturbation kernel의 variance인 $\\sigma_i^2$였던 것을 생각하면 두 식의 공통점을 찾을 수 있다.

보다 엄밀하게 말하자면 noised score estimation인 $\\nabla_x \\log p(\\tilde{x} \\vert x)$에 대해 $1/\\mathbb{E}\\left[  \\parallel \\nabla_x \\log p(\\tilde{x} \\vert x) \\parallel_2^2 \\right]$에 비례하게 된다.

$$
\\theta^\\ast = \\underset{\\theta}{\\arg\\min} \\sum_{i=1}^N (1-\\bar{\\alpha}_i) \\mathbb{E}_{p_\\text{data}(x)} \\mathbb{E}_{p_{\\bar{\\alpha}_i}(\\tilde{x} \\vert x)} \\left( \\parallel s_\\theta(\\tilde{x}, i) - \\nabla_{\\tilde{x}} \\log p_{\\bar{\\alpha}_i}(\\tilde{x} \\vert x) \\parallel_2^2 \\right)
$$

따라서 해당 loss term을 만족하는 $\\theta^\\ast$에 대해서 역으로 Markov chain process를 통해 샘플링을 진행한다.

$$
x_{i-1} = \\frac{1}{\\sqrt{1-\\beta_i}} (x_i + \\beta_i s_{\\theta^\\ast}(x_i, i)) + \\sqrt{\\beta_i}z_i,~i=N,~N-1,~\\cdots,~1
$$

이러한 샘플링 방법을 저자들은 **ancestral sampling(**조금씩 거슬러 올라오는 샘플링 과정을 reverse process인 $p_\\theta$에 대해 명명**)**이라고 부른다. 사실상 DDPM의 loss 식은 위와 구조를 다르게 하지만(아래 참고) 실제로 DDPM이 score estimation model을 학습시키는 과정이라는 것을 보여주기 위해 위와 같이 수식화함.

$$
\\mathbb{E}_{x_0,\\epsilon} \\left( \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t(1-\\bar{\\alpha}_t)} \\parallel \\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t) \\parallel^2\\right)
$$

결국 말하고자 하는 것은 기존의 score-based generation 접근법이던 SMLD나 DDPM 모두 inference(forwarding)이 SDE로 표현되었던 것처럼, 역과정도 마찬가지로 score matching을 통해 SDE를 풀어가는 것으로 표현될 수 있다는 것이다.


# 디퓨전 모델링의 확장

기존 방식들을 보면 data를 여러 noise scale로 구성된 perturbation kernel를 사용한 연구들이라고 요약할 수 있다. 이 논문에서는 score based generation 방법을 미리 정해진 noise scale(discrete value)가 아닌 모든(continuous value) noise scale에 일반화시키고자 하며,

이는 곧 SDE를 **연속 변수 방정식**으로 풀고자 하는 목표로 작용한다(아래 그림 참고).


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/233843045-c8923c29-280a-4101-b8b0-01316dae6958.png" width="900">
</div>


앞서 디퓨전 프로세스의 forward와 reverse 모두를 SDE로 표현할 수 있다고 했는데, 이를 다시 언급하면 다음과 같다.

### 미분 방정식을 통해 데이터셋을 망가뜨리기(노이즈로 만들기)

Diffusion의 모델링의 가장 기본은 우리가 가지고 있는 임의의 i.i.d. 데이터셋인 $x(0) \\sim p_0$에 continous/discrete time 변수 $t \\in [0, T]$로 인덱싱되는 diffusion process를 기반으로 가우시안 노이즈에 가까운 $x(T) \\sim p_T$를 만드는 것이다. 데이터 분포 $p_0$로부터prior 분포 $p_T$를 만드는 diffusion process는 앞서 소개했던 것과 같이 확률 미분 방정식의 solution으로 모델링할 수 있다.

$$
dx = f(x,~t)dt + g(t) dw
$$

$w$는 Brownian motion이라고도 불리는 standard Wiener process를 의미한다. $f(x, t)$는 $x, t$에 대한 drift coefficient이며 $g(t)$는 diffusion coefficient이다. SDE는 $f(x, t)$ 및 $g(t)$가 state와 time에 대해 모두 Lipshitz를 만족한다는 가정 하에(미분 값이 bounded되어 있다고 할 때) 한정된 변화율 사이에서 unique strong solution을 가질 수 있다([참고 링크](https://link.springer.com/book/10.1007/978-3-642-14394-6)).

$$
\\begin{aligned}
&\\vert \\nabla_{x,~t} f(x,~t) \\vert \\le \\epsilon_1 \\newline
&\\vert \\nabla_{t} g(t) \\vert \\le \\epsilon_2
\\end{aligned}
$$

### 미분방정식을 역으로 풀면서 샘플링하기

Prior를 알고 있다면(사전에 정의를 했다면) prior sample인 $x(T) \\sim p_T$을 샘플링할 수 있다. 예컨데 우리가 prior를 다변수 가우시안으로 정의했다면 가우시안 샘플링을 통해 $x_T$를 구할 수 있다. 그리고 증명을 통해 diffusion SDE의 역과정 또한 SDE임을 다음과 같이 수식화할 수 있다([참고 링크](https://www.sciencedirect.com/science/article/pii/0304414982900515)).

$$
dx = \\left(f(x,~t) -g^2(t)\\mathbf{\\nabla_x \\log p_t(x)}\\right) dt + g(t) d\\bar{w}
$$

$\\bar{w}$는 표현만 다를 뿐 앞서 forward process에서 봤던 Wiener process인 $w$와 동일하다고 보면 된다. 앞서 noise를 점차 만들어가는 과정은 아주 작은 단위로 $t$가 증가하면서 미분 방정식의 궤도를 그리지만, reverse 식에서는 아주 작은 단위의 $t$가 감소하면서 미분 방정식의 궤도를 그려나간다. 만약 위의 식에서 marginal distribution $p_t(x)$에서 score를 알 수 있다면 임의의 노이즈 샘플 $x_T$을 통해 $x_0$를 구할 수 있게 된다.

### Reverse SDE를 풀기 위해 score 예측하기

결국 샘플링을 위해서는 score를 예측해야한다는 것인데, 실질적으로 원래 데이터셋 분포의 marginal에 대한 score를 구할 수 없기 때문에 score-matching에 기반한 time-dependendent model인 $s_\\theta(x, t)$를 앞서 본 SMLD와 DDPM의 loss term과 같이 최적화할 수 있다. 다만 앞서 본 SMLD와 DDPM은 연속되지 않은 perturbation sequence $N$에 대해 계산을 했다면, 이를 연속 변수 $t$에 대해 일반화하게 되면 다음과 같다.

$$
\\theta^\\ast = \\underset{\\theta}{\\arg\\min} \\mathbb{E}_t\\left(\\lambda(t) \\mathbb{E}_{x(0)} \\mathbb{E}_{x(t) \\vert x(0)} \\left( \\parallel s_\\theta(x(t), t) - \\nabla_{x(t)} \\log p_{0t}(x(t) \\vert x(0)) \\parallel_2^2 \\right)\\right)
$$

앞서 언급했던 것과 같이 $\\lambda(t) \\propto  1/\\mathbb{E} \\left[ \\parallel \\nabla_{x(t)} \\log p_{0t}(x(t) \\vert x(0)) \\parallel_2^2 \\right]$인 값으로 고르게 된다. 물론 굳이 이 식과 같이 score matching을 하는 방식이 denoising score matching이 아니더라도 다른 방법들 중 하나인 **sliced score matching**이나 **finite-difference score matching**이어도 상관이 없다.

### VE 및 VP SDE의 확장

SMLD와 DDPM의 noise perturbation 방식은  SDE를 discrete하게 바꾼 것이라고 볼 수 있다. $N$개의 noise scale이 있을 때, SMLD의 Markov chain에 기반한 forward process는 다음과 같다.

$$
x_i = x_{i-1} + \\sqrt{\\sigma_i^2 - \\sigma_{i-1}^2} z_{i-1},~i = 1,~\\cdots,~N
$$

해당 식에서 $N$이 극한으로 증가한다면($\\infty$) noise scale은 연속 시간에 대한 함수 $\\sigma(t)$로 표현할 수 있으며 마찬가지로 standard gaussian variable $z_{i-1}$ 또한 $z(t)$로 나타낼 수 있다. 이때의 Markov chain을 continuous stochastic process $x(t)$로 바꾼다면,

$$
x(t+\\Delta) = x(t) + \\sqrt{\\sigma(t+\\Delta)^2 - \\sigma(t)^2}z(t)
$$

$\\Delta$가 매우 작다는 가정 하에 <U>테일러 1차 근사</U>를 통해 미분 방정식으로 근사할 수 있다. 일반적인 오일러 메소드라고 보면 된다.

$$
dx = \\sqrt{\\frac{d\\sigma^2(t)}{dt}} dw
$$

마찬가지로 DDPM의 경우에는

$$
x_i = \\sqrt{1-\\beta_i} x_{i-1} + \\sqrt{\\beta_i}z_{i-1},~i=1,~\\cdots,~N
$$

위와 같은 forward process를 따르기 때문에 $N$이 극한으로 증가한다면,

$$
x(t+\\Delta) = \\sqrt{1-\\beta(t+\\Delta) \\cdot \\Delta}x(t) + \\sqrt{\\beta(t+\\Delta)\\cdot \\Delta}z(t)
$$

위와 같다. 이때 $\\beta$는 원래 scheduling된 각 Markov process에서의 노이즈이기 때문에 미소 단위의 variance 변화에 대해 $\\Delta$가 추가로 곱해지는 형태가 된다. 역시 오일러 메소드를 통해 미분 방정식으로 근사하면

$$
dx = -\\frac{1}{2} \\beta(t) x dt + \\sqrt{\\beta(t)} dw
$$

위와 같이 정리된다. 논문에서는 SMLD의 미분 방정식은 variance가 계속 증가하는 VE(Variance Exploding) SDE이고 DDPM의 미분방정식은 variance가 유지되는 VP(Variance Preserving) SDE라고 명명하였다.

이때, drift coefficient와 drift coefficient에 대해 affine 형태를 가지기 때문에 variance 미분 방정식으로 발전시킬 수 있다(다음 식을 사용함). 

$$
\\frac{d\\Sigma_{\\text{VP}}(t)}{dt} = \\beta (t) (I - \\Sigma_{\\text{VP}}(t))
$$

참고로 위와 같은 수식 전개가 가능한 것은 gaussian 분포를 가정하고 있고 non-linear case가 아니기 때문이다. 해당 ODE를 풀게 되면 $x(t$ )의 covariance function을 구할 수 있다.

$$
\\Sigma_\\text{VP} (t) = I+\\exp\\left(\\int_0^t -\\beta(s) ds\\right)(\\Sigma_\\text{VP}(0) - I)
$$

이 식은 모든 $t$에 대한 variance $\\Sigma_\\text{VP}$가 $\\Sigma_{\\text{VP}}(0)$을 기준으로 bounded 된다는 사실이다. 예컨데 DDPM에서와 같이 $\\Sigma_\\text{VP}(0) = I$라면 모든 $t$에서 $I$로 유지된다. 그리고 저자가 밝힌 새로운 sub-VP SDE는 다음과 같다.

$$
dx = -\\frac{1}{2} \\beta(t) x dt + \\sqrt{\\beta(t) (1 - \\exp\\left(-2\\int_0^t \\beta(s) ds\\right))}dw
$$

그리고 같은 방식으로 Covariance에 대한 ODE solution을 구하면 새로운 미분 방정식에 대한 covariance를 구할 수 있다.

$$
\\Sigma_\\text{sub-VP}(t) = I+\\exp\\left(-2\\int_0^t \\beta(s) ds \\right)I + \\exp\\left(-\\int_0^t \\beta(s) ds \\right) (\\Sigma_\\text{sub-VP}(0)-2I)
$$

sub-VP SDE는 사실상 VP SDE를 upper bound로 가진다고 생각하면 좋다. Diffusion proces는 $dx = f(x,~t)dt + g(t)dw$ 형태의 모든 식이 가능하기 때문에 처음부터 discrete diffusion으로 접근한 것이 아닌 continuous function의 관점으로 접근했기 때문에 해당 SDE를 제시할 수 있었다고 판단된다. sub-VP의 경우 특정 실험에서 좋은 결과를 보였다고 한다(likelihood).

### 논문에서 사용한 Stochastic Differential Equations

지금까지 총 3개의 서로 다른 SDE를 소개했는데, 각각의 perturbation kernel을 가우시안 형태의 marginal distribution으로 나타내면 다음과 같다.

$$
p_{0t}(x(t) \\vert x(0)) = \\begin{cases}
\\mathcal{N}(x(t); x(0), (\\sigma^2(t)-\\sigma^2(0))I),& \\text{(VE SDE)} \\newline
\\mathcal{N}(x(t);x(0)e^{-\\frac{1}{2}\\int_0^t \\beta(s)ds}, I-Ie^{-\\int_0^t \\beta(s) ds}),&\\text{(VP SDE)} \\newline
\\mathcal{N}(x(t);x(0)e^{-\\frac{1}{2}\\int_0^t \\beta(s)ds}, \\left(1-e^{-\\int_0^t \\beta(s) ds}\\right)^2I),&\\text{(sub-VP SDE)} 
\\end{cases}
$$

sub-VP SDE의 전개 방식이 조금 복잡해서 어렵게 느껴지는데, 단순히 원래 DDPM에서 노이즈를 만들었던 방식이

$$
p(x_i \\vert x_0) = \\mathcal{N}(x_i; \\sqrt{\\bar{\\alpha}_i}x_0,~(1-\\bar{\\alpha}_i)I),~\\bar{\\alpha}_i := \\prod_{j=1}^i (1-\\beta_j)
$$

다음과 같았다면, variance 부분을 quadratic하게 만들어준 것과 같다.

$$
p(x_i \\vert x_0) = \\mathcal{N}(x_i; \\sqrt{\\bar{\\alpha}_i}x_0,~(1-\\bar{\\alpha}_i)^2I),~\\bar{\\alpha}_i := \\prod_{j=1}^i (1-\\beta_j)
$$


# Solving the reverse SDE

미분 방정식에서 score 예측 모델을 통해각 time point에 대해 $s_\\theta(\\cdot)$만 알아낼 수 있다면 어쩌구저쩌구 이론에 의해 reverse-SDE를 구성할 수 있다는 것은 앞에서 입아프게 짚고 넘어왔다. Reverse SDE를 numerical하게 풀어내어 $x_0$을 찾는 과정이 결국 샘플링하는 것과 같다.

SDE를 numerical하게 풀어내는 것은 일종의 solution이 되는 함수의 형태(trajectory)를 예측하는 것과 같다. SDE를 풀어내는 방법으로는 Euler-Maruyama나 stochastic Runge-Kutta 방법 등등(사실 뭔지는 잘 모른다) 다양하게 있고, 암튼 말하고자 하는 것은 score predictor만 있다면 어느 형태의 SDE solver든 사용해서 sample을 만들어낼 수 있게 된다.

DDPM과 같은 ancestral sampling 방법은 특이 케이스인데, SDE가 조금만 달라지더라도 적용할 수 없다는 문제가 있다(일반화가 어렵다). 따라서 이를 좀 완화하기 위해 reverse diffusion sampler를 제안했고(아래 식과 같음), 해당 방법이 단순히 SMLD/DDPM 모델의 ancestral sampling보다 약간 더 좋은 성능을 보였다고 한다.

$$
x_i = x_{i+1} - f_{i+1}(x_{i+1}) + G_{i+1}G_{i+1}^\\top s_{\\theta^\\ast}(x_{i+1}, i+1) + G_{i+1}z_{i+1}
$$

아무래도 보다 0에 가까운 미소 단위의 $\\Delta$(time 변화)에 대해 정의할수록 ancestral sampling과 <U>실제 SDE solution과의 차이가 줄어들게</U> 되는데, 기존 ancestral sampler는 SDE의 원형을 고려하지 않기 때문인 듯하다($p_\\theta(x_i \\vert x_{i+1})$에 의존).


# Predictor-corrector samplers

일반적으로 SDE를 풀어내는 것은 score prediction만 진행하게 되는 과정인데, 여기에 추가로 MCMC approach를 더해주게 된다(Langevin dynamics). 방법을 간단하게 numerical SDE가 다음 time step의 solution을 예측하면, 해당 point에서 score estimation을 토대로 correction sampling(조정 작업)을 진행하는 것이다. **Numerical SDE solver**가 **predictor** 역할을 수행하고 **score model**이 **corrector** 역할을 수행한다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/233843046-232508c8-1d46-4906-b9a3-61b85b1c324f.png" width="800">
</div>


각각 알고리즘(Predictor만 사용/Corrector만 사용/둘 다 사용)에 대해 SMLD, DDPM 모델을 적용해봤을 때, PC를 같이 사용하는 것이 predictor만 사용하는 것이나 corrector를 사용하는 것보다 샘플링 효율성이 상당히 증대되었다고 한다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/233843048-dfae2905-b393-467d-9ffc-fb86e3bbc6ec.png" width="800">
</div>



# Probability flow and Neural ODE

Score-based model을 활용하여 SDE를 numerical하게 풀어내는 것에 사용하게 된다. 그런데 사실 모든 diffusion process는 marginal likelihood $p(x_{0:T})$를 동일하게 가지는 ODE를 찾을 수 있다. 예컨데 원래의 diffusion SDE가 다음과 같다면,

$$
dx = f(x, t)dt + g(t)dw 
$$

이 SDE와 동일한 marginal likelihood를 가지는 ODE는

$$
dx = \\left( f(x, t) - \\frac{1}{2}g(t)^2 \\nabla_x \\log p_\\theta(x) \\right) dt
$$

아래와 같이 나타내어진다. 해당 부분에 대한 증명은 너무 복잡해서 아직 이해는 못했다… 아무튼$\\tilde{f}(x, t) = f(x, t) - \\frac{1}{2}g(t)^2 \\nabla_x \\log p_\\theta(x)$에 대해 Wiener process term인 $dw$를 없앤 ODE

$$
dx = \\tilde{f}(x, t)dt
$$

는 원래의 SDE와 동일한 marginal likelihood를 가진다. 즉, sampling을 했을때 가지는 결과가 같다는 것이다. 이럴 경우 DDPM에서와 같이 강제로 discrete한 likelihood를 어림짐작하는 것이 아니라(아래 참고), 

$$
p_\\theta(x_0 \\vert x_1) = \\prod_{i=1}^D \\int_{\\delta_{-}(x_0^i)}^{\\delta_+(x_0^i)} \\mathcal{N}(x; \\mu_\\theta^i (x_1, 1), \\sigma_1^2) dx
$$

$$
\\delta_+(x) = \\begin{cases}
\\infty,&\\text{if }x=1 \\newline
x+\\frac{1}{255},&\\text{if }x<1
\\end{cases},~~\\delta_-(x) = \\begin{cases}
-\\infty,&\\text{if }x=-1 \\newline
x-\\frac{1}{255},&\\text{if }x>-1
\\end{cases}
$$

실제 likelihood를 계산할 수 있게 된다.

$$
\\log p_0(x(t)) = \\log p_T(x(T)) + \\int_0^T \\nabla \\cdot \\tilde{f}_\\theta(x(t),t)dt
$$

그리고 ODE의 경우 $dw$(랜덤한 부분)을 아예 무시할 수 있기 때문에 특정 datapoint와 $x(0)$와 경로 상의 $x(T)$가 $1$대 $1$ mapping된다는 장점이 있다. 이는 마치 flow based model이나 neural ODE와 같은 invertible model로 생각할 수 있는데, 이럴 경우 latent representation을 사용하여 image editing이나 interpolation 등등이 손쉽게 가능해진다. 마찬가지로 각 이미지가 유일한 latent로 mapping되기 때문에 forward SDE를 일종의 encoder로도 생각할 수 있다(실제로 forwarding 과정은 파라미터 학습과 전혀 무관하다).

마지막으로 neural ODE는 stochastic한 부분이 사라지므로 빠른 샘플링이 가능하다.


# Conditional sampling

Continous SDE를 해결하는 과정에서 또다른 장점 중 하나는 샘플을 생성하는 것 뿐만 아니라 조건부 확률인 $p_0(x(0) \\vert y)$에 대한 샘플을 생성할 수 있다는 것이다. 이때,  각 time step의 class 확률 $p_t(y \\vert x(t))$가 있어야한다.

$$
dx = \\left( f(x, t) - g(t)^2(\\nabla_x \\log p_t (x) + \\nabla_x \\log p_t(y \\vert x)) \\right)dt + g(t)d\\bar{w}
$$

클래스 conditional probability는 class guidance 논문인 Diffusion beats GAN과 동일한 방식으로 noised sample에 대한 supervised learning을 통해 인코더를 학습시킨다. 나머지 방법은 Appendix에 있는데 딱히 디테일하게 볼 필요는 없을 것 같아서 패스했다. 


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/233843049-83e1b687-74f2-4af8-8dbe-63528f8871db.png" width="">
</div>
`,FO=`---
title: "Consistency models 논문 리뷰"
category: "ai papers"
publishedAt: "2023-04-26"
thumbnail: "https://user-images.githubusercontent.com/79881119/234603447-6a5db911-d522-479e-96c9-321d0e009ac2.png"
---


# 들어가며…

**Yang Song**씨의 논문은 항상 읽다보면 **피가 말린다**. 안그래도 수식이 방대한 딥러닝 세상에서 더욱 수식을 멋지게 활용(?)하여 **Appendix를 화려하게 채워주기 때문이다**. 바로 이전에 리뷰했던 논문인 score based diffusion의 기초 논문들 중 하나인 [‘Score-based generative modeling through stochastic differential equations’](https://6unoyunr.github.io/blog/scoresde) 또한 Appendix가 굉장했던 기억이 있다. 아무튼 diffusion을 공부하는 사람이라면 대체 어디서부터 읽어야할지 막막하기도 하고, 가장 베이스라인이라고 여겨질 수 있는 DDPM이나 NCSN 등등을 읽다보면 대체 무슨 근본으로 이러한 수식을 전개하는거지 싶은 순간들이 온다. 본인은 diffusion을 공부하기 시작한 이후로 수없이 많은 기초 논문들, 블로그 및 유튜브와 Bishop의 pattern recognition 서적의 이런저런 수식들을 참고했었다. 다만 많은 시간동안 느꼈던 점은 제대로 이해하지 못하고 대충 넘어간 애들은 결국 내 것이 되지 못한 채 이후 논문들을 이해하는 과정에서 발목을 잡는다는 사실이었다. 또한 공부하면서 느꼈던 점은 생성 모델로서 무언갈 구현했다기 보다는 수학적 모델을 통해 생성 모델을 도출한다는 흐름이 논문의 수식 이해에 보다 도움이 되었다는 것이다. 

따라서 무작정 수식 전개를 이해하기보다는 근본적으로 diffusion이 대체 왜 생성 모델로 사용될 수 있는지, 그리고 그 <U>한계점과 해결책이 무엇인지</U> 이해하는 것이 가장 중요하다고 생각된다.


# Diffusion model의 문제점

여전히 **diffusion model**은 생성 속도가 느리다는 점을 극복하지 못했다. 근래에 이미지 쪽에서의 image manipulation이나 텍스트 쪽에서의 large language model based chatbot이 보다 다양한 사람들에게 서비스로 보급되기 시작한 이후, 사용자에게는 쾌적한 서비스의 공급/편의성이라는 측면과 사업자에게는 적은 리소스/비용이라는 이해관계가 맞붙기 시작했다. 결국 AI로 하여금 고퀄리티의 생산물을 만들어내는 것은 좋은데, 그게 오래 걸리면 무슨 소용일까. 마치 <U>배차간격이 긴 광역버스를 기다리는 퇴근길</U>과 같다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603450-76d2b35e-993c-42f7-aae3-de2ba2acc664.jpg" width="800">
</div>


GAN과 같은 implicit 생성 모델에 비해 가지는 모달리티의 안정성은 좋은데, 그걸 보장하기 위해서는 **trade-off**로 <U>시간과 연산량</U>을 지불할 수 밖에 없다는 것이다.


# 빠른 샘플링의 고안


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603561-58d2adc4-f683-4124-9bc2-1dda75a6f580.png" width="400">
    <img src="https://user-images.githubusercontent.com/79881119/234603471-f7852e02-c77f-4bfe-b43b-e43a19667484.png" width="700">
</div>


빠르게 생성한다는 것은 직관적으로 표현될 수 있다. 기존의 디퓨전이 **장인 정신**으로 한땀한땀(시간축 $t$에 따라서) 노이즈를 제거해가는 방식 대신에, **하이패스를 달아버려서** 한방에 노이즈로부터 샘플을 생성하는 것이다. 즉 기존의 GAN이 가능했던 빠른 샘플링인 $G(z)$를 디퓨전에 대해서도 가능하게 하고 싶다는 것이다. 이러한 생각에서 나온 방법들 중 일부는 다음과 같다.

- DDIM : Markovian process와 동일한 marginal likelihood를 가지는 Non-Markovian forward process를 정의하고, 이를 통해 샘플링 시퀀스의 time step을 간소화
- Diffusion model distillation : 샘플링 성능이 좋은 디퓨전 모델 ex) DDPM 을 사용하여 단일 step으로 좋은 샘플링이 가능하게끔 probability flow ODE를 학습

하지만 여전히 DDIM을 포함하여 probability flow ODE의 경우에도 샘플링의 속도를 빠르게 하면 할수록 발생하는 샘플 퀄리티의 하락을 무시할 수 없다. 샘플링 단계를 최소화하면서 샘플링 성능의 저하를 막는 것이 주요 포인트인데, 이게 기존 방식으로 해결하기에는 벅차다.  그나마 distillation 방법이 probability flow ODE에 대해 좋은 디퓨전 모델의 성능을 transfer하기 좋은 방법이긴 하지만, 결국 디퓨전 모델의 생성에 의존해야한다는 점 때문에 <U>학습 속도가 현저히 느려지게 된다</U>는 **bottleneck**에서 벗어날 수 없다.


# Related works

저자가 주장하는 consistency model의 개요는 이전의 Yang Song이 풀어냈던 diffusion 방식과 달라지지는 않았다. 이전 논문에서의 내용을 인용하면, 모든 diffusion process는 marginal likelihood $p(x_{0:T})$를 동일하게 가지는 ODE를 찾을 수 있다. 예컨데 원래의 diffusion SDE가 다음과 같다면,

$$
dx_t = \\mu(x_t, t)dt + \\sigma(t)dw_t 
$$

이 SDE와 동일한 marginal likelihood를 가지는 ODE는

$$
dx_t = \\left( \\mu(x_t, t) - \\frac{1}{2}\\sigma(t)^2 \\nabla_x \\log p_t(x_t) \\right) dt
$$

위와 같이 표현할 수 있다. 해당 내용에 대한 증명은 **Appendix**로 Yang Song의 논문(SDE diffusion 논문)에 첨부되어 있다.

ODE로 변형했을 때 SDE에 대해 가지는 장점은 <U>stochastic한 diffusion coefficient를 가지지 않기 때문에</U>($dw$), probability flow ODE를 기준으로 starting point $x_0$를 잡는다면 미분 방정식의 solution이 그리는 trajectory를 따라가는 $x_T$까지의 모든 점 $x_t$에 대해 하나의 선으로 이을 수 있게 된다(아래 그림 참고). 확률 미분 방정식은 drift term이 방향만 정해줄 뿐, 실질적으로 뻗어나가는 구조는 랜덤한 요소가 좌우하기 때문에 starting point와 ending point만 알 뿐, 그 내부에서 각각의 $x_t$가 서로 교차하고 얽히는 과정을 알 수 없기 때문에 $1$대 $1$ mapping이 불가능하다는 단점이 있다. 하지만 ODE의 경우에는 trajectory를 그리는 요소에 시간축이라는 단일 변수가 관여할 수 있게 된다. 만약 특정 시점의 데이터인 $x_t$에 대해 score를 예측할 수 있는 모델인 $s_\\phi(x, t) \\approx \\nabla \\log p_t(x)$가 있다면, 위의 방정식은 perturbation kernel $p_t(x) = p_\\text{data}(x) \\otimes \\mathcal{N}(0, t^2I)$에 대해 다음과 같은 form으로 나타낼 수 있다.

$$
\\frac{dx_t}{dt} = -ts_\\phi (x_t, t)
$$

참고로 이 configuration은 ‘Elucidating the design space of diffusion-based generative models’이라는 논문에 따른 것으로, **diffusion 확률 미분 방정식을 정의**할 때 drift term과 diffusion term을 디자인하게 되는 방식이 각 논문마다 다른 것을 알 수 있다. 위의 공식은 $\\mu = 0, \\sigma = \\sqrt{2t}$를 따르는 미분 방정식의 solution이다.

고로 만약 $x_T\\sim \\mathcal{N}(0,T^2I)$를 정의하고 이에 따른 probability flow ODE $\\frac{dx_t}{dt} = -ts_\\phi (x_t, t)$를 풀어낸다면, $x_0$과 $x_T$를 잇는 하나의 trajectory를 구할 수 있게 된다. 미분 방정식을 푸는 방식은 Euler나 Heun solver와 같은 numerical 방법을 통해 함수의 형상을 예측하는 형태가 된다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603477-6678e3ad-a648-435c-89c2-bb7961322e32.png" width="">
</div>


이는 우리가 실제로 Analytic하게 풀어낼 수 없는(정해진 solution이 없는) 미분 방정식을 마주했을때, 아주 작은 변수의 변화에 대한 함숫값의 변화를 예측하는 과정을 의미한다. 그러나 그림을 보면 알 수 있듯이 실제로 numerical하게 풀어낸 미분 방정식의 해는 실제 solution과 오차가 클 수 밖에 없으며, 이는 시간 축이 길어지면 길어질수록, 샘플링 간격이 늘어나면 늘어날수록 variance가 높아지게 된다.

$$
\\hat{x}_t,~t \\in (0,~T)
$$

따라서 논문에서는 numerical instability를 보완할 목적으로 $t = \\epsilon(0.002)$의 위치에서의 solution을 실제 데이터 샘플인 $x_0$에 근사한 값으로 간주했으며, time step의 총 수는 $T = 80$을 사용하였다.

Diffusion model은 결국 느린 sampling 속도가 가장 큰 문제점이라고 하였다. 물론 마찬가지로 ODE solver를 sampling에 사용하는 과정에서도 앞서 본 식과 같이 score model의 score 예측에 해당되는 $s_\\phi(x, t)$가 발목을 잡게된다. 결국 numerical ODE solver 또한 퀄리티를 포기함으로써 속도를 증가시키는 방법이나 distillation을 사용할 수 밖에 없다.

하지만 이러한 노력에도 불구하고 기존 ODE solver는 꽤 좋은 퀄리티의 데이터를 생성하기 위해서는 단일 step으로는 불가능하다는 문제점이 발생하였다. Distillation을 하는 방식은 보통 DDPM과 같은 디퓨전의 prior에 의존하게 되는데, 결국 DDPM에서 각 time step에 대한 노이즈 데이터를 샘플링 해야하기 때문에 사용할 때 <U>연산량이 부담된다는 것을 해결할 수 없다</U>는 문제가 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603480-db2f2eb3-5e5e-4532-8aa1-70c66e000835.png" width="500">
</div>


바로 이러한 문제를 해결하고자 했던 논문 중 하나가 점진적으로 distillation을 수행하는 time step 수를 줄이는 progressive distillation이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603487-edb4fe29-d5cb-402b-b7ef-0866993120e1.png" width="500">
</div>


처음부터 단일 trajectory을 모두 학습하려면 score 예측에 필요한 샘플이 그만큼 늘어나게 된다. 하지만 만약 여러 trajectory에 대해 부분적으로 학습된 ODE score estimator가 서로 연결되게끔 distillation 하면서 그 수를 줄여나가면, **굳이 처음부터 엄청난 수의 샘플을 사용하지 않고도** 충분히 좋은 성능을 보일 수 있다는 것이 그 방법이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603492-70a828eb-da05-4c7f-bf2e-cfb1695ea092.png" width="600">
</div>


이 논문에서는 위와 같이 progressive distillation을 사용하지는 않지만 consistency distillation을 사용하여 ODE solver에 대한 예측과 prior에 대한 예측을 일치시키는 작업을 진행하게 된다. Consistency 논문에서 주된 비교 타겟으로 삼은 논문이 바로 [progressive distillation 논문](https://arxiv.org/pdf/2202.00512.pdf)이다.


# Consistency models

**Consistency model**을 언급할 적에 ‘새로운 생성 모델’이라고 언급하면서 논문이 시작된다. Consistency model은 diffusion process의 SDE를 기반으로 하는 probability flow ODE를 수학적 접근 프레임으로 삼는데, 이때 ODE를 풀어가는 방식에 만약 굳이 사전 학습된 DDPM에 의한 distillation이 불필요하게 된다면 이는 곧 scratch 부터 학습될 수 있는 새로운 생성 모델의 기본이 되는 것이다. 얼핏 보면 normalizing flow랑 비슷해보이기도 하지만 근본이 <U>디퓨전 확률 미분 방정식으로부터 출발</U>했기 때문에 확실히 다르다고 말할 수 있을 것 같다.

### Consistency model의 정의

 Probability flow ODE인

$$
dx_t = \\left( \\mu(x_t, t) - \\frac{1}{2}\\sigma(t)^2 \\nabla_x \\log p_t(x_t) \\right) dt
$$

의 해가 되는 trajectory(궤도)를 $\\{x_t \\}_{t \\in [\\epsilon, T]}$ 라고 해보자. Consistency function은 함수 궤도 상에 있는 모든 점들을 $x_\\epsilon$ 으로 한번에 보내는 함수를 의미한다.

$$
f : (x_t, t) \\rightarrow x_\\epsilon
$$


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603497-b5c391e5-7fc7-4c33-9e6b-63ad8529232d.png" width="550">
</div>


바로 위의 그림과 같이 표현할 수 있다. 초록색이 시간축 상에서 $\\epsilon$부터 $T$까지 뻗어있는 PF ODE의 솔루션 궤도이며, 모든 시간축 상의 점들을 **태초마을로 귀환**시켜버리는 것이 논문에서 학습시키고자 하는 consistency model의 주된 목적이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603447-6a5db911-d522-479e-96c9-321d0e009ac2.png" width="500">
</div>


그렇다는 의미는 다음과 같이 궤도 상의 모든 점은 함수 결과에 대해 consistency를 가진다고 볼 수 있다.

$$
f(x, t) = f(x_{t^\\prime},t^\\prime),~\\forall t,~t^\\prime \\in \\{\\tau \\vert \\epsilon \\le \\tau \\le T\\}
$$

만약 <U>time argument가 고정</U>되어있다면(시간축 상에 발자국이 남아있다면), 역과정에 대해서도 invertible function이 된다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603500-a362aee4-fbfb-4943-9ff8-3cf4e9939f5b.png" width="600">
</div>


따라서 consistency model은 ODE를 통해 궤도를 예측하면서 남은 **발자국**의 출발점을 똑같은 곳인 $x_\\epsilon$으로 보내는 과정이 된다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603457-969dd89b-f352-45d3-952e-bbc823803b5e.gif" width="500">
</div>


### Parameterization

딥러닝에서 가장 중요한 것은 학습이 가능하게끔 함수 parameter를 설정해주어야 한다는 점이다. 모든 형태의 consistency function은 boundary constraints(가장자리 조건)을 다음과 같이 가진다. 상당히 심플한데,

$$
f(x_\\epsilon, \\epsilon) = x_\\epsilon 
$$

쉽게 말하자면 **태초 마을($x_\\epsilon$)**에서 귀환($f(\\cdot)$)을 하면 **태초 마을이** 나와야한다는 것이다. 굉장히 당연한 조건이라고 생각이 들 수도 있지만 현재 다루고 있는 내용이 미분 방정식의 solution인 연속 함수에 대한 내용이기 때문에 constraint를 제대로 설정하는 것이 매우 중요하다. 하는 방법은 총 두가지가 있을 수 있는데, 첫번째로는 다음처럼 함수를 case로 분류하거나

$$
f_\\theta(x, t) = \\begin{cases}x,&t = \\epsilon \\newline F_\\theta(x, t),& \\epsilon <t<T \\end{cases}
$$

Skip point인 $t = \\epsilon$에서 $c_\\text{skip}(\\epsilon) = 1$ 이고 $c_\\text{out}(\\epsilon) = 0$인 **미분 가능한 함수**를 통해 구현하는 방법이 있다.

$$
f_\\theta(x, t) = c_\\text{skip}(t)x + c_\\text{out}(t) F_\\theta(x, t)
$$


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603502-8ca6c9df-af07-4480-836c-c110b22cdca2.png" width="900">
</div>


표는 consistency model과 같은 방법론으로 접근한 여러 방법들에 대해 [관련 논문](https://arxiv.org/pdf/2206.00364.pdf)에서 참고하였는데, 보게 되면대부분 두번째 방법을 사용하는 것을 알 수 있고, 이 논문에서도 마찬가지로 두번째 방법을 사용하였다.

### Sampling

 잘 학습된 consistency model $f_\\theta(\\cdot, \\cdot)$이 있다고 가정하게 되면, 단순히 알고 있는 prior로부터 샘플링을 진행한 뒤

$$
\\hat{x}_T \\sim \\mathcal{N}(0, T^2I)
$$

그대로 함수(딥러닝 모델)에 넣으면 구할 수 있게 된다.

$$
\\hat{x}_\\epsilon = f_\\theta(\\hat{x}_T, T)
$$

따라서 single step generation을 할 수 있게 되는 것이다. 근데 만약 이런저런 이유로 **consistency 모델을 사용**하여 기존 디퓨전 모델과 같이 **multiple step generation**을 하고 싶다면 단순히 <U>태초 마을로 데려갔다가 다시 노이즈를 더했다가 하는 과정</U>을 반복하면 된다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603506-0f846807-dea8-4e77-9a81-f576867dc715.png" width="600">
</div>



<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603509-ca5b4bde-cd26-46a0-be61-591b039491b8.png" width="550">
</div>


### Zero shot data editing

이와 같은 consistency model의 특징(prior를 기준으로 data와 대응되는 궤도 상의 어떤 점에서 출발하더라도 원래의 $x_0$로 수렴하는 성질)을 사용하게 된다면 image editing이나 manipulation을 zero shot으로 수행할 수 있다. 가장 간단하게 생각해볼 수 있는 것은 GAN, VAE와 같은 latent variable model에서 할 수 있는 interpolation이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603454-c1fe1ac7-aacf-4aed-9040-bb4ada32befa.jpg" width="600">
</div>


Laent와 생성되는 sample이 parameter로 구성된 implicit decoder의 출력이 되는 GAN이나 VAE의 경우에는 샘플 $x_0$를 만들어내는 latent $z_0$ 그리고 샘플 $x_1$을 만들어내는 latent $z_1$사이의 보간을 통해 중간 이미지($\\text{Image}(x_0, x_1)$)를 생성할 수 있고, 이는 곧 특징자 벡터를 자유롭게 사용하여 생성되는 이미지를 바꿀 수 있다는 장점이 된다.

$$
F_\\Theta(\\alpha \\cdot z_0 + (1-\\alpha) \\cdot z_1) = \\text{Image}(x_0, x_1) 
$$

확률 미분 방정식에서의 diffusion process를 그대로 사용하는 DDPM baseline의 경우에 prior sample인 $x_T$와 이에 대해 생성한 샘플 $F_{\\Theta_{1:T}}(x_T) = x_0$이 $1$대 $1$ 대응이 아니라는 점을 생각해보자. 하나의 latent sample $x_T$가 포함된 모달리티에서 이에 대응될 수 있는 dataset 모달리티 샘플 $x_0^1, \\cdots x_0^N$ 은 Markov process를 전제로 샘플링하기 때문에 latent interpolation이 image에서 유의미한 interpolation으로 이어지지 않는다는 문제가 있다. 그런데 이를 consistency model과 같이 Probability flow ODE의 solution에 대해 풀게 된다면 $x_T$는 더이상 data modallity에 대해 one to many mapping이 아니게 된다. 따라서 GAN이 가지는 장점 중 하나인 latent manipulation을 통한 <U>이미지 manipulation이 용이</U>하다는 특징을 가져갈 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603514-6ef113b5-a82d-4ef0-9626-8f4dbf318c19.png" width="600">
</div>


또한 추가적으로 sample의 modality와 더불어 condition이 들어가는 경우에도 zero-shot으로 사용할 수 있다는 장점이 발생한다. 예컨데 좋은 성능의 image inpainting, colorization 그리고 super-resolution 등등을 수행할 수 있는 디퓨전 기반의 모델은 모두 해당 task에 대한 목적을 가지고 explicit하게 학습이 전제되어야한다. 하지만 앞서 말했던 것과 같이 consistency model은 어떠한 수준의 noise에서도 $x_\\epsilon$을 복구할 수 있게끔 학습되기 때문에 여러 noise level에 대한 denoising이 가능하며,


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603517-ef598c62-a4a4-4883-93b3-60bb501bbae1.png" width="700">
</div>




이를 다르게 생각한다면 어떠한 input이 들어가더라도**multiple step generation**을 수행하게 되면 임의의 input에 대해 그 시작점을 찾을 수 있게 되는 것이다(condition이 들어갈 때는 단순히 prior sampling 부분만 스킵하면 될 것 같음). 만약 input이 **grey image**라면 이를 consistency model에 대해 multistep(노이즈를 더하고 $x_0$를 예측하고를 수차례 반복)을 적용할 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603527-bcd67b05-4c1c-4cc0-b062-a8db6a1fc8fe.png" width="700">
</div>


이렇듯 딱히 <U>condition에 대해 따로 학습할 필요가 없다</U>는 부분은 아래와 같이 inpainting, super-resolution 그리고 SDEdit(painting to image)와 같은 task에 자연스럽게 사용될 수 있다는 장점을 부여해준다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603536-91cb5030-69f6-4067-8cc9-495050027f7b.png" width="700">
</div>



<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603538-1e75c3b7-b61c-4239-96b2-62d87dc09eb1.png" width="700">
</div>



<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603541-223f5440-e14f-4bf1-b06b-ff6a180c6380.png" width="700">
</div>



# Train consistency model

저자들이 앞서 밝힌 것과 같이 consistency model을 학습하는 방법으로는 꽤나 좋은 pre-trained source model의 score를 사용하여 distillation하는 방식과, 처음부터 학습하는 방식이 있다고 했었다. 그 두가지 방법에 대해 각각 소개하면 다음과 같다.

### Distillation을 통해 학습시키기

$$
dx_t = \\left( \\mu(x_t, t) - \\frac{1}{2}\\sigma(t)^2 \\nabla_x \\log p_t(x_t) \\right) dt,~~\\frac{dx_t}{dt} = -ts_\\phi (x_t, t)
$$

앞서 봤던 PF ODE식을 생각해보자. 여기서 좌측 식만 보게되면 실제 데이터 분포에 대한 score를 구할 수 없기 때문에 학습된 네트워크의 score prediction을 대입하면 우측 식과 같이 empirical PF-ODE를 문제로 가져올  수 있다. 시간축 $[\\epsilon,~T]$을 $N-1$개의 sub-interval로 분리한다고 생각해보자. 자르는 기준선에 대한 boundary condition $t_1 = \\epsilon$ 그리고 $t_N = T$에 대해 증가하는 sequence $[t_1,~t_2,~\\cdots,~t_N]$를 정할 수 있다. 시간축을 나누는 기준은 임의로 정할 수 있지만, 관련 논문 중 하나의 setting을 따라갔다.

$$
t_i = \\left(\\epsilon^{1/\\rho} + \\frac{i-1}{N-1} (T^{1/\\rho} - \\epsilon^{1/\\rho}  )\\right)^\\rho,~\\rho = 7
$$

물론 <U>샘플링이 촘촘할수록</U> numerical ODE solver가 실제 solution에 가까워지기 때문에 $N$의 값이 클수록 더 정확한 예측을 할 수 있게 된다. 아무튼 이렇게 solver가 예측한 특정 시점에서의 함숫값을 $\\hat{x}_{t_n}^\\phi$라 한다면,

$$
\\hat{x}_{t_n}^\\phi := x_{t_{n-1}} + (t_n - t_{n-1}) \\Phi(x_{t_{n+1}}, t_{n+1}; \\phi)
$$

단일 step ODE solver의 update function인 $\\Phi(\\cdots; \\phi)$에 대해 예측된 다음 함숫값은 위와 같다.  이때 $\\phi$라는 파라미터가 ODE solving에 관여하는 이유는 지금 적용하고자 하는 score estimator가 empirical PF ODE를 풀고자하며, 이는 곧 사전 학습된 score estimator를 사용할 것임을 알려준다. Numerical ODE solver 중 가장 흔히 사용할 수 있는 <U>오일러 방식</U>을 적용하면 위의 식은,

$$
\\hat{x}_{t_n}^\\phi := x_{t_{n-1}} - (t_n - t_{n-1})t_{n+1} s_\\phi(x_{t_{n+1}},~t_{n+1})
$$

간단하게 이처럼 표현할 수 있다. 그런데 사실 SDE를 PF-ODE로 바꾸면서 생기는 오차는 실제 score estimate function 과의오차와 부합하게 된다. 따라서 이 부분에 대한 connection을 해주기 위해서 강제로 $1$ to many mapping을 만들어줄 수 있다.

$$
x \\sim p_\\text{data},~x = x+\\eta \\text{(Gaussian noise)} 
$$

이런 식으로 설정한 data point $x$를 기준으로, PF ODE 상의 인접한 data point $(\\hat{x}_{t_n}^\\phi,~x_{t_{n+1}})$를 구할 수 있고, 이때 $x_{t_{n+1}}$은 SDE의 transition kernel에 따라 $\\mathcal{N}(x,~t^2_{n+1}I)$의 분포에서 샘플링하게 된다(대충 아래 그림).


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603543-2607ff4a-647b-474c-835e-eb9551974072.png" width="600">
</div>


그리고 이렇게 샘플링한 adjacent point들에 대해 consistency network를 학습한다. 학습 방법은 간단하게 두 인접한 sample point(하나는 forward SDE에 따라 샘플링, 하나는 이렇게 샘플링된 애를 score estimator와 numerical ODE solver를 통해 궤도 예측)를 각각 네트워크에 통과한 결과가 서로 같게끔하면 된다.

$$
\\mathcal{L}^N_{CD}(\\theta, \\theta^-;\\phi) := \\mathbb{E}(\\lambda(t_n)d(f_\\theta(x_{t_{n+1}}, t_{n+1}),~f_{\\theta^-}(\\hat{x}^\\phi_{t_{n}},t_n)))
$$

$\\lambda(\\cdot)$는 시간에 따른 kenrel 분포 변화때문에 loss에 weight를 주기 위한 term이고 $d$는 두 예측 사이의 거리 metric, 학습의 주체가 되는 $\\theta$가 student parameter로 loss에 대한 gradient descent를 받게 되고  $\\theta^-$는 teacher parameter로 student parameter를 EMA 방식으로 가져간다. 흔히 알고있는 distillation 방법이랑 동일하다. 거리 메트릭은 이것저것 다 가능한데 이미지 생성에 주로 사용되는 MSE, L1 그리고 LPIPS를 해당 논문에서는 모두 실험했으며 weight term인 $\\lambda(\\cdot)$는 <U>심플하게 $1$로 고정해서 사용하는 것</U>이 모든 task 및 dataset에 대해 괜찮은 성능을 보였다고 밝힌다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603546-c1e191df-b7c5-4db4-9c1b-8208cd6cc737.png" width="500">
</div>


지금껏 정리한 학습 과정을 나타낸 **알고리즘 pseudo code**는 위와 같음. Numerical ODE가 가지는 bounded condition(numerical하게 푼 solution이 실제 solution과 가지는 오차가 특정 범위 내에 존재한다는 가정)과 consistency network $f$가 가지는 Lipshitz condition을 만족한다는 조건 상에서 loss function의 supremum 또한 수렴한다는 증명을 할 수 있다. 이는 곧 **empirical PF ODE(**Consistency model**), 보다 엄밀히 말하자면 consistency network**가 distillation되는 실제 SDE 궤도에 따라 Numerical ODE와 함께 수렴이 가능하다는 증거가 된다.

$$
\\begin{aligned}
&\\text{If local error uniformly bounded by }O((t_{n+1} - t_n)^{p+1}),\\newline
&\\sup_{n, x} \\parallel f_\\theta(x, t_n) - f(x, t_n;\\phi)\\parallel_2 = O((\\Delta t)^p)
\\end{aligned}
$$

해당 내용은 논문의 Appendix A.2 절에 수록되어있는데, 증명법은 간단하게 귀납법을 사용하면 가능하다(증명은 이 글에서 생략하겠다).

참고로 논문에서는 학습 주체가 되는 $f_\\theta$를 student network가 아닌 online(학습되는) network, 그리고 EMA로 파라미터를 받는 $f_{\\theta^-}$를 teacher network가 아닌 target(목적이 되는) network라고 이름지었다. Consistency distillation loss는 무한히 증가하는 time step sample $N$에 대해 학습될 때 target과 online parameter를 같게 만들 수 있으며, 이는 곧 distillation의 주체가 되는 **consistency network**가 완벽하게 <U>모든 정보를 이어받았다</U>고 이해할 수 있다.

### Isolation(단독으로) 학습시키기

위에서 소개한 방법은 consistency network를 score network의 정보와 ODE solver를 사용하여 어떤 식으로 consistency loss를 수렴시킬 수 있는지에 대해 증명하는 과정이었다. 이번에는 consistency model이 기존 diffusion 방식에서 벗어난 PF ODE 자체로의 가능성을 보여주며, 새로운 생성 모델의 시작이라는 기준이 된 학습법에 대해 언급하도록 하겠다.

Distillation 방식의 경우 사전 학습된 diffusion process model이 필요하고, 이를 통해 score estimation $s_\\phi(x, t)$를 미분 방정식의 한 요소로 사용할 수 밖에 없었다. 만약 consistency model을 단독으로 학습시키고자 한다면 <U>해당 의존성을 없애버려야한다</U>(아래의 식에서 $\\nabla_x \\log p_t(x_t)$를 구해야함).

$$
dx_t = \\left( \\mu(x_t, t) - \\frac{1}{2}\\sigma(t)^2 \\nabla_x \\log p_t(x_t) \\right) dt
$$

이를 score estimator 없이 구하는 방법은 다음과 같다.

구하고 싶은 **score**를실제 data의 **marginal distribution**에 대해 역으로 projection하면 적분식이 나온다.

$$
\\nabla \\log p_t(x_t) = \\nabla_{x_t} \\log \\int p_\\text{data}(x) p(x_t \\vert x) dx
$$

그리고 $\\log$에 대한 미분은 closed form으로 정리된다.

$$
\\nabla \\log p_t(x_t) = \\frac{ \\int p_\\text{data}(x) \\nabla_{x_t}p(x_t \\vert x) dx}{\\int p_\\text{data}(x)p(x_t \\vert x)dx}
$$

그리고 확률 분포 $p(x_t \\vert x)$에 대한 미분은 log likelihood $\\log (p(x_t \\vert x))$에 대한 미분으로 치환할 수 있다.

$$
\\nabla \\log p_t(x_t) = \\frac{ \\int p_\\text{data}(x) p(x_t \\vert x)\\nabla_{x_t}\\log p(x_t \\vert x) dx}{\\int p_\\text{data}(x)p(x_t \\vert x)dx}
$$

분모와 분자를 정리하게 되면,

$$
\\nabla \\log p_t(x_t) = \\frac{ \\int p_\\text{data}(x) p(x_t \\vert x)\\nabla_{x_t}\\log p(x_t \\vert x) dx}{p_t(x_t)}
$$

그리고 $x_t$는 적분의 주체가 되는 변수 
$x$와 상관없기 때문에 상수로 취급 가능하다.

$$
\\nabla \\log p_t(x_t) = \\int\\frac{  p_\\text{data}(x) p(x_t \\vert x)}{p_t(x_t)}\\nabla_{x_t}\\log p(x_t \\vert x) dx
$$

앞부분의 식은 Bayes’ rule에 따라 조건부의 위치가 바뀌게 되고,

$$
\\nabla \\log p_t(x_t) = \\int p(x \\vert x_t) \\nabla_{x_t} \\log p(x_t \\vert x) dx
$$

이는 $x_t$를 조건으로 하는 확률 분포에 따른 $x$에 대해 평균을 구하는 것과 같다.

$$
\\nabla \\log p_t(x_t) = \\mathbb{E}(\\nabla_{x_t} \\log p(x_t \\vert x) \\vert x_t)
$$

조건부 확률은 diffusion process에서 가우시안 커널로 정의가 되었기 때문에

$$
-\\mathbb{E}\\left(\\frac{x_t - x}{t^2} \\vert x_t\\right)
$$

이처럼 근사시킬 수 있다. 물론 가지고 있는 샘플 내에서 평균을 구하는 과정이 되기 때문에 numerical error는 존재할 수 밖에 없다. 아무튼 이렇게 구한 score를 사용하게 되면 score estimation을 해주는 pre-trained network 없이 샘플링이 가능하고, 이 샘플들을 통해 consistency network 학습이 가능하다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603552-c24a46a3-00b3-4db2-942f-329b570711f5.png" width="500">
</div>


길게 증명과정이 있었지만 process는 간단하게도 인접 샘플들을 모두 사전 정의한 diffusion SDE에 따라 생성, 이를 사용하여 consistency model을 학습시키게 된다. 해당 process를 따르는 consistency network 학습 loss는 다음과 같이 변한다. 다변수 standard gaussian 변수 $z \\sim \\mathcal{N}(0, I)$에 대해,

$$
\\mathcal{L}^N_{CT}(\\theta, \\theta^-) := \\mathbb{E}(\\lambda(t_n)d(f_\\theta(x + t_{n+1}z, t_{n+1}),~f_{\\theta^-}(x + t_nz,t_n)))
$$

이와 같고, 마찬가지로 해당 loss를 수렴시키는 과정이 distillation loss를 수렴시키는 것과 결과적으로 동일함을 증명할 수 있다. 이 부분 증명이 진짜 중요하긴 한데 Taylor expansion을 통해 $o(\\Delta t)$ 에 대한 term을 뽑아내는 방식으로 증명이 이루어져서 수식 길이가 너무 길어서 이것도 이 글에서는 패스.. 증명은 Appendix에서 Theorem 2.를 참고하면 된다.


# Experiments

실험은 비교적 직관적으로 각 factor에 대해 차례대로 실험해서 좋은 factor를 선별하는 과정을 거치게 된다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/234603556-63854500-a4b1-40fa-80b2-513fe79bd466.png" width="900">
</div>


우선 (a)를 보게 되면 <U>LIPS loss가 가장 효과적인 distance metric</U> $d(\\cdot)$임을 알 수 있고, 바로 다음 실험인 (b)를 보게 되면 LPIPS를 고정 metric으로 활용하는 식으로 단계단계 실험을 진행한다. (b)에서는 solver에 대한 학습 효과를 보는데, 1차 근사만 고려하는 오일러보다는 2차 근사를 고려하는 Heun이 좀 더 좋은 성능을 보이는 것을 알 수 있다.

(c)에서는 앞서 bias를 줄이기 위해 테스트한 time step sample 수 $N$에 대한 경향성을 <U>조금 더 촘촘하게</U> 늘려서 실험했는데, 당연하게도 $N$이 커질수록 성능이 좋아진다. 이건 수식 증명에서도 볼 수 있는 내용. 
$N$이 어느 정도 증가하면 그 이후로는 <U>성능 수렴이 발생하는 것</U>도 함께 확인할 수 있다. 아무래도 numerical ODE에 따른 성능 향상의 bottleneck이지 않을까 생각해봄.

(d)는 마지막으로 CT를 사용한 학습 과정인데, 일단 FID가 현저히 떨어지는건 어쩔 수 없는 한계점. CT의 경우에는 CD와는 다르게 특정 <U>numerical ODE solver에 성능이 좌우되지 않기 때문</U>에(학습에 사용되는 샘플링은 사전에 정의된 커널로 함) solver를 사용할 필요가 없다. CT의 경우에는 distillation이 사용되지 않기 때문에 $N$에 대한 효과가 두드러졌는데, 예컨데 $N$이 너무 작으면 빠른 수렴은 가능했지만 샘플링 성능이 구리고 키우면 수렴은 좀 느려지지만 그대신 샘플링 성능은 오른다. 이 두 가지 장점을 같이 사용하기 위해 $N$을 <U>조금씩 증가시키면서 학습</U>시키는 방법(보라색)을 고안하였고, EMA factor $\\mu$또한 이에 맞춰 점차 증가시키는 방법을 사용하였다. 그래프를 보면 빠른 성능 수렴 + 높은 샘플링 퀄리티(FID)를 보이는 것을 확인할 수 있다.


# 결론

실험 결과에는 few-step image generation, direct generation 및 zero-shot image editing과 관련된 여러 결과들이 첨부되어있다. 아무래도 consistency network가 첫번째로 empirical PF ODE의 수렴을 이용하여 한 번에 샘플링이 가능한 네트워크 학습을 고안한 만큼, 앞으로 기존 diffusion 샘플링이 하지 못했던 빠른 샘플링과 관련된 새로운 방향이지 않을까 생각된다. 만약 해당 방법론이 stable diffusion과 같은 zero-shot text to image generation과 결합되어 높은 퀄리티의 샘플링이 가능하다면 <U>새로운 사업상의 게임 체인저</U>로 등장할 수 있지 않을까 싶다.
`,jO=`---
title: "Improved DDPM + Diffusion beats GAN + Classifier free diffusion guidance 논문 리뷰"
category: "ai papers"
publishedAt: "2023-04-30"
thumbnail: "https://user-images.githubusercontent.com/79881119/235350616-84dda90c-0784-4dd0-a515-78d964a55385.png"
---


# 들어가며…

이번에 리뷰할 논문은 총 3개 시리즈로, 전체적으로 디퓨전 모델(DDPM, DDIM)에 기반하여 **작성되었다는 점**이 공통점이고, 모두 diffusion model의 sampling quality를 높이기 위한 노력으로 이어진다고 볼 수 있다. 그 중 [Improved DDPM](https://arxiv.org/abs/2102.09672)의 경우 DDPM에서 가장 baseline 실험만 진행된 점에 추가로 몇몇의 modification을 통해 샘플의 log likelihood를 높일 수 있음을 보인 논문이며 [Diffusion beats GAN](https://arxiv.org/abs/2105.05233) 논문은 보다 다양한 architecture ablation과 classifier guidance를 제시하여디퓨전 모델이 GAN 이상의 샘플링 퀄리티를 보일 수 있음을 보여주었다. 마지막으로 [classifier-free diffusion](https://arxiv.org/abs/2207.12598)은 앞서 언급한 classifier guidance가 가지는 한계점과 문제점을 언급하며 conditional generation의 장점과 classifier의 explicit한 학습으로부터 자유로워질 수 있는 방법을 제시한다. 증명할 부분 자체는 많지 않기 때문에 한번에 다루는 것이 좋을 것 같다고 생각하여 정리해보려고 한다.


# Improved DDPM

DDPM이 <U>새로운 생성 모델의 학습법</U> 및 <U>샘플링 모델</U>로 딥러닝 씬에서 주목받기 시작하면서 CIFAR-10이나 LSUN같은 데이터셋 외에도 샘플의 diversity가 다양한 ImageNet과 같은 데이터셋의 학습에도 유용할 지 의문이 발생하기 시작했다. GAN의 경우에는 빠른 샘플링 속도 및 높은 샘플링 퀄리티로 주목을 받아왔었지만 학습의 불안정성과 샘플 생성 시 다양성이 떨어진다는 문제를 해결하기 힘들었고, VAE와 같은 likelihood 방법은 안정성과 샘플 다양성을 보장할 수 있지만 그 대신 샘플링 속도나 퀄리티가 GAN에 비해 떨어진다는 문제를 해결하기 힘들었다. 이런 와중에 DDPM이 등장한 것이다. 디퓨전 모델도 지금도 그렇긴 하지만 만능이라고 할 수 없었다. 새로운 방법론으로 제시가 되었을 뿐, 네트워크 구조나 학습법에 대한 future work는 여전히 과제로 남아있는 상태였던 것이다.

이 논문에서는 크게 두 가지 방법을 통해 샘플링 퀄리티를 높인다. 첫번째는 hybrid objective로, 기존 DDPM에서는 Variational lower bound(VLB)를 수정한 simplified loss를 사용했으나, 이 논문에서는 여기에 추가로 VLB loss를 사용하여 최적화를 하였다.

두번째는 고정된 variance가 아난 학습된 variance를 사용하였고, 이를 통해 DDPM이 수백의 forward process를 통해 좋은 퀄리티의 샘플들을 만들었던 점과 비교하여 더 적은 forward pass($50$)로도 이를 달성할 수 있었다. DDIM에서는 non-Markovian process에 기반한 새로운 샘플링 방법을 고안했었는데, 이와는 다르게 DDPM의 Markovian process 자체는 유지하는 방향으로 연구를 진행한 것이다.

### 기존 DDPM

DDPM 공식에 대한 모든 정리는 이전 게시글에 다뤄놓았기 때문에 증명 부분을 제외하고 보면 다음과 같다. DDPM의 기본 원리는 data distribution $x_0 \\sim q(x_0)$가 있을 때, forwarding noise process를 아주 작은 가우시안 노이즈를 더해가는 방식으로 정의한다.

$$
q(x_t \\vert x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I)
$$

이때 충분히 큰 시간 $T$ 동안 잘 scheduling된 $\\beta_t$가 있다면 parameterized prior를 $x_T \\sim \\mathcal{N}(0, I)$에서 샘플링할 수 있게된다. 즉, forward process에 의해 점차 가우시안 노이즈에 가까워진다는 것이다. 그렇다면 이 조건부 확률의 반대 방향인 $q(x_{t-1} \\vert x_t)$를 알 수만 있다면 임의의 가우시안 노이즈로부터 $x_0$를 샘플링할 수 있게 되는데, 식을 보면 알 수 있겠지만 이 부분은 <U>tractable하지 않다</U>.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350616-84dda90c-0784-4dd0-a515-78d964a55385.png" width="600">
</div>


그렇기에 파라미터를 가진 neural network를 통해 각 step의 noised sample $x_t$로부터  $p_\\theta(x_{t-1} \\vert x_t)$를 예측하고, 이를 통해 점차 노이즈를 제거하여 $x_0$를 샘플링하고자 하는 것이다. 그리고 아주 작은 가우시안 노이즈를 더하는 과정의 역과정은 곧 <U>가우시안 노이즈를 빼는 과정</U>으로 approximate이 가능하다.

$$
p_\\theta(x_{t-1} \\vert x_t) := \\mathcal{N}(x_{t-1}; \\mu_\\theta (x_t, t), \\Sigma_\\theta (x_t, t))
$$

그리고 이를 최적화하는 variational lower bound 공식은 다음과 같이 정리된다.

$$
\\mathcal{L} 
\\le D_{KL}(q(x_T \\vert x_0) \\vert\\vert p_\\theta(x_T)) -\\sum_{t > 1} D_{KL} (q(x_{t-1} \\vert x_t, x_0) \\vert\\vert p_\\theta(x_{t-1} \\vert x_t)) -\\mathbb{E}_q\\left(\\log p_{\\theta}(x_0 \\vert x_1) \\right)
$$

맨 앞부분은 충분한 시간 $T$에 대해 임의의 데이터셋 $x_0$를 perturbation하게 되면 자연스럽게 만족하는 식이므로 $0$에 가깝다고 생각할 수 있다. 따라서 최적화에 필요한 식이 아니게 된다. 중간의 식은 역과정을 예측하는 네트워크가 forward process의 posterior를 잘 따라갈 수 있게끔 설정한 KL divergence 식이 된다. 마지막으로 $x_1$에서 $x_0$를 생성하는 과정은 $256$의 RGB 데이터로 구성되는 이미지의 확률을 projection하기 위해 설정된 식이다. 디테일한 증명 및 loss 각 term에 대한 설명은 DDPM 게시글에서 확인하면 된다.  아무튼 이 식을 단순화하여 나타낸 것이 곧 다음과 같은 simplified loss이다.

$$
\\begin{aligned}
&\\mathbb{E}_{x_0,\\epsilon} \\left( \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t(1-\\bar{\\alpha}_t)} \\parallel \\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t) \\parallel^2\\right) \\newline
\\approx& \\mathbb{E}_{t, x_0, \\epsilon} \\left( \\parallel \\epsilon - \\epsilon_\\theta(x_t, t) \\parallel^2\\right) 
\\end{aligned}
$$

### DDPM의 log-likelihood 증가시키기

DDPM에서 흔히 생성된 샘플의 퀄리티를 평가하는 메트릭인 FID나 IS(Inception Score)는 좋은 수치를 보여주었으나 log-likelihood 수치는 잘 달성하지 못한 모습을 보여주었다. Log likelihood는 generative model이 data distribution의 mode를 얼마나 잘 반영하는지 나타내는 지표이다. 쉽게 설명하자면  log-likelihood를 최적화하는 것이 곧 generative model로 하여금 data distribution의 전체적인 형태를 잘 잡아내도록 하게 할 수 있다. 아무리 샘플을 잘 만들어내더라도 실제 데이터 분포의 일부분만 잘 반영하는 네트워크는 해당 데이터를 ‘잘’ 만들어낸다고 판단하기 어렵기 때문이다. DDPM이 대체 왜 log likelihood를 제대로 반영하지 못하는지에 대해 분석한 것이 바로 이 논문이며, log likelihood를 높이고자 방법을 찾고 이를 적용하는 것이 <U>실제 샘플링 퀄리티에 큰 도움이 될 수 있다</U>는 전개 방향이 된다.

### Learnable Standard deviation(Variance)

DDPM에서는 저자가 사전에 정의한 variance를 고정으로 사용한다($\\sigma_t^2I$). 여기서 신기한 점은 $\\sigma^2_t$를 $\\beta_t$를 정의하는 것과 forward process의 posterior로 유도된 $\\tilde{\\beta}_t = \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t} \\beta_t$를 사용했을 때의 샘플링 퀄리티가 큰 차이가 없다는 사실이다. 차이가 난다고 하면 전자를 사용하면 $q(x_0)$에 대해서 $t$번째 kernel의 variance가 isotropic Gaussian이 되고 후자를 사용하면 delta function이 된다는 점이다. $\\beta_t$와 $\\tilde{\\beta}_t$를 variance가 가질 수 있는 양단의 기준점이라고 한다면, 왜 해당 파트가 샘플링 성능에 큰 영향을 끼치지 않는지에 대한 이유가 중요해진다. 사실 이 부분에 대한 논의를 기존 DDPM 게시글에서 제대로 언급하지 못했었는데, 그때 나름대로 생각했던 말을 인용하자면

> 실제 실험에서는 단순히 $t$번째 step에서의 variance인 $\\beta_t$를 $\\alpha_t^2$로 사용해도 큰 차이가 없다고 언급하는데, 이는 사실상 variance 누적곱에 해당되는 $\\bar{\\alpha}_t$가 step 수가 크기 때문에 $t$에 따라 큰 차이를 보이지 않기 때문이라고 생각했다(아니라면 말고).
> 

라고 했었다. 자기 피드백을 해보자면 한 10% 정도만 맞는 말을 한 것 같다. 실제로 Improved DDPM 저자가 분석한 내용을 살펴보도록 하자.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350647-b458c10f-a842-40eb-ab2e-6550b8fff47c.png" width="500">
</div>


Diffusion step이 커질수록 $\\beta_t$와 $\\tilde{\\beta}_t$ 값이 거의 동일해진다. 이는 결국 diffusion step이 점차 커지면 커질수록 $\\sigma_t$를 설정하는 것은 샘플 퀄리티에 큰 영향을 끼치지 않을 것이라는 사실을 의미한다. 두 값이 유의미한 차이를 보이는 곳은 $t = 0$ 근방인데, 이 부분에서는 이미지가 <U>거의 완성된</U>(small noise) 영역이기 때문에 샘플 퀄리티가 크게 차이나지 않게 된다.

즉, Diffusion step 수가 늘어날수록 $\\Sigma_\\theta(x_t, t)$를 바꾸는 것은 image distribution를 결정하는 과정에 크게 영향을 주지 못한다. 그렇다면 결국 $\\sigma_t$를 고정하는 것이 diffusion process에서 최선이라는 것이라는 걸 언급하고 싶은 것일까(?)는 아니다. 아래 그래프를 보도록 하자.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350618-13bb612e-572c-41b2-9c6d-ac0d3c79dcf5.png" width="500">
</div>


Log likelihood를 최적화하는 방법은 diffusion process에 맞는 VLB(Variance Lower Bound)를 최적화하는 과정인데, 실질적으로 노이즈에 가까운 뒷부분($T = 4000$)보다 샘플에 가까운 앞부분($T=0$)에 올수록 loss가 차지하는 중요도(importance)가 올라가게 되는 것을 볼 수 있다. 이는 DDPM이 log likelihood를 효과적으로 개선시키지 못한 이유 중 하나가 바로 variance의 양단이 차이가 많이 나는 앞부분에서 제대로 $\\Sigma_\\theta(\\cdot)$를 적용하지 못한 것으로 해석할 수 있고, DDPM에서 굳이 loss term에 넣지 않았던 $\\Sigma_\\theta(x_t, t)$ 또한 예측이 필요하다는 사실을 보여준다. 논문에서는 만약 $\\Sigma_\\theta(x_t, t)$를 예측하고자 하는 범위가 너무 작다면($t$가 어느 정도 증가하고 나면 $\\log$ 범위에 대해서도 infimum/supremum의 차이가 거의 안남) neural network로 이를 예측하는 것이 어려울 것이라고 보았다(변동이 크면 수렴에 방해될 수 있기 때문). 따라서 그대신 variance를 $\\beta_t$와 $\\tilde{\\beta}_t$의 보간을 통해 parameterization하였다. 네트워크는 <U>하나의 dimension마다 특정 요소를 내뱉는</U> interpolation용 vector $v$를 예측하고, 해당 output은 variance를 다음과 같이 interpolate한다.

$$
\\Sigma_\\theta(x_t, t) = \\exp(v\\log \\beta_t + (1-v)\\log \\tilde{\\beta}_t)
$$

앞서 말했던 것처럼 $\\beta_t$ 자체를 interpolation 하는 것보다 $\\log$를 씌워서 interpolation 하는 것이 numerical 관점에서 안정적이기 때문에 위와 같이 수식화된다.  참고로 $v$는 꼭 내적에만 국한되지 않고 $0\\sim1$ 이외의 값들을 가질 수 있게 설정되었지만, 학습 후에 네트워크의 동작을 보았을 때 실제로 <U>외적을 예측하는 경우는 없었다</U>고 한다. Simplify된 loss는 VLB loss에서 variance에 대해 normalize되는 부분을 모두 무시하기 때문에 parameterized된 variance를 고려할 수 없게 된다. 따라서 $L_\\text{simple}$ 대신 variance에 대한 고려를 할 수 있는 $L_\\text{VLB}$를 weighted summation한 loss를 사용하였다.

$$
L_\\text{hybrid} = L_\\text{simple} + \\lambda L_\\text{vlb}
$$

물론 DDPM에서 학습이 용이했던 이유 중 하나는 $L_\\text{simple}$의 영향도 있기 때문에 이를 방해하지 않도록 $\\lambda = 0.001$의 작은 값을 사용하였으며, $\\mu_\\theta(x_t, t)$ term이 $L_\\text{vlb}$에 영향을 받지 않도록 해당 loss를 최적화할 때는 stop gradient를 사용하였다. 즉 $\\Sigma_\\theta$는 $L_\\text{hybrid}$를 통해 simplified loss에 guided된 상태로 안정적인 variance 학습을 할 수 있게 하며 그와 동시에 $\\mu_\\theta$는 오로지 simplified loss로만 최적화하게 된다.

### Better noise scheduling

DDPM에서 사용했던 **noise scheduling**를 기준으로 새로운 noise scheduling 방법도 제시한다. Linear scheduling 방식은 고차원(높은 resoltuion) 이미지에는 잘 적용되지만, 오히려 저차원($32 \\times 32$) 이미지에는 효과적이지 않은 것을 알 수 있다. 이는 forward process에 의해 gaussian noise에 가까워지는 속도가 low resolution image의 경우 더 심하기 때문으로,


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350619-7cdd3163-4cfc-49e5-b7e3-c82f25024500.png" width="700">
</div>


위의 그림을 보게 되면 **linear scheduling**(upper row)을 적용했을 경우에 몇 prcoess가 지나지 않아도 <U>이미지 정보가 거의 유실되는 것</U>을 볼 수 있다. 샘플링을 잘하기 위한 diffusion process를 학습하려면 variance의 누적인 $\\bar{\\beta}_t$가 보다 단계적으로 샘플들을 noisy하게 만들어야한다는 것이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350620-a77002e6-61e7-41d9-bb59-f901518a64ac.png" width="500">
</div>


실제로 Linear schedule로 학습된 애들은 diffusion process를 $20\\% \\sim 30\\%$까지 skip하더라도 FID에 큰 손실이 없는 것을 알 수 있고, 이는 diffusion step 수가 늘어나는 것과 샘플링 퀄리티가 좋아지는 것에 아무런 도움이 되질 않는다는 것을 암시한다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350622-3af084f8-02bc-4480-8f3f-1b4c297d4346.png" width="500">
</div>


따라서 저자는 cosine schedule 방식을 사용하였고, 이를 통해 보다 점진적으로(linear하게) 줄어드는 형태의 variance를 구현할 수 있었다고 한다.

$$
\\bar{\\alpha}_t = \\frac{f(t)}{f(0)},~f(t) = \\cos \\left( \\frac{t/T + s}{1+s} \\cdot \\frac{\\pi}{2}\\right)^2
$$

이 정의에 따르면 $\\beta$가 $t = T$에 가까워질수록 $1$에 지나치게 가까워진다는 문제가 발생하는데, 이러한 singularity(계속 같은 modality에서 샘플링 되는 문제)를 없애기 위해 $0.999$보다는 커지지 않도록 clip해서 썼다고 한다. 마찬가지로 작은 offset $s$를 사용함으로써 $t = 0$ 근방에서 너무 작아지지 않도록 설정해주었는데, 이때 픽셀의 bin(확률 분포에서 확률로 넘어가는 공식)을 고려하여 $1/127.5$로 설정하였으며, 이는 곧 $s = 0.008$이라는 값으로 결정된다.

### Gradient noise 줄이기

Hybrid loss를 사용한 이유는 다음과 같다. 사실 대놓고 log likelihood를 줄이고자 한다면 VLB로 optimize하는게 가장 좋을 것이다. 허나 결과는 그리 호락호락하지 않았더라..


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350623-087f6934-e107-411a-8b5e-44bbd8c12fd5.png" width="500">
</div>


실제로 <U>log likelihood</U>에 관련이 있는 loss는 VLB loss 그 자체인데 막상 그래프를 보면 hybrid보다 훨씬 수렴시키기 어렵고 noisy한 학습이 진행되는 것을 볼 수 있다. 애초에 전반적으로 hybrid가 더 낮은 loss curve를 보여주는 것을 볼 수 있다. 상식선에서 VLB loss를 사용했을 때 왜 더 log-likelihood가 나쁘게 나오는지 저자들은 하나의 가설을 세웠고, 이는 VLB loss로부터 오는 gradient가 Hybird에서 오는 gradient보다 **noise가 많다는 사실**이었다(아래 그림 참고). 


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350624-bfe1dd94-a7a0-420b-9fa6-356a3e639339.png" width="500">
</div>


예컨데 $L_\\text{VLB}$는 simplified loss와는 다르게 각 step마다 magnitude가 다르다. 즉, loss가 어느 time step $t$를 기준으로 계산되냐에 따라 차이가 나므로, 단순히 $t$를 <U>uniform하게 샘플링하는 것이 VLB objective에 도움이 되지 않는다</U>는 것이다. 이를 완화하고자, 다음과 같은 importance sampling을 제시하였다.

$$
\\begin{aligned}
&L_\\text{VLB} = \\mathbb{E}_{t \\sim p_t} \\left( \\frac{L_t}{p_t} \\right) \\newline
&\\text{Where }p_t \\propto \\sqrt{\\mathbb{E}(L_t^2)} \\text{ and }\\sum p_t = 1
\\end{aligned}
$$

일종의 focal loss와 비슷하다고 생각할 수 있는데, 학습 시 $10$개의 previous loss에 대한 기록을 통해 지속적으로 업데이트되는 각 loss의 확률에 따라 loss sampling이 된다고 생각하면 된다. 물론 처음에는 각 $t$에 대해 $10$개의 샘플이 모일 때까지는 uniformly 추출하는 과정을 거친다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350625-0d11d863-2ce3-4458-a2da-2e5a73ce5828.png" width="500">
</div>


논문을 읽다보니 느낀 것은, Improved DDPM에서 집중한 것은 어떻게 likelihood based network에 비교하여 DDPM의 log likelihood를 효과적으로 개선시킬까에 대해 다룬 논문인 것 같다. Variance를 parameterize하는 과정에서 FID도 꽤나 많이 올라간 듯하다.


# Diffusion beats GAN

다음 논문은 GAN의 샘플링 퀄리티를 넘어설 수 있는(사실 DDPM baseline은 ImageNet과 같은 복잡한 데이터셋에 대한 샘플링은 성능이 상대적으로 좋지 않았다) 방법을 제시한 ‘Diffusion beats GAN’ 논문이다. 디퓨전을 굉장히 사랑하는 듯한(?) OpenAI에서 낸 자극적인 제목의 논문이다 보니 <U>디퓨전이 유명해지게끔 한 논문들 중 하나</U>로 손꼽힌다. 기존 GAN 중 SOTA였던 BigGAN의 생성 성능보다 좋은 ImageNet 샘플링 성능을 보여주었으며, unconditional/conditional image generation 모두 다뤘다는 점이 인상깊은 논문이다.

### Why diffusion is not good enough?

분명 디퓨전이 sample diversity도 좋고, 학습 안정성이 높은데도 불구하고 <U>왜 샘플링 성능이 충분히 올라오지 못했을까?</U> 이에 대해서 저자들이 문제점을 가설로 설정하고, 그리고 이 가설을 풀어나가는 과정이 곧 이 논문의 핵심이라고 할 수 있겠다. 저자들이 생각한 diffusion이 다른 generative model에 비해 여전히 좋은 샘플링을 내지 못하는 이유는 다음과 같다.

1. GAN(Generative Adversarial Networks)의 경우 diffusion에 비해 오랜 연구가 진행되었고, 이에 따라 최적의 네트워크 구조나 학습법, 하이퍼 파라미터 등등 리서치가 충분히 진행되었기 때문이다.
2. GAN은 fidelity를 높이는 대신 diversity가 trade off로 지불되었기 때문에, 샘플링 성능 자체만 놓고 보자면 GAN을 이기기 힘들다는 것이다.

결국 이유를 분석하자면 DDPM은 생성 모델로서 연구된 기간이 아직 짧다는 점(실제로 diffusion beats GAN 논문은 DDPM 이후 약 $1$년 뒤에 나온 논문), 그리고 GAN은 샘플링되는 데이터의 다양성을 포기하는 대신 높은 퀄리티의 데이터를 만들게끔 설계되었다는 점이다. 따라서 이 논문에서는 GAN이 가지는 두 장점(최적의 네트워크 구조 + 샘플링 퀄리티)를 diffusion에 접목시키고, 아예 GAN을 뛰어넘겠다는 포부를 담고 시작하게 된다.

### Background

이 논문에서 background로 사용된 기본 프레임워크는 DDPM인데, 이에 추가로 위에서 설명한 improved DDPM(trainable variance) 그리고 빠른 샘플링을 위해 제시된 DDIM(Denoising  diffusion implicit models)를 메인으로 한다. 앞에서도 언급했지만 Improved DDPM 논문에서도 적은 time step을 통한 높은 샘플링을 획득할 수 있었지만 이를 해결하는 방식이 ‘학습 과정을 바꿨다는 점’이고, DDIM은 이와는 다르게 동일한 marginal distribution을 가지는 non-Markovian process를 기반으로  ‘샘플링 과정을 바꿨다는 점’에서 서로 다른 연구라고 할 수 있다. DDIM에 대한 글은 본인 포스팅에도 있기 때문에 미리 읽고 오는 것을 추천한다.

결론부터 말하자면 학습 방법은 Improved DDPM의 hybrid loss를, 샘플링의 경우 50 step보다 작은 sequence를 통해 생성할 경우에는 DDIM을 적용하게 된다. 사실 이 내용은 위에서 미처 설명하지 못한 <U>Improved DDPM에서의 실험</U>과 관련이 있다(아래 그래프 참고).


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350626-de4e219c-8302-40b1-b59d-44430094be02.png" width="">
</div>


해당 그래프를 보게 되면 약 $50$ step 이후로는 DDIM의 샘플링 퀄리티가 더 좋아지는 것을 볼 수 있다. 

### Sample quality metrics

Sampling quality를 측정하는 대표적인 방식은 기존 GAN에서 사용하는 IS, FID가 있지만, 여전히 모두 완벽하지 않고 단점이 있다는 치명적인 문제를 안고 있다. 사실상 생성 모델 연구가 **정성적 평가**로는 설득력을 가지는데 그에 비해 **정량적 평가**로 <U>설득력을 가지기 힘든 이유 중 하나</U>라고 볼 수 있다. 정말로 두 이미지 중에서 ‘잘 만든’ 이미지를 평가하는 것은 특이점을 넘어서게 되면 사실상 큰 의미가 없기 때문이다. 특히나 GAN과 같이 adversarial network로 학습하는 경우 fake sample이 gradient에 attack을 수행하기 때문에 자연스럽게 FID score가 높아질 수 밖에 없다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350613-d14a2024-bd45-4d3f-a4c2-565c297fc8bf.png" width="700">
</div>


$$
IS(G) = \\exp(\\mathbb{E}_{x \\sim G})(D_{KL}(p(y \\vert x, p(y))))
$$

Inception score는 위와 같이 측정된다. $p(y)$는 실제로 생성되는 샘플들이 전체 class에 대해 고르게 잘 만들어내는지 측정하고 $p(y \\vert x)$는 생성된 샘플의 퀄리티를 측정한다. 하지만 IS가 반영하지 못하는 것은 각 클래스 별로 다양한 이미지를 생성하지 못하는 상황이다. 예컨데 CIFAR-10 dataset에 대해 10개의 클래스 각각 한가지 샘플만 찍어내더라도 그 퀄리티가 높으면 IS 상으로는 흡족한 결과가 나오게 된다(collapse를 판별할 수 없음). 이를 극복하기 위해 inception network를 사용하여 layer에서의 feature를 사용, 평균 및 공분산을 사용하여 다변수 가우시안 분포를 모델링하는 FID 방식이 소개되었다.

$$
FID(x, g) = \\parallel \\mu_x - \\mu_g \\parallel^2_2 + Tr(\\Sigma_x + \\Sigma_g - 2(\\Sigma_x \\Sigma_g)^{1/2})  
$$


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350630-f9820d26-0213-40c2-b50f-cc206ca1f2e2.png" width="650">
</div>


그리고 또다른 방법으로는 precision recall metric으로 sample fidelity(precision)과 sample diversity(recall)을 분리하는 방법을 제시한 논문도 있다. 예컨데 모델이 학습한 implicit probability가 $P_g$이고 실제 샘플의 분포가 $P_r$이라고 했을 때, 모델이 생성한 샘플 중 실제 샘플의 분포 내에 들어가는 정도를 측정하는 것이 샘플링 성능이랑 관련이 있고 이와는 반대로 실제 분포의 샘플 중 모델이 생성한 샘플에 들어가는 정도를 측정하는 것이 샘플링 다양성과 관련이 있다.

$$
(Precision) = \\frac{TP}{TP+FP}
$$

True positive($P_r$에 해당되는 샘플이면서 $P_g$에 포함되는 것)  + False positive($P_r$에 해당되는 샘플이 아닌데 $P_g$에 포함되는 것) 중 True positive($P_r$에 해당되는 샘플이면서 $P_g$에 포함되는 것)의 비율이 샘플링 퀄리티와 직결되고,

$$
(Recall) = \\frac{TP}{TP+FN}
$$

True positive($P_r$에 해당되는 샘플이면서 $P_g$에 포함되는 것) + False negative($P_r$에 해당되는 샘플이지만 $P_g$에 포함되지 않는 것) 중 True positive($P_r$에 해당되는 샘플이면서 $P_g$에 포함되는 것)의 비율이 샘플링 다양성과 직결된다고 해석하면 된다. 이 논문에서는 Precision, IS를 fidelity를 측정하는 목적으로, Recall을 diversity를 측정하는 목적으로 사용하였다.

### Architecture improvement

앞서 DDPM을 baseline으로 하는 디퓨전 연구가 가지고 있던 한계의 원인 중 하나가 네트워크 구조에 따른 충분한 리서치가 진행되지 않은 점을 들 수 있다고 했다. 이에 저자들은 diffusion model에서 sampling quality를 높일 수 있는 구조를 다음과 같이 서칭하였다.

- Depth(네트워크 깊이) 대비 Width(채널 수) 를 늘린다. 이때 모델 크기는 상대적으로 일정하게 유지하게끔 증가시킨다.
- Attention head의 갯수를 늘린다(베이스라인이 되는 UNet의 residual block에 attention이 들어간다).
- Attention을 원래 $16 \\times 16$의 feature map level에만 적용했었는데, 이걸 $32 \\times 32$, $8 \\times 8$의 feature map에도 적용한다.
- Activation upsampling 및 downsampling 시에 BigGAN의 residual block을 사용한다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350631-2f69801a-2bb9-49d4-95de-77ef0cd49169.png" width="700">
</div>


- Residual connection을 $\\frac{1}{\\sqrt{2}}$만큼 수행한다.

비교를 위해 ImageNet $128 \\times 128$ 크기의 이미지에 대해 $256$의 batch size, $250$의 sampling step으로 통일하고 FID를 기준으로 실험을 진행하였다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350632-8ba35f51-f417-4e88-b698-f16e3c56fb8c.png" width="500">
    <img src="https://user-images.githubusercontent.com/79881119/235350633-37db8891-bdad-42e5-8fec-a90d7b99533e.png" width="500">
</div>


좌측 테이블에서는 rescaling 부분을 제외하고는 모든 구조적 제안이 FID 성능을 높이는데 기여하는 것을 볼 수 있다. 또한 아래 그래프에서 보게 되면 depth를 증가시키는 선택 또한 성능 향상에 도움이 되는 경향을 보았지만, 학습 시간이 지나치게 증가한다는 문제 때문에 더이상 실험을 진행하지 않았다고 한다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350635-ce72acaf-3c88-4dc0-9b36-8ac82719d400.png" width="750">
</div>


그리고 attention configuration에 대한 실험도 진행하였는데, 실험 결과를 보게 되면 head의 개수를 늘리고 각 head의 channel 수를 줄이는 것이 가장 좋은 FID를 보여주었다. 그래프에서 확인해보면 $64$ channel을 사용할 때가 학습 속도 면에서 가장 성능 효율이 좋았기 때문에 이를 사용하게 되었다. 신기하게도 이러한 구조적 장점(성능 경향성)은 transformer의 구조와 동일하다고 한다.

### Adaptive group normalization

AdaGN이라고 불리는 이 친구는 time step과 class embedding을 각 residual block에 stylization해주기 위해 사용되었다. 예컨데 hidden layer activation $h$가 있고 time step과 class embedding의 linear projection $y = [y_s,~y_b]$가 있을 때,

$$
\\text{AdaGN}(h,~y) = y_s \\cdot\\text{GroupNorm}(h) + y_b
$$

위와 같이 정의된다. 아마도 StyleGAN을 읽어본 사람이라면 GroupNorm 부분만 제외하고는 AdaIN과 동일한 것을 확인할 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350636-39e8c778-bbe4-4e29-b846-2fdbfefed0c5.png" width="">
</div>


AdaGN이 좋은 성능을 보이는 것을 기준으로 모든 네트워크 학습에 사용하였지만, 특별히 ablation을 진행한 결과는 위와 같다. 아무튼 위의 여러 과정을 거쳐 결정된 네트워크 구조는 다음과 같다.

- 각 resolution마다 2개의 residual block(BigGAN)을 가지며, width도 resolution에 맞게 조정됨
- Attention head마다 $64$의 channel 수를 가지는데, resolution $32, 16, 8$에 모두 attention layer가 있음
- BigGAN residual block을 upsampling, downsampling할 때 사용하며 AdaGN이 들어가서 timestep과 class embedding을 넣어줌

### Classifier guidance

GAN과 같은 아키텍쳐에서 conditional image synthesis가 label이 한정된 데이터셋에는 높은 퀄리티를 보장할 수 있는 방법 중 하나로 증명되었다. 예컨데 GAN을 하나의 확률 분포라고 생각하면 단순히 실제 데이터인지 아닌지 구분하는 것보다 discriminator가 $p(y \\vert x)$가 explicit하게 정보를 주는 것이 각 label에 맞는 이미지를 잘 생성할 수 있게끔 generator를 유도할 수 있다는 것이다.

근데 생각해보면 앞서 우리는 이미 AdaGN을 통해 class embedding을 time step과 더불어 일종의 style 정보로 넣어주었다는 사실이 있다. 하지만 class embedding을 넣어주는 과정은 실제로 discriminator의 정보를 explicit하게주는 것과 차이가 있다. 따라서 저자는 해당 부분에 대한 방법을 발전시켜서 실험을 진행한다. 예컨데 사전 학습된 classifier가 있다고 생각해보자. 이 classifier는 각 time step $t$에 해당되는 noisy image $x_t$에 대해 classification task에 학습된 상태로 가정한다($p_\\phi(y \\vert x_t,~t)$).  이때의 log likelihood gradient $\\nabla_{x_t} \\log p_\\phi(y \\vert x_t,~t)$를 diffusion sampling의 guidance로 사용하겠다는 것이다. 이 부분에서 DDPM sampler인 Markovian process에 적용될 수 있는 conditional guidance와 DDIM sampler인 non-Markovian process에 적용될 수 있는 conditional guidance를 구분하여 설명한다. 각각을 수식으로 보면 다음과 같다.

### Conditional reverse noising process

각 noised image에 대해서 사전 학습된 pre-trained classifier network $p_\\phi(y \\vert x_t,~t)$는 diffusion pcoess에 완전히 explicit한 정보이기 때문에 다음과 같이 normalizing factor $Z$에 대해 constant 취급이 가능하다. 자세한 증명은 논문 Appendix에 있으므로 여기서는 생략.

$$
p_{\\theta,\\phi}(x_t \\vert x_{t+1} , y) = Zp_\\theta (x_t \\vert x_{t+1}) p_\\phi(y \\vert x_t)
$$

여기서 원래의 공식을 recall해보자면 일반적인(class condition 없는) diffusion process는 다음과 같이 정의가 되었었다. 각 time process에 대해 예측된 $\\mu, \\Sigma$에 대해서,

$$
\\log p_\\theta(x_t \\vert x_{t+1}) = -\\frac{1}{2}(x_t-\\mu)^\\top \\Sigma^{-1} (x_t - \\mu)+C
$$

이때 상대적으로 $\\log p_\\phi(y \\vert x_t)$가 가지는 curvature가 $\\Sigma^{-1}$에 비해 작을 것으로 예상된다. 이에 대한 해석은 다음과 같다. $\\log p_\\theta(x_t \\vert x_{t+1})$은 $1/2\\parallel \\Sigma \\parallel$을 계수로 갖는 quadratic function이다. 따라서 이 quadratic function의 곡률을 결정하는 부분이 곧 $\\Sigma$의 크기와 연관이 있는데, diffusion step 대부분에서 $\\Sigma$는 $0$에 가까운 작은 값을 가지게 되므로 계수가 매우 커지게 된다. 따라서 $p_\\phi$가 가지는 function이 이에 비해 적은 곡률을 가질 것이라고 가정할 수 있게 되는 것이다.

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235352170-23a9d778-f0ab-4fc6-ad1b-3cbe2efa7517.png" width="500">
</div>

이렇게 되면 $x_t = \\mu$인 점에서(diffusion reverse process의 quadratic function의 꼭짓점 부분) classifier guidance 부분을 테일러 1차 근사를 통해 나타낼 수 있다.

쨌든 샘플링되는 파트는 $\\mu$가 메인인데, 어차피 그 부분에서 $p_\\theta$에 대비해서 $p_\\phi$가 가지는 <U>곡률이 상대적으로 매우 작기 때문에</U> 무시할 수 있다는 개념이다.

$$
\\log p_\\phi(y \\vert x_t) \\approx \\log p_\\phi(y \\vert x_t) \\vert_{x_t = \\mu}+(x_t - \\mu)\\nabla_{x_t} \\log p_\\phi (y \\vert x_t) \\vert_{x_t = \\mu} = (x_t - \\mu)g+C_1
$$

여기서의 $g$는 $x_t = \\mu$에서의 classifier에 의한 log likelihood의 gradient와 같다. 이를 위의 공식에 대입하게 되면,

$$
\\begin{aligned}
\\log(p_\\theta(x_t \\vert x_{t+1}) p_\\phi(y \\vert x_t)) \\approx& -\\frac{1}{2}(x_t-\\mu)^\\top \\Sigma^{-1} (x_t - \\mu)+(x_t - \\mu)g + C_2 \\newline
=& -\\frac{1}{2}(x_t-\\mu-\\Sigma g)^\\top \\Sigma^{-1} (x_t - \\mu -\\Sigma g)+ \\frac{1}{2}g^\\top \\Sigma g + C_2 \\newline
=& -\\frac{1}{2}(x_t-\\mu-\\Sigma g)^\\top \\Sigma^{-1} (x_t - \\mu -\\Sigma g)+ C_3 \\newline
=& \\log p(z) + C_4,~z \\sim \\mathcal{N}(\\mu + \\Sigma g, \\Sigma)
\\end{aligned}
$$

결국 classifier에 의한 guidance는 샘플링할 때 gradient 방향을 틀어준다고 생각할 수 있다(drift 조정).


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350637-0aa972ae-3e28-4476-99b9-afa4de6b4442.png" width="700">
</div>


### Conditional sampling for DDIM

그러나 위의 샘플링 방법은 drift를 조정해주는 과정이 들어가고, 수식 상에서 <U>Markov process임을 가정</U>하고 있으므로 DDIM처럼 deterministic한 sampling을 하는 경우에는 사용할 수 없다. 

$$
x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}\\underset{\\text{predicted }x_0}{\\left( \\frac{x_t - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_\\theta^{(t)}(x_t)}{\\sqrt{\\bar{\\alpha}_t}} \\right)} + \\underset{\\text{direction pointing to }x_t}{\\sqrt{1-\\bar{\\alpha}_{t-1} - \\sigma_t^2} \\cdot \\epsilon_\\theta^{(t)}(x_t)} + \\underset{\\text{random noise}}{\\sigma_t z},~z \\sim \\mathcal{N}(0, I)
$$

위의 식을 보면 알 수 있듯이 deterministic DDIM은 $x_0$로부터 $x_t$를 예측하는 형태로 샘플링이 진행되다보니 $x_t$에 대한 classifier gradient를 적용할 수가 없게 되는 것이다. 여기서 바로 이전에 살펴봤던 논문인 SDE와 diffusion model을 연결했던 논문이 힘을 발휘한다. 해당 내용도 포스팅되어있다. 해당 논문에서 VP-SDE라고 명시된 확률 미분 방정식에 대해 보면 다음과 같다. 예컨데 원래의 DDPM은 다음과 같은 process를 통해 샘플링을 진행한다.

$$
x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\left( x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}}_t} \\epsilon_\\theta(x_t, t)\\right)+\\sigma_tz
$$

그런데 이때, 이 식을 score estimate function $s_{\\theta^\\ast}(\\cdot)$에 대한 확률 미분 방정식으로 포현하면 다음과 같다.

$$
x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} (x_i + \\beta_i s_{\\theta^\\ast}(x_i, i)) + \\sqrt{\\beta_i}z_i
$$

고로 기존의 ancestral sampling에서 벗어나서 score function을 time $t$에 대해 정의할 수 있게 된다. 

$$
\\nabla_{x_t} \\log p_\\theta (x_t) = -\\frac{1}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta (x_t)
$$

이를 앞서 정의했던 $p(x_t)p(y \\vert x_t)$의 score function에 적용하게 되면 다음과 같다.

$$
\\begin{aligned}
\\nabla_{x_t}\\log \\left( p_\\theta(x_t)p_\\phi(y \\vert x_t)\\right) =& \\nabla_{x_t} \\log p_\\theta(x_t) + \\nabla_{x_t} \\log p_\\phi(y \\vert x_t) \\newline
=& -\\frac{1}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(x_t) + \\nabla_{x_t} \\log p_\\phi(y \\vert x_t)
\\end{aligned}
$$

즉, epsilon을 다음과 같이 새롭게 정의할 수 있게 된다. 앞서 DDPM의 경우와 동일하게 gradient를 바꾸는 느낌이다.

$$
\\hat{\\epsilon}_\\theta(x_t) := \\epsilon_\\theta(x_t) - \\sqrt{1-\\bar{\\alpha}_t}\\nabla_{x_t} \\log p_\\phi (y \\vert x_t) 
$$

### Classifier gradient scaling

Classfier $p_\\phi$에 의한 score guide를 주기 위해서는 classification model을 학습시켜야 한다. Classifier architecture는 UNet model의 downsampling 부분에서 추출된 feature map에 attention pooling($8 \\times 8$)을 통해 최종 output을 추출하게 된다. Classifier는 각 노이즈 스텝에 대해 분류할 수 있어야하므로 각각의 time step에 대한 noised input을 학습하게 된다. 학습 이후에는 앞서 언급한 gradient 영향을 주면서 샘플링을 진행한다고 보면 된다.

초반 unconditional ImageNet model(class condition을 따로 embedding으로 주지 않은 네트워크)로 실험했을때, classifier guidance $s$를 $1$보다 크게 하지 않으면 원하는 class의 샘플이 나올 확률이 절반으로 뚝 떨어지는 것을 확인하였고, 심지어 이 확률로 샘플을 만들어도 <U>시각적으로 그다지 해당 클래스의 범주에 속하지 않는 것</U>을 확인하였다. 


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350638-8036a8c0-dedf-424f-9130-085fcdee73e5.png" width="">
</div>


예컨데 “Pembroke Welsh corgi”의 class에 대한 scale을 $1.0$으로 주었을 때(좌측) 제대로 생성되지 않던 웰시코기 이미지가 $10.0$으로 키웠을 때 유의미하게 좋아지는 것을 볼 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350639-d330f2db-523f-4e8c-b4a9-6c419e2ff940.png" width="">
</div>


위의 표에서 주목할 점은 unconditional model에 classifier guidance를 충분히 큰 값으로 주게 되면(guidance = $10.0$) conditional model에 필적하는 FID 및 IS를 보여주는 것을 확인할 수 있다.

논문에서 추가로 언급한 내용 중에 low resolution image를 condition으로 하는 2-stage diffusion process를 사용했을 때 BigGAN의 성능을 넘어선 것을 알 수 있는데, 여전히 샘플링 속도가 문제가 된다는 점과 classifier training으로부터 자유롭지 않기 때문에 labeled sample에 한정된다는 문제가 발생한다.


# Classifier-free diffusion guidance

### Low Temperature Sampling

Classifier guidance 논문은 classifier에 의한 gradient 조절을 통해 샘플의 <U>다양성을 조금 희생</U>하는 대신 **fidelity**를 얻을 수 있었다. Classifier guidance의 주목적은 샘플링의 다양성보다 샘플링의 퀄리티에 대한 연구라는 것은 앞선 설명을 통해 명확해졌을 것이다.

이처럼 샘플링의 다양성과 퀄리티에 대한 trade-off는 GAN을 비롯한 generative model에서 이미 연구가 된 바가 있다. 이러한 방법론들을 ‘Low temperature sampling’이라고 하는데, 해당 용어는 <U>energy based model</U>인 볼츠만 머신에서 파생된 것이다.

Samping이 되는 prior를 에너지에 기반한 state의 집합($S(\\tau)$)이라고 생각해보자. 예컨데 에너지가 높은 상태는 불안정하기 때문에 그만큼 존재할 수 있는 state도 많아진다. 온도가  높아지면 높아질수록($\\tau \\uparrow$) 샘플링 다양성이 증가한다는 경향성과 묶어서 생각할 수 있다. 이와 반대로 에너지가 낮은 상태($\\tau \\downarrow$)는 안정적이기 때문에 그만큼 존재할 수 있는 state의 영역이 줄어든다. 샘플링 다양성이 감소하는 대신, 한정된 state에서 더 많은 샘플링을 통해 state 밀도를 높일 수 있기 때문에 더욱 그럴 듯한 샘플을 만들어내는 fidelity라는 경향성과 묶어서 생각해볼 수 있다.

이렇듯 “Low temperature sampling”은 <U>다양성을 희생하는 대신 fidelity를 높이는 전략</U>으로, truncation trick을 쓰는(feasibility가 높은 영역에서 샘플링하는 전략) 방법을 사용하거나 Glow와 같은 autoregressive model에서 부적절한 샘플들을 rejection하는 전략들을 사용하는 등이 이러한 방법론의 한 메소드로 제시가 된다.

Diffusion beat GANs라는 논문이 제시되면서 해당 문제에 대해 두 가지 접근법을 제시했으나(각 process마다 gaussian noise를 줄이는 방법/Score 예측을 줄이는 방법), 두 방법 모두 그다지 효과적이지 못했다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350640-205bdc8e-b12b-4f64-8ed1-b59488f5effd.png" width="">
</div>


Temperature가 낮아질수록 fidelity가 좋아지거나 predicion이 좋아지는 경향을 보여야하는데 전혀 그렇지 못한 것을 확인할 수 있다. 따라서 class guidance scale $s$를 통해 이를 trade-off로 조절할 수 밖에 없었다.

### Classifier 없는 guidance?

이는 마치 다음과 같다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235351438-cac45ec0-b464-4ade-b500-725a40fc9e16.png" width="">
</div>


논문에서 task를 정하는데 있어 <U>이러한 pipeline이 된 맥락</U>은 다음과 같다. Classifier guidance는 diffusion model의 학습 pipeline을 보다 복잡하게 만든다. 왜냐하면 앞서 언급했던 바와 같이 UNet 형태의 diffusion model을 학습하면서 각 time step에서의 noised sample의 downsampled feature을 토대로 classifier를 따로 학습해야하기 때문이다. 사전 학습된 classifier를 사용할 수 없다는 문제는 아무리 time step을 최소화한다고 하더라도 학습을 복잡하게 만드는 과정으로 나타난다.

또한 classifier guidance sampling은 image classifier를 속이는 형태의 gradient based adversarial attack으로 해석할 수 있다. 결국 FID나 IS와 같은 metric은 어쩔 수 없이 classifier-based metric인데, 샘플링 과정에서 classifier를 잘 속이도록(classifier 상으로 유의미한 image가 나오도록)하는 과정은 FID나 IS와 같은 <U>metric을 높이기 위한 직접적인 목적 함수</U>가 되기 때문이다. 고로 정말 classifier guidance라는 방법이 샘플링 효과를 높일 수 있는 방법이었기 때문에 FID나 IS score가 좋은게 아니라, 방법 자체가 metric을 개선시키기 좋은 환경이므로 성능을 높일 수 있지 않았나라고 판단한 것이다. 참으로 똑똑한 사람들.. 참고로 diffusion beat GANs는 OpenAI에서 쓴 논문이고 얘는 Googlebrain에서 쓴 논문이다. 이정도면 거의 세기의 대결……


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350617-fa8547b8-cacd-4eac-91bc-9f3192cf3877.jpg" width="">
</div>


### Background

학습 방법은 의외로(?) 심플하게 정리된다. 물론  기본 학습 셋팅 자체가 DDPM과는 약간 다르기 때문에 동일한 수식으로 비교하기는 애매하지만 방법에 대해서만 언급하면 다음과 같다. 학습은 continuous time diffusion model을 학습한다. 데이터셋 $p(x)$으로부터의 샘플 $x$와 정해진 하이퍼파라미터 범위$\\lambda \\in [\\lambda_{\\min},~\\lambda_{\\max}]$의 latent인 $z_\\lambda$에 대해서, forward process $q(z \\vert x)$는 variance preserving(VP) markov process로 표현할 수 있다.

$$
q(z_\\lambda \\vert x) = \\mathcal{N}(\\alpha_\\lambda x, \\sigma_\\lambda^2I), \\text{ where }\\alpha_\\lambda^2 = 1/(1+e^{-\\lambda}),~\\sigma_\\lambda^2 = 1-\\alpha_\\lambda^2
$$

Continuous한 임의의 $z_\\lambda$에 대해 위와 같이 marginal을 정의하게 되면, 인접한 latent에 대한 조건부 그래프는 다음과 같이 표현 가능하다.

$$
q(z_\\lambda \\vert z_{\\lambda^\\prime}) = \\mathcal{N}((\\alpha_\\lambda/\\alpha_{\\lambda^\\prime})z_{\\lambda^\\prime}, \\sigma_{\\lambda \\vert \\lambda^\\prime}^2) , \\text{ where }\\lambda < \\lambda^\\prime,~\\sigma^2_{\\lambda \\vert \\lambda^\\prime} = (1-e^{\\lambda-\\lambda^\\prime})\\sigma_\\lambda^2
$$

$\\lambda$를 실제로 계산하게 되면 $\\alpha_\\lambda$와 $\\sigma_\\lambda$에 대해 데시벨 단위의 SNR과 같은 맥락으로 표현이 가능하기 때문에, 이전 process의 input을 signal로서 점차 줄여가면서 더해지는 노이즈를 증가시키는 방식을 표현한 것을 알 수 있다. 이를 input $x$에 대해 조건화하여 Bayes’ rule을 사용하여 posterior로 바꾸는 과정과 이를 통해 parameterized reverse process $p_\\theta$와의 loss를 구하는 과정은 DDPM과 동일하므로 따로 언급하지는 않겠다. 결국 학습하고자 하는 네트워크는 다음과 같은 목적함수를 가진다.

$$
\\mathbb{E}_{\\epsilon, \\lambda}(\\parallel \\epsilon_\\theta(z_\\lambda)- \\epsilon \\parallel_2^2)
$$

$\\epsilon \\sim \\mathcal{N}(0, I)$이며 $z_\\lambda = \\alpha_\\lambda x + \\sigma_\\lambda \\epsilon$로 추출하게 된다. Continous function에 대한 score mathinc으로 학습이 진행된다고 보면 될 것 같다. $p(\\lambda)$가 일정하면 평소에 보는 variational lower bound 식이 되는데 저자들은 classifier guidance 논문에서 밝힌 것처럼 cosine schedule에서 아이디어를 얻어 사용했다고 한다. 해당 내용은 위에서 언급했던 바와 같이 보다 점진적으로 감소하는 noise를 구현하여 네트워크가 모든 노이즈 분포에 대해 골고루 학습될 수 있도록 하는 것이다.

$$
\\begin{aligned}
&\\lambda = -2\\log \\tan(au+b),~u \\sim \\mathcal{U}(0, 1) \\newline
&a = \\arctan (e^{-\\lambda_{\\min}/2})-b,~b = \\arctan(e^{-\\lambda_{\\max}/2})
\\end{aligned}
$$

### Classifier guidance

앞서 low temperature sampling에서 언급했던 바와 같이 GAN이나 Flow based model의 경우에 FID score와 IS 간의 trade-off를 할 수 있다는 장점이 있지만, 이를 디퓨전 모델에 가져오는 것이 상당히 힘들다고 언급했었다. 가장 주된 이유 중 하나는 prior를 만드는 과정이 diffusion process로 고정되기 때문이다. 이러한 비슷한 효과를 주기 위해 앞서 리뷰했던 classifier guidance 논문에서는 diffusion score에 noised image에 대한 classifier guidance를 주는 모델링을 통해 해결하고 하였다.

$$
\\hat{\\epsilon}_{\\theta, \\phi}(z_\\lambda, c) := \\epsilon_\\theta(z_\\lambda, c) - w\\sigma_\\lambda\\nabla_{z_\\lambda} \\log p_\\phi (c \\vert z_\\lambda) 
$$

해당 모델링에서 classifier의 영향력을 행사하는 $w$가 곧 probability의 scale factor로, log likelihood에 대해 보다 생성되는 데이터가 해당 label을 가지는 이미지 범주에 들게끔 학습시키기 때문에 diversity를 희생하고 fidelity를 높이는 기능을 한다.

$$
\\tilde{p}_{\\theta, \\phi}(z_\\lambda \\vert c) \\propto p_\\theta(z_\\lambda \\vert c)p_\\phi(c \\vert z_\\lambda)^w
$$


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350641-c99563f7-e436-48f9-bd57-fd7b2b53deb6.png" width="">
</div>


이에 대한 효과는 위와 같은 toy experiment에 대해 관찰하게 되면 더 명확하게 드러나는데, 각각의 가우시안 분포가 classifier guidance에 의해 멀어질수록 구분이 잘 되는 특징은 높아지지만 그에 비해 각 분포가 차지하는 부피는 줄어드는 것을 볼 수 있다.

### Classifier free guidance

Classifier guidance가 앞서 본 실험에서와 같이 IS와 FID 간의 trade off를 잘 보여주기는 했지만, 그럼에도 불구하고 완벽하지는 않은 low temperature sampling이며 가장 큰 문제는 image classifier로부터 자유롭지 못하다는 것이다. 저자들이 주장하는 classifier free guidance 방법은 기존의 $\\epsilon_\\theta(z_\\lambda, c)$를 $\\hat{\\epsilon}_{\\theta, \\phi}(z_\\lambda, c)$ 로 auxiliary하게 바꾸지 않더라도 classifier guidance와 같은 효과를 주고 싶게 한다는 것이다. 가장 큰 차이는 classifier parameter $\\phi$의 의존성을 없애고 싶은 것이다.

따라서 논문에서는 classifier를 사용하는 대신, unconditional diffusion model $p_\\theta(z)$ 그리고 conditional model $p_\\theta(z, c)$를 함께 학습하는 전략을 취한다. 이때 개별적인 네트워크를 구성하고 각각을 훈련시키는 것이 아닌, 두 probability 모두를 parameterize하는 방법을 생각해낸다. 그 방법은 다음과 같다.

1. Unconditional model은 class identifier $c$ 대신 $\\emptyset$을 null token으로 넣어준다. 즉, $\\epsilon_\\theta(z_\\lambda) = \\epsilon_\\theta(z_\\lambda, \\emptyset)$
2. $p_\\text{uncond}$ 만큼의 hyperparameter probability 만큼 null class sample을 생성 및 학습에 사용하여 unconditional model 학습에 사용한다.
3. Conditional과 unconditional의 weight를 다음과 같이 벡터로 조정한다. $\\tilde{\\epsilon}(z_\\lambda, c) = (1+w)\\epsilon_\\theta(z_\\lambda, c) - w\\epsilon_\\theta(z_\\lambda)$

해당 식은 classifier gradient $\\phi$에 대한 식이 전혀 포함되지 않기 때문에 기존 논문에서 했던 approximation(테일러 1차 근사)와 같은 문제에서도 해결된다. 또한 gradient를 직접 건드는 샘플링이 아니므로 adversarial attack이 아니다.

### 실험 결과


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350642-63f54983-7e25-4ea0-9cd0-43d5de9a6a11.png" width="">
</div>


완전히 unconditional이랑 conditional이랑 동일한 확률로 샘플링할 줄 알았는데 실제 결과를 보니 $0.5$가 마냥 좋지는 않아보인다. 아무튼 해당 논문에서는 총 3개의 확률에 대해 실험을 진행하였다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235350644-5713b6a9-d3c6-40c4-b0cd-d243a8273876.png" width="500">
    <img src="https://user-images.githubusercontent.com/79881119/235350645-6227a31e-e75a-4de2-b3d6-123d1f194454.png" width="500">
</div>



# 결론

총 3개의 논문에 대해 봤는데 각각 논문들이 문제시한 점이 어느 정도 이어진다고 생각해볼 수 있다. 가장 먼저 improved DDPM에서는 단순히 DDPM의 기존 방식이 왜 샘플링 성능이 좋지 않은지를 여러 요소들을 종합적으로 판단 후에 이런저런 실험을 진행한 것이 특징이라고 할 수 있을 것 같다.

그와는 별개로 classifier관련 두 논문 중 첫번째인 OpenAI의 논문은 classifier의 guidance를 사용하게 되면 GAN이나 flow based model에서 가능한 고퀄의 샘플링이 가능하다는 점에 집중했으며 그와 동시에 diffusion model architecture를 최적화하는 연구를 진행했다는 점이 contribution이 될 것 같다.

마지막으로 classifier guidance free 논문은 굳이 classifier 학습이 없이도 class condition을 주고 학습시키거나 주지 않고 학습시키는 동시 최적화를 통해 디퓨전 단일 네트워크가 unconditional diffusion model $p_\\theta(z)$ 그리고 conditional model $p_\\theta(z, c)$ 모두 학습할 수 있으며, 이를 기반으로 classifier guidance의 score estimation을 classifier parameter $\\phi$에 무관하게 구성할 수 있음을 입증하였다.
`,VO=`---
title: "ControlNet 논문 이해하기 및 사용해보기"
category: "ai papers"
publishedAt: "2023-05-01"
thumbnail: "https://user-images.githubusercontent.com/79881119/235462207-4ecbc531-2eae-4da1-a9be-b2dcfc6cca9c.png"
---


# 들어가며
ControlNet의 논문 제목 풀네임은 'Adding conditional control to text-to-image diffusion models'이다. 이른바 <U>ControlNet</U>이라고 불리는 이번 연구는 사전 학습된 large diffusion model을 어떻게 하면 **input condition**에 맞게 <U>효율적인 knowledge transfer</U>이 가능할지에 대해 논의한 페이퍼이다.  Diffusion model이라는 말이 들어갔지만 기존에 리뷰했던 디퓨전 베이스 페이퍼와는 완전히 다른 방향의 연구에 해당된다. 오히려 최근 LLM(Large Language Model)을 파라미터 효율적으로 학습하는 연구 방향인 Parameter efficient fine tuning과 연결짓는 편이 더 합리적이다. 실제로 코드를 받아서 실험해보았을 때 저자들이 제시한 ControlNet 구조를 학습시키는 과정은 서버용 GPU가 아닌 <U>개인 GPU로도 충분히 학습 가능</U>하며, 가장 눈에 띄는 장점은 ControlNet은 어떠한 input condition에 대해서도 학습이 가능하다는 점이다. 방법론으로 들어가게 되면 ControlNet의 가장 메인 포인트라고 할 수 있는 ‘zero convolution’이 등장하는데, 과연 어떠한 방식으로 input condition을 자유롭게 조정할 수 있게 되었는지 차근차근 살펴보도록 하자.


# Input condition in diffusion models

Input condition을 diffusion model에 주는 방식은 사실 이미 존재했었다. 아직 본인 블로그에서는 요즘 가장 핫한 stable diffusion의 근간이 되는 연구인 [latent diffusion 논문](https://arxiv.org/abs/2112.10752)을 따로 다루지는 않았지만 간단하게 소개하자면, 


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235462245-c6711e21-c0b8-435f-80e9-09fea31ea502.png" width="700">
</div>


예컨데 이미지를 <U>유의미한 semantic 정보만 유지</U>하고 이미지 생성에 크게 필요하지 않은 high frequency feature를 거르는 vector quantized encoder/decoder를 학습한 상태로 생각하자(즉, 이미지 $x$를 작은 크기의 resolution을 가지는 latent image로 축소한다고 생각하면 된다). 이렇게 축소된 latent를 diffusion process를 통해 복구하는 과정을 학습하는 것이 우리가 일반적으로 이해하고 있는 **DDPM** 혹은 **DDIM**의 학습 및 샘플링 프로세스이다.

우리가 기존에 살펴본 내용 중에서 attention pooling에 시간 정보와 class label 정보를 projection embedding으로 넣어주는 방법론이 있었다([diffusion process conditioning 논문 리뷰글](https://6unoyunr.github.io/blog/diffusionpapers)). 이를 확장시켜 생각하면, 만약 특정 목적을 가지고 condition을 임베딩으로 사영시킬 수 있는 task specific encoder $\\tau_\\theta$만 있다면, 각 디퓨전 모델 학습 시에 $\\tau_\\theta$를 통해 추출된 condition vector를 attention layer를 통해 조건화해줄 수 있다. 예컨데 만약 다음과 같은 이미지와 텍스트 description 쌍이 있다고 생각하면(출처 : BLIP 논문),

> Description : The car is driving past a small old building
> 


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235462200-a917f64d-6bd3-4c05-9f05-29ef14451152.png" width="400">
</div>


CLIP의 text encoder와 같은 <U>임의의 텍스트 인코더</U>를 통해 추출한 embedding을 이미지 생성 시(reverse process)에 조건부로 넣어주게 되면 해당 디퓨전 모델은 샘플링 과정에서 prior에 prompt 조건만 추가해주게 되면 text to image task를 수행할 수 있게 되는 것이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235462194-52d7d549-92c8-4f76-a8eb-a31384addbbd.png" width="800">
</div>


단순히 prompt를 통해 위와 같은 고퀄리티의 이미지를 만들 뿐만 아니라, <U>다양한 모달리티에 대한 학습된 encoder</U>만 있다면 attention pooling 조건화를 통해 diffusion process를 학습시킬 수 있다.

# Naive conditioning의 단점

이러한 방법들이 가지는 문제점은 상당히 명확하다.

첫번째로는 diffusion model이 특정 condition에 맞게 학습되려면 그만큼 score network가 <U>해당 condition을 이미지 생성에 잘 반영</U>해야하는데, 이를 달성하기 위한 **학습 데이터**가 상당히 <U>많이 필요하다는 것</U>이다. 예컨데 Vision-Language(VL) task는 멀티모달에서 활발히 연구가 되었기 때문에 CLIP, ALIGN과 같은 대량의 데이터셋이 구축되었지만 다른 모달리티(pose to image, semantic to image 등등)은 그렇지가 않다는 것이다. 실제로 LAION-5B와 같이 stable diffusion의 학습 base가 된 데이터셋에 비해서 object shape나 pose 같이 특정 목적성을 가진 데이터셋은 여러 가지 한계점 때문에 대량으로 구축하기 힘들기 때문이다. 대략 <U>수만배 정도 차이</U>가 난다.

두번째로, 이미지 생성이나 manipulation 같은 processing 과정이 대량의 데이터를 통해 솔루션을 획득하는 과정은 굉장히 리소스가 많이 든다는 점이다. 첫번째 문제였던 데이터 갯수의 차이를 극복하더라도 <U>사전 학습된 네트워크를 학습하는 것은 장벽</U>으로 작용하게 된다.

마지막으로 processing 과정은 problem 정의에 있어 그 형태의 boundary를 예측할 수 없을 정도로 다양하고, 더욱이 발전할 수 있다. 즉 한계가 없는 문제를 해결하는데 있어 greedy한 선택만 취하게 된다면(디퓨전 프로세스를 제한하거나 attention activation을 바꾸는 것) 이는 결국 고차원의 이해가 필요한 작업들(depth, pose 등등)에는 최적화가 힘들다는 것을 의미한다.  말이 조금 복잡하게 표현된 것 같은데 이를 latent diffusion의 방법론을 통해 다시 한 번 언급하자면, latent diffusion process는 사전 학습된 task specific encoder의 embedding output에 conditioning을 의존하게 되므로(embedding을 단순히 샘플링 부분에 넣어주는 과정을 통해 constraints를 줌) 보다 다양한 task에 대한 학습 과정에서 최적의 선택이 아닐 수 밖에 없다는 것이다. 고로 <U>end-to-end 학습을 할 수 있는 방법을 강구</U>해야한다. 아래의 그림과 같이 기존 방식은 conditioning part와 실제 디퓨전 모델 학습이 end-to-end가 아닌 분리된 형태를 가진다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235462202-abcf9761-fbf6-41cd-b966-3d012c7433e9.png" width="1000">
</div>



# ControlNet, end-to-end neural network

따라서 논문이 문제로 삼은 기존 conditioning의 한계점을 극복하기 위해 저자는 새로운 <U>transfer learning 구조를 제안</U>하였다.  ControlNet을 간단하게 묘사하면 다음과 같다.

- diffusion model의 parameter를 복사하여 새로운 학습 프레임워크를 원래 parameter와 병렬로 구성한다. 이를 각각 “trainable(학습 가능한)  copy”와 “locked(학습 불가능한) copy”라고 부른다.
- Locked copy는 기존 network의 성능인 이미지 생성에 필요한 representation을 유지하고 있다고 생각할 수 있다.
- Trainable copy는 conditional control을 위해 여러 task-specific dataset에 대해 학습되는 프레임워크다.
- Locked copy와 Trainable copy는 zero convolution을 통해 서로 연결된다. Zero convolution 또한 학습 가능한 레이어에 속한다.

대충만 쭉 묘사했는데 사실 이 부분은 그림을 보면 이해가 쉽다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235462204-48bf3f2f-bc94-423d-a3f7-a6f4a900382a.png" width="700">
</div>


$x$가 들어가서 $y$가 나오는 구조는 diffusion process에 접목시키게 되면 특정 시점의 noised latent vector $z_{t}$가 input으로 들어가서 다음 시점의 noised latent vector $z_{t-1}$를 예측하는 것과 같다. 회색으로 된 neural network는 원래의 diffusion model로 파라미터가 고정된 채 변하지 않게끔 하면 사전 학습된 디퓨전 모델의 <U>이미지를 만드는 성능을 해치지 않고</U> 가만히 놔둘 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235462207-4ecbc531-2eae-4da1-a9be-b2dcfc6cca9c.png" width="500">
</div>


좌측의 얼어있는 친구는 가만 놔두고 우측의 불타는 친구만 condition에 대해 학습한다고 생각하면 된다. Trainable copy이므로 fine-tuning 과정인데 원래의 parameter를 최대한 손상시키기 않겠다는 의도가 보이는 학습 구조가 된다.


# Method

그렇다면 구체적으로 어떻게 해당 학습이 효과적으로 conditioning을 할 수 있는지 수식적으로 살펴보도록 하자. 예컨데 conditioning을 하는 neural network block은 흔히 우리가 알고있는 resnet에서의 bottleneck block이나 transformer의 multi-head attention block을 생각하면 된다.

2D(이미지와 같은 형태)의 feature를 예시로 들어보자. 만약 feature map $x \\in \\mathbb{R}^{h \\times w \\times c}$가 정의되어 있다면, neural network block $\\mathcal{F}_\\Theta(\\cdot)$는 블록에 포함되는 parameter $\\Theta$를 통해 input feature map $x$를 transform하게 된다.

$$
y = \\mathcal{F}_\\Theta(x)
$$

 바로 이 과정이 앞서 그림에서 봤던 (a)에 해당된다. 이제부터 해당 parameter $\\Theta$는 잠궈놓을 것이다(학습하지 않을 것). 그리고 이를 똑같이 복사한 trainable parameter $\\Theta_c$는 잠궈놓은 친구와는 다르게 input condition $c$를 input으로 받아 학습에 사용될 것이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235462208-12e86232-15ef-4f0f-b15d-f4d66b69869a.png" width="500">
</div>


참고로 더해지는 부분에 대해서는 네트워크가 <U>activation을 저장해놓을 필요가 없기 때문에</U> 학습 시에 메모리를 $2$배로 가질 필요성도 없어진다. Backpropagation을 통해 계산된 gradient는 학습 가능한 모델에 대해서만 optimization을 진행할 것이기 때문이다.

### Zero convolution

이때 더해질 때 바로바로 이 논문에서 가장 중요한 녀석인 zero convolution이라는 개념이 사용되는데, 각 neural block의 앞/뒤로 하나씩 붙는다고 생각하면 된다. 앞/뒤에 붙는 녀석들을 각각 $\\mathcal{Z}_{\\Theta_1}(\\cdot), \\mathcal{Z}_{\\Theta_2}(\\cdot)$라고 해보자. 물론 zero-convolution은 feature map의 크기를 변화시키면 안되기 때문에 $1\\times 1$ 크기를 가지는 convolution이며 weight와 bias 모두 zero로 초기화된 상태로 학습이 시작된다.

위의 그림대로 원래의 output $y$에 conditioning 함수를 거친 output을 더하면 다음과 같다.

$$
y_c = \\mathcal{F}_\\Theta(x) + \\mathcal{Z}_{\\Theta_2}(\\mathcal{F}_{\\Theta_c}(x + \\mathcal{Z}_{\\Theta_1}(c)))
$$

여기에서 대체 왜 weight 및 bias가 $0$으로 초기화된 ‘Zero convolution’이 사용되었는지 이유가 등장한다. Zero-convolution은 weight 및 bias가 모두 $0$이므로, input에 상관없이 처음엔 모두 $0$을 output으로 내뱉는다.

$$
\\begin{cases}
\\mathcal{Z}_{\\Theta_1}(c) = 0 \\newline
\\mathcal{F}_{\\Theta_c}(x+\\mathcal{Z}_{\\Theta_1}(c)) = \\mathcal{F}_{\\Theta_c}(x) = \\mathcal{F}_{\\Theta}(x) \\newline
\\mathcal{Z}_{\\Theta_2}(\\mathcal{F}_{\\Theta_c}(x + \\mathcal{Z}_{\\Theta_1}(c))) = \\mathcal{Z}_{\\Theta_2}(\\mathcal{F}_{\\Theta_c}(x)) = 0
\\end{cases}
$$

즉 처음에는 $y_c = y$로 시작하게 된다. 해당 내용이 암시하는 것은 training이 시작되는 당시에는 ControlNet 구조에 의한 input/output 관계가 사전 학습된 diffusion의 input/output과 전혀 차이가 없다는 것이고, 이로 인해 optimization이 진행되기 전까지는 neural network 깊이가 증가함에 따라 영향을 끼치지 않는다는 것을 알 수 있다.

### Gradient flow in zero convolution

$1 \\times 1$ convolution 구조를 가지는 zero convolution에 대한 연산 과정에 local gradient를 유도할 수 있다. 예컨데 input feature map $I \\in \\mathbb{R}^{h \\times w \\times c}$가 있을때 forward pass는

$$
\\mathcal{Z}(I,; \\{W, B\\})_{p, i} = B_i + \\sum_{j}^c I_{p, i}W_{i, j}
$$

이처럼 표현되고, zero convolution은 최적화 전까지는 $W = 0, B = 0$이기 때문에 $I_{p, i}$가 $0$이 아닌 모든 point에 대해서

$$
\\begin{cases}
\\frac{\\partial \\mathcal{Z}(I; \\{W, B\\})_{p, i}}{\\partial B_i} = 1\\newline
\\frac{\\partial \\mathcal{Z}(I; \\{W, B\\})_{p, i}}{\\partial I_{p, i}} = \\sum_{j}^cW_{i,j} = 0 \\newline
\\frac{\\partial \\mathcal{Z}(I; \\{W, B\\})_{p, i}}{\\partial W_{i, j}} = I_{p, i} \\neq 0
\\end{cases}
$$

위와 같이 정리된다. Input에 대한 gradient는 $0$으로 만들지만 weight나 bias에 대한 gradient는 $0$이 아니기 때문에 학습이 가능하다. 왜냐하면 first step만 지나게 되면 Hadamard product 기호인 $\\odot$에 대해

$$
W^\\ast = W-\\beta_\\text{lr} \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathcal{Z}(I; \\{W, B\\})} \\odot \\frac{\\partial \\mathcal{Z}(I; \\{W, B\\})}{\\partial W} \\neq 0
$$

$0$이 아닌 weight를 만들기 때문에 바로 다음 step에서는

$$
\\frac{\\partial \\mathcal{Z}(I; \\{W^\\ast, B\\})_{p, i}}{\\partial I_{p, i}} = \\sum_{j}^cW^\\ast_{i,j} \\neq 0
$$

학습이 잘된다.


# Stable diffusion + ControlNet


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235462212-cafb40f1-f222-4560-9491-52d370dd512f.png" width="">
</div>


위에서 설명한 구조를 기존 stable diffusion에 구현한 구조는 위와 같다. Loss는 기존 diffusion algorithm에 task specific condition $c_f$만 추가된 형태가 된다.

$$
\\mathcal{L} = \\mathbb{E}_{z_0, t, c_t, c_f, \\epsilon \\sim \\mathcal{N}(0, 1)}\\left( \\parallel \\epsilon - \\epsilon_\\theta(z_t, t, c_t, c_f) \\parallel_2^2 \\right)
$$


# 결과

### Canny Edge


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235462216-b7291a87-32f6-4ac2-983f-b7193936fd35.png" width="">
</div>


### Hough Line


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235462217-4d045efa-eddb-4450-9ec2-92a3ad0cb070.png" width="">
</div>


### Scribble


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235462218-4986d394-2c30-4e23-b3fd-18f65c34bcb9.png" width="">
</div>


### HED edge


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235462222-f9f9d56d-ee9f-4e1c-af07-9fb29cd10880.png" width="">
</div>


### Pose


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235462224-f1f5b4d8-579f-473e-b033-aad1b955a387.png" width="">
</div>


### Segmentation


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235462226-5209cf47-c377-400e-add5-07738a63644b.png" width="">
</div>


### Depth


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235462230-9f60b1a4-d1d7-41c3-a5d8-a3b74f07af57.png" width="">
</div>


### Cartoon line drawing


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235462232-8dbc6b0e-ad70-41f6-aa4b-fb2d91c40f60.png" width="">
</div>



# Official Code로 직접 실행해보기

현재 official code는 [깃허브 소스](https://github.com/lllyasviel/ControlNet.git)로 제공되고 있다. 엥간하면 로컬 서버에서 돌아가기는 하는데 안정적으로 돌릴라면 서버에서 돌리는게 좋다. 여기다가 실행법은 올리겠지만 원본 페이지에 들어가서 ⭐ 한번씩 눌러주면 좋을 것 같다. 다음 repository를 클론 후

\`\`\`bash
git clone https://github.com/lllyasviel/ControlNet.git
\`\`\`

Conda 가상 환경을 설치해준다.

\`\`\`bash
cd ControlNet
conda env create -f environment.yaml
conda activate control
\`\`\`

그런 뒤 사용하고자 하는 모델과 stable diffusion을 [Hugging Face Page](https://huggingface.co/lllyasviel/ControlNet)로부터 다운받으면 된다. 다운받는 위치는 ControlNet/models에 stable diffusion ckpt를 넣고 detector를 ControlNet/annotator/ckpts에 넣으면 된다.

### Detector(모두) 다운받는 코드(ControlNet 레포지에서 실행)

\`\`\`bash
cd ./annotator/ckpts
curl -LO https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/body_pose_model.pth
curl -LO https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/dpt_hybrid-midas-501f0c75.pt
curl -LO https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/hand_pose_model.pth
curl -LO https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/mlsd_large_512_fp32.pth
curl -LO https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/mlsd_tiny_512_fp32.pth
curl -LO https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/network-bsds500.pth
curl -LO https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/upernet_global_small.pth
\`\`\`

굳이 다 다운받고 싶지 않으면 원하는 파일에 대한 curl만 실행하면 된다.

### Models(모두) 다운받는 코드(ControlNet 레포지에서 실행)

\`\`\`bash
cd ./models
curl -LO https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_canny.pth
curl -LO https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_depth.pth
curl -LO https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_hed.pth
curl -LO https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_mlsd.pth
curl -LO https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_normal.pth
curl -LO https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_openpose.pth
curl -LO https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_scribble.pth
curl -LO https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_seg.pth
\`\`\`

마찬가지로 굳이 다 다운받고 싶지 않으면 원하는 파일에 대한 curl만 실행하면 된다.

### 데모 버전 API 실행하기

원하는 모델을 실행하는 코드는 간단하게

\`\`\`bash
python gradio_어쩌구2어쩌구.py
\`\`\`

를 실행하면 되는데, 만약 서버컴에서 이걸 실행하고 로컬에서 접속하고 싶다면 코드를 살짝만 바꿔주면 된다. 예컨데 모든 \`gradio_어쩌구2저쩌구.py\` 파일 코드를 보게 되면 가장 마지막 줄에

\`\`\`python
block.launch(server_name='0.0.0.0')
\`\`\`

요 친구가 있는데 이걸 다음과 같이 바꿔주면 된다.

\`\`\`python
block.launch(server_name='0.0.0.0', share=True)
\`\`\`

본인은 대충 

\`\`\`python
python gradio_scribble2image_interactive.py
\`\`\`

이걸 실행해보겠다. 제대로 실행되면 다음처럼 나온다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235462235-7dd62696-6f57-4ae8-a0ee-640a2b1cf4e8.png" width="">
</div>


대강 public URL은 72시간 동안 유효하다는 뜻, 본인은 연세 vpn으로 서버컴에 접속한 상태지만 노트북으로 들어가보겠다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235462236-0d8e2563-80b2-45a8-9b61-46aa33e51c64.png" width="900">
</div>


다음과 같은 화면이 뜬다. 실제로 잘 되는지 확인해보자.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235462237-2678713e-f6ad-441a-aed9-0debfa373a47.png" width="700">
</div>


비루한 그림실력.. 힘내라 ControlNet

Run 버튼을 누르자 DDIM sampler가 동작하기 시작한다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235462239-0268dd2a-2e93-45fa-aff1-f9261b635f62.png" width="">
</div>



<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235462241-f63bdb2f-ddd6-437f-9aa0-e27094bbe81c.png" width="">
</div>


그림을 못그려도 인생 살기 큰 문제 없다는 긍정적인 희망이 생기는 논문이었다... 암튼 이렇게 하면 된다. 넉넉잡아 10기가 이상의 GPU면 다 돌아가는 듯하다.
`,HO=`---
title: "GLIDE(Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models) 논문 및 코드 리뷰"
category: "ai papers"
publishedAt: "2023-05-02"
thumbnail: "https://user-images.githubusercontent.com/79881119/235698021-b1c810ca-e900-404c-bf79-f4ae880b3c1b.png"
---


# 들어가며

최근 DDPM 수식 조지기 게시글과 더불어 DDIM 등등 여러 diffusion model 관련 논문들을 리뷰했었다. 그 중 diffusion에 condition을 추가하는 논문인 classifier guided/free 논문과 최근 가장 핫한 conditioning paper인 ControlNet 또한 다뤘었다. 이번에 소개할 논문인 [GLIDE](https://arxiv.org/abs/2112.10741) 또한 conditional diffusion에 관련된 논문이며, 기존에 소개했던 classifier 관련 논문들은 label이 존재하는 discretized category에만 적용될 수 있었던 방법이라면 GLIDE는 **text description**을  어떻게 하면 diffusion sampling에 효과적으로 **조건으로써 사용**할 수 있는지 논의한 페이퍼이다.

DALLE-2가 출시되고 난 후 부랴부랴 diffusion에 대해 알아보고 그제서야 GLIDE가 관련 논문으로 눈에 들어왔으나 그 때 당시 diffusion에 대한 지식이 전무하기도 했고 구글에 검색해봐도 그다지 **도움이 되는 리뷰글**이 없어 고생했었다. Diffusion에 대해 공부를 시작한 지 어느덧 거의 1년이 되어가는데, GLIDE는 사실 조금은 철 지난 논문이긴 하지만 어떻게 **디퓨전 모델**이 GAN과 같은 기존 generative method를 넘어설 수 있었는지 그 <U>흐름을 볼 수 있는 과정 중간에 있는</U> 좋은 페이퍼라고 생각한다.


# Diffusion model의 성장

**Diffusion model**의 가장 기본이 되는 SMLD와 DDPM는 score based generative model로 여러 관련 연구들을 파생하며 디퓨전 모델링의 가능성을 딥러닝 생성 모델 소사이어티에 널리 알려지기 시작하였다. 그러한 연구들 중 **GLIDE**는 diffusion도 GAN에서의 연구 방향처럼 언젠가는 다양한 task에 적용이 될 수 있을 것이라고 생각하고, 가장 대표적인 생성 관련 멀티모달 연구인 <U>T2I(Text to Image synthesis)</U> 분야를 파고들기 시작했다.

### CLIP 연구와 T2I

**T2I 연구**는 기존 SOTA 및 우수한 성능을 가지는 generative model을 활용하여 활발히 진행되었다. 대표적으로 이를 가속화할 수 있었던 대표주자가 transformer 구조와 이를 기반으로 한 Image/Text corresponding model인 CLIP이다. CLIP에 대한 글은 포스팅한 내용을 보면 보다 이해하기 빠를 것이다. 이를 간단하게 설명하자면,


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235698004-59d9e19f-d74a-4b51-a774-5d5616fad369.png" width="700">
</div>


**이미지**와 해당 이미지를 잘 설명하는 **caption**(text prompt)가 pair로 존재하는 대량의 데이터셋에서, 이미지와 함께 positive pair가 되는 text prompt 각각의 embedding 유사도를 높이게끔 학습하며 그와 동시에 negative pair가 되는 나머지 prompt 각각의 embedding과의 유사도는 낮아지게끔 학습한다. **Contrastive learning**을 사용하여 기존 classification task에서의 discrete label(one hot encoding label)에서 벗어나, 다양한 <U>텍스트 형태와 이미지를 연관</U>지을 수 있는 학습 구조를 소개한 것이다.

### Embedding similarity into classifier

이러한 구조적 변경을 통한 사용 가능한 텍스트의 확장은 시사하는 바가 굉장히 컸다. 예컨데 <U>사용될 수 있는 텍스트가 많아진다는 것</U>은 네트워크의 크기를 부담없이 크게 가져가면서 dataset domain $p(x)$를 확장할 수 있다는 뜻이 되고, semantic understanding의 이해가 여타의 연구들에 **snowball effect**를 가져올 수 있는 딥러닝 연구 특성상 이전과는 비교할 수 없을 정도로 <U>multimodal에 대한 연구 및 성능이 급증할 것</U>임을 암시하였다. GLIDE에서는 classifier를 사용한 디퓨전 모델의 조건화와 CLIP을 사용한 이미지와 텍스트 간의 유사도를 기반으로 한 understanding 두 연구를 통해 아이디어를 확립하기 시작했다.

### Guided diffusion models

디퓨전 모델 조건화의 대표격 연구인 [Diffusion beat GANs](https://arxiv.org/abs/2105.05233) 그리고 [Classifier free diffusion](https://arxiv.org/abs/2207.12598)는 기존 GAN 및 autoregressive modeling에서만 가능했던 다양성/퀄리티 간의 trade-off(low temperature sampling)를 디퓨전 모델에 접목할 수 있게 하였고, 디퓨전 생성 모델의 controllability 및 latent manipulation을 용이하게 할 수 있었다.

$$
\\log p_\\phi(y \\vert x_t) \\approx \\log p_\\phi(y \\vert x_t) \\vert_{x_t = \\mu}+(x_t - \\mu)\\nabla_{x_t} \\log p_\\phi (y \\vert x_t) \\vert_{x_t = \\mu} = (x_t - \\mu)g+C_1
$$

평균에 가까운 point에서 상대적으로 **curvature**이 작다고 가정할 수 있는 classifier의 score를 <U>테일러 1차 근사</U>를 통해 gradient guidance를 주는 방식인 classifier guidance는 디퓨전 모델로 하여금 샘플 퀄리티를 높일 수 있는 효과적인 방법으로 제시되었다.

$$
\\tilde{\\epsilon}(z_\\lambda, c) = (1+w)\\epsilon_\\theta(z_\\lambda, c) - w\\epsilon_\\theta(z_\\lambda)
$$

그에 대응하여 나온 연구인 classifier free 연구는 classifier guidance를 위해 classifier에 의존하는 것은 classifier를 모든 scale의 noise에 대해 학습해야하기 때문에 <U>pipeline을 복잡하게</U> 만들고, 무엇보다 classifier에 의한 gradient 학습은 결국 FID나 IS 메트릭 상 직접적인 목적 함수로 작용하기 때문에 실제 샘플링 성능을 높이는데 방법론 자체가 주된 역할을 수행하지 않는다는 비판 속에서 등장하였다.

### CLIP guidance and classifier-free guidance

저자는 이러한 두 경향(classifier를 사용하자/classifier를 사용하지 말자)에 대해 <U>모두 실험이 가능한 프레임워크</U>를 만들고, 실제로 각 방법이 **text to image sample quality**에 어떤 영향을 미치는지 관찰하였다.

다만 text description에 대해서는 단일 task에 대한 classifier를 사용할 수 없기 때문에 이를 보완하고자 CLIP score(유사성)을 사용하는 방법을 고안하였고, classifier-free guidance는 classifier의 의존성이 불필요하기 때문에 기존 ADM(Diffusion beat GANs)구조와 해당 논문에서 사용된 conditioning 방법을 사용하게 된다. 해당 내용은 뒤에서 오피셜 코드와 함께 자세히 리뷰할 예정이다.


# Diffusion model에 대한 간략한 소개

### DDPM as Score estimator

GLIDE는 이해하고 싶지만 아직 diffusion에 대해서 다뤄본 적이 없는 사람들에게는 갑자기 이상한 입실론이 나오기 시작하면 골이 땡기기 시작한다. 본인 게시글 중 DDPM 소개 및 Score based generative modeling 이해하기를 읽고 오면 좋지만, 간단하게 소개하면 다음과 같다.

일반적으로 샘플링을 하기 위해 intractable solution을 풀기 위해 가정해야하는 variational inference의 기초가 바로 샘플링이 용이한 prior $p_\\theta(z)$를 잘 설정하는 것이다. 기존 GAN에서는 단순 샘플링으로 해결하거나 VAE에서는 auto-encoder를 함께 학습하면서 KL divergence 정규화를 하는 방법을 쓰게 되는데, 이걸 diffusion model에서는 이름에서 알 수 있듯이 <U>‘diffusion process’</U>로 해결한다.

$$
q(x_t \\vert x_{t-1}) := \\mathcal{N}(x_t; \\sqrt{\\alpha_t}x_{t-1}, (1-\\alpha_t)I) 
$$

마치 향수가 공기 중에서 점차 확산해가는 운동을 Brownian motion으로 정의하는데, 해당 운동을 설명하는 방정식이 바로 stochastic differential equation이고, 이 SDE의 solution이 바로 위에서 보이는 diffusion process가 된다. 이렇게 작은 gaussian noise를 점차 더해가다보면, 충분한 시간 $t$가 지난 후에는 $x_T$가 가우시안 노이즈가 되어있는 것이다.

가우시안 노이즈를 이번에는 reverse process(역과정)를 통해 데이터를 만들고 싶다고 생각해보자. 하지만 확률 분포가 정의된 정방향과는 다르게 역방향인 $q(x_{t-1} \\vert x_t)$ 는 tractable하지 않기 때문에, 이를 예측하는 parametric function $p_\\theta(x_{t-1} \\vert x_t)$를 딥러닝을 통해 해결하고자 하는 것이다.

이를 수식화하여 정리한 것이 다음과 같은 diffusion process(DDPM)의 학습 목적함수인 simplified loss이며,

$$
L_\\text{simple} := \\mathbb{E}_{x_0 \\sim q(x_0), \\epsilon \\sim \\mathcal{N}(0, I)} \\left( \\parallel \\epsilon - \\epsilon_\\theta(x_t, t) \\parallel_2^2 \\right)
$$

이는 곧 score matching 수식화를 통해 DDPM network가 예측하고자 하는 epsilon은 score function의 normalized 버전임을 알 수 있다(증명 생략).

$$
\\begin{aligned}
&x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\left( x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}}_t} \\epsilon_\\theta(x_t, t)\\right)+\\sigma_tz \\newline
\\rightarrow~~&x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} (x_i + \\beta_i s_{\\theta^\\ast}(x_i, i)) + \\sqrt{\\beta_i}z_i
\\end{aligned}
$$

이런 베이스라인에서 variance까지 학습하고자 한 논문이 <U>improved DDPM 논문</U>이다. 위의 베이스 식에 이런저런 condition을 넣을 수 있는데, 이러한 조건화 중 diffusion model을 보다 고차원 이미지에 대해 성능을 높일 수 있었던 방법이 downsampled input $x$를 채널 단위로 concatenate하여 조건화를 시킨 <U>superresolution diffusion model</U>이다. 이 내용은 뒤이어 코드 리뷰와 함께 확인해볼 수 있다.

$$
p_{\\theta}(y_{t-1} \\vert y_t,~x) 
$$

### Guided diffusion

Noise가 더해진 각 process 단계에서의 sample에 대한 classifier $p_\\phi(y \\vert x_t)$를 사용하여 생성 과정에서 gradient를 틀어주는 방법($s$가 guidance의 power를 결정)이다.

$$
\\hat{\\mu}_\\theta(x_t \\vert y) := \\mu_\\theta(x_t \\vert y) + s \\cdot \\Sigma_\\theta(x_t \\vert y)\\nabla_{x_t} \\log p_\\phi (y \\vert x_t) 
$$

### Classifier-free guidance

앞서 소개한 방법은 noise sample에 대해 classifier를 학습시켜야 한다는 번거로움이 있다. 만약 classification에 대한 implicit classifier $p^i(y \\vert x_t)$을 가정하면, 해당 implicit classifier는 다음 비례식을 가진다(Bayes’ rule).

$$
p^i(y \\vert x_t) \\propto \\frac{p(x_t \\vert y)}{p(x_t)}
$$

log likelihood에 대한 gradient(score)를 표현하면 다음과 같이 나타낼 수 있다.

$$
\\nabla_{x_t} \\log p^i(y \\vert x_t) \\propto \\nabla_{x_t} \\log p(x_t \\vert y) - \\nabla_{x_t} \\log p(x_t) \\propto \\epsilon^\\ast(x_t \\vert y) - \\epsilon^\\ast(x_t)
$$

고로, 네트워크를 classifier conditioned sample 그리고 unconditional sample에 대해 모두 학습하면 다음과 같은 extrapolated direction을 구할 수 있다.

$$
\\hat{\\epsilon}_\\theta(x_t \\vert y) = \\epsilon_\\theta(x_t \\vert \\emptyset) + s \\cdot (\\epsilon_\\theta(x_t \\vert y) - \\epsilon_\\theta(x_t \\vert \\emptyset))
$$

### CLIP guidance

앞서 설명한 내용을 사용하여 이 논문에서 사용한 CLIP guidance에 대해 살펴보면 다음과 같다. CLIP은 image encoder $f(x)$를 통해 추출한 이미지 임베딩과 text encoder $g(c)$를 통해 추출한 텍스트 임베딩 간의 similarity를 내적을 통해 계산한다. 정확히 말하자면 cosine similarity에 해당된다.

$$
f(x) \\cdot g(c)
$$

만약 image와 text 사이에 유사도가 높다면 inner product 역시 큰 값을 가지게 된다. CLIP은 이러한 방식으로 캡션과 이미지 사이의 유사도를 측정할 수 있기 때문에, 해당 value를 일종의 classification probability로 바꿔 생각해볼 수 있다.

$$
\\hat{\\mu}_\\theta(x_t \\vert y) := \\mu_\\theta(x_t \\vert y) + s \\cdot \\Sigma_\\theta(x_t \\vert y)\\nabla_{x_t} (f(x_t) \\cdot g(c))
$$

기존 classifier guidance 논문에서 했던 방법처럼 CLIP 또한 noised image $x_t$에 대해 학습을 했고, 이를 통해 reverse process를 하는 과정에서 보다 정확한 gradient를 구할 수 있게 된다. 이를 페이퍼에서는 ‘Noised CLIP’이라 부른다.

기존에 커뮤니티에서 CLIP을 사용한 diffusion의 text 조건화에 대해 실험한 경우에는 <U>굳이 noised CLIP 없이도 fine tuning이 가능하다고 주장</U>했었지만, 그대신 data augmentation이나 perceptual loss와 같은 **추가적인 메트릭**이 필요하다. CIFAR-10C와 같은 corruption dataset이 domain shift problem과 같은 task에서 사용되는 것을 보면, noised input이 classifier로 하여금 out of distribution domain의 이미지에 해당되고, 이는 곧 제대로 된 classification에 방해가 되기 때문이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235698044-d3810eb0-0921-4997-80c4-6a3e8a866020.png" width="600">
</div>



# Related works

### Text conditional image generation

Text를 조건부로 image를 생성하는 task는 기존에도 수없이 존재해왔다. 예컨데 기존의 가장 SOTA였던 GAN을 아키텍쳐 베이스로 잡은 수많은 방법들이 captioning dataset을 기반으로 제안되었고, 비교적 가장 해당 논문과 시기적으로 가까운 DALLE의 경우 Vector Quantized learning을 베이스라인으로 삼았다. 기존 연구들은 diffusion baseline을 사용하지 않았지만, 디퓨전 모델이 발전하기 시작하면서 text conditioning 연구를 접목하려는 시도가 점차 증가하기 시작했다. 사실상 DALLE의 아이디어를 많이 참고했다고 볼 수 있는 vector quantized diffusion 연구가 diffusion based T2I 연구의 초창기에 해당된다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235698010-5d52e1aa-0efe-4246-8660-b3abd2e76c7b.png" width="600">
</div>


### Diffusion model based generation tasks

Diffusion 관련하여 text to image 이외에도 다른 형태의 task 또한 발전하기 시작했다.  [SDEdit](https://sde-image-editing.github.io/)과 같은 논문에서는 diffusion process인 SDE를 통해 단순히 inpainting과 같은 task 말고도 rough sketch(stroke) to image와 같은 conditioned task를 해결할 수 있음을 보였다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235698012-e5a084b7-69c8-4e16-9938-8b93cee78de5.png" width="600">
</div>


또한 [Palette](https://arxiv.org/pdf/2111.05826.pdf) 논문에서는 image to image translation task 각각을 목적으로 diffusion model을 학습시켰을 때 모두 성공적으로 task를 수행할 수 있음을 보였다.

### Use CLIP guidance into image generation(GAN)

GAN 모델을 기반으로 한 CLIP guided image generation 또한 활발하게 연구되기 시작했다. 예컨데 이전에 리뷰했던 논문들을 포함하여 StyleCLIP(StyleGAN + CLIP), BigSleep(BigGAN + CLIP) 그리고 StyleGAN-NADA와 같은 내용이 바로 이에 해당된다. GAN과 CLIP guidance를 어떻게 엮어서 사용할 수 있었는지에 대한 자세한 내용은 해당 게시글에 정리되어있다. [LAFITE paper](https://arxiv.org/pdf/2111.13792.pdf)에서는 CLIP text embedding에 조건화를 위해 사전 정의된 노이즈로 perturb한 CLIP image embedding을 사용하여 GAN model을 학습하는 방식으로, 고퀄리티의 image/text pair dataset이 없더라도(language free) text to image 조건화가 가능하다는 연구를 진행한 바 있다.

### Use CLIP guidance into image generation(Diffusion)

그러나 물론 diffusion에도 CLIP guidance를 접목하고자 한 시도가 GLIDE 논문이 처음은 아니었다. Crowson이 커뮤니티에서 공개한 [코랩 코드](https://colab.research.google.com/drive/12a_Wrfi2_gwwAuN3VvMTwVMz9TfqctNj)라던지, DDIM reconstruction process 과정에서 CLIP loss를 따라가게끔 diffusion model을 fine tuning한 [DiffusionCLIP](https://arxiv.org/pdf/2110.02711.pdf) 연구가 진행되었다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235698017-8e20d77b-42a5-4cbb-b307-022372c776e3.png" width="700">
</div>


### Text based image editing

[Paint by Word](https://arxiv.org/pdf/2103.10951.pdf)나 [Diffusion based image editing](https://arxiv.org/pdf/2111.14818.pdf)  논문에서는 CLIP을 사용하여 GAN이나 Diffusion model로 하여금 이미지를 원하는 prompt에 맞춰 변환을 할 수 있도록 학습사는 방법을 소개하였다.


# Training 방법

GLIDE는 diffusion을 사용한 T2I를 다룬 논문이므로 학습법을 이해하는 것이 매우 중요하다. 가장 메인이 되는 실험에서는 일반적인 크기의 $64 \\times 64$ image에 대해 대용량(3.5B)의 diffusion  model(+text condition)을 학습시키고, 이에 추가로 대용량(1.5B)의 upsampling diffusion model($64 \\times 64 \\rightarrow 256 \\times 256$)을 학습하였다. 또한 CLIP guidance를 위해 앞서 말했던 것처럼 $64 \\times 64$의 image에 대해 ViT-L CLIP 모델을 noised input과 함께 학습하였다.

### Training text conditional diffusion models

이 논문에서는 Diffusion beat GANs 논문에서 밝힌 ADM model을 사용하였다. 해당 구조를 서칭하는 과정은 diffusion 관련 논문들을 정리한 게시글에 있지만 결론부터 가져오자면,

- 각 resolution마다 2개의 residual block(BigGAN)을 가지며, width도 resolution에 맞게 조정됨
- Attention head마다 $64$의 channel 수를 가지는데, resolution $32, 16, 8$에 모두 attention layer가 있음
- BigGAN residual block을 upsampling, downsampling할 때 사용하며 AdaGN이 들어가서 timestep과 class embedding을 넣어줌

이렇게 된다. 이때 기존 ADM에서는 학습 과정에 class embedding과 time embedding을 attention으로 넣어주는 과정을 통해 class conditioning을 진행했는데, GLIDE에서는 해당 부분을 transformer로부터 나오는 text embedding으로 대체하였다. 이 부분은 논문에서 설명이 충분치 않기 때문에 github official code와 함께 보도록 해보자.

### Transformer를 통해 text embedding 뽑아내기

\`\`\`python
xf_in = self.token_embedding(tokens.long())
xf_in = xf_in + self.positional_embedding[None]
if self.xf_padding:
    assert mask is not None
    xf_in = th.where(mask[..., None], xf_in, self.padding_embedding[None])
xf_out = self.transformer(xf_in.to(self.dtype))
if self.final_ln is not None:
    xf_out = self.final_ln(xf_out)
xf_proj = self.transformer_proj(xf_out[:, -1])
xf_out = xf_out.permute(0, 2, 1)  # NLC -> NCL

outputs = dict(xf_proj=xf_proj, xf_out=xf_out)
\`\`\`

\`Text2ImUNet\` 클래스에 속한 메소드 중 \`get_text_emb\`에 대한 내용이다. \`xf_in\`이 transformer에 들어가는 prompt input을 tokenize 및 임베딩화 + positional encoding으로 바꾸게 되고 모든 transformer 모듈 연산이 끝난 후 실제로 conditioning에 사용되는 것은 output의 가장 마지막 token에 대한 embedding(\`xf_out[:, -1]\`)를 dimension에 맞게끔 projection한 결과가 된다. Output의 가장 마지막 token이 가지는 의미는 다음과 같다.

\`\`\`python
tokens = [self.start_token] + tokens[: text_ctx - 2] + [self.end_token]
\`\`\`

Transformer tokenizer는 인코딩 과정에서 BPE를 사용하게 되는데, 이때 시퀀스를 같은 길이로 맞춰주면서 가장 뒷부분에 시퀀스의 마지막을 의미하는 \`<EOS>\`를 추가해준다. 따라서 ViT에서 가장 앞부분에 class token을 추가한 뒤 encoder를 통과시켜 해당 token의 feature map을 class에 대한 정보로 사용했던 것처럼, Text embedding 또한 transformer를 통과한 뒤 \`<EOS>\` 토큰에 남은 나머지 시퀀스에 대한 attention 정보를 사용하면 이는 곧 전체 text를 요약한 feature로 해석할 수 있다.  즉, class embedding 대신 conditioning에 사용될 수 있게 된다.

### UNet module conditioning 하기

\`\`\`python
emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))
if self.xf_width:
    text_outputs = self.get_text_emb(tokens, mask)
    xf_proj, xf_out = text_outputs["xf_proj"], text_outputs["xf_out"]
    emb = emb + xf_proj.to(emb)
\`\`\`

실제 모델 포워딩(UNet)에서 사용하는 과정을 보게 되면, 위와 같은 방식으로 추출한 \`xf_proj\`와 feature dimension과 token index의 위치는 permute한 \`xf_out\`를 사용한다.

\`xf_proj\`는 시간에 대한 정보인 timestep embedding과 더해지고, \`xf_out\`는 그대로 모듈에 들어가게 된다.

\`\`\`python
for module in self.input_blocks:
    h = module(h, emb, xf_out)
    hs.append(h)
h = self.middle_block(h, emb, xf_out)
for module in self.output_blocks:
    h = th.cat([h, hs.pop()], dim=1)
    h = module(h, emb, xf_out)
\`\`\`

module이라고 되어있는 부분은 상속된 상위 클래스인 \`UNet\` 에서 확인할 수 있듯이 TimestepBlock 혹은 AttentionBlock으로 이어지게 된다.

\`\`\`python
class TimestepEmbedSequential(nn.Sequential, TimestepBlock):
    def forward(self, x, emb, encoder_out=None):
        for layer in self:
            if isinstance(layer, TimestepBlock):
                x = layer(x, emb)
            elif isinstance(layer, AttentionBlock):
                x = layer(x, encoder_out)
            else:
                x = layer(x)
        return
\`\`\`

레이어의 속성에 따라 사용되는 text condition이 서로 다른 것을 볼 수 있는데, 예컨데 ResBlock과 같이 TimestepBlock을 상속 클래스로 하는 녀석들은 time+text embedding(\`xf_proj\`) 정보를 받아서 group normalization할 때 사용하고(기존 ADM과 똑같다),

\`\`\`python
if self.use_scale_shift_norm:
    out_norm, out_rest = self.out_layers[0], self.out_layers[1:]
    scale, shift = th.chunk(emb_out, 2, dim=1)
    h = out_norm(h) * (1 + scale) + shift
    h = out_rest(h)
else:
    h = h + emb_out
    h = self.out_layers(h)
\`\`\`

AttentionBlock을 클래스로 하는 녀석들은 모든 text token information을 key/value로 cross-attention에 사용하여 query의 주체가 되는 noised image \`x\`가 text 정보랑 잘 엮일 수 있게 연산된단.

\`\`\`python
def forward(self, x, encoder_out=None):
    b, c, *spatial = x.shape
    qkv = self.qkv(self.norm(x).view(b, c, -1))
    if encoder_out is not None:
        encoder_out = self.encoder_kv(encoder_out)
        h = self.attention(qkv, encoder_out)
    else:
        h = self.attention(qkv)
    h = self.proj_out(h)
    return x + h.reshape(b, c, *spatial)
\`\`\`

그리고 구조 설명에서 말했던 바와 같이 resolution $32, 16, 8$에 모두 attention layer가 있다고 했기 때문에 각각의 attention map에 맞게 projection되어(\`self.qkv(self.norm(x).view(b, c, -1))\`) key/value 역할을 수행한다.

### Training dataset and architecture

Dataset은 DALL-E에서 사용한 것과 동일한 데이터셋으로 학습하였다. 다만 Diffusion beat GAN 논문에서는 width를 샘플링 효율 대비 연산량 때문에 크게 증가시키지 않았지만 이 논문에서는 보다 풍부한 text 정보를 담기 위해 과감하게 $512$ 채널로 증가시켜버린다. Transformer의 경우 채널 수가 $2048$인  $24$개의 residual block을 사용하였고, 이 두 개(UNet + Transformer) 파라미터를 총 합한 것이 3.5B가 된다.

이에 추가로 upsampling diffusion model은 마찬가지로 같은 conditioning을 사용하는데, 이미지 생성 UNet보다는 적은 규모의 transformer($2048 \\rightarrow 1024$)를 사용하게 된다. 이외의 training detail은 논문에 있기 때문에 따로 언급은 하지 않겠다.

### Classifier free guidance

위의 구조는 transformer output에 대해 text conditioning만 해줄 뿐 따로 CLIP guidance나 classifier free 방법이 들어간 것은 아니다. 사전 학습이 모두 끝난 뒤에, $20\\%$의 비율에 해당되는 text token sequence를 empty sequence(NULL = $\\emptyset$)로 바꾸어 text에 대해 unconditional representation도 함께 학습시킨다(fine-tuning). 이러한 방법을 통해 기존 classifier free guidance 논문에서처럼 네트워크는 unconditional diffusion model $p_\\theta(z)$ 그리고 conditional model $p_\\theta(z, c)$ 두 확률분포를 모두 가지게 된다. 다만 여기서 $c$가 class embedding에서 text embedding으로 바뀌었다고 생각하면 된다.

### Image inpainting

앞서 related에서 설명했듯이 [Palette](https://arxiv.org/pdf/2111.05826.pdf) 논문에서는 image to image translation task 각각을 목적으로 학습했다고 하였다. 이외의 논문들은 직접 diffusion model을 해당 task를 목적으로 학습시키지 않았는데, 이렇게 단순히 다른 목적으로 학습된 diffusion forward로 perturbed input을 넣어 noise로 만든 다음 복구하는 작업을 취하게 되면 perturbed edge 부분이 불연속적이고 artifact가 발생한다는 문제가 있다.

따라서 이 논문에서는 Palette 논문과 유사하게 inpainting에 대해 학습시키게 된다.  Impaint에 사용된 UNet 구조를 보면 포워딩이 다음과 같이 진행된다.

\`\`\`python
def forward(self, x, timesteps, inpaint_image=None, inpaint_mask=None, **kwargs):
    if inpaint_image is None:
        inpaint_image = th.zeros_like(x)
    if inpaint_mask is None:
        inpaint_mask = th.zeros_like(x[:, :1])
    return super().forward(
        th.cat([x, inpaint_image * inpaint_mask, inpaint_mask], dim=1),
        timesteps,
        **kwargs,
    )
\`\`\`

x(마스킹 안된 원래의 이미지)에 x를 고대로 복사한 RGB에 masking을 적용한 이미지를 concat한다. 여기에 추가로 inpaint mask를 붙인 총 $7$개의 channel input이 사용된다고 생각하면 된다. (RGB(원래 이미지) + RGB(가려진 이미지) + Mask).

Input의 channel의 갯수가 바뀌기 때문에 이에 따라 받아들이는 module의 channel 수도 달라지게 되는데, fine-tuning이기 때문에 원래 $3$개의 채널은 사전 학습된 녀석을 가져오고 나머지 channel은 $0$으로 초기화했다고 생각하면 된다. 이에 상응하는 upsampling model은 fine tuning 과정에서 low-resolution 이미지의 전체(원본 RGB)를 넣어주되, high resolution image는 masking된 부분을 제외하고 넣어주었다(아래 코드 참고).

\`\`\`python
def forward(self, x, timesteps, inpaint_image=None,
			      inpaint_mask=None, low_res=None, **kwargs):
    if inpaint_image is None:
        inpaint_image = th.zeros_like(x)
    if inpaint_mask is None:
        inpaint_mask = th.zeros_like(x[:, :1])
    _, _, new_height, new_width = x.shape
    upsampled = F.interpolate(
        low_res, (new_height, new_width), mode="bilinear", align_corners=False
    )
    return super().forward(
        th.cat([x, inpaint_image * inpaint_mask, inpaint_mask, upsampled], dim=1),
        timesteps,
        **kwargs,
    )
\`\`\`

즉, 기존 upsampling module이 $256 \\times 256$  크기의 noised $x_t$와 $64 \\times 64$  GT를 bicubic으로 upsampling한 녀석을 concatenate해서 프로세스를 진행했다면, inpainting의 경우 두 concatenate 사이에 inpainting된 high resolution GT를 mask와 함께 조건부로 넣어준다.

### Noised CLIP models

Diffusion beat GANs 논문에서는 보다 정확한 classifier guidance를 위해 classifier를 각 noise input에 대해 학습했던 것과 같이, GLIDE에서는 CLIP guidance를 위해 CLIP model을 noised image $x_t$에 대해 학습하게 된다.


# 결과

### CLIP guidance vs. Classifier-free guidance


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235698021-b1c810ca-e900-404c-bf79-f4ae880b3c1b.png" width="700">
</div>


정성적으로 나온 결과는 위의 그림과 같다. 보다 디테일이 잘 살아있고, artifact나 부자연스러운 부분이 CF Guide가 더 적은 것을 확인할 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235698028-34cbeb76-9949-45e8-b0fe-4b39afb04e0f.png" width="700">
</div>


메트릭 평가에서도 대체로 Classifier-free guidance가 더 좋다는 결과가 나온다. Precision/Recall을 보게 되면 Precision 파트에서 CLIP guidance는 꼬랑지가 휘는 걸 볼 수 있는데, 앞서 리뷰했던 논문에서 밝힌 것과 같이 Recall을 희생하는 만큼 Precision이 오르는 것이 정상적인 trade-off 경향성이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235698031-bf332432-d596-4cfe-99e5-5cf16734508d.png" width="700">
</div>


결국 precision이 제대로 증가하지 않았다는 것은 그만큼 $P_r$에 $P_g$가 포함되지 않는다는 소리며, 이는 곧 다양한 text prompt를 커버할 수 없는 샘플링이라고 할 수 있다. IS/FID 역시 Classifier free guidance 기준으로 더 넓은 범위를 커버하는 것을 볼 수 있으며, 가장 인상깊은 결과는 CLIP score(유사도)에 따른 FID 경향성이 CLIP으로 직접 guidance를 주는 것보다 classifier free로 가는게 더 효과적인 것을 알 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/235698035-f6822431-04d9-42e2-9ebf-7a695732f510.png" width="500">
    <img src="https://user-images.githubusercontent.com/79881119/235698039-b32dd194-cb2a-4f15-8cbc-d2f9cb5b5ea0.png" width="500">
</div>


사람이 평가한 Elo score 상으로도 Classifier free guidance가 상당히 높은 점수를 획득하였으며, 사실적 이미지(photorealism)와 캡션 유사도(Caption similarity) 측면에서 DALL-E 보다도 좋은 성능을 보였다. DALL-E에 reranking(CLIP 점수를 이용하여 좋은 샘플을 우선시하여 picking하는 것)을 적용하고 GLIDE는 reranking을 적용하지 않고 단순 비교를 한 결과이며 $\\%$는 승률을 의미한다. 마지막 row는 GLIDE의 output에 DALL-E의 d-VAE를 적용한 구조가 된다.
`,qO=`---
title: "MERU(Hyperbolic Image-Text Representations) 논문 리뷰"
category: "ai papers"
publishedAt: "2023-07-12"
thumbnail: "https://github.com/user-attachments/assets/26b67f08-0036-4e9d-936d-dd1ca659a7f4"
---


# 들어가며…

논문을 소개하기 전, **CLIP**과 **ALIGN**과 같은 기존 **Vision-Language Modeling**의 문제점을 짚는 것이 우선이다. 만약 두 논문에 대한 사전 지식이 없다면 **MERU** 라는 이름을 가지는 이 모델이 문제시하고자 했던 유클리디안 space(모든 datapoint에 대해 동일한 거리 기준을 삼는 것)에 대한 이해를 하기 힘들기 때문에 적어도 본인 블로그의 CLIP 논문에 대한 내용을 짚고 넘어오는 것이 좋다.

유클리디안 기하학과 비유클리디안 기하학은 다른 거리 기준을 가진다. 예컨데 유클리디안(이를 보통 평평한 평면에 비유하기도 함) 공간에서는 삼각형의 세 내각의 합이 $180^\\circ$ 인 것이 당연하지만, Spherical 공간에서는 성립하지 않는 것을 볼 수 있다. 마찬가지로 Modality가 놓인 공간이 얼마나 휘어있느냐(이를 하이퍼볼릭에서는 **Curvature**라는 값으로 정의함)에 따라 각 임베딩 간의 관계성이 다르게 성립한다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/044c8c53-8fcd-4392-9324-c29a4e3e0507" width="700">
</div>


논문에서는 다행(?)인지 모르겠지만 수식을 방대하게 풀어놓는 형태로 우리를 괴롭히지는 않는다. 다만 단순하게 요약해서 ‘이미지/텍스트’ 간의 관계성을 설명하는 기하학은 **Euclidean**이 아닌 **non-Euclidean**이 보다 적합하다는 것. 그렇다면 대체 어떤 이유에서 기존 VLM과 다르게 다른 기하학을 도입하고자 하였는지 살펴보도록 하자.


# 텍스트와 이미지의 계층적 관계

예컨데 다음과 같은 이미지가 있다고 생각해보자. 


<div align="center">
    <img src="https://github.com/user-attachments/assets/1de1e8fe-7f13-4fb6-bd7f-eb4325c4f0e3" width="300">
</div>


길거리에 나가 $n$명의 사람들에게 “이 그림을 한 문장으로 요약해주시거나, 느낀 점을 말씀해주세요.” 라고 부탁해보자. 도를 아십니까로 착각하여 내치고 지나가지 않는 이상 $n$명의 사람들은 서로 다른 대답을 할 것이다. 예컨데 다음과 같은 후보군이 있을 수 있다.

- 강아지와 고양이 사진입니다.
- 고양이가 강아지한테 덤비고 있네요.
- 너무 귀여운 사진이네요.

사람이 이미지와 텍스트를 인지하는 과정은 말하지 않더라도 자연스럽다. 이미지와 텍스트와의 관계성을 고려할 때 우리는 내재된 ‘계층적 관계’를 이해할 수 있다는 것이다. 위에서 예시로 든 첫번째 문장과 두번째 문장을 비교해보자. ‘강아지와 고양이 사진입니다.’라는 설명은 단순히 이미지에 속한 semantic한 정보만을 고려한다. 말 그대로 이미지에 ‘강아지’와 ‘고양이’라는 객체가 포함되어 있다는 정보만 줄 뿐 두 객체의 관계성에 대해서는 설명하지 않는다. 그럼에도 불구하고 ‘강아지와 고양이 사진입니다.’는 이미지를 잘 설명하는 텍스트에 해당되고, 우리가 설계하고자 하는 이미지/텍스트 간의 관계성 메트릭 공간에서는 서로 가까운 거리를 유지해야할 것이다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/0d9ab3a9-6554-416b-913c-9e986afc5706" width="300">
</div>


 ‘고양이가 강아지한테 덤비고 있네요.’라는 문장은 단순히 객체만을 묘사하지 않고 두 객체 간의 관계에 대한 정의를 내리고 있다. 실제로 이미지만 보고서 고양이가 강아지한테 덤비고 있다는 사실 여부까지는 확인할 수 없지만, 적어도 ‘고양이가 강아지한테 덤비고 있네요.’라는 문장이 해당 이미지를 묘사할 수 있는 합리적인 결과물이라고 인지할 수 있다. 마찬가지로 우리가 설계하고자 하는 이미지/텍스트 간의 관계성 메트릭 공간에서는 서로 가까운 거리를 유지해야할 것이다. 위의 그림 상에서 초록색으로 표시된 부분이 곧 그 관계성을 표현하는 부분이라고 할 수 있다.

‘그래서 하고픈 말이 뭔데?’라고 물어볼 당신을 위해 마지막 문장을 예시로 들면서 언어의 계층적 구성에 대해 언급하고자 한다. ‘너무 귀여운 사진이네요’라는 표현 자체는 큰 문제가 없다. 그나마 문제시될 만한 점은 고양이랑 강아지를 그다지 귀여워하지 않는 사람도 있다는 것인데, 그런 사소한 취향 차이는 무시하고 고양이랑 강아지는 무조건 귀여운 존재라는 가정 하에 각 표현력이 가지는 정보를 계층적으로 구성하면 다음과 같다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/11646a36-d907-41c6-8d0a-f74afe8ae0a5" width="700">
</div>


텍스트가 묘사하는 내용이 보다 이미지에 특화될수록(이미지에 잘 부합할수록), 그만큼 이미지에서 디테일하게 볼 semantic한 정보에 대한 공간력의 크기 또한 달라지게 되는 것이다. 이걸 단순히 유클리디안 공간에서 표현하게 되면 이미지를 묘사하는 모든 문장들과의 관계성이 cosine similarity(벡터의 각도)에 대해서만 정의되기 때문에 계층적으로 구성된 임베딩을 전혀 고려하지 않게 된다.  

왜냐하면 기존 VLM이 baseline으로 가져가는 contrastive learning objective에는 거리 메트릭이 **오직 embedding 유사성에만 의존**하기 때문이다.


# 하이퍼볼릭 공간에 대한 짧은 Recap


<div align="center">
    <img src="https://github.com/user-attachments/assets/d2d1694b-ab09-408a-a2d3-840c97d39faf" width="800">
</div>


사실 비유클리디안 모달리티를 표현하기 위한 하이퍼볼릭 geometry에 대한 연구는 이전에도 꾸준히 진행되었다. 계층적 임베딩을 학습하기 위해 정의한 푸앙카레 disk(혹은 확장한 ball)에 대해 정의한 paper([참고 링크](https://papers.nips.cc/paper/2017/file/59dfa2df42d9e3d41f5b02bfc32229dd-Paper.pdf))을 먼저 살펴볼 수 있다. 푸앙카레 곡면이 가지는 특징은 우리가 매핑하고자 하는 쌍곡면(하이퍼볼릭) 공간의 모든 점을 해당 곡면 상으로 projection이 가능하다는 점이다. 사영할 때의 관계성만 정의된다면(각각의 점을 Analytic한 위상 함수의 $X, Y$ 위상 각각의 집합이라고 보면 된다) 임베딩을 푸앙카레 곡면에서 이해할 수 있다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/f0f7697a-4661-41bb-ace6-70514de129a7" width="700">
</div>


쌍곡면 공간의 이점은 곧 비유클리디안 모달리티의 대표적 형태인 ‘그래프’의 왜곡과 관련된다([도움될만한 링크](https://chumji.tistory.com/3)). 이에 대한 내용은 [‘Curvature Regularization to Prevent Distortion in Graph Embedding’](https://arxiv.org/abs/2011.14211)에 잘 정리되어있는데, 간단하게 보자면 위의 그림에서 **A**와 같은 노드/엣지 관계를 가진  데이터를 고려해보자. 단순히 파란색으로 표현된 노드와 빨간색으로 표시된 노드에 대한 node classification 이외에도, 각 노드들의 연결성(어떤 노드와 엣지로 관련을 가지는지)에 따라 각각의 모달리티는 특정 표현자(embedding)를 가지면서 학습될 것이다. 기존 그래프 구조의 학습법의 경우 proximity preserving(인접한 노드일수록, 임베딩 유사성을 높게 가져간다)라는 목적 함수를 가지기 때문에 인접한 노드 간의 유사성 매핑과는 별개로 동떨어진 노드 간의 representation에 대해서는 아무런 고려를 하지 못한다(노드가 넘어갈수록 diffusion process와 같은 random walk를 생각하면 된다). 결론적으로 모든 노드들의 확률 그래프는 central limit theorem에 의해 가우시안 분포를 따르게 되어, 얼추 **B1**에서 보이는 것과 같이 둥그스럼한 형태의 임베딩 space를 구성하게 된다. 노드 간의 유사성을 고려한 mapping 방법 자체는 크게 잘못되지는 않았으나 기존에는 멀었던 노드들의 관계성이나 그래프 자체의 구조를 전혀 고려하지 못한다는 문제가 발생한다.

B1에서의 mapping법(Proximity Preserving을 목적으로 학습된 경우)와 B2에서의 mapping 법에 대해 distance를 기준으로 비교하면, **Geodesic distance**와 **Shortest path distance**는 두 경우 모두 유의미한 양의 상관관계를 가지지만(그림 **C**), **Euclidean distance**에서는(그림 **D**) 경향성이 무너지는 것을 확인할 수 있다. 하지만 Oracle Embedding으로 가정한 매핑 방법이 완벽하게 존재할 수는 없고, 임베딩 공간을 objective function으로 수렴시키고자 하는 딥러닝 메소드에서는 이런 학습법을 찾을 수 없는 것이 당연하다. 바로 이러한 **학습법의 제약을 위상에 대한 제약으로 바꾸어 학습하고자 하는 것이 하이퍼볼릭 임베딩 리만 학습법이다**.


### Riemannian manifolds(리만 다양체)

부드러운 다양체는 일종의 부드러운 천을 생각하면 생각하기 편하다. 부드러운 천이 평평한 바닥에 놓여있다면 이는 곧 유클리디안 space에서의 리만 다양체가 될 것이고, 그렇지 않다면 일반화된 space 상에서의 다양체가 된다.  다양체라고 하니까 기분이 좀 이상하네 걍 manifold로 쓰는게 나을듯.

암튼 Smooth surface에 대한 Manifold를 생각해보면 결국 2차원의 sheet가 이리저리 얽혀있다고 보면 되고, 우리가 특정 함수를 해석학적으로 읽을 때의 느낌과 비슷하게 smooth surface 또한 Locally Euclidean(아주 작은 양수 $\\epsilon$에 대해서 manifold를 확장시키면 해당 공간은 로컬 좌표계에서 유클리디안 모달리티를 가질 수 밖에 없다. $d\\vec{x}*d\\vec{y}$ 느낌으로다가)


<div align="center">
    <img src="https://github.com/user-attachments/assets/ff4f14fa-cd8e-4bee-832a-5cb13b7cef20" width="300">
</div>


그렇다면 Riemannian manifold를 정의하기 위해서는 딱 두 가지만 있으면 되는데 그게 바로 위에서 언급한 smooth manifold $\\mathcal{M}$ 그리고 manifold 상에서 거리를 잴 수 있는 metric $g$이다. 고로 이 메트릭을 Euclidean inner product로 정의해버리면 그 Riemannian manifold는 곧 유클리디안 space로 수렴하는 것. 바로 **기존 방식인 CLIP 학습법**이 **manifold는 implicit**하게 가져가면서 metric $g$(이미지 임베딩과 텍스트 임베딩 간의 거리)를 Euclidean inner product(**코사인 유사도**)로 정의해버린 것과 같다.

하지만 제목에서부터 알 수 있듯이 이 논문의 목적은 하이퍼볼릭 space이고 하이퍼볼릭 space의 특징은 manifold가 ‘constant negative curvature’를 가진다는 점이다. 앞서 이런저런 소개를 통해 여러 모델링을 언급했지만 MERU 페이퍼에서는 차원을 하나 확장시켜서 해당 위상의 부분 위상으로서 정의되는 로렌츠 모델(Lorentz model)을 기반으로 한다.


### 로렌츠 모델에 대한 하이퍼볼릭 공간적 정의


<div align="center">
    <img src="https://github.com/user-attachments/assets/b2e0de57-e031-484c-843f-0ce89034e1f4" width="">
</div>


MERU는 로렌츠 모델을 기반으로 한다. 로렌츠 모델을 머리 속에 그릴 때는 고등학교 때 배웠던(요즘도 배우나..?) 이차 곡선 중 쌍곡선에 대한 그래프를 $x$축을 기준으로 회전시켰다고 보면 된다. 이때의 $x$축이 additional axis을 의미하게 되며, 해당 공간 상의 텐서를 \`permute\` 시키게 되면 위에서 보는 것과 같이 $\\mathbb{R}^{3}$ 공간에서의 두 smooth manifold 중 upper half를 볼 수 있게 된다($z < 0$인 부분은 무시). 이때 hyperboloid의 중심이 되는 축(대칭축이라고도 부른다)인 $z$를 기존 수학 및 물리학에서 정의하는 것과 같이 시간에 대한 차원(time dimension)으로 정의할 수 있고, 이를 제외한 나머지 $n$개의 차원은 공간에 대한 차원(space dimension)으로 정의할 수 있다. 고로 위상의 모든 벡터는 $\\rm{x}_\\text{space} \\in \\mathbb{R}^n$이고 $x_\\text{time} \\in \\mathbb{R}$인 좌표 $[\\rm{x}_\\text{space}, \\it{x}_\\text{time}]$로 벡터를 표현할 수 있다. 모든 수식에 서 로마 문자($\\rm{x}, \\rm{y}$)로 표현된 친구들은 2차원 이상의 좌표계를 가지는 벡터를 의미하고 이텔릭($x, y$)로 표현된 친구들은 1차원의 스칼라 값을 의미한다. Riemannian manifold를 구성하기 위해 필요한 smooth sheet는 마련되었고, 남은 건 거리 메트릭이다. 거리 메트릭은 Euclidean inner product인 $\\left< \\cdot, \\cdot \\right>$에 대해 다음과 같이 정의된다.

$$
    \\left< \\rm{x}, \\rm{y} \\right>_\\mathcal{L} = \\left< \\rm{x}_\\text{space},\\rm{y}_\\text{space} \\right> - x_\\text{time} y_\\text{time}
$$

마찬가지로 로렌츠 norm은 inner product에 대해 $\\parallel \\rm{x} \\parallel_\\mathcal{L} = \\sqrt{\\vert \\left< x, x \\right>_\\mathcal{L} \\vert}$ 로 유도된다. 고등학교 수학을 열심히 들었다면 쌍곡선의 특성에 대해서 잘 알고 있겠지만, 이차곡선의 방정식을 전미분하여 curvature를 구할 수 있고 해당 curvature에 해당되는 음의 scalar value $-c$에 대해 로렌츠 모델 상의 모든 벡터 조건(constraints)을 다음과 같이 줄 수 있다.

$$
    \\begin{aligned}
    &\\mathcal{L}^n \\{ \\rm{x} \\in \\mathbb{R}^{n+1}: \\left< x, x \\right>_\\mathcal{L} = -1/c,c > 0 \\} \\newline
    &x_\\text{time} = \\sqrt{1/c + \\parallel \\rm{x}_\\text{space} \\parallel^2}
    \\end{aligned}
$$

이제 로렌츠 모델링을 통한 리만 다양체를 구성하는 과정이 거의 마무리되었다.


# 로렌츠 모델링에서의 거리 메트릭

### Geodesics.

Geodesic은 다양체 내부의 두 점에 대해 가장 짧은 path를 의미한다. 

질량이 없는 빛이 중력 가속도의 영향을 받아 휜다는 상대성 이론의 설명에서 휘어진 시공간을 통해 빛이 진행하는 얘기를 들어본 적이 있을 것이다. 우주 공간을 하나의 리만 다양체로 생각하고 만약 이 다양체가 여러 천체의 상호작용에 의해 휘어진 천과 같이 구성되어 있다면 직진하는 성질을 가진 빛은 사실 직진하는 성질을 가지는 것이 아니라 목표 지점까지 최단 루트로 가는 것과 같다. 바로 이렇게 빛이 그리는 궤도를 Geodesics라고 이해해볼 수 있다. 로렌츠 모델에서의 Geodesics는 hyperboloid와 두 벡터 및 원점이 그리는 평면의 intersection curve에 해당된다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/fa0c1b6b-008b-4e08-aaf6-7cbc520a6992" width="700">
</div>


이를 유도해서 풀면

$$
    d_\\mathcal{L}(\\rm{x}, \\rm{y}) = \\sqrt{1/c} \\cdot \\cosh^{-1}(-c \\left< \\rm{x}, \\rm{y} \\right>_\\mathcal{L})
$$

### Tangent space.

로렌츠 공간의 spacial vector $z$에 대해 tangent space는 해당 벡터와 Lorentzian inner product(앞에서 정의했던거)가 $0$이 되는 모든 모든 벡터가 모인 공간(span)이다. 해당 공간은 Euclidean space가 된다.

$$
    \\mathcal{T}_{\\rm{z}} = \\{ \\rm{v} \\in \\mathbb{R}^{n+1}: \\left< z, v\\right>_\\mathcal{L} = 0 \\} 
$$

따라서 Ambient space(특정 차원의 공간을 포함하는 그 이상의 모든 공간을 의미한다)의 모든 벡터 $\\rm{u}$는 orthogonal projection을 통해 저차원 공간인 Tangent space로 사영할 수 있다.

$$
    \\rm{v} = \\text{proj}_{\\rm{z}}(\\rm{u}) = \\rm{u} + \\it{c}\\rm{z} \\left<\\rm{z},\\rm{u} \\right>_\\mathcal{L}
$$

### Exponential and logarithm maps.

Exponential map은 tangent space의 벡터들을 manifold 상으로 올려주는 역할을 한다. 위에서 언급한 projection의 역과정은 아니고, ambient space가 아닌 공간에 대한 sub-space에서 공간 차원에 대한 벡터를 통해 함수 및 역함수를 구하는 과정이라고 생각해볼 수 있다.

$$
    \\begin{aligned}
    &\\text{expm}_{\\rm{z}} : \\mathcal{T}_{\\rm{z}}\\mathcal{L}^n  \\rightarrow \\mathcal{L}^n \\newline
    \\rm{x} = \\text{expm}_{\\rm{z}}(\\rm{v}) &= \\cosh(\\sqrt{c} \\parallel v \\parallel_\\mathcal{L})\\rm{z} + \\frac{\\sinh (\\sqrt{c} \\parallel v \\parallel_\\mathcal{L})}{\\sqrt{c}\\parallel v \\parallel_\\mathcal{L}}\\rm{v}
    \\end{aligned}
$$

뒤에서 보면 알겠지만 실제 MERU에서는 space component만 써먹는 단순화 작업을 통해 하이퍼볼릭 코사인 텀 하나를 날려버린다. 하이퍼볼릭 함수는 모두 역함수가 있어서 반대로  manifold 상에서 tangent 공간으로 내리는 공식도 가능하다.

$$
    \\begin{aligned}
    &\\text{logm}_{\\rm{z}} : \\mathcal{L}^n  \\rightarrow \\mathcal{T}_{\\rm{z}}\\mathcal{L}^n \\newline
    \\rm{v} = \\text{logm}_{\\rm{z}}(\\rm{x}) &= \\frac{\\cosh^{-1}(-c\\left<\\rm{z},\\rm{x} \\right>_\\mathcal{L})}{\\sqrt{(c\\left< \\rm{z}, \\rm{x} \\right>_\\mathcal{L})^2-1}} \\text{proj}_{\\rm{z}}(\\rm{x})
    \\end{aligned}
$$


# 방법론

모델링은 CLIP과 같이 이미지와 텍스트가 독립적인 인코더를 가지는 형태가 된다.  그런데 이제 여기서 그칠게 아니고 실질적으로 이 논문이 제시한 문제점을 해결하기 위해서는 다음과 같은 방법론을 적용해야한다.

1. 임베딩을 Euclidean space가 아니라 Lorentz space로 바꿀 방법
2. CLIP과 같이 이미지의 semantic 정보와 텍스트의 semantic 정보를 함께 학습할 방법


<div align="center">
    <img src="https://github.com/user-attachments/assets/a8ab5c13-2f1a-492c-8f58-1f6b50670f80" width="600">
</div>


### Lifting embeddings

임베딩을 올리는 방법은 다음과 같다. $n$차원으로 나온 이미지 및 텍스트 임베딩이 있다고 해보자. 기존 CLIP과 동일한 프레임워크라면 두 임베딩은 같은 형태의 텐서가 될 것이다. 로렌츠 모델은 설명했던 바와 같이 추가된 축(시간축)으로 확장시키는 작업이 필요하기 때문에, 우선은 임시로 원점에서의 tangent space(Euclidean)에 대한 벡터를 구한 뒤 이를 hyperboloid로 올리는 공식($\\text{expm}$)을 적용하면 된다.

### Expand dims

$$
    \\rm{v} = (\\rm{v}_\\text{enc}, 0) \\in \\mathbb{R}^{n+1}
$$

### Onto **Hyperboloid**

여기는 잘보면 우리가 지금 올려놓은 tangent space가 $\\rm{z} = 0$에 기반하므로 하이퍼볼릭 코사인이 없어져도 된다.

$$
    \\rm{x}_\\text{space} = \\frac{\\sinh (\\sqrt{c} \\parallel v \\parallel_\\mathcal{L})}{\\sqrt{c}\\parallel v \\parallel_\\mathcal{L}} \\rm{v}_\\text{space}
$$

그리고 **시간에 대한 스칼라**는 **쌍곡면의 곡면에 대한 constraints**로 구할 수 있다.

$$
    \\begin{aligned}
    &\\mathcal{L}^n \\{ \\rm{x} \\in \\mathbb{R}^{n+1}: \\left< x, x \\right>_\\mathcal{L} = -1/c,c > 0 \\} \\newline
    &x_\\text{time} = \\sqrt{1/c + \\parallel \\rm{x}_\\text{space} \\parallel^2}
    \\end{aligned}
$$

즉 기존의 CLIP embedding을 하이퍼볼릭 embedding으로 바꾸는 작업 끝.

### Numerical Issue

다만 문제가 되는 점은 기존 임베딩 벡터를 하이퍼볼릭으로 올리면서 exponential 계산이 추가되는데, CLIP 기반의 weight 초기화가 진행된 경우 유클리디안 공간 벡터의 norm은 대략 $\\sqrt{n}$을 가지게 된다. 그 말인 즉슨 이걸 exponential하게 올리면 수치적으로 $e^{\\sqrt{n}}$가 초기값이 되어 학습이 불안정해지기 때문에 이를 해결할 방법이 필요하다. 그래서 이걸 해결하기 위해 scaling 스칼라인 $\\alpha$를 각각 이미지 및 텍스트 임베딩 output에 적용하여 이러한 문제를 해결한다. 알파는 학습 가능한 파라미터.

### Entailment loss

Entailment loss는 CLIP에서 임베딩 관계성을 implicit하게 학습하기 위해 사용되는 contrastive loss에 추가적으로 하이퍼볼릭 공간의 위상 특성을 고려하여 이미지-텍스트 관계를 더해주는 역할이다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/a407a6e2-d4b3-489b-8b4d-19d9464ccd5e" width="600">
</div>


그림을 보면 알겠지만 각도에 대한 내용이기 때문에 로렌츠 모델을 위에서 내려다본 구조를 생각하면 되고, 예를 들어 특정 텍스트가 존재할 때 해당 텍스트가 반영하는 모든 이미지 임베딩의 공간을 aperture로 가정하면(원뿔 형태), 만약 이미지가 이 내부에 들어와있다면 굳이 끌어들일 필요가 없지만(이는 아마도 각도가 too much align되면 embedding collapse가 발생하여 representation이 subfold된다고 생각한 것으로 예상) 외부에 있다면 각도를 줄여주는 loss를 통해 aperture 내부로 임베딩을 끌어들이는 것이다. 서로 다른 이미지/텍스트를 인코딩한 임베딩 간의 거리 조절은 contrastive learning에서 주로 담당하고 있고, entailment loss는 하이퍼볼릭의 curvature를 고려하여 계층적 구조를 탄탄히 하려는 목적인 듯하다. **모델링이 생각보다 너무 심플해서 놀랐던 부분**.

$$
    \\mathcal{L}_\\text{entail}(\\rm{x}, \\rm{y}) = \\max (0, ext(\\rm{x},\\rm{y})-\\text{aper}(\\rm{x}))
$$


# Experimental Results

### Image/Text retrieval


<div align="center">
    <img src="https://github.com/user-attachments/assets/44111971-567f-4621-9fb0-35e0c600b4cd" width="500">
</div>


전반적으로 retrieval 성능을 CLIP에 비해 끌어올린 것을 볼 수 있는데, 확실히 유클리디안에서 비유클리디안으로의 공간 확장이 가지는 장점이 가장 잘 드러날 수 있는 실험 결과가 아닐까 생각된다.

### Zero-shot image classification


<div align="center">
    <img src="https://github.com/user-attachments/assets/bde9f6b2-e57b-4ca2-ad36-5055b8abc241" width="700">
</div>


Representation 학습에는 어떤 도움이 되는지. Retrieval 결과가 본인들 모델링에 대한 근본적 main contribution을 보여주는 실험이었다면 이건 비교적 sub contribution을 보여주는 결과라고 본다. 그래서 그런지 SOTA를 찍을 필요는 없고 성능 향상의 가능성만 간단하게 보여주고 넘어가는 듯.

# Ablations

이런저런 ablation도 많이 진행했다. Qualitative results도 많다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/aa91fb8f-698c-4ee4-86ee-097c36fac5b0" width="400">
    <img src="https://github.com/user-attachments/assets/42610df4-c06f-44e5-afdf-2622162355f6" width="400">
</div>


좌측은 임베딩 길이에 따라 성능 본 것. 길이랑 무관하게 모두 성능이 좋게 나왔다. 우측은 논문에서 제시한 방법론들을 하나씩 빼고 한 것. Curvature를 고정하는 건 원래 푸앙카레 기법에서 사용한 것인데 그렇게 하면 놀랍게도 Large model에서 성능이 개판이 되는 걸 볼 수 있다. Contrastive learning 시에 Large model에서 로렌츠 norm을 사용하지 않으면 심지어 수렴이 되지 않고 발산을 하는데, 이는 아마도 트랜스포머 백본과의 수렴 속도 차이 때문의 문제로 생각된다.


# Discussion and conclusion

CLIP의 retrieval 및 linear probing/fine tuning 등 representation 자체를 모달리티 robust하게 만들고자 여러 연구가 나오고 있는 중인 것 같다. 그와 동시에 이미지와 텍스트 간의 semantic한 정보들을 보다 풍부하게 제공할 수 있는 학습 objective나 constraints를 제시하는 것이 앞으로의 multimodal task에서 가장 challenging한 부분이 되지 않을까 싶다.

MERU에서 제시한 로렌츠 모델링에 contribution이 있는 것은 아니고 기존 CLIP representation을 확장시킬 수 있는 방향을 제시한 것이 contribution로 보인다. Hyperbolic Riemannian manifold를 위한 모듈을 기존 framework 끝부분에 사용하고, 새로운 manifold에서 제시할 수 있는 추가적인 objective를 제시한 방향도 paper contribution에 적합하다고 생각했다. 기존 인코딩 방식을 바꾸지 않으면서 projection하는 방법만 제시했기 때문에 비슷한 형태의 embedding을 추출하는 인코더 기반 네트워크들에 대해 hierarchy embedding에 대한 추가 실험 및 연구가 진행되면 해당 방법론에 대한 정당성이 조금씩 확립될 것으로 보인다.
`,GO=`---
title: "DINO(Emerging Properties in Self-Supervised Vision Transformers) 논문 리뷰"
category: "ai papers"
publishedAt: "2023-11-12"
thumbnail: "https://github.com/user-attachments/assets/7e1685db-4c89-44c9-8c0a-34c6af0f7d9f"
---



# 들어가며 …

제목에서 알 수 있듯이 이 논문은 **Vision Transformer**가 자기 학습을 통해 **습득할 수 있는 능력이나 특성**에 대해 논의한다. ViT의 프레임워크가 제안된 배경에는 자연어 분야의 Transformer 구조가 존재하는데, 이미 GPT나 BERT와 같은 후속 연구를 기반으로 NLP에서는 Large Dataset의 self-supervised learning이 downstream task에서 보다 풍부한 semantic information을 제공한다는 사실이 증명된 바 있다. 이와는 다르게 ViT의 학습 구조를 보게 되면 언어 모델과 같이 대용량의 이미지 데이터셋을 사용하여 사전 학습하는 과정이 이후의 downstream task에도 도움이 된다는 사실은 증명이 되었으나, 여전히 <U>supervised learning 구조</U>에서 벗어나지 못한 것을 알 수 있다.


# 발견한 특성들

논문 구성은 간단하게도 아이디어를 develop하는 과정(self-supervised learning 방법론에 대한 approach)에 대한 motivation으로 시작하게 되고, 해당 방법론으로부터 온 **효과**를 언급하면서 이를 증명할 **여러 실험 결과들**을 보여주게 된다.  논문에서 발견한 ViT의 self-supervised learning 특성을 요약하면 다음과 같다.

- 자기 학습을 통해 획득한 ViT의 feature는 이미지의 semantic segmentation 정보를 가지게 되고, 이는 지도 학습으로 학습된 ViT나 convnet에서도 발견되지 않은 특성이다.  실제로 아래 그림과 같이 attention 정보를 통해 네트워크가 각 이미지 단위로 포커싱하고있는 영역이 곧 이미지 상에서 object의 semantic한 정보 그 자체라는 것을 확인할 수 있다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/7e1685db-4c89-44c9-8c0a-34c6af0f7d9f" width="1000">
</div>


- 또한 이렇게 획득된 feature는 유사도에 기반한 $k$*-NN* classifier로 활용될 수 있고 small ViT로도 ImageNet(recognition task)에서 좋은 정확도를 보임을 확인하였다.
- 마지막으로 여러 셋팅에서의 실험 및 충분한 ablation을 통해 ViT의 자기 학습 과정에서 효과적으로 쓰일 수 있는 방법론들을 직접 실험들을 통해 규명했으며, momentum encoder, multi-crop training 그리고 smaller patch(more number of patches)가 중요하게 쓰인다.


# Self-supervised learning Frameworks

대표적인 label이 없는 환경에서의 unsupervised(self-supervised) learning 접근법으로는 [SimCLR](https://arxiv.org/abs/2002.05709), [MoCo](https://arxiv.org/abs/1911.05722) 그리고 [BYOL](https://arxiv.org/abs/2006.07733)가 있었다. 갑자기 이 얘기를 꺼낸 이유는 이 페이퍼에서 논하고자 했던 property가 곧 ViT의 self-supervised learning으로부터 나오기 때문에, 제안된 structure의 근거를 알기 위해서는 이전 논문들의 참고가 필수적이기 때문이다. DINO 논문에서는  SimCLR, MoCo 그리고 BYOL 중 BYOL에서 inspiration을 얻었다고 한다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/651b0185-c302-4a21-a63e-5a576142efa2" width="500">
    <img src="https://github.com/user-attachments/assets/50169b98-880b-4c54-89f4-350917af5b60" width="500">
</div>


SimCLR의 구조는 첫번째 그림과 같다. Input $x$가 주어지면 이를 정해진 random augmentation을 적용한 각각의 샘플 $\\tilde{x}_i$ 그리고 $\\tilde{x}_j$로 만들게 되고, 이를 뉴럴 네트워크 $f(\\cdot)$에 통과시킨 output $h_i$, $h_j$ 를 하나의 representation/embedding이라고 했을 때 이를 Linear operation($g(\\cdot)$)으로 mapping한 최종 latent인 $z_i$ 그리고 $z_j$를 contrastive하게 학습하는 방법을 사용하였다.  Moco도 큰 틀에서는 contrastive learning과 두 개의 branch를 사용한다는 점에서 SimCLR와 거의 동일하지만, 차이점이라고 한다면 SimCLR은 배치 내에서 동일한 인코더를 기준으로 representation 학습을 진행하지만 MoCo는 학습이 되지 않고 EMA 방식으로 업데이트되는 momentum encoder가 사용된다는 점이다. 쿼리에 사용되는 배치는 매 학습마다 \`enqueue\` 및 \`dequeue\` 를 통해 최신 mini-batch가 지속적으로 업데이트되며, positive logit은 동일 배치의 샘플에 대해, negative logit은 이전 queue의 샘플에 대해 연산을 진행하게 된다. Querying에 사용되는 encoder를 점진적으로 학습하는 방법을 적용했다는 점에서 차이가 생긴다.


# BYOL paper에 대한 짧은 논문 리뷰


<div align="center">
    <img src="https://github.com/user-attachments/assets/ffc4d429-a0d6-4b75-9c58-9b6b68ea941d" width="900">
</div>


BYOL은 momentum encoder의 장점을 가져오면서 학습의 전반적 형태는 SimCLR와 같은 방식을 가져왔다. 그러나 알고리즘 측면에서 큰 차이가 있는데, 이는 바로 BYOL은 contrastive learning을 하지 않는다는 점, 즉 negative pair가 필요하지 않다는 것이다.

BYOL에서 가장 크게 주목할 점은 어떻게 negative pair와 같이 collapse를 방지할 만한 장치가 없이도 안정적인 학습이 가능한가에 대한 부분이다. 바로 이 부분에서 대체 왜 online/target 두 브랜치가 서로 assymetric(비대칭)하게 구성되었는가를 확인할 수 있다. 예컨데 predictor $q_\\theta(\\cdot)$는 projection 목적이 되는 $g_\\theta$와 동일한 신경망 구조를 가진다.  단순하게 생각했을때 prediction은 하나의 classifier라고 생각할 수 있지만 그렇지는 않고 projection network와 같은 dimension의 output을 내보낸다. 하지만 바로 이 projector 부분이 학습되면서 optimal point에 놓여있는 것이 가장 주요한 학습 키포인트로 작용한다.

Projector가 수렴했다는 것은 곧 projector가 어느 정도 optimal한 영역에 있다고 볼 수 있고 이를 $q^\\ast_\\theta$라고  한다면 online branch의  input이 되는 $z_\\theta$에 대해 $q^\\ast_\\theta(z_\\theta) = \\mathbb{E}[z^\\prime_\\xi \\vert z_\\theta]$로 표현이 가능하다.  수식 상에서 조건부 expectation은 $z_\\theta$에 대한 함수가 되며, 조건부 확률 분포와 동일한 의미를 가진다. 즉 우리가 흔히 optimal하게 학습된 neural network를 특정 도메인의 데이터셋 $\\{X, Y\\}$에 대해 parameterized posterior $p_\\theta(Y \\vert X)$로 표현하는 것처럼 projector가 수렴했다는 가정 하에 $z^\\prime_\\xi$와의 수식으로 해석할 수 있다. 결국 이 가정을 통해 수식을 다시 전개하게 되면 simplified BYOL loss(원래 BYOL에서는 view를 교차하는 형태로 symmetric한 cost function을 구성하는 것과 더불어 latent의 정규화 작업이 추가됨)은 다음과 같이 표현 가능하며

$$
\\mathcal{L}_\\text{BYOL} = \\mathbb{E}\\left(\\parallel \\mathbb{E}(z^\\prime_\\xi\\vert z_\\theta)-z_\\xi^\\prime\\parallel_2^2\\right)
$$

결국 학습 파라미터(online branch) $\\theta$에 대한 gradient는 다음과 같이 expected variance의 gradient로 수렴하게 된다.

$$
\\nabla_\\theta \\mathcal{L}_\\text{BYOL}= \\nabla_\\theta\\mathbb{E}\\left(\\sum_i \\text{Var}(z^\\prime_{\\xi, i} \\vert z_\\theta) \\right)
$$

이러한 가정은 optimal projector가 수렴했다는 전제에서 성립하게 되는데, 이를 통해 BYOL loss는 수렴된 projector를 변화시키지 않고 online network를 업데이트할 수 있다. 위의 수식은 파라미터 및 projection을 다변수로 가지는 최적화 함수를 **Lagrangian으로 표현했을 때**의 envelop theorem 그리고 optimality condition에 기반한다.

BYOL에서는 이렇게 업데이트되는 $\\theta$에 대해 online branch와 target branch $\\xi$가 동시에 감소하는 방향은 loss surface $\\mathcal{L}$에는 정의될 수 없다는 것이다. Target branch에서의 projection $z^\\prime_\\xi$와 online branch에서의 $z_\\theta$에 대한 Variance로 loss 최적화 식을 만들었었고 이게 의미하는 바는 projector가 어느 정도 수렴한 상황에서 가장 말단의 posterior는 고정된 상태라고 보는 것이다. 임의의 random variable에 대해 조건부 분산은 조건 변수가 추가될수록 이전 분산의 lower bound가 된다. 만약 BYOL을 통한 최적화 과정이 collapse를 일으킨다면 online network의 projection인 $z_\\theta$는 더이상 무작위로 분포한 확률 랜덤 변수가 아닌 constant $c$로 고정될 수 있고, 이는 parameter space에서기존 업데이트 과정이 lower bound가 됨을 명시할 수 있는 근거가 된다.

$$
\\text{Var}(z^\\prime_\\xi \\vert z_\\theta) \\le  \\text{Var}(z^\\prime_\\xi \\vert c)
$$

즉 collapse가 일어날 수 있는 환경이 parameter surface에서 보다 큰 값을 가지기 때문에 더 불안정(unstable), collapse가 발생하지 않는다.

만약 반대라면 어떻게 될까? 이 논문에서는 $\\xi$를 loss function을 기준으로 업데이트하지 않고 EMA를 사용하여 점진적 과부하를 걸었는데, 이는 같은 위의 수식으로 그 이유를 찾을 수 있다. Target network에 collapse가 발생한다면 이번에는 $z^\\prime_\\xi = c$ 인 deterministic constant가 되고, 이번에는 조건부 변수가 아닌 메인 변수에 해당되므로 분산이 0이 된다.

$$
\\text{Var}(c \\vert z_\\theta) = 0 \\le \\text{Var}(z^\\prime_\\xi \\vert z_\\theta)
$$

즉 $\\xi$에 대해서 학습하게 되면 무조건 collapse가 발생하게 된다는 것을 볼 수 있다. BYOL는 이러한 이론적 배경에 근거하여 negative pair를 굳이 구하지 않더라도 similarity loss를 기반으로 점진적으로 latent를 bootstrapping (과거의 online parameter가 미래의 online parameter의 학습에 도움이 되는 과정)하는 방법을 제시하였고, 이는 batch size로부터의 자유 및 자기 학습 방법의 지평을 보다 넓힐 수 있는 계기가 되었다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/bae8b348-da0e-45c5-b219-84484c24318c" width="800">
</div>



# DINO approach


<div align="center">
    <img src="https://github.com/user-attachments/assets/88eb8d19-7cd7-4274-a8a8-566bdecc7930" width="500">
</div>


DINO는 ‘knowledge distillation with no labels’의 줄임말로, 말 그대로 ViT 학습에 SSL 프레임 워크를 제안한 형태가 된다. 이 방법 역시 student/teacher(혹은 online/target)의 두 브랜치 간의 학습이 진행되는데, 안정적인 pseudo label을 만들어내는 teacher은 loss term에 대한 파라미터 최적화가 발생하지 않고consistency를 통해 지속적으로 update되는 student parameter를 지수 평균 이동(exponential moving average)으로 가져온다.

### Knowledge Distillation

논문에서 접근한 SSL은 다음과 같다. Student model과 teacher model은 학습되는 중간에는 데이터가 매핑되는 함수로 작용하고, 이는 곧 probability mapper로 해석 가능하다. 만약 student network($g$)의 파라미터를 $\\theta_s$라 한다면 입력 신호 $x$에 대한 output logit $g_{\\theta_s}(x)$를 구할 수 있다. 그리고 이 logit에 softmax function을 적용하면 확률로의 직접 매핑이 가능하다. 이때 \`softmax\`가 적용되는 dimension은 특징자(feature embedding) 축이라고 생각하면 된다.

$$
P_s(x)^{(i)} = \\frac{\\exp(g_{\\theta_s}(x)^{(i)}/\\tau_s)}{\\sum_{k=1}^K\\exp(g_{\\theta_s}(x)^{(k)}/\\tau_s)}
$$

여기서 temperature $\\tau_s$가 사용되는데, 이는 다양한 논문들에서 probability distribution의 분포를 결정하는 하이퍼파라미터 혹은 학습 가능한 파라미터로 많이 사용된다. 이 논문에서의 temperature parameter의 목적은 student network에 의한 probability의 sharpness 조절 역할을 하게 된다. 마찬가지로 teacher network에 대해서도 다음과 같은 formulation이 가능하다.

$$
P_t(x)^{(i)} = \\frac{\\exp(g_{\\theta_t}(x)^{(i)}/\\tau_t)}{\\sum_{k=1}^K\\exp(g_{\\theta_t}(x)^{(k)}/\\tau_t)}
$$

Knowledge distillation에서 학습은 teacher의 output을 일종의 ground truth로 가정한 student output과의 consistency loss이다. 즉 cross entropy에서 one-hot label을 teacher network의 output으로 바꿨다고 생각하면 된다.

$$
\\underset{\\theta_s}{\\min} H(P_t(x),~P_s(x)) = \\min_{\\theta_s} \\{-P_t(x) \\log P_s(x)\\}
$$

크로스 엔트로피가 의미하는 것이 정보이론에서 “하나의 확률분포”가 “또다른 확률분포”가 가지는 정보와 얼마나 가까운지에 따른 거리 metric이기 때문에 결국 학습 목적은 학생으로 하여금 선생의 지식을 잘 모방하도록 하는 것이 된다. 하지만 단순히 이 방법론으로 마무리되는 알고리즘은 아니고, DINO는 효과적인 학습을 위해 다양한 방법들을 추가하게 된다. 예컨데 위의 수식은 앞서 보여준 framework와는 다르게 augmentation에 대한 내용이 없지만, 저자는 바로 이 수식 전개 직후 단순 distillation을 사용함에 따라 생기는 문제점들을 언급한다.

### Data augmentation

Transformer 구조가 가지는 가장 큰 문제점 중 하나가 local-to-global correspondence가 적다는 것이다. Transformer는 attention을 기반으로 단번에 global information을 인지하기 때문에 convolution network에 비해 가지는 장점도 있겠지만, local information을 포착하기 전에 모든 attention map들이 global feature에서 수렴해버린다면 CNN이 가지는 계층적 구조에 의한 correspondency(feature간 상관관계에서 얻을 수 있는 high to low level 효과)를 SSL에서 기대할 수 없다는 문제가 있다.

따라서 이를 해결하는 방법으로 augmentation의 비대칭을 사용하였다. 이 프로세스를 요약하면 다음과 같다:

1. Teacher가 local/global에 대한 consistency를 가지고 어느 정도 수렴했다는 가정 하에, teacher는 이미지의 global한 형태를 보고 ‘그럴 듯한’ 예측을 한다.
2. 위의 가정이 있다면 teacher network에는 계속 global한 image 정보만 주면 된다.
3. Teacher는 student의 파라미터로부터 EMA된다. 즉, teacher의 바람직한 수렴을 위해서는 student가 앞서 말했던 local/global에 대한 consistency 정보를 학습할 수 있는 환경이 되어야한다.
4. 따라서 student에는 local image 정보를 같이 준다.

길게 설명했지만 풀어쓰자면 teacher network에는 이미지에 큰 범위에서의 augmentation이 들어간 global view $x_1^g$, $x_2^g$만 예측에 사용되고, student network에는 multi-crop strategy와 같은 이미지의 작은 범위까지의 augmentation이 적용된 local/global  정보가 예측에 사용된다.

$$
\\underset{\\theta_s}{\\min}\\sum_{x \\in \\{x_1^g,~x_2^g\\}} ~~\\sum_{x^\\prime\\in V,~x^\\prime \\neq x} H(P_t(x), P_s(x^\\prime))
$$

논문에서는 보통의 방식과 같이  multi-crop image들을 생성했는데, 2개의 global views는 원본 이미지 대비 $50\\%$보다 큰 크기만큼 잘라서 쓰고 여러 local view는 반대로 $50\\%$보다 작은 크기만큼 잘라서 쓴다.

### Avoiding collapse

Self-supervised learning의 문제점은 representation 학습에 대한 ground truth가 없기 때문에 collapse가 발생할 수 있다는 것이다. 사실상 우리가 많이 알고 있는 contrastive learning이든, clustering 방식이든, predictor를 다는 BYOL과 같은 구조라던지 Batch Normalization을 도입하는 등등의 approach는 공통적으로 collapse를 막는 역할을 하게 된다. 물론 DINO 또한 normalization 구조라던지 앞서 언급한 여러 방법론으로 stabilization을 수행할 수 있었지만, 이 논문에서는 momentum teacher network의 output을 centering 및 sharpening하는 구조를 통해 이러한 효과를 얻을 수 있다고 한다.  Sharpening/Centering에 대한 내용은 조금 알아보기 쉽게 나타내면 Sharpening은 temperature 조절을 통해 softmax 예측값 분포를 보다 명확하게 드러내는 것이고 centering은 teacher output에 center value $c$를 bias term으로 더해주어 예측값 사이의 차이를 조절해주게 된다. 즉 sharpening과 centering은 효과만 보게 되면 서로 반대의 역할을 수행한다. 여기서 드는 의문점은, 굳이 sharpening을 통해 prediction의 entropy minimization을 수행할 목적이었다면 왜 다시 centering이라는 방법으로 다시금 prediction을 재조정하는 과정을 거치는지에 대한 부분이다. 이 부분에 대해서 나름대로 이해한 것은 다음과 같다.

우선 centering에 사용될 bias term $c$는 다음 식을 통해 exponentially update가 된다. EMA 방식으로 teacher parameter가 업데이트되는 것과 동일하다.

$$
c \\leftarrow mc + (1-m)\\frac{1}{B}\\sum_{i=1}^B g_{\\theta_t}(x_i)
$$

식을 자세히 보면 center $c$에는 결국 학습 시 사용되는 batch size랑은 무관하게, 기존 input에 대한 model의 output(prediction) 정보를 평균으로 저장하게 된다. 이제 centering에 대한 맥락은 얼추 이해했고, 다시 sharpening으로 돌아가보도록 하자.


<div align="center">
    <img src="https://github.com/user-attachments/assets/a3bef87c-f0d2-4efe-9c43-bc01c7a7622f" width="950">
</div>


예컨데 고양이라는 이미지에 대해 모델이 낸 prediction을 sharpening하는 작업을 하게 되면 뭉뚱그려진 예측값을 어느 정도 명확하게 하면서 feature map을 선명하게 만들어 준다는 장점이 있지만, 만약 배치 단위로 들어오는 특정 input이 모델로 하여금 지속적으로 collapse가 발생하게 한다면, 이는 불난 집에 부채질하는 격이 된다.  


<div align="center">
    <img src="https://github.com/user-attachments/assets/fd5860a0-d41f-4160-ac81-a440237a1337" width="950">
</div>


원래 목적이라 함은 다른 input에 대해 특징을 잘 잡아내는 feature를 뽑고자 sharpening을 도입했는데, contrastive learning과 같은 제어장치가 없다면 모델은 그냥 단순히 네트워크 예측 자체의 entropy를 낮추는 방향으로 끊임없이 학습이 될 것이기 때문에 이미지의 종류에 상관없이 단일의 feature를 뽑게 되고, 이러한 문제를 trivial solution이 발생한다고 한다. 결국 기존 SSL approach를 사용하지 않고는 이를 근본적으로 해결하기 어렵다는 문제가 발생한다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/9e925219-7bfa-4398-8210-00dc08db1da8" width="950">
</div>


그렇기 때문에 만약 centering term이 있다면 이를 단순히 model prediction에 더해주는 것만으로도 이전 배치들의 정보를 가져올 수 있으며, batch size의 크기에 robust한 학습 효과를 보여주는 것이다. 예컨데 contrastive learning에서는 positive sample과 negative sample 쌍을 얻기 위해 최대한 많은 배치 수가 필요했고, 그 이유는 모델이 학습할 때 metric learning을 적은 단위의 배치 내에서 진행하는 것보다는 큰 배치 내에서 진행하는 것이 전체 데이터셋의 확률 분포를 잘 나타낼 수 있기 때문이었다. 하지만 위와 같이 output을 뽑아서 배치 단위의 prediction을 저장하고, 이를 이후의 output을 sharpning할 때 smoothing에 사용하는 것만으로도 배치 사이즈를 키우지 않고 이러한 효과를 볼 수 있다는 것이 바로 sharpning/centering이 가지는 장점이다. 사실 까놓고 말하자면 단순하게 이전 prediction을 일종의 prototype으로 저장해놓고 쓴다는 느낌인데, 저자들은 이 방법론이 실제로 학습에 미치는 영향을 보여주기 위해 실험을 진행하였다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/f11b6374-ef64-455f-808f-9d523fb5784e" width="950">
</div>


Sharpening의 효과는 entropy를 0으로 만든다. 그리고 centering의 효과는 smoothing을 통해 어떤 input이 들어오든 entropy를 유지시킨다. 둘 중 하나만 사용하면 epoch에 따라 representation overfitting/underfitting이 발생하는모습을 잘 확인할 수 있다. 무엇보다 이를 잘 보여주는 실험이 KL divergence에 있지 않을까 싶다.

Teacher/student 구조를 쓰면서 얻고 싶은 장점은 EMA 방식으로 기존 representation 정보를 차곡차곡 모아가는 teacher network의 prediction을 student가 따라가면서 서로 간의 학습에 bootstrapping이 일어날 수 있다는 것인데 만약 representation이 collapse가 된다면 이러한 효과를 볼 수 없을 것이고, 결국 bootstrapping이 없다는 것은 학습이 진행되면서 student/teacher prediction 차이가 없어진다는 것이다.


# 실험 결과

### Classification

SSL/Unsupervised Learning의 경우 학습된 feature를 증명하는 과정이 여러 가지로 분류된다.
우선 downstream task에 맞게 head를 다는 과정이 필요하고, 이 head를 어떻게 써먹냐에 따라 linear classifier/fine-tuning/k-NN classifier로 분류된다.

- Linear classifier : 학습된 backbone을 frozen한 채로 linear classifier만 학습해서 representation의 효과를 보고자 하는 것
- Fine tuning : 학습된 backbone을 head에 붙인 채로 fine tuning하여 representation의 효과를 보고자 하는 것
- k-NN classifier : classifier 같은 부수적인 요소 없이 단순히 embedding으로 retrieval해서 representation/metric learning 자체 효과를 보고자 하는 것


<div align="center">
    <img src="https://github.com/user-attachments/assets/ea1bb3ef-b531-49e5-87f0-0ef27496dda9" width="400">
    <img src="https://github.com/user-attachments/assets/ead481df-286d-4085-95fc-e083a49fef93" width="400">
</div>


뭐 성능 자체와 관련해서는 상당히 좋게 나온 것을 확인할 수 있고, ViT baseline의 다른 SSL 방식과 비교했을 때도 유의미하게 높은 classification 성능을 보여준다.

### ViT Attention map

하지만 classification 보다는 DINO의 가장 큰 특징은 ViT의 attention map을 보면 잘 드러나는데, 바로 local feature에 attention을 집중할 수 있다(localization)는 것이다. 이는 기존 ViT 방식으로는 얻을 수 없는 feature map에 해당된다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/22b364b5-29c1-49eb-b071-af0c9bad81b3" width="350">
    <img src="https://github.com/user-attachments/assets/e7bf8362-48f6-4f63-a0e8-3d2e54a6c2fe" width="400">
    <img src="https://github.com/user-attachments/assets/54771b26-f8e8-4978-b829-8090832ee7c8" width="400">
</div>


Segmentation처럼 high-level image task의 경우 모델의 예측이 정교해야하기 때문에 상대적으로 classification task에 비해 SSL이 달성할 수 있는 성능 수치가 그리 높지 않았다. 그럼에도 불구하고 DINO의 attention map을 보면 알 수 있듯이 이 페이퍼에서 제안한 학습 방법은 ViT의 input에 대한 attention을 효과적으로 localization하는 것에 성공하였고, 정량적으로도 그 수치를 증명하였다.


# 결론

DINO는 BYOL을 비롯한 기존 SSL 방식에서 motivation된 self-distillation 구조와 더불어 collapse를 방지하고 학습 안정화를 위해 sharpening/centering을 도입하여 ViT를 효과적으로 학습하였다. 그런데 이렇게 안정적으로 ViT를 라벨 없이 SSL로만 학습하고 보니 이게 무슨 일이람. ViT의 attention map이 localization되는 중요한 변화를 확인할 수 있었다. 이러한 점이 시사하는 바는 상당히 크다.

지금까지는 supervised learning이 절대적인 학습법이었으며, 사실 SSL이 학습 안정화를 토대로 가끔 supervised learning의 성능을 넘는 경우도 있긴 했지만은 모든 task에 정통으로 사용될 수 있는 방법은 아니었으며 linear probing이나 fine tuning 시에 미리 학습된 representation의 효과를 강하게 보여주었지 실질적으로 SSL로 학습된 representation이 가능성을 보여주는 경우는 많지 않았다. 하지만 애초에 구조상 inductive bias가 없어 localization이 힘든 transformer baseline인 ViT를 SSL하였더니 attention map이 segmentation 효과를 보여주었고, 이는 NLP가 아닌 Computer Vision 분야에서도 classification 뿐만 아니라 여러 task에 SSL이 우월한 성능을 보여줄 수 있음을 증명하였다.
`,WO=`---
title: "DINOv2(Learning Robust Visual Features without Supervision) 논문 리뷰"
category: "ai papers"
publishedAt: "2023-12-04"
thumbnail: "https://github.com/user-attachments/assets/aa0121a3-1c4d-4e34-a85d-8a9bae6caf95"
---


# Supervised 학습의 한계점

이전 게시글 중 DINO를 리뷰한 내용에서는  Self-supervised learning은 NLP 뿐만 아니라 CV에서도 적절한 전략을 잘 사용한다면 기존 ViT/CNN 구조에서 발견하지 못한 **유의미한 visual feature를 획득할 수 있음**이 증명되었다는 점을 소개했었다. Supervised learning은 label이 존재하는 형태의 학습에서 손실 함수로 적용되는 objective value가 명확하다는 점, 그렇기 때문에 학습되는 encoder 및 decoder의 hypothesis를 정확하게 align할 수 있다는 장점이 있었으나 특정 task에 최적화된 파라미터는 label space가 조금만 달라지거나 input image의 domain이 조금만 달라지더라도 optimal point에서 크게 벗어날 수 있기 때문에 일반화된 성능을 보여주기 힘들다는 명확한 한계점이 존재했다. 특히 classification과 같이 이미지를 전반적으로 해석하는 task에서는 큰 문제가 없지만 segmentation과 같이 입력 이미지와 동일한 resolution에서 픽셀별 prediction이 진행되는 high-level task에서는 더 큰 문제를 불러오게 된다. NLP가 상대적으로 성능 수렴을 빠르게 달성하고 거대 모델의 property를 찾거나, tuning 방법과 관련된 연구가 진행된 바탕에는 바로 supervised learning에서 벗어났다는 사실이 존재한다. 즉 task 마다 직접 생성해주는 ground truth는 high level로 올라갈수록 cost가 높아진다는 superficial한 단점 말고도 궁극적으로 얻고자 하는 robust한 visual feature를 얻는 과정에 악영향을 준다는 문제가 있다.


# Self-supervised Learning

 DINO 첫번째 논문을 간단하게 요약하면 EMA 구조를 가져가면서 batch size에 따라 overfitting/underfitting되지 않도록 무관한 안정적인 학습을 위해 model output의 entropy를 조절하는 centering/sharpening 작업을 도입했었다. DINO에서는 ‘ ViT를 위한 SSL, 그리고 이를 통해 획득할 수 있었던 visual feature의 특징에 대해 서술했다.’라고 한다면 [DINOv2](https://arxiv.org/abs/2304.07193)는 ‘이러한 SSL 구조를 어떻게 확장시킬지 데이터/모델링 관점에서 서술했다.’고 요약할 수 있다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/aa0121a3-1c4d-4e34-a85d-8a9bae6caf95" width="900">
</div>
 

SSL이 가지는 장점을 보여주는 하나의 qualitative 예시는 위의 그림에서 확인할 수 있다. 첫번째 칼럼에서 ‘새’와 ‘비행기’는 low level에서 비슷한 semantic 특징을 가지지만 엄연히 다른 도메인에 속한다. 그럼에도 불구하고 각 이미지의 pixel 사이의 PCA를 수행하고 가장 큰 값을 가지는 3개의 component를 visualize하면 비행기의 날개 부분이 새의 날개 부분에 매칭되거나 몸통 부분은 몸통 부분에, 꼬리는 꼬리 부분에 매칭되는 것을 확인할 수 있다(색을 보면). 마찬가지로 세번째 칼럼에서 하나의 말이 존재하는 이미지, 여러 말이 존재하는 이미지와 같이 이미지 인스턴스 내에 특정 물체를 나타내는 feature가 많이 존재하는 상황에서도 이러한 correspondence가 잘 유지된다던지, sketch(drawing)과 같이 input에 대한 natural shift가 발생한 상황에서도 PCA 결과가 합리적인 것을 볼 수 있다. 학습된 모델 스스로가 같은 종류의 instance를 포함하는 여러 도메인 사진에 대해서도 correspondence를 잘 인지할 수 있고, 이는 곧 추가적인 fine-tuning 작업 없이도 학습된 visual feature를 다양한 downstream task/dataset에 활용할 수 있음을 보여준다. 이를 흔히 표현하는 방식으로는 **“Out of Box model”**라고 부른다. 한국말로 **“우물 안 개구리 형태의 모델에서 벗어남”**이라고 하고 싶다.


# Dataset filtering/Curating


<div align="center">
    <img src="https://github.com/user-attachments/assets/a124cc5f-9906-4e4b-b630-b4ed779b8427" width="900">
</div>


대용량의 데이터에 대해 SSL을 수행하기에 앞서 paper에서는 **data curation(데이터 정제) 작업의 중요성**을 언급한다. 특정 dataset을 semantic하게 분석했을 때 bias가 존재한다면 다양한 이미지에 대한 visual feature를 뽑을 때 bias가 포함될 수 있기 때문이다. 이러한 ‘문맥상’의 정규화에 대한 중요성은 이미 SSL이 만연하게 적용된 NLP에서는 어느 정도 당연하게 인식하고 있는 사실이며, computer vision에서도 비슷한 맥락의 효과를 얻고자 한다면 data curation이 필수적이라는 것이다. 사실 생각해보면 supervised learning에서도 이와 같은 data curation이 필요하기는 하지만 label space 상에서 카테고리 간의 비율만 얼추 맞으면 학습 수렴에 큰 문제가 없었기 때문에 치명적으로 작용하는 문제가 아니었으나, self-supervised/unsupervised learning에서는 모델은 오로지 ‘이미지’에만 의존한 학습을 하기 때문이라고 볼 수 있다. 데이터를 처리한 과정은 다음과 같다. 위의 framework figure를 참고하면서 보면 이해하기가 편하다.

1. Curated dataset은 다음과 같이 다양한 데이터셋을 사용한다. Uncurated dataset으로는 Crawling이 가능한 웹사이트에서 img 태그에 포함된 url source를 통해 가져오게 되며, 이렇게 획득한 소스 이미지들을 PCA hash deduplication(중복 제거), NSFW filtering(성적인/폭력적인 이미지 필터링) 그리고 초상권 문제가 있기 때문에 사람 얼굴을 blurring하는 등의 작업을 추가로 진행한다. 이렇게 수집한 이미지는 대략 1.2B 이미지이다.
    

<div align="center">
    <img src="https://github.com/user-attachments/assets/dfb7d742-ca92-4c09-be43-d8471c5f8f71" width="800">
</div>

    
2. Uncurated dataset 자체적으로 진행한 중복 제거 및 이런저런 필터링을 제외하고도, 기존 curated dataset을 기준으로 curation을 진행하기 전 이미 존재하는 데이터셋과 중복되는 이미지를 없애는 작업을 시작한다. 굳이 dataset에 이미 존재하는 이미지를 다시 retrieval하는 과정을 진행할 필요는 없기 때문이다.
3. Self-supervised image retrieval
중복 제거가 완료된 uncurated dataset을 기준으로 이미지 retrieval은 curated dataset과 잘 align되는 샘플들을 추출하는 과정이다. 각 이미지에 대한 임베딩을 ImageNet-22k에 사전 학습된 ViT-H/16 네트워크로 추출하고, 이미지 벡터 간의 코사인 유사도를 통해 벡터 간 거리를 계산하게 된다. 만약 retrieval의 구심점이 되는 이미지가 충분하다면 query를 기준으로 $N$개의 가장 가까운 이미지들을 찾은 뒤 이를 그대로 데이터셋에 넣고(위의 표에서 sample에 해당), 충분하지 않다면 cluster로부터 샘플링하는 방법을 채택한다. Cluster 방식에서는 uncurated data source를 $100,000$개의 분리된 cluster로 구성한 뒤 retrived image가 포함된 cluster에서 $10,000$개의 이미지를 가져온다.


# Self-supervised pre-training

데이터셋 정제와 더불어 학습법도 DINO에 비해 일부 추가된 점이 있는데, 각 요소들에 대해 간단히 요약하면 다음과 같다.

### Image-level objective

DINO 원래 논문에서 사용하기로는 student/teacher network에 각각 local/global feature를 구분해서 넣고, 나오는 output에 대해 consistency loss를 cross entropy term으로 적용했었다. Loss에 대한 최적화는 student에만 적용하고 teacher는 EMA로 파라미터 업데이트하는 것까지 동일하게 사용하였다.

### Patch-level objective

Student model로 들어가는 일부 patch를 랜덤하게 마스킹하고,  각 mask patch 위치의 feature 간의 cross-entropy loss를 추가하였다. Mask에 의한 augmentation 효과가 더해졌다고 보면 된다.

### Untying head weights between both objectives

image/patch loss를 적용할 때 같은 head(classifier)를 사용하면 image-level loss는 overfitting되고 patch-level loss는 underfitting되는 문제가 발생하였고, 각 loss가 적용되는 헤드의 분리를 통해 이 문제를 해결할 수 있었다고 한다.

### Sinkhorn-Knopp centering

DINO에서 teacher softmax-centering하는 방식을 SWaV로 바꾸게 된다. 


<div align="center">
    <img src="https://github.com/user-attachments/assets/6bae75bc-4fbf-4d6c-b37b-03351e86a356" width="800">
</div>

 

SWaV 논문에서는 prototype $C$가 일종의 code book 역할이 되는 $Q$로 향하는 경로로서 학습이 진행된다.  서로 다르게 augmentation된 이미지는 각자 prototype에 의해 코드북으로 매핑이 진행되고, 각자 본인의 코드북을 예측하는 것이 아니라 다르게 augmentation된 이미지에 의해 매핑된 코드북을 예측한다. 이로써 일반적인 contrative learning을 대체할 수 있다는 논리 전개가 가능하다. Collapse (모든 latent인 $z$가 동일한 code $q$로 매핑되는 케이스)를 막기 위해 code book $Q$는 각 배치 단위에서 구성되는 모든 샘플들을 각각의 코드북에 균등하게 배분하는 과정을 거친다. 즉 $B$만큼의 배치 사이즈로 $K$개의 코드북에 매핑될 때, 각 iteration 마다 코드북 하나는 **최소한** $B/K$ 만큼 선택될 수 있어야한다는 조건이 필요하다.  해당 조건 내에서 최적화 문제를 풀어내는 과정을 쭉 요약하는 것이  SWaV 에 대한 내용이다. 이 논문에서 제안된 방법은 centering을 위한  빌드업이었다. 


<div align="center">
    <img src="https://github.com/user-attachments/assets/28f03922-c353-486a-b16b-4e7ec58e9df6" width="400">
    <img src="https://github.com/user-attachments/assets/30dfb38a-1dd6-4c4e-89bf-dd300398abf9" width="600">
</div>


Centering은 단일 헤드의 학습 불안정성을 해소하기 위한 일종의 앙상블 장치였다. 과거의 prediction 정보를 accumulation 함에 따라 이전 input들의 정보가 이후 input의 prediction을 보다 bias되지 않게 해줄 수 있었다. 이러한 방법 대신 representation에 head를 $m$개씩 달아두고, 해당 예측 정보들에 대한 SWaV에 weight를 주어 앙상블하는 방법을 제안한 것이 바로 [Weighted Ensemble Self-supervised learning](https://arxiv.org/abs/2211.09981)이다. 엔트로피(예측의 확실성 지표)에 따른 weight가 가장 좋은 성능을 보였고, centering을 SWaV 방식에 여러 head로부터의 앙상블로 대체한 것이 DINO의 representation 성능을 증가시키는 것을 확인할 수 있었다.

### KoLeo regularizer

배치 내에서 각 샘플들이 embedding되는 point 간의 간격을 동일하게 유지하고자 하는 정규화 term이다. 예컨데 embedding point $n$개 모두에 대해 가장 가까운 다른 point와의 거리를 log 분포로 나타내면, 이 분포가 균등하면 균등할수록 엔트로피는 커질 것이고 regularizer는 \`reduce\` 가 되게끔 역수를 취해 사용한다. 물론 embedding point 간의 거리 자체가 확률 분포를 표방하는 값으로 간주되므로 정규화 작업 전 normalization을 통해  scale을 조정해준다. 즉 거리를 균등하게 함으로써 필요한 정보량을 최소로 하고자 하는 것이다.

$$
    \\mathcal{L}_{\\text{koleo}}= -\\frac{1}{n}\\sum_{i=1}^n \\log (d_{n, i}),~~d_{n, i} = \\min_{j \\neq i} \\parallel x_i -x_j \\parallel
$$

### Adapting the resolution

보다 높은 해상도의 이미지를 사용했을 때 segmentation/detection과 같은 pixel level task에서의 성능이 올라간다. 이는 small object의 경우 low resolution에서 백본에 연산을 돌리면 작은 물체의 feature가 일종의 노이즈처럼 사라지는 현상이 발생되기 때문이다. 그렇다고 해서 무작정 고차원의 이미지를 가지고 모델을 학습시키는 건 메모리나 시간이 투머치로 소모적이기 때문에 권장되지는 않는다.

따라서 DINOv2에서는 pretraining 마지막 일부만 이미지의 해상도를 $518 \\times 518$로 증가시켜서 학습시키는 전략을 사용했다. 


# Efficient Training Details

DINO-v2는 일종의 테크니컬 리포트다. 사실 앞부분만 보아도 데이터 정제 과정이나 학습에 사용한 프레임워크를 방법론으로 제시했다기보단 기존의 연구로부터 이어지는 여러 insight 및 approach를 사용해서 좋은 모델을 만들어보겠다는 노력이 보이기 때문이다.

### Utilizing faster transformer : Flash attention


<div align="center">
    <img src="https://github.com/user-attachments/assets/a00ed3b1-7599-45e9-8833-614b7ed0dab7" width="900">
</div>


[Flash attention](https://arxiv.org/abs/2205.14135)에서 제안한 방법을 사용하면 하드웨어에 최적화해서 적용되기 때문에 더 빠르고 효율적인 연산이 가능하다. 이 논문 저자의 경우 논문에 나온 FlashAttention을 직접 구현하여 사용하였다. 사실 아직 본인은 FlashAttention 논문 자체는 이해하지 못하고 있다. 하드웨어 요소에 대한 이해가 부족한데, 어떻게 하면 이쪽으로 지식 및 기술 스택을 쌓을 수 있으려나…??

### Nested tensors in self-attention

이전의 implementation에서는 서로 다른 patch token 수를 가지게 되는 global crop/local crop이 서로 따로 forward passing 및 backward passing 과정을 거쳤었다. 그러나 새롭게 구현된 버전에서는 이를 동시에 수행함으로써 그만큼의 연산량을 줄일 수 있었다.

### Efficient stochastic depth

[Stochastic depth](https://arxiv.org/abs/1603.09382)는 네크워크 깊이에 따른 layer dropout을 수행하여 레이어 간의 의존성 문제를 해결하고 학습 시 각 layer의 feature map을 빠르게 최적화하는 것이 주된 contribution이었다. DINO-v2에서는 이를 다르게 수행하는데, 레이어를 skip한다는 개념이 아니라 batch 단위로 랜덤하게 들어오는 샘플들을 각 레이어에서 drop out rate $d$에 따라 $(1-d)$만큼의 batch만 block에 통과시키는 전략을 취한다. 어차피 unsupervised setting이기 때문에 batch 순서에 따른 label space의 영향을 무시할 수 있으며, 학습마다 네트워크의 모든 block에 대해 stochastic하게 drop-out을 해줄 경우 오히려 학습 시간이 늘어날 수 있기 때문에 이를 최소화한 전략으로 보인다.

### Fully-shared data parallel (FSDP)

AdamW로 EMA 구조를 가지는 모델을 최적화할 때, model로 하여금 4개의 replica가 필요하다. Student, teacher와 이에 추가로 Adam/AdamW 최적화에 사용되는 first momentum 그리고 second momentum이 필요하기 때문이다. 보다 큰 모델을 사용할 때 memory footprint가 급격히 증가할 수 밖에 없는데, 이를 해소하기 위해 data parallel을 사용하였다. 그리고 이렇게 replica를 gpu에 분리를 할 때 GPU memory가 분리된다는 점에서 또다른 장점이 생기는데, 이는 weight 저장 자체는 float32로 하고 최적화 시 gpu 간 통신에서는 float16으로 부동 소숫점 절반을 날려버려도 학습 성능의 큰 저하 없이 메모리를 줄일 수 있다는 것이다. 원래대로라면 GPU 갯수가 증가하더라도 메모리 총합은 같은게 DDP의 특징이었는데, FSDP를 사용하면 communication 단의 메모리를 절반으로 줄여버리니까, 결론적으로는 보다 많은 GPU를 분리해서 사용할수록 학습 전체 메모리는 줄어드는 장점이 생긴다.

### Model distillation

큰 모델이 가지고 있는representation을 효과적으로 작은 모델에 넘겨줄 때, 작은 모델을 scratch부터 학습시키는 것보다는 큰 모델의 prediction에 align하는 distillation 학습법이 효과적인 것은 어느 정도 알려진 사실이다. 따라서 DINO-v2에서도 결론적으로 작은 모델에 넘겨주는 방식을 distillation으로 했는데, 이때 기존 학습 framework인 EMA는 그대로 가되, 약간의 차이가 발생한다. 우선 self-distillation 구조가 아니므로 teacher는 학습된 large model을 frozen한 채로, 작은 모델을 student로 잡아 spare EMA를 수행한다. 또한 앞서 설명했던 stochastic depth, masking 같은 방법론은 제외한다.


# 실험 결과


<div align="center">
    <img src="https://github.com/user-attachments/assets/d4185e15-b44b-4068-b34c-9de7eb4fc404" width="700">
</div>


DINO-v2는 베이스라인 논문인  [iBOT](https://arxiv.org/pdf/2111.07832.pdf)(image BERT 논문)을 기준으로 짜잘한 방법들이 추가로 사용되었는데, 그래서 제안한 방법들이 얼마나 효과적인지를 task를 고정한 채로 ablation을 진행한 표이다. 이외에 여러 실험 결과들에 대한 내용이 페이퍼에 있는데, 대부분 성능이 올라갔다는 점을 드러내고 있다.


# 결론

DINO-v2 논문을 보면 주인공이 모든 무술을 연마하여 마스터해버리는 무협 영화(?)장르가 생각난다. 메타의 이 연구는 단순히 scaling-up하는데 목적을 두지 않고 보다 효율적인 학습법과 그러면서도 어떻게 좋은 representation을 얻을 수 있는지 다양한 방법들을 적용해보고 실험해본 결과물로 보인다. 어느새 학계에서 성능 좋은 베이스라인 모델을 만드는 것은 불가능에 가까운 게 아니라 불가능이 되어버렸다. 이제는 논문을 쓰는 과정에서 집중해야 할 곳들은 이런 SSL 자체보다는 학습된 representation을 어떻게 활용하고, mapping하고 혹은 tuning할 것인가에 있어 보인다.
`,KO=`---
title: "Mamba modeling의 기초 (1) - Linear State-Space Layer (LSSL)에 대하여"
category: "ai papers"
publishedAt: "2024-02-01"
thumbnail: "https://github.com/user-attachments/assets/4e6b47fa-4749-4dbb-9733-4236cbd95187"
---


# 연속 데이터 구조에 대한 DNN의 발전

Sequential한 데이터를 처리하기 위해 딥러닝 모델은 수많은 변화와 발전을 이루었다. 그 중 요즘 대표적으로 **LLM** 및 **multimodal** 연구에서 활발하게 활용되는 것은 <U>Transformer 구조</U>이지만, 그 이전에는 **LSTM**이나 **GRU**같이 Long term(거리가 먼 문맥 간의 관계성 파악) 모듈과 함께 연구된 Recurrent Neural Network (RNN), 그리고 가장 베이직한 DNN 구조인 CNN(Convolutional Neural Network)를 temporal dataset에 적절하게 변형시켜사용하는 방법이 있었다(예컨데, 비디오 데이터셋에는 temporal information 간의 정보도 사용하기 위해 시간 축을 추가한 3D convolution을 사용하였다).

이외의 방법으로는 신경망 자체의 발전으로는 유명하지는 않지만 <U>보다 복잡한 continuous data를 처리하기 위해</U> neural differential equations (NDEs)를 직접 모델링하는 방법이 주로 사용되었다.

하지만 모든 네트워크는 **나름의 장단점이 확실**했다. RNN (Recurrent Neural Network)은 모두가 알다시피 Long-term module의 발전이 있었음에도 불구하고 여전히 긴 문맥을 처리하는데 연산량이나 시간이 비례해서 증가한다는 문제점이 있었으며, 또한 Long-term 모듈에 의존하기에 복잡한 데이터에서의 문맥 파악을 학습시키기 어렵다는 근본적인 문제가 있었다.

대체로 <U>gradient vanishing problem</U>이나 <U>gradient exploding problem</U>은 continual learning에서와 더불어 RNN과 같은 연속 데이터를 학습함에 있어 catastrophic forgetting의 주된 이유로 등장하기도 했다.

CNN(Convolutional Neural Network)는 local한 정보에 대해 (서로 차원이 붙어있는 특징) 최적화가 빠르다는 장점이 있으며, 어느 정도 문맥이 명확한 비디오 데이터셋이라던지, 이미지와 같은 object centric/semantic centric 데이터에 대해 inductive bias를 가진다는 장점이 있었다. 하지만 연산 자체가 sequence에 대응할 수 있는 구조가 아니다보니, 길이가 길어질수록 RNN과 같은 문제가 발생하였다. 결국 convolution 연산 또한 <U>정해진 context 내에서의 local information만 뽑아내는 구조</U>다 보니, context length에 따라 연산량이나 시간이 비례한다는 문제는 똑같이 생기게 되었다.

NDE (Neural Differential Equation) 모델링은 특정 modality나 정해진 문제를 수학 모델링을 통해 이론화했지만, 그리 효율적이지 않다는 문제가 있다. 대표적으로는 diffusion modeling을 생각해볼 수 있는데, 생성 모델인 diffusion을 이런 효율의 문제를 score function의 이산화로 해결했다. Implicit model의 부담을 줄여주어서 간단한 U-Net 구조를 사용했고, consistency modeling과 같이 또다른 implicit mapping을 통해 해결할 수 있었지만, 이는 각 구간에서의 미분 방정식 solution을 numerical하게 구할 수 있었기 때문이었고 모든 형태의 미분 방정식에서 **일괄적**으로 신경망이 <U>효율적으로 학습될 수 있는 구조를 찾는 것</U>은 불가능하다.

결국 가장 이상적인 모델 구조의 발전 방향은

- Convolution과 같이 병렬화 연산이 가능한 구조여야 효율적일 수 있음.
- Recurrence 형태의 상태 추론과정을 통한 문맥 처리가 되어야함.
- Differential equation과 같이 이산화된 신호가 아닌 time-scale에 적용 가능해야함.

로 요약할 수 있다. 이러한 모델링을 찾기 위해 끊임없는 시도가 있었다.


# 여러 모델링 방법들 소개

### CKConv

그 중 하나인 [CKConv(Continuous Kernel Convolution)](https://arxiv.org/abs/2102.02611)는 **콘볼루션 커널**을 일종의 vector continuous function $\\psi : \\mathbb{R} \\rightarrow \\mathbb{R}^{N_{out} \\times N_{in}}$ 으로 보는 방식이다. 이때 연속 함수 $\\psi$는 작은 신경망 MLP로 parameterize하여 학습시키게 되는데, MLP는 value로 time-step을 **스칼라 값**으로 받아 해당 position에서의 <U>convolution kernel을 벡터로 내보내는 형식</U>이 된다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/67214052-2cbd-4aa5-84aa-565a8b2e7d61" width="900">
</div>


### UnICORNN

RNN 계열에서 ODE 기반의 모델링 (time-scaling을 통한 long-time dependency 확보)에서는 [UnICORNN](UnICORNN)과 같은 연구가 진행되기도 하였다. 간단하게 방법만 소개하면 해당 RNN은 2차 ODE(일반 미방)을 시간 축으로 이산화 (discretization)할 수 있는 오일러 메소드를 사용한다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/219f4ae0-eab7-4051-ba3f-2163361e9c55" width="400">
</div>


위의 그림에서 나와있는 $y$가 얻고자 하는 함수이고 $z$는 얻고자 하는 함수의 1차 미분 함수에 해당된다. 2차 ODE를 직접 풀어서 <U>원하는 함수를 얻기가 힘들기 때문</U>에 $y$의 1차 미분 함수인 $y^\\prime$을 $z$라는 임시 변수로 선언함으로써 2차 미분 ODE를 $z, y$ 간의 1차 미방으로 바꿀 수 있다.

이렇게 변경된 ODE 시스템을 **“Hamiltonian system”**이라고 부른다. 이 Hamiltonian system을 풀어내는 과정에서 시간별 input에 의존하는 연속 함수가 구현이 되고,

$$
    H(y, z, t) = \\frac{\\alpha}{2} \\parallel y \\parallel^2 + \\frac{1}{2}\\parallel z \\parallel^2 + \\sum_{i=1}^m\\frac{1}{w_i} \\log (\\cosh (w_iy_i + (Vu(t))_i + b_i))
$$

각 벡터 $y, z$의 유클리디안 norm 연산인 $\\parallel \\cdot \\parallel$ 을 통해 위와 같이 정리된다. 이 연속 신호 미분 방정식을 오일러 메소드를 통해 이산화하면 얻고자 하는 discrete dynamical system이 추출된다.

### LMU

[Parallelizing Legendre Memory Unit Training (LMU)](https://arxiv.org/abs/2102.11417) 에서는 RNN의 단점 중 하나인 병렬화 불가능 문제를 linear recurrence convolution으로 해결하는 시도를 보였다. 만약 우리가 특정 input의 이전/이후 state를 가져올 수 있는 딜레이 구조의 시스템을 구축할 수 있다면, 해당 시스템의 output으로 input의 recurrence 구조를 확보할 수 있다는 장점이 생긴다. 우리는 Linear system을 찾고자 하기 때문에 (애초에 학습하고자 하는 신경망 연산 자체가 텐서 및 행렬 기반이기 때문이라 생각하면 편하다), 다음과 같이 네 개의 matrices $<A, B, C, D>$ 로 표현되는 **LTI system을 찾는 것**이 목표가 된다. 

$$
    \\begin{aligned}
    \\dot{m} =& Am + Bu \\newline
    y =& Cm + Du
    \\end{aligned}
$$

그리고 I/O 의 라플라스 변환 형태인 $u(s), y(s)$로 SISO system의 transfer function $G(s)$를 정의할 수 있게 된다. 하지만 해당 **transfer function**이 내포하는 어려움은 infinite dimensional하며, continous delay $\\theta$를 모두 커버치기 불가능하다는 문제가 생긴다.

$$
    G(s) = \\frac{y(s)}{u(s)} = e^{-\\theta s}
$$

이제 finite하고 causal한 state space realization 차원으로 가져오기 위해서는 transfer function $G(s)$를 $s$에 대한 polynomial로 구성을 해야한다. 보통 transfer function은 분자와 분모가 각각 특정 차수를 가지는 $s$의 다항식으로 표현되는데, proper 한 dimension을 가지는 시스템은 분모의 차수가 더 높아야한다(그래야 시스템의 convergence를 보장할 수 있기 때문이다). 아무튼 위에 있는 저 식을 approximation 해야한다는 결론에 다다르게 된다. 이를 Linear system에서 구현하기 위해서 앞서 확인했던 것처럼 matrices를 구해야하고, $i,~j \\in [0,d-1]$ 에 대해서 **다음이 성립하는 행렬 요소**를 사용하게 된다.

디테일한 내용이나 증명 과정은 해당 페이퍼의 이전 논문인 [LMU](https://papers.nips.cc/paper_files/paper/2019/file/952285b9b7e7a1be5aa7849f32ffff05-Paper.pdf)를 보거나 아래에 있는 증명 과정을 보면 된다. 

$$
    \\begin{aligned}
    A_{i,j} =& \\frac{(2i+1)}{\\theta}\\begin{cases}
    -1 & i < j \\newline
    (-1)^{i-j+1} & i \\ge j
    \\end{cases}\\newline
    B_i =& \\frac{(2i+1)(-1)^i}{\\theta}
    \\newline
    C_i =& (-1)^i \\sum_{l=0}^i {i \\choose l}{i+l \\choose j}(-1)^l \\newline
    D =& 0
    \\end{aligned}
$$

해당 매트릭스들 중 세번째 matrix인 $C$가 <U>가장 주요 아이디어</U>에 해당된다. $C$는 풀게 되면 르장드르 다항식으로 표현되며, $D = 0$이기 때문에 shifted input $u(t-\\theta)$ 의 정확도를 현재 state $m_t$를 기준으로 판별할 수 있다. 예컨데, $\\theta^\\prime$만큼의 phase가 이동된 신호를 예측하고자 한다면 다음과 같이 **shifted Legendre polynomial**를 통해 근사할 수 있다.

$$
    \\begin{aligned}
    C_i(\\theta^\\prime) = (-1)^i \\sum_{l=0}^i {i \\choose l}{i+1 \\choose j}&\\left(-\\frac{\\theta^\\prime}{\\theta}\\right)^l,~0 \\le \\theta^\\prime \\le \\theta \\newline
    u(t-\\theta^\\prime) \\approx& C(\\theta^\\prime)^\\top m_t
    \\end{aligned}
$$

설명이 길었지만 다시 풀어서 설명하자면, 이상적인 딜레이 시스템을 LTI 시스템으로 구축하여 표현한 것이 기존의 Linear state machine 디자인이었고, 이를 다시 non-linear neural network system을 사용하여 학습한 것이 LMU 구조가 되겠다. <U>딜레이 시스템을 솔루션으로 삼아</U> 네트워크를 학습하려고 한 것이다. 

### HiPPO

[HiPPO](https://arxiv.org/pdf/2008.07669.pdf)는 LMU를 **일반화한 구조**에 해당된다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/0751e256-9966-40b9-b3c8-9f122cec709a" width="800">
</div>


**HiPPO의 방법**은 다음과 같다:

원래 함숫값과 예측된 함숫값 사이의 차이를 measure할 수 있는 Hilbert space $\\mu$상에서 각 구간의 연속 함수인 $f$를 $g$라는 subspace로 보내는 과정을 거친 뒤, 이를 적당한 vector basis의 coefficient의 배열로 표현한다. 그렇게 되면 Continous-time ODE를 LTI system의 미분 방정식으로 표현할 수 있게 되며, 이때 system의 주축이 되는 $A(t)$와 $B(t)$ 함수의 형태를 결정하여 시퀀스 메모리에 대한 중요도를 매핑한다. 이를 통해 기존 LMU를 continuous-time memorization으로 일반화시켰다. 왜냐하면 기존 LMU(르장드르 메모리 유닛)에서는 특정 슬라딩 윈도우 크기($\\theta$)를 가지는 이상적인 delay system의 <U>LTI 미분 방정식을 그대로 이산화하여 사용</U>하기 때문이다. 


# LSSL 모델링


<div align="center">
    <img src="https://github.com/user-attachments/assets/4e6b47fa-4749-4dbb-9733-4236cbd95187" width="800">
</div>


이런 이전 모듈들의 발전은 모두 공통적으로 기존 CNN/RNN의 구조 및 단점을 <U>time-step 차원에서 접근했다</U>는 점이다. 하지만 모든 방법들은 **convolutional/recurrent model**의 문제점을 근본적으로 해결하지 못했다는 점이 한계점으로 작용했다.

Linear State-Space Layer (LSSL)은 위의 그림에서 나오는 각각의 장점을 통합한 구조를 고안하는 것을 주된 목적으로 삼았다. 결국 formulation은 이전 approach와 큰 차이는 없다. LSSL은 1-dimensional function 혹은 sequence $u(t) \\rightarrow y(t)$를 implicit function $x(t)$를 통해 mapping하고자 하는 방법이다.

$A$는 앞서 봤던 LMU에서와 같이 system의 **implicit function** $x(t)$의 **evolution**을 조정하는 matrix이며, $B, C, D$는 **projection**에 사용된다.

$$
    \\begin{aligned}
    \\dot{x}(t) =& Ax(t) + Bu(t) \\newline
    y(t) =& Cx(t) + Du(t)
    \\end{aligned}
$$

만약 $\\Delta t$를 <U>discrete step-size</U>로 정한다면, LSSL은 정해진 갯수의 메모리와 연산으로 각 시간 축에 따라 state를 변화시키는 **recurrent model**로 해석할 수 있으며, LTI system인 위의 두 수식은 결국 **continous convolution**으로 표현될 수 있다. 고로 discrete-time version의 LTI system 또한 convolution으로 병렬화가 가능하다. 학습 속도가 빨라질 수 있다는 것이다. 마지막으로 LSSL은 LTI system의 모델링 자체가 differential equation이기 때문에 continous-time model의 모든 적용 가능한 상황을 그대로 모방할 수 있다. 

결국 이 논문에서 밝히고자 한 내용은 위의 LSSL이 고전적인 제어 이론으로부터 익히 알려져있는 사실과 같이 모든 형태의 1-D Convolution을 표현할 수 있을 뿐만 아니라, 적절한 step size인 $\\Delta t$ 그리고 적절한 state matrix $A$를 가지고 RNN 및 ODE가 가지는 특성(특히 장점에 집중)을 그대로 가져올 수 있다는 것이다. $A$는 다시 말하지면 시스템의 변화를 주도하는 학습 행렬로 사용되는데, HiPPO와 같은 이전 연구에서 드러났던 것처럼 연속 시간에 대한 memory를 고려하면서 동시에 long dependency를 고려해야한다.


# Continuous-time memorization

Continuous time memorization 에 대한 근사화(approximation)는 HiPPO 그리고 LSSL 논문에서 공통적으로 가지는 이론적/기술적 배경에 해당된다.

필연적으로 연속 시간 모델링을 그대로 적용할 수 없기 때문에 이를 이산 시간 모델로 근사화 혹은 다운 샘플링하는 과정을 거치게 된다.

디퓨전 모델링에서도 확인할 수 있었던 것처럼 결국 연속 시간 미분 방정식의 $dt$를 얼마나 조정하냐에 따라 생성 성능이 달라졌기 때문에, 결국 연속 시간 모델링을 이산화할 때는 step size/time scale인 $\\Delta t$를 조절하는 것이 중요하다.

해당 섹션에서는 LSSL 모델링으로부터 여러 property에 대한 insight를 얻을 수 있는 **근거**라고 볼 수 있는 개념들에 대해서 정리하도록 하겠다.

### Approximations of differential equations

모든 형태의 differential equation $\\dot{x}(t) = f(t, x(t))$는 integral equation $x(t) = x(t_0) + \\int_{t_0}^t f(s, x(s))ds$을 동치로 가지게 된다. 해당 integral solution은 함수 $x$의 근사치를 $f(s, x(s))$에 넣고 계속 연산을 하는 방식으로 풀어낼 수 있다. 예컨데 $x_0(t) = x(t_0)$라는 함수 초기 조건을 가지고 있다면,

$$
    x_{i+1} (t) = x_0 (t) + \\int_{t_0}^t f(s, x_{i}(t))ds
$$

위와 같이 근사화할 수 있다. 이를 *[Picard iteration](https://en.wikipedia.org/wiki/Picard%E2%80%93Lindel%C3%B6f_theorem)* 이라고 부른다.

### Discretization

그리고 이산화 과정에서 함수를 직접 적분해낼 수 없기 때문에 discrete times $t_i$에 대해, $x(t_i)$를 쪼개서 얻어내야한다. Integral equation의 form을 closed form으로 정확히 계산할 수 있다면 단순히 downsampling하는 방법으로 각 $x(t_0), x(t_1), \\cdots$ 를 얻어내거나, closed form으로 알지 못하더라도 *picard iteration*을 각 구간별 integral equation인

$$
    x(t_{k+1}) = x(t_k) + \\int_{t_k}^{t_{k+1}} f(s, x(s)) ds
$$

에 적용하여 각 $t_k$ 시점의 함숫값들을 샘플링할 수 있다. 다른 방법으로는 **generalized bilinear transform (GBT)**가 있는데, 이는 현재 우리가 관심있는 Linear ODE에 적용될 수 있는 방법이다. 풀고자하는 Linear ODE의 형태가 다음과 같을때,

$$
    \\begin{aligned}
    \\dot{x}(t) =& Ax(t) + Bu(t) \\newline
    y(t) =& Cx(t) + Du(t)
    \\end{aligned}
$$

GBT update는 다음의 수식으로 진행된다. 수식에서의 $\\Delta t$는 step size를 의미한다.

$$
    x(t+\\Delta t) = (I-\\alpha \\Delta t \\cdot A)^{-1}(I+(1-\\alpha)\\Delta t \\cdot A)x(t) +\\Delta t(I-\\alpha \\Delta t \\cdot A)^{-1}B \\cdot u(t)
$$

수식이 조금 복잡해서 한번에 잘 이해가 되질 않지만 특별한 케이스를 보면 이해하기 어렵지 않다. $\\alpha = 0$을 위 수식에 대입하면,

$$
    \\begin{aligned}
    x(t+\\Delta t) =& x(t) + \\Delta t \\cdot (Ax(t) + Bu(t)) \\newline
    =& x(t) + \\Delta t \\cdot \\dot{x}(t)
    \\end{aligned}
$$

위와 같이 표현되며 이는 가장 대표적인 방법인 *Euler method*임을 알 수 있다. 결국 $\\alpha$는 동일하게 함수를 구하는 방식에서 어느 위치에서의 미분값을 사용하냐에 따라 달려있다. $\\alpha=1$이 되면 *backward Euler method* 가 되는데, 이는 동일하게 함수를 예측할 때 특정 위치에서의 도함수에 기반한 first order approximation이라는 점은 같지만 특정 위치가 $t$ 가 아닌 $t + \\Delta t$ 라는 점에서 차이가 있다.

$$
x(t+\\Delta t) = (I-\\Delta t A)^{-1}x(t) + \\Delta t (I - \\Delta t A)^{-1} B \\cdot \\dot{x}(t)
$$

따라서 $\\alpha = \\frac{1}{2}$를 사용하게 되면 서로 다른 두 위치의 도함수 평균을 쓰게 되므로, 만약 곡률이 큰 복잡도가 높은 함수가 솔루션을 구성하는 상황에서는 같은 $\\Delta t$를 사용하더라도 보다 안정적인 함수 예측이 가능해진다. 이를 *bilinear* 방법이라고 부른다.

$$
x(t+\\Delta t) = (I-\\Delta t / 2A)^{-1}(I+\\Delta t / 2A) x(t) + \\Delta t (I - \\Delta t / 2A)^{-1} B\\cdot\\dot{x}(t)
$$

이렇게 *bilinear* 방법에 사용되는 matrix A와 B를 $\\bar{A}, \\bar{B}$ 라고 했을 때, 이를 통해 위의 시스템을 discretize하게 되면 다음과 같은 discrete-time state-space model을 구할 수 있다. 

$$
\\begin{aligned}
x_t =& \\bar{A}x_{t-1} + \\bar{B}u_t \\newline
y_t =& Cx_t + Du_t
\\end{aligned}
$$

### Timescale factor

시퀀스 길이에 따른 dependency는 길이가 길어질수록 줄어든다. 예컨데 $\\Delta t$ 만큼을 시간 간격으로 잡는다면 의존도는 그에 반비례하게 된다. 대부분의 ODE 기반 RNN 구조에서는 $\\Delta t$를 고정값으로 사용하였지만, classical RNN의 gating 메커니즘은 이를 학습하는 것과 같은 효과를 지닌다. 그리고 CNN 관점에서의 $\\Delta t$는 convolution kernel의 크기를 조절하는 형태로 해석이 가능하다. 즉, CNN이든 RNN이든 ODE 기반으로 해석한다면 모두 시간 간격인 $\\Delta t$를 어떻게하면 최적화할 수 있을까에 대한 문제로 해석이 가능하다는 것이다.

### Continuous-time memory

입력되는 함수 $u(t)$와 고정된 probability measure(메트릭) $\\omega(t)$가 있을 때, 함수의 기본꼴이 되는 $N$개의 basis가 있다고 가정해보자. 각 time step $t$마다 이전까지의 input들인 $u(\\tau)\\vert_{\\tau < t}$ 는 $N$개의 basis의 조합으로 표현이 가능하고, 이는 곧 함수를 projection하여 획득한 coefficient vector $x(t) \\in \\mathbb{R}^{N}$ 이다. 이때 각 time step마다의 최적의 솔루션은 거리 메트릭 $\\omega(t)$에 의존하게 된다. 이렇듯 함수 $u(t)$를 coefficient $x(t)$로 표현하는 과정이 앞서 소개했던 HiPPO (High-Order Polynomial Projection Operator)가 된다.  

HiPPO의 경우에는 두 가지 경우(해당 논문에서는 LegT, LagT라는 이름으로 제안된 메트릭)를 제안하였는데, 모든 time step에 같은 중요도를 매핑하는 uniform measure $\\omega = \\mathbb{I}\\{[0, 1]\\}$ 와, 가까운 time step에 보다 높은 중요도를 매핑하는 exponential-decaying measure $\\omega(t) = \\exp(-t)$ 가 있다. 논외긴 하지만 HiPPO에서는 정해진 sliding window 크기를 가지는 translated Legendre (LegT) 대신 long dependency 및 forgetting 문제를 해결하고자 scaled Legendre (LegS)를 사용하였다. 둘의 공통점은 window 안에서 균일한 measure weight을 가진다는 점이지만, LegS는 시간이 흐를수록 window 크기가 커진다는 차이점이 있다. 아무튼 중요한 점은 measure 종류에 따라 matrix $A$를 closed form으로 풀어낼 수 있으며, 이를 토대로 long dependency에 대한 모델링이 가능하다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/fb29570d-a928-4d60-9151-57c968e217ee" width="1000">
</div>


각 메트릭에 따른 matrix $A$를 정하는 과정은 HiPPO 논문의 Appendix를 참고하면 되는데, 이를 조금 간단하게 정리해보고자 한다. 관련 내용을 이해하는데 필요한 사전 지식이 너무 방대하여 완벽한 증명 과정을 담기에는 무리가 있지만 그럼에도 HiPPO 전반적인 내용을 이해해야 LSSL 모델링을 해석할 수 있기 때문이다.

### Orthogonal Polynomials

Orthogonal polynomials (서로 수직 관계에 있는 다항식)은 함수를 해석하는데 사용되는 기본적인 툴이다. 모든 measure $\\mu$ 상에서 해당 OP에 대응되는 unique한 함수 시퀀스가 나오게 된다. 여기서 measure metric은 적분이 이루어지는 서브 공간으로 이해하면 된다. OP의 특징은, 서로 다른 OP들을 measure 상에서 적분했을때 0이 나와야한다는 것이다. 그리고 $i$번째 Polynomial은 차수가 $i$라는 constraints도 포함된다.

$$
\\langle P_i, P_j \\rangle_\\mu = \\int P_i(x) P_j(x) d\\mu (x) = 0~~(i \\neq j),~\\deg (P_i) = i
$$

이러한 조건에서 $f$라는 이상적인 함수에 근사하는 최적의 솔루션은 다음과 같이 계산된다.

$$
\\sum_{i=0}^{N-1} c_i P_i(x) / \\parallel P_i \\parallel_\\mu^2,~\\text{where }c_i = \\langle f,P_i \\rangle_\\mu = \\int f(x)P_i (x) d\\mu(x)
$$

가장 대표적으로 유명한 OP에는 Fourier series basis를 생각해볼 수 있고, Jacobi, Laguerre 혹은 Hermite Polynomial도 이에 포함된다. 여기에서 소개할 OP는 Jacobi Polynomial에 속하는 르장드르 다항식이다.

### Legendre Polynomials

르장드르 다항식은 흔히 구면 좌표계에서 많이 사용한다. 공학 수학을 배울 때의 악몽이 떠오르는 기분이다. 암튼 orthogonal 관계는 익히 알려진대로 구간 $[-1, 1]$ 내에서 $L^2$ 내적을 취했을 때 $\\frac{2}{2n+1}$ 만큼 스케일링된 크로네커 델타를 획득할 수 있다. 그리고 유명한 성질 중 하나가 $P_n(1) = 1, P_n(-1) = (-1)^n$ 라는 경계조건을 가진다는 것.

여기서 일종의 선형성을 통해 다양한 time-scale 축에 대한 Polynomial 또한 구할 수 있다. 결국 르장드르 다항식이 성립하는 measure 공간 자체도 균일 확률 분포였기 때문에 가능한 일이다. 

원래의 orthogonality는 $[-1, 1]$에서 성립했고, 이를 $[0, t]$ 구간에서 성립하게 하기 위해 함수 구간을 맞춰주게 되면 다음과 같다.

$$
\\begin{aligned}
(2n+1)\\int_0^t P_n \\left( \\frac{2x}{t} - 1 \\right) P_m \\left( \\frac{2x}{t}-1 \\right) \\frac{1}{t} dx = \\frac{2n+1}{2}\\int P_n P_m \\omega_\\text{leg} dx
\\end{aligned}
$$

적분 구간만 맞춰줬는데 다시 크로네커 델타를 획득할 수 있다. 고로 measure가 스케일링된 경우 르장드르 다항식은 원래의 다항식을 스케일링 해주면 쉽게 얻을 수 있다.

$$
(2n+1)^{1/2} P_n \\left(\\frac{2x}{t} - 1\\right)
$$

### Translated Legendre

Translated Legendre는 윈도우 크기가 $\\theta$이고, 현재 지점이 $t$인 경우의 Legendre measure를 의미한다.

$$
\\begin{aligned}
\\omega(t, x) =& \\frac{1}{\\theta} \\mathbb{I}_{[t-\\theta, t]} \\newline
p_n(t, x) =& (2n+1)^{1/2}P_n\\left(\\frac{2(x-t)}{\\theta} + 1\\right) \\newline
g_n(t, x) =& \\lambda_n p_n (t, x)
\\end{aligned}
$$

그리고 원래 여기에서 tilting 개념이 등장하면서 굳이 OP를 쓰지 않을 때 사용하는 함수가 등장한다. 이를 $\\chi$라고 하는데, 만약 $p_n(t, x)$ 대신 조합된 함수 형태인 $p_n(x)\\chi(x)$를 쓴다고 가정한다면 각 time step에서 이번에는 $\\omega/\\chi^2$에 orthogonal해지게 된다 (OP 곱하기 OP 곱하기 $\\chi^2$이 되므로). 만약 normalized된 measure와 orthonormal basis를 구한다치면,

$$
\\zeta(t) = \\int \\frac{\\omega}{\\chi^2} = \\int \\frac{\\omega^{(t)}(x)}{(\\chi^{(t)}(x))^2}dx
$$

해당 함수가 곧 normalization constant가 된다. 그렇기에 normalized된 measure인 $\\nu^{(t)}$는 $\\frac{\\omega^{(t)}(x)}{\\zeta(t)\\cdot(\\chi^{(t)}(x))^2}$를 density로 가진다. 이렇게까지 하는 이유는 결국 tilted OP를 orthonormal하게 맞춰주기 위함이다. 위의 수식을 사용하여 orthogonality를 확인하면 르장드르에서의 orthogonality가 원래의 measure $\\omega$에 대해 정규화가 됨을 알 수 있다. 하지만 이건 특수한 경우에 formulation을 위해 사용하게 되지만, 르장드르에 의한 projection에는 사용되지 않는다. 따라서 그냥 일반적인 수식을 생각해주면 된다. 앞서 추가로 언급했던 르장드르 다항식의 특성을 활용하면 마찬가지로 shifted and scaled Legendre에 대해,

$$
\\begin{aligned}
g_n(t, t) =& \\lambda_n (2n+1)^{1/2} \\newline
g_n(t,t-\\theta) =& \\lambda_n (-1)^n (2n+1)^{1/2}
\\end{aligned}
$$

위의 경계조건을 가진다.

### Projection and Coefficients

$A$ 하나 유도하는데 너무 돌아가는 듯 하지만 HiPPO를 완전히 정복하기 위해선 필수적인 수식들이다. 앞서 tilting을 고려한 measure를 유도했었는데, 이를 사용하여 coefficient를 계산하기 위해 measure에 projection한 결과는 다음과 같다.

$$
c_n(t) = \\zeta(t)^{-1/2} \\lambda_n \\int fp_n^{(t)} \\frac{\\omega^{(t)}}{\\chi^{(t)}}
$$

해당 수식을 토대로 end-to-end model을 구성하고, 해당 네트워크가 online prediction에 기반에서 이전의 함숫값 $f$ 그리고 현재의 함수를 제대로 대변하게 하기 위해서는 $c(t)$를 벡터로 표현해야하고, 이는 곧 coefficient의 벡터 형태로 얻고자 하는 목적에 부합한다.

Coefficient는 항상 현재의 예측에 기반하여 업데이트되어야한다. 즉 coefficient는 고정되어있지 않고 지속적으로 변하는 함수로 고려해야하며, 이에 맞는 미분 방정식을 생각해볼 수 있다.

$$
\\begin{aligned}
\\frac{d}{dt} c_n(t) &= \\zeta(t)^{-1/2} \\lambda_n \\int f(x) \\left(\\frac{\\partial}{\\partial t}p_n (t, x) \\right) \\frac{\\omega}{\\chi} (t, x) dx \\newline
&+\\int f(x) \\left( \\zeta^{-1/2}\\lambda_n p_n(t, x) \\right)\\left(\\frac{\\partial}{\\partial t} \\frac{\\omega}{\\chi} (t, x)\\right) dx
\\end{aligned}
$$

### Coefficient dynamics with Translated Legendre

르장드르 다항식의 projection을 구할 때 tilting을 무시한다고 했다. 그러면 위의 수식을 풀어낼 때 필요한 것은 OP의 편미분과 measure의 편미분이다. OP의 편미분은 자세한 과정은 생략하고 결과만 언급하자면 $n$번째 르장드르의 미분은 $n-1$번째의 르장드르까지의 linear combination으로 표현할 수 있다. 놀라운 르장드르의 세계.

그래서 정말 다행이지만 $\\lambda_n p_n(t, x)$의 미분은 수많은 $g$들로 간단하게 표현 가능하다.

$$
\\frac{\\partial}{\\partial t} g_n (t, x) = -\\lambda_n (2n+1)^{1/2} \\frac{2}{\\theta} \\left( \\lambda_{n-1}^{-1}  (2n-1)^{1/2}g_{n-1} + \\lambda_{n-3}^{-1} (2n-1)^{1/2} g_{n-3} + \\cdots \\right)
$$

그리고 measure에 대한 편미분은 rectangular function에 대한 미분과 같다.

$$
\\frac{\\partial}{\\partial t} \\omega (t, x) = \\frac{1}{\\theta}\\delta_t - \\frac{1}{\\theta} \\delta_{t-\\theta}
$$

준비물이 모두 완료되었기 때문에 이를 통해 앞서 구했던 coefficient dynamics를 표현한 미분 방정식에 대입이 가능하다.

$$
\\frac{d}{dt}c_n(t) = -\\frac{\\lambda_n}{\\theta} (2n+1)^{1/2} \\sum_{k=0}^{N-1} M_{nk} (2k+1)^{1/2} \\frac{c_k(t)}{\\lambda_k} + (2n+1)^{1/2} \\frac{\\lambda_n}{\\theta} f(t)
$$

이며 이 때 $M_{nk}$는 $k$가 $n$보다 작거나 같으면 무조건 $1$이고 $k$가 $n$보다 크면 $(-1)^{n-k}$의 값을 가지는 value이다. 이제 임의로 정해줄 수 있는 $\\lambda_n = (2n+1)^{1/2}(-1)^n$를 적용하면

$$
\\frac{d}{dt} c(t) = -\\frac{1}{\\theta} Ac(t) + \\frac{1}{\\theta} B f(t)
$$

의 수식에서

$$
A_{nk} = (2n+1)\\begin{cases}
(-1)^{n-k}& \\text{if }k < n \\newline
1 & \\text{if }k \\ge n
\\end{cases},~~B_n = (2n+1)(-1)^n
$$

앞서 소개했던 LMU가 그대로 나오는 것을 확인할 수 있다.


# LSSL 해석해보기

다시 LSSL로 돌아와서 Fixed state space representation $A, B, C, D$가 주어진 상황을 가정해보자. 간단하게도 LSSL은 input sequence를 output sequence로 매핑하는 과정이 된다. LSSL는 이러한 매핑 과정에서 파라미터 행렬 $A, B, C, D$ 그리고 discretize에 필수적인 $\\Delta t$로 정의된다. 이제 이러한 LSSL이 대체 어떻게 RNN, CNN 그리고 Neural ODE의 모든 특징을 가질 수 있는지 해석해보도록 하겠다.

### LSSL to RNN

LSSL에서의 recurrent state는 각 time step 이전의 input context를 내포하는 state인 $x_{t-1}$에 해당한다. 현재 state $x_t$ 그리고 output $y_t$는 이산화된 LSSL formulation에 의해 계산된다.

$$
\\begin{aligned}
x_t =& \\bar{A}x_{t-1} + \\bar{B}u_t \\newline
y_t =& Cx_t + Du_t
\\end{aligned}
$$

따라서 RNN 구조와 같이 동작하는 것을 알 수 있다. 심지어 RNN 구조에서의 gated recurrence 도 만족한다. 예컨데 1차원의 gated recurrence 구조 $(1-\\sigma (z))x_{t-1} + \\sigma(z) u_t$는 backward-Euler method로 $\\dot{x}(t) = -x(t) + u(t)$를 이산화한 것과 동일하다. $z$는 임의의 expression이 모두 가능한데, sigmoid function 특성과 앞서 소개한 GBT를 생각하면 $\\Delta t = \\exp (z)$로 표현했을때 gated recurrence가 $A = -1, B = 1$인 backward-Euler method임을 증명할 수 있다. 그런데 여기서 의문이 생길 수 있는 점은, Linear system에서 구축한 state layer가 과연 일반적인 deep RNN이 가지는 non-linearity 및 복잡도를 표현할 수 있는가에 대한 문제이다.

앞서 단순히 $\\dot{x}(t) = -x(t) + u(t)$의 이산화에 대해 언급했었는데, 이를 다르게 해석해서 *Picard iteration*  을 사용한다고 생각하면, 결국 deep RNN은 학습 과정에서 *Picard iteration* 을 거치면서 함수를 찾아간다고 생각할 수 있다. 즉, 만약 linear recurrence가 아닌 non-linear recurrence를 사용한다면 LSSL 또한 non-linearity를 학습할 수 있게 된다. 이를 통해 RNN 구조와 LSSL는 필요충분 관계에 놓여있다고 볼 수 있다.

### LSSL to CNN

간단한 상황을 가정하기 위해 initial state를 $0$이라 가정해보자. 그렇게 되면 Linear state system을 풀어낸 output을

$$
y_k = C(\\bar{A})^k\\bar{B}u_0 + C(\\bar{A})^{k-1}\\bar{B}u_1 + \\cdots + C\\bar{A} \\bar{B}u_{k-1} + \\bar{B}u_k + Du_k
$$

이처럼 정리할 수 있으며, 이는 곧 discrete-time convolution으로 표현 가능하다.

$$
\\begin{aligned}
&y = \\mathcal{K}_L (\\bar{A}, \\bar{B}, C) \\ast u + Du \\newline
&\\mathcal{K}_L (\\bar{A}, \\bar{B}, C) = (CA^iB)_{i \\in [L]} \\in \\mathbb{R}^L
\\end{aligned}
$$

따라서 LSSL은 output이 convolution에 의해 연산되는 모델로 해석 가능하며, 콘볼루션 연산은 FFT로 가속화가 가능하다.

일반적인 continous state-space system의 관점에서 output $y$는 input $u$에 대해 시스템의 impulse response function $h$와의 콘볼루션 연산으로 표현된다.

$$
y(t) = \\int h(\\tau)u(t-\\tau) d\\tau
$$

이와는 조금 다르게, convolutional filter가 만약 rational functional degree ($N$)를 가지는 경우, 크기가 $N$인 state-space model로 필터를 나타낼 수 있다. 기존 연구들에서 밝혔던 점을 토대로 임의의 convolutional filter $h$는 유한한 degree 값을 가지는 rational function으로 표현이 가능하다. 앞서 봤던 HiPPO matrix의 케이스를 예로 들어보도록 하자. 필요한 사전지식을 정리할 때 Translated Legendre의 경우를 보게 되면, $A$는 특정 구간($\\theta$) 내에서 동일한 확률 분포를 가지는 measure에서 정의되었다. 일반적인 LSSL에서 $dt$를 고정시켜서 생각했을 때, 첫번째 식인

$$
\\dot{x}(t) = Ax(t) + Bu(t)
$$

은 history element를 기억하는 과정에 해당되고 두번째 식인

$$
y(t) = Cx(t) + Du(t)
$$

은 해당 윈도우 내에서 유의미한 feature를 뽑는 작업이다. 그렇기 때문에 LSSL은 결국 width가 학습 가능한 convolutional kernel filter를 학습하는 과정과 동치라고 생각할 수 있다.


# Deep Linear State-System Layers

일반적인 LSSL은 간단하게 요약하면 입력 시퀀스를 출력 시퀀스로 매핑하는 시스템이었다. 예컨데 길이가 $L$인 신호가 있다면, LSSL은 $\\mathbb{R}^L \\rightarrow \\mathbb{R}^L$을 수행하는 하나의 vec to vec 함수 구조이며 이때 함수 자체는 parameterized 되어있다. 만약 LSSL을 $\\psi$라고 한다면,

$$
\\psi(\\cdot \\vert A, B, C, D, \\Delta t),~A \\in \\mathbb{R}^{N \\times N},~B \\in \\mathbb{R}^{N \\times 1},~C \\in \\mathbb{R}^{1 \\times N},~D \\in \\mathbb{R}^{1 \\times 1}
$$

이처럼 표현할 수 있다. 앞서 언급했던 것처럼 단일 LSSL은 Recurrence, Convolution의 특징을 모두 가지고 있기 때문에 RNN과 CNN의 대표적인 레이어인 recurent unit이나 convolution kernel처럼 사용할 수 있다.

또한 입력 시퀀스가 transformer의 input처럼 $H$의 hidden dimension을 가지고 있다고 하면($L \\times H$), LSSL은 $H$만큼의 LSSL을 독립적으로 학습하게 되고, Transformer의 multi-head 효과 또한 그대로 적용할 수 있다.

말하고자 했던 것은 LSSL를 stacking하는 과정으로 기존 DNN 방법론과 같이 다양한 함수를 모사할 수 있으며 동시에  normalization, residual connection과 같은 방법론과 함께 모델링될 수 있다는 사실이다.


# LSSL과 Continuous-time Memorization

LSSL이 기존 DNN 모델링의 특징을 살리면서 사용될 수 있다고 해서 무작정 사용할 수는 없는 노릇이고, LSSL이 장점을 보일 수 있어야 한다.

### Long dependency into LSSLs

Discretized Linear system ODE에서, 시스템은 이산화된 parameter $\\bar{A}$가 계속 곱해지며 발전해간다.

$$
x_t = \\bar{A}x_{t-1} + \\bar{B}u_t
$$

그 말은 gradient descent로 학습하게 되면vanishing gradient 문제를 피할 수 없다는 것이다. 이처럼 만약 $A$를 랜덤하게 초기화한 후 학습하는 형태를 사용하면, 기대하는 성능이 나오지 않을 것이라는 말이 된다.

하지만 HiPPO와 같은 framework에서는 measure $\\omega$에 따라 어떤 방식으로 이전 function을 기억할 지에 대한 문제를 언급했었다 (projection/coefficient화 과정을 통해). 그러나 HiPPO의 문제점이라고 한다면 이렇게 매뉴얼하게 정한 hippo matrix를 학습하지 못하고 그대로 사용해야한다는 점이다. 왜냐하면 HiPPO에서는 르장드르를 포함한 일부 measure에 대해서만 이를 풀어낼 수 있는 structured solution matrix $A$가 존재했고, 모든 일반적인measure에도 다른 형태의 $A$가 존재할 수 있다는 사실을 밝히지 못했기 때문이다.

따라서 LSSL에서는 이를 arbitrary measure $\\omega$로 확장시키고, 이때 Low-recurrence width $A$에 대한 미분 방정식을 찾을 수 있다고 증명하였다.

### Efficient Algorithms for LSSLs

그러나 A와 $\\Delta t$가 상당히 중요한 parameter임이 드러났음에도 불구하고, naive LSSL에서는 학습하기 어렵다는 문제가 발생한다. LSSL은 MVM(Matrix Vector Multiplication) 그리고 Krylov function을 연산할 때 (각각 convolution/recurrence에 해당) 전자의 경우에는 matrix inversion이 필요하다는 어려움과,

$$
x(t+\\Delta t) = (I-\\alpha \\Delta t \\cdot A)^{-1}(I+(1-\\alpha)\\Delta t \\cdot A)x(t) +\\Delta t(I-\\alpha \\Delta t \\cdot A)^{-1}B \\cdot u(t)
$$

후자는 $\\bar{A}$를 feautre의 길이인 $L$만큼 곱해야 한다는 문제가 발생한다.

$$
\\mathcal{K}_L (\\bar{A}, \\bar{B}, C) = (CB, CAB, \\ldots, CA^{L-1}B)
$$

이를 해결하기 위해 $A$를 학습할 때의 효율성을 증대하기 위한 조건이 하나 더 발생한다. 모든 기존의 fixed LSSL의 $A$는 *3-quasiseparable*함이 증명되었다. 만약 학습되는 $A$ 또한 *quasiseparable* 특성을 유지할 경우, MVM과 krylov function 연산이 보다 적은 연산량으로 처리될 수 있다.


# Evaluations and Demonstrations

실제로 특정 조건을 가지는 $A$를 학습할 수 있으면, 이는 이전 HiPPO system $A$보다 더 좋은 성능을 보임을 확인하였다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/b7369d07-aa59-4040-9aa1-28ba9d774e57" width="900">
</div>


또한 길이가 긴 음성 신호의 classification 성능을 통해 long time dependency 또한 입증하였다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/3298c702-4968-41bc-a739-b42807d2e26f" width="600">
</div>


또한 기존 SoTA에 필적하는 성능을 보이기까지 학습 epoch가 훨씬 적어질 수 있음을 보여주었다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/4f2ba576-38d9-4a3d-a79f-5a46b60febc3" width="700">
</div>



# Conclusion

Mamba modeling의 가장 기초가 되는 LSSL을 살펴보았으며, LSSL의 이해에는 HiPPO의 이해가 필수적이기 때문에 해당 논문도 함께 다루었다. 앞으로 몇개의 포스팅을 통해 Mamba를 리뷰하게 될지는 모르겠지만 State Modeling에 대해서는 아무도 제대로 정리를 안해놓을 것 같아서..
`,YO=`---
title: "Mamba modeling의 기초 (2) - (S4) Efficiently Modeling Long Sequences with Structured State Spaces에 대하여"
category: "ai papers"
publishedAt: "2024-02-22"
thumbnail: "https://github.com/user-attachments/assets/448874c2-a202-4c9b-9b74-03131178e257"
---

# 시작하기 전에,

HiPPO는 SSM 구조에서 Long-term range를 구축하기 위한 matrix $A$ 구조의 중요성을 언급하였고, LSSL에서는 SSM을 연속적으로 존재하는 $A$ 전부에 대해 이를 일반화하였다. 이전 글에서 Mamba modeling의 기초가 되는 LSSL에 대해서 설명했었고, 해당 글에 간단한 수식과 관련된 증명을 첨부했었다. 솔직한 심정을 담아 말해보자면, 아직 본인은 이러한 기본 내용들을 전부 이해하지 못했다고 생각하고 있고 이 글을 통해서 맘바를 이해하고자 하는 것은 너무 돌아가는 과정이라고 생각되기도 한다. 자신감 없이 말한 감이 없지 않아 있지만 결국  맘바 모델링 자체를 이해하는데 있어서 state-space modeling을 완전히 뼛속부터 이해할 필요는 없다고 생각한다 (Bottom-up 보다는 Top-down이 맞는 방향이라는 개인적인 의견).

그리고 여러 블로그에 보면 시각화와 함께 맘바 모델링을 한큐에 이해할 수 있게 쉽게 정리해둔 글도 많이 보인다. 그럼에도 불구하고 굳이 글을 길게 써서 리뷰했던 이유는 앞선 글에서 말했던 것처럼 맘바의 근본적인 내용에 대해 이해해보려 노력하는 과정이 의미가 있다고 생각했기 때문이다. 맘바를 간단하게만 이해한다면 맘바가 도대체 왜 transformer 구조가 가지는 문제들을 해결할 수 있었는지, 그리고 단순히 새로운 아키텍쳐로서 등장했다고 해서 무조건 좋다고 생각해야하는 것은 아니기 때문이다.

*무엇이든 쉽게 얻으면 그만큼 쉽게 잃는 법이니까.* 분명이 맘바가 가지는 특징을 제대로 이해할 수 있다면 맘바가 가지는 근본적인 장단점을 발견할 수 있을 것이고, 이를 통해 향후 연구 및 모델링 개발의 기반이 될 것이다. 이번 글에서는 **LSSL가 가지는 연산량과 연산 불안정성을 해결하고자 한** **S4 모델링**에 대해서 정리해보도록 하겠다.

### 다시 시퀀스 모델링으로 돌아가서,

시퀀스 모델링에서 가장 중요한 것은 적당한 길이의 시퀀스를 얼마나 효율적으로 처리하는가와 시퀀스의 길이가 길어질수록 참조할 수 있는 문맥의 길이도 그에 따라 길어져야한다는 것이다. 트랜스포머는 연산량을 희생하여 단일 연산으로 전체 시퀀스에 대한 어텐션 정보를 획득할 수 있고, 토큰 임베딩의 길이를 늘임으로써 이를 해결할 수 있었으나 결국 연산량의 한계가 있다는 문제가 있다. RNN 및 CNN 각각이 가지는 특징들도 있지만 모든 모델링은 각자가 가지는 장단점 때문에 만능일 수는 없었고, 이에 대한 대응으로 SSM(state-space modeling)을 제안한 것이 바로 LMU, HiPPO를 비롯한 논문들이었던 것이다.

### 그래서 SSM을 정리하자면,

SSM은 한마디로 Linear Time-Invariant System으로 latent space를 구축하고자 한 것이다. LTI system의 미분 방정식을 구성하는 matrix가 시간 불변성을 가진다는 특징은 결국 연속 신호를 이산화한 관측 단위에서 미분 방정식은 동일한 식으로 구축되며, 따라서 시퀀스 길이에 무관하게 동일한 latent space $x(t)$를 만들어낼 수 있다는 것이다. gate에 의존하는 RNN과는 다르게 **특정 구조를 가지는** Matrix $A$가 있다면 실제 딥러닝 모델의 예측에 가장 중요한 특징 벡터인 latent를 long range dependent하게 뽑아낼 수 있게 된다. 트랜스포머로 각 토큰 단위로 어텐션을 구하든, RNN 구조로 연속으로 들어오는 데이터로 implicit latent를 만들든 그런 방법들이 아니라 실제로 시간 불변성이 성립하는 latent space를 예측해낼 수 있다면 아무리 오랜 시간이 흘러도 (관측 범위가 처음과 크게 벗어나도) hidden space는 동일한 함수로 구현될 것이기 때문에 Long Range Dependency를 보장할 수 있다는 것이다. 따라서 어텐션 연산에 대해 고정된 문맥 길이를 가지는 Transformer 구조에 비해 상대적으로 더 긴 길이의 시퀀스 데이터를 처리할 수 있고 이론상으로는 무한한 길이의 인풋을 감당 가능하다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/448874c2-a202-4c9b-9b74-03131178e257" width="800">
</div>


논문에 나와있는 그림을 통해 이해해보도록 하자. 우선 좌측의 “Continuous State Space” 부분을 확인해보면, SSM 구조에서 인풋에 해당하는 $u(t)$가 행렬 $A, B, C, D$로 구성된 Linear system의 상태(이를 hidden state, 혹은 latent state $x(t)$라 부른다)를 통해 모델링된 출력값 $y(t)$를 내게 된다. 이때 $x(t)$가 지속적으로 변화하는 input to output $\\mathbf{u} \\rightarrow \\mathbf{y}$를 저장하는 메모리 역할을 수행하게 된다면, 이론상 불변하는(고정된 요소를 가지는) 행렬 $A, B, C, D$에 대해 꾸준이 이전 정보를 저장할 수 있게 된다. 바로 이것이 중앙에 보이는 “Long-Range Dependencies”에 해당되고, 실제로 이에 대한 구조화된 행렬의 효과성을 입증한 것이 “**HiPPO: Recurrent Memory with Optimal Polynomial Projections”** 논문에 해당된다.

이때 기본적으로 SSM은 “Recurrent system”을 가지게 되는데, 이는 시스템의 구조가 입력에 대한 출력 구조로 이어져있으며, 이전 입력 대비 출력 결과에 따른 시스템 변화가 이후 입출력에 영향을 미치게 된다는 것이다. 이를 연산하는 방식으로는 귀납적으로(Recurrent) 연산 후 연산을 하는 방식으로도 구현이 가능하지만 단순화하여 콘볼루션 연산으로 수행하는 것도 가능하다. 그러나 여전히 고차원의 데이터에 대해 필연적으로 증가하는 행렬 연산($e.g.$ $A, B, C, D$ 행렬이나 $\\bar{K}$) 때문에 **연산량이 높다는 문제**가 생긴다.

### “잘” 구조화된 SSM을 사용하면 되지 않을까?? (S4)

따라서 지금 글에서 다루고자 하는 논문인 “**Efficiently Modeling Long Sequences with Structured State Spaces**” (S4)에서의 목적은 다음과 같다. 기존에 LSSL(단순 SSM)의 높은 연산량을 해결하기 위한 방법들이 제안되었지만, 모두 연산상에 numerical stability(연산 안정성 혹은 연산 엄밀성)이 부족했다. 그렇기 때문에 연산 안정성도 높임과 동시에 기존에 존재하던 “잘” 구조화된 행렬에 적용 가능한 알고리즘들을 활용하기 위해 SSM의 토대가 되는 matrix $A$를 **다시 구조화하는 전략을 사용**하였다. 바로 원래 $A$의 rank보다 훨씬 낮은 rank(서로 독립인 column의 갯수를 의미하며, 낮은 rank를 가지는 matrix는 독립이 아닌 column을 모두 배제할 경우 그만큼 차원 수를 줄여서 표현 가능하다)를 가지는 요소와 normal matrix(특수한 케이스의 정사각 행렬로, commutable한 특징이나 diagonal 요소로 분리가 가능하다는 등등의 특징을 사용하여 non-normal matrix에 비해 빠르게 연산이 가능하다) 요소로 분리가 가능하다는 점을 사용한다.

또한 기존의 SSM이 coefficient space(latent를 표현하는 함수는 사전 정의된 여러 orthogonal한 함수들의 coefficient 가중합으로 표현하고자 했었다.)로 접근하는 방식을 사용했다면, 이번에는 주파수 차원으로 올려서 계산하게 된다. 시간 단위에서의 콘볼루션은 주파수 단위에서의 곱연산으로 표현된다.

이를 통해 **Low-Rank 행렬은 Woodbury identity로**, **Normal 행렬은 Cauchy kernel로** 교정 가능하며 이를 토대로 연산량을 $O(N^2L)$에서 $\\tilde{O}(N+L)$로, 메모리는 $O(NL)$에서 $O(N+L)$로 줄일 수 있었다.

# 기존 SSM과 표현 방식

**긴 문맥을 보존할 수 있게끔 모델링**된 matrix $A$ (e$.g.$ HiPPO 행렬)을 사용한다. 그렇게 되면 다음과 같은 연립 미분 방정식으로 표현되는 시스템을 구축할 수 있다.

$$
\\begin{aligned}
\\mathbf{x}^\\prime = A\\mathbf{x} + B\\mathbf{u} \\newline
\\mathbf{y} = C\\mathbf{x}+D\\mathbf{u}
\\end{aligned},\\quad A^{\\text{HiPPO}}_{n, k}= -\\begin{cases}
(2n+1)^{1/2}(2k+1)^{1/2},&\\text{if }n > k \\newline
n+1,&\\text{if }n = k \\newline
0,&\\text{if } n < k
\\end{cases}.
$$

이때 일반적인 컴퓨팅 시스템에서는 Continuous system을 Discrete(이산화된 입력)으로 변화하는 과정이 필요하다. 이를 적용한 식이 실제 SSM에서 적용될 Discrete SSM이다.

$$
\\begin{aligned}
x_k = \\overline{A}x_{k-1} + \\overline{B}u_k\\quad &\\overline{A} = (I-\\Delta/2 \\cdot A)^{-1} (I+\\Delta/2\\cdot A)  \\newline
y_k = \\overline{C}x_k,\\quad &\\overline{B} = (1-\\Delta/2\\cdot A)^{-1}\\Delta B \\newline
&C = \\overline{C}
\\end{aligned}
$$

이에 대한 증명이나 보다 자세한 내용은 [이전 게시글인 LSSL](https://6unoyunr.github.io/blog/lssl)을 보고 오면 좋다. 아무튼 Discrete SSM이 의미하는 바는 SSM이 결국 recurrent 연산 구조를 가지기 때문에 **RNN**의 연산 특징을 가진다는 것. 이산화된 Matrix $\\overline{A}$가 hidden state $x$의 transition matrix 역할을 수행한다. 헌데, 위의 식에 대한 hidden state와 output을 $x_{-1} = 0$라 가정한 후에 전개하면, convolution kernel에 대한 연산으로도 표현이 가능하다.

$$
\\begin{aligned}
x_0 =& \\overline{B}u_0,\\quad x_1 = \\overline{AB}u_0 + \\overline{B}u_1,\\quad x_2 = \\overline{A}^2\\overline{B}u_0+\\overline{A}\\overline{B}u_1 +\\overline{B}u_2,~\\cdots \\newline
y_0 =& \\overline{CB}u_0,\\quad x_1 = \\overline{CAB}u_0 + \\overline{CB}u_1,\\quad x_2 = \\overline{CA}^2\\overline{B}u_0+\\overline{CA}\\overline{B}u_1 +\\overline{CB}u_2,~\\cdots \\newline
y_k =& \\overline{K} \\ast \\mathbf{u},\\quad \\overline{K}\\in\\mathbb{R}^L := \\mathcal{K}_L(\\overline{A}, \\overline{B}, \\overline{C}) := (\\overline{C}\\overline{A}^i\\overline{B})_{0\\le i < L}
\\end{aligned}
$$

그러므로 만약 convolutional filter $\\overline{K}$에 대해 알고 있다는 가정을  한다면 FFT(빠른 콘볼루션 연산 알고리즘)을 통해 연산 속도를 개선할 수 있지만, 이 필터를 연산하는 과정 자체도 행렬곱이 필요하며 non-normal matrix $\\overline{A}$에 대해 일반화된 연산을 하기에는 어려움이 따르게 된다. 즉, 이 논문에서 하고자 하는 것은 해당 필터를 어떻게 효율적으로 연산하는가에 대한 부분이다.


# 방법론 : Diagonalization (대각화)

행렬의 대각화 (Diagonalization)는 행렬의 고유값(eigenvalue)인 $\\lambda$와 고유벡터(eigenvalue)를 활용하여 대상이 되는 행렬을 고유값이 대각선 성분인 행렬로 만드는 과정이다. 예컨데 대각화(Diagonalization)이 가능한 행렬 $A \\in \\mathbb{R^{n \\times n}}$가 존재한다면, 이 행렬의 eigenvalue $n$개와 이에 대응되는 eigenvector $n$에 대해서 $\\Lambda = V^{-1}AV$로 표현 가능하다. 이때, $\\Lambda$와 $V$는 각각 다음과 같다.

$$
\\text{For eigenvalues }\\{ \\lambda_i\\}_{i=1}^n \\text{ and eigenvectors } \\{v_i\\}_{i=1}^n,\\quad\\Lambda = \\begin{bmatrix}
\\lambda_1 & 0 & 0 & \\cdots & 0 \\newline
0 & \\lambda_2 & 0 & \\cdots & 0 \\newline
\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\newline
0 & 0 & 0& \\lambda_{n-1} & 0 \\newline
0 & 0 & 0 & 0 & \\lambda_n 
\\end{bmatrix},\\quad V = \\begin{bmatrix}
\\mid & \\mid & \\cdots & \\mid \\newline
v_1 & v_2 & \\cdots & v_n \\newline
\\mid & \\mid & \\cdots & \\mid \\newline
\\end{bmatrix}
$$

이런 상황에서, 기존의 식을 조금 바꿔보면 다음과 같이 정리할 수 있다. 우선, 일반적으로 SSM에서 $D = 0$으로 간소화하여 사용하기 때문에 $\\mathbf{y} = C\\mathbf{x}$로 표현하도록 하겠다.

$$
\\begin{aligned}
\\mathbf{x}^\\prime =& A\\mathbf{x} + B\\mathbf{u} \\newline
\\mathbf{y} =& C\\mathbf{x}
\\end{aligned}\\quad\\rightarrow\\quad
\\begin{aligned}
\\tilde{\\mathbf{x}}^\\prime =& V^{-1}AV\\tilde{\\mathbf{x}} + V^{-1}B\\mathbf{u} \\newline
\\mathbf{y} =& CV\\tilde{\\mathbf{x}}
\\end{aligned} 
$$

두 식이 서로 조금 달라보이지만, 우측과 같이 전개된 시스템의 좌측에 전부 $V$를 곱하게 되면, $x = V\\tilde{x}$인 SSM과 동치인 것을 알 수 있다. 즉 입력 대비 출력으로 이어지는 관계성 $\\mathbf{u} \\rightarrow \\mathbf{y}$은 동일한 SSM 시스템이고, 이때의 system latent는 행렬 $A$의 eigenvector matrix $V$에 의해 바뀌게 된다. 위의 식처럼 새로운 시스템에서의 $A$행렬에 해당되는 대각 행렬 $V^{-1}AV$을 구축할 수만 있다면, 앞서 보았던 콘볼루션에서 $A$의 곱연산의 연산량을 효과적으로 줄일 수 있다. (이를 **Vandermonde product**라 한다. 그냥 그렇다고 생각하고 넘어가자)

그러나 유감이지만, **HiPPO 행렬에 대한 대각화는 진행할 수 없다**.


<div align="center">
    <img src="https://github.com/user-attachments/assets/2fe8e285-6e46-4997-a4ac-2966d1692fdc" width="600">
</div>


그 이유는 슬프게도 HiPPO 행렬이 대각화되면서 고유벡터로 이루어진 matrix $V$의 성분이 **너무 커지기 때문**이다. 쉽게 말하면, 컴퓨터 상에서 연산이 안정적이려면(실제 수학적 계산값과 일치하려면), 행렬 연산 과정에서 행렬 요소가 너무 큰 값을 가지면 안되기 때문에 불가능하다는 것이다. 앞서 보여준 HiPPO 행렬 $A$를 대각화하면 요소가 $V_{ij} = {i+j \\choose i-j}$가 되는데 (combination), state size $N$이 커지면 커질수록 최대 $2^{4N/3}$까지 커지는 요소를 감당할 수 없다 ($e.g.$ 출력값인 $CV\\tilde{\\mathbf{x}}$ 연산에 문제가 생길 수 있다.).

# 방법론 : Normal + Low Rank Matrix

위에서 다룬 것은 대각화를 통해 연산의 용이성을 올려보자!라는 내용이지만, 결국 기본적인 HiPPO 행렬에 대해서 적용하기는 힘들고 추가 작업이 필요하다는 것을 암시한다. 가장 이상적인 조건은 대각화의 대상이 되는 행렬 $A$가 unitary matrix와 같이 특수 행렬로 대각화가 가능한 경우에 해당된다. 선형 대수에서 이러한 조건이 만족하는 행렬 $A$의 모음을 **“normal matrices”**라 부른다. 눈치챘을 수도 있지만 당연히 HiPPO matrix는 normal matrix가 아니고, 그렇기 때문에 대각화할 때 **고유벡터 요소가 커지는 문제**가 발생한다.

다행이지만 저자는 HiPPO matrix $A$는 normal matrix가 아니지만, 해당 행렬을 normal matrix와 low rank matrix의 합으로 나타낼 수 있음을 발견하였다. 하지만 문제는 콘볼루션 필터 연산 시 합(Normal + Low Rank)에 대한 제곱 연산이 필요한데, 이 역시 시간이 오래 걸리고 최적화가 필요한 부분이라는 문제가 발생한다.

$$
\\overline{K}\\in\\mathbb{R}^L := \\mathcal{K}_L(A, B, C) := (CA^iB)_{0\\le i < L}
$$

이 문제를 해결하기 위해 **세 가지** 알고리즘을 추가로 적용하여 커널 필터를 계산하게 된다. 각 알고리즘에 대한 내용을 아래 그림과 관련지어 정리하면 다음과 같다. 아직 디테일하게 설명한 부분이 없어 그림의 내용에 대한 이해는 못하지만 순차적으로 보도록 하자.


<div align="center">
    <img src="https://github.com/user-attachments/assets/f63bc9a0-dceb-4d5a-bc47-e9fa453a2e74" width="900">
</div>


위의 알고리즘을 따라가기 위해서는 행렬 $A$가 NPLR (Normal + Low-Rank) 혹은 DPLR (Diagonal + Low-Rank)로 표현이 가능함을 이해하고 넘어가야한다. 이 부분은 논문에 Appendix C.1. 파트를 보면 empirical하게 모든 HiPPO 행렬에 대해 입증해 놓았다. 사실 수식으로의 이해는 필요가 없는 부분이고, **그냥 받아들이면 되는 파트**다.

그런 뒤, 기존의 콘볼루션 커널을 계산하던 방식에서 차이를 두게 된다. $\\overline{K}$를 직접 계산하지 않고  $\\overline{K}$의 Discrete Fourier Transform(DFT) 변환 형태인 $\\hat{K}$룰 사용한다. DFT는 이산화된 시간 축에서의 신호를 (여기서는 필터의 요소인 $CB, CAB, CA^2B, \\cdots$ 를 연속된 시간 축에서의 신호로 생각하면 된다) 이산화된 주파수 축으로의 스펙트럼으로 바꿔주는 변환에 해당되고, DFT 변환 및 이의 역변환 IDFT 알고리즘은 Fast Fourier Transform (FFT)라 하며 연산 속도는 $O(L\\log L)$ 선에서 해결 가능하다.

$$
\\hat{K}_j = \\sum_{k=0}^{L-1} \\overline{K}_k\\exp(-2\\pi j\\frac{k}{L})
$$

뒤에서 더 자세히 정리하겠지만, Truncate SSM을 구성하여 **스펙트럼 단위에서 연산**하게 되면 필터 연산 시 $A$를 여러 번 곱하는 방식에서 벗어나 **한번의 행렬 연산으로도 연산이 가능**하게 된다. 이 과정에서 골칫거리인 term인 $(1-\\overline{A}^L)$가 발생하는데 (결국 powering이 필요), 이를 **reparameterization**을 통해 반복된 연산을 피해 메모리 절약 및 속도 향상을 할 수 있게 된다. 그리고 위에서 가정한 구조화를 통해 스펙트럼 커널 $\\hat{K}$를 연산하는 것이 “Cauchy kernel”과 동일함을 알 수 있고, 효율적인 알고리즘을 적용할 수 있다. 짧게 정리했지만 실제 순서대로 간단하게 표현하면 다음과 같다.

1. 모든 HiPPO 행렬 $A$를 **NPLR (DPLR)로 표현이 가능**하고, 이를 적용하여 $A \\rightarrow \\overline{A}$ (Discretize)를 $O(N)$ 연산으로 줄일 수 있다.
2. $\\overline{K}$의 truncate SSM generating function은 DFT랑 동일하다. 따라서 **주파수 축으로의 변환 및 역변환을 통해 연산 가능**하며, 이때 $\\overline{A}$의 반복된 제곱 연산 대신 단일 연산으로 바꿀 수 있다.
3. 위의 연산 과정이 **Cauchy kernel 연산 구조와 동일**하므로 **효율적인 알고리즘이 적용 가능**하다.
4. 이때 inverse는 **Woodbury’s Identity를 사용하면 간소화가 가능**하다.


# 방법론에 대한 보다 자세한 설명

앞서 소개한 효율화 과정에 대해서 자세하게 살펴보자. 개인적으로는 디퓨전 논문보다 어려운 것 같다. 그렇지만 힘내보자. 


<div align="center">
    <img src="https://github.com/user-attachments/assets/44895afe-576d-4912-9451-e45ab84e8295" width="">
</div>


HiPPO 행렬은 모두 “NPLR (Normal Plus Low-Rank)”로 표현 가능하다. HiPPO 행렬은 총 4가지가 존재하지만, 가장 보편적인 케이스인 LegS에 대해서만 살펴보면 다음과 같다.

$$
A^{\\text{HiPPO}} _{n, k}= -\\begin{cases}
(2n+1)^{1/2}(2k+1)^{1/2},&\\text{if }n > k \\newline
n+1,&\\text{if }n = k \\newline
0,&\\text{if } n < k
\\end{cases}.
$$

요 식 모든 요소에 $\\frac{1}{2}(2n+1)^{1/2}(2k+1)^{1/2}$를 더해주게 되면 다음과 같은 식이 된다.

$$
-\\begin{cases}
\\frac{1}{2}(2n+1)^{1/2}(2k+1)^{1/2},&\\text{if }n > k \\newline
\\frac{1}{2},&\\text{if }n = k \\newline
-\\frac{1}{2}(2n+1)^{1/2}(2k+1)^{1/2},&\\text{if } n < k
\\end{cases}.
$$

이 식의 대각 성분을 따로 분리하게 되면 $-\\frac{1}{2}I + S$로 표현 가능하고, 이때 $S$는 normal matrix에 속하는 **skew-symmetric matrix**가 된다. 또한 원래의 행렬의 모든 요소에 같은 값을 더하는 행렬은 rank가 $1$인 행렬이다. 모든 HiPPO 행렬에 대한 증명은 논문을 참고하면 되고, 이를 통해 암시할 수 있는 사실은 모든 HiPPO state 행렬을 Diagonal part + Low-Rank part로 분리 가능하다는 사실이다. 이를 다음과 같이 표현하도록 하겠다. 논문에서는 증명 과정에서 conjugate($\\ast$) 표시를 사용했는데, 본인은 이 기호가 조금 익숙치 않아서 **transpose 기호($\\top$)로 대체하여 사용**하도록 하겠다.

$$
A = \\Lambda - PQ^\\top
$$

이제 이렇게 대체된 식으로 discrete system matrix를 표현해보면 다음과 같다.

$$
\\begin{aligned}
\\overline{A} &= (I-\\Delta / 2 \\cdot A)^{-1} (I+\\Delta / 2 \\cdot A) \\newline
\\overline{B} &= (1-\\Delta / 2 \\cdot A)^{-1} \\Delta B \\newline \\newline
I + \\Delta/2\\cdot A &= I + \\Delta / 2 \\cdot\\left(\\Lambda - PQ^\\top\\right) \\newline
&= \\frac{\\Delta}{2} \\left(\\frac{2}{\\Delta}I + \\Lambda - PQ^\\top\\right) \\newline
&= \\frac{\\Delta}{2}A_0 \\newline \\newline
(I-\\Delta/2 \\cdot A)^{-1} &= \\frac{2}{\\Delta}\\left(\\frac{2}{\\Delta}-\\Lambda+PQ^\\top\\right)^{-1} \\newline
&= \\frac{2}{\\Delta}\\left(D-DP(I+Q^\\top DP)^{-1}Q^\\top D \\right) \\newline
&= \\frac{2}{\\Delta}A_1, \\text{ where } D = \\left(\\frac{2}{\\Delta}-\\Lambda\\right)^{-1} \\text{ (Diagonal term)}
\\end{aligned}
$$

$A_0$을 계산하는 것은 크게 문제가 없는데 $A_1$ 연산에는 큰 문제가 있는데, 바로 역행렬 연산이다. 행렬 차원 수 $N$이 증가할수록 연산량이 기하급수적으로 늘어난다 . 따라서 역행렬 연산을 DPLR 행렬에 대해 위와 같이 **Woodbury’s Identity를 통해 단순화**할 수 있다. 대각화된 행렬에 대한 inverse는 쉽게 구할 수 있으며, 뒤에 붙는 low-rank term에는 무관하게 연산이 가능하므로 전체 계산식에 대한 역행렬 연산보다 단순화할 수 있다는 것이다. Woodbury’s Identity의 경우에는 앞으로 전개될 증명 과정에 계속 활용되기 때문에 계속 인지하고 있는 편이 용이하다 (DPLR 구조의 행렬만 가지면 계속 효율적으로 적용이 가능).

**Woodbury's Identity**는 다음과 같이 적용할 수 있다. **교환 법칙이 성립하는 세 행렬** $A\\in\\mathcal{R}^{N \\times N}, U, V \\in \\mathcal{R}^{N \\times p}$에 대해 (여기서 $\\mathcal{R}$은 commutative ring으로, 이에 속하는 원소들에 대해서는 곱연산에 대해 교환 법칙이 성립한다고 생각하면 된다.)

$$
(A+UV^\\top)^{-1} = A^{-1}-A^{-1}U(I_p + V^\\top A^{-1}U)^{-1}V^\\top A.
$$

위의 식을 만족하게 된다.

암튼 구한 식으로 다시 discrete system을 정의해보면, DPLR인 행렬 $A_1,$ $A_0$에 대해 $O(N)$의 연산량으로 해결 가능하다.

$$
\\begin{aligned}
x_k =& A_1A_0x_{k-1} + 2A_{1}Bu_k \\newline
y_k =& C^\\top x_k
\\end{aligned}.
$$

그리고 저자는 이 부분에서 row vector였던 $C$를 column vector로 간주하여 다른 파라미터($B, P, Q$)들과 shape을 맞추었기 때문에, 필자도 이에 따라 요 부분부터는 기존 시스템 식의 $C$를 $C^\\top$으로 바꿔 표기하도록 하겠다.

이제 이산화된 시스템 행렬은 얼추 알겠고, 사실 가장 중요한 것은 Recurrent system에서의 콘볼루션 필터 $\\overline{K}$를 빠르게 연산하는 것이다. DPLR이 행렬의 이산화 과정에서 Woodbury’s Identity를 활용할 수 있게 되면서 효율성을 올려준다는 사실을 인지하였으나, 실제로 콘볼루션 필터를 연산하는 과정에서의 반복곱 연산에서는 큰 도움이 되지 않는다는 것을 알 수 있다. 반복곱 연산 대신, DPLR를 활용하기 위해서는 역행렬 연산이 필요하므로 스펙트럼 단위로 넘기는 (coefficients) generating function을 생각해볼 수 있다. 예컨데 무한한 길이의 콘볼루션 필터 신호가 있다고 가정해보자.

$$
\\mathcal{K}(\\overline{A}, \\overline{B}, \\overline{C}) = \\{\\overline{C}^\\top\\overline{B},\\overline{C}^\\top \\overline{A}\\overline{B},\\overline{C}^\\top \\overline{A}^2\\overline{B},\\ldots\\}
$$

우리가 현재 신호에 대해 가질 수 있는 것은 이상적인 콘볼루션 필터 중 $L$의 길이를 가진 한정된 길이의 필터이다.

$$
\\mathcal{K}_L(\\overline{A}, \\overline{B}, \\overline{C}) = \\{\\overline{C}^\\top\\overline{B},\\overline{C}^\\top \\overline{A}\\overline{B},\\overline{C}^\\top \\overline{A}^2\\overline{B},\\ldots,\\overline{C}^\\top \\overline{A}^{L-1}\\overline{B}\\}
$$

길이가 $L$인 이산(discrete) 신호는 주파수 $2\\pi\\times\\frac{0}{L} \\sim 2\\pi \\times \\frac{L-1}{L}$의 성분으로 분해가 가능하다. 이 주파수를 표현하는 unit $z$라는 변수로 표현한다면, 이를 $z$함수에 대한 coefficient의 집합으로 대체 가능하며 이를 $z$-transform이라고 부른다. 이때, 일반적으로 $z$는 복소수 단위(**Real + Imag**)를 의미하며 주파수 단위에서는 이를 오일러 각도 변환 식인 ($e^{-i\\Omega}$)에서 $\\Omega =\\{\\frac{2\\pi l}{L}\\}_{l=0}^{L-1}$의 합으로 표현 가능하다.

$$
\\hat{K}_L(z; \\overline{A}, \\overline{B}, \\overline{C}) := \\sum_{i=0}^{L-1} \\overline{C}^\\top\\overline{A}^i\\overline{B}z^i = \\overline{C}^\\top(I-\\overline{A}^L)(I-\\overline{A}z)^{-1}\\overline{B}
$$

이러한 변환을 DFT(Discrete Fourier Transform)이라 부르며, **이산화된 시간 축의 신호를 이산화된 주파수 축으로 변환하는 과정**에 주로 활용된다. 맨 앞단의 $\\overline{C}^\\top(I-\\overline{A}^L) = \\tilde{C}$로 두게 되면,

$$
\\hat{K}_L(z; \\overline{A}, \\overline{B}, \\overline{C}) = \\tilde{C} (1-\\overline{A}z)^{-1}\\overline{B}
$$

이 식에서 discretized matrix $\\overline{A}, \\overline{B}$ 의 closed form으로 대체하여 $A, B$에 대해 표현 가능하다.

$$
\\begin{aligned}
\\overline{A} =& (I-\\Delta/2 \\cdot A)^{-1} (I+\\Delta/2\\cdot A) \\newline
\\overline{B} =& (1-\\Delta/2\\cdot A)^{-1}\\Delta B
\\end{aligned}
$$

이 식을 그대로 위에 대입하게 되면,

$$
\\begin{aligned}
\\tilde{C}^(I-\\overline{A}z)^{-1}\\overline{B} =& \\tilde{C}\\left(I-(I-\\Delta/2 \\cdot A)^{-1} (I+\\Delta/2\\cdot A)z \\right)^{-1}\\overline{B} \\newline
=& \\tilde{C}\\overline{B}\\left(I-\\frac{\\Delta}{2}A\\right)\\left(\\left(I-\\frac{\\Delta}{2}A\\right)-\\left(I+\\frac{\\Delta}{2}A\\right)z\\right)^{-1} \\newline
=& \\tilde{C} \\Delta B\\left( I(1-z) - \\frac{\\Delta}{2}A(1+z)\\right) \\newline
=& \\frac{\\Delta}{1-z}\\tilde{C} \\left( I - \\frac{\\Delta A}{2\\frac{1-z}{1+z}} \\right)^{-1}B \\newline
=& \\frac{2}{1+z}\\tilde{C}\\left(\\frac{2}{\\Delta}\\frac{1-z}{1+z}I -A \\right)^{-1}B
\\end{aligned}
$$

+앞서 우리는 시스템 행렬이 DPLR(Diagonal Plus Low-Rank)임을 보였기 때문에 다시 표현하게 되면,

$$
\\frac{2}{1+z}\\tilde{C}\\left(\\frac{2}{\\Delta}\\frac{1-z}{1+z}I -A \\right)^{-1}B = \\frac{2}{1+z}\\tilde{C}\\left(\\frac{2}{\\Delta}\\frac{1-z}{1+z}I -\\Lambda+PQ^\\top  \\right)^{-1}B
$$

이제 앞서 언급했던 Woodbury’s Identity를 사용해볼 수 있다. 식을 간소화하기 위해 다음과 같이 두개 되면,

$$
R(z) = \\left( \\frac{2}{\\Delta}\\frac{1-z}{1+z} - \\Lambda \\right)^{-1}
$$

최종적으로는 다음 식으로 전개할 수 있다.

$$
\\tilde{C}(1-\\overline{A}z)^{-1}\\overline{B} = \\frac{2}{1+z}\\left(\\tilde{C}R(z)B - \\tilde{C}R(z)P(1+Q^\\top R(z)P)^{-1} Q^\\top R(z)B \\right).
$$

여기서 생길 수 있는 의문점은, 앞서 식을 전개하는 과정 상에서 앞단의 $\\overline{C}^\\top(I-\\overline{A}^L) = \\tilde{C}$로 정의된 부분이다. 원래대로라면 매번 $\\overline{A}^L$를 연산해야 하지만, 단순히 학습 파라미터를 초기에 $\\tilde{C}$로 초기화한 상태로 학습 가능하게끔 **reparameterization**을 하게될 경우 이에 따른 연산 코스트를 줄일 수 있다.

이제 마지막 단계까지 왔다. 결국 전개한 식을 요약하자면, $\\overline{K}$를 연산하는 부분을 generating function $\\hat{K}_L(\\Omega; \\overline{A}, \\overline{B}, \\overline{C})$로 계산하고, 이때 $\\overline{A}$가 Diagonal 성분을 가짐을 사용하여 풀어낸 식이다. 그런데 이렇게 풀어낸 식이 결국 Cauchy kernel이랑 정확하게 일치하고, Cauchy kernel은 효율적으로 연산할 수 있는 알고리즘이 존재한다. 우선 Cauchy matrix / kernel의 구조는 다음과 같이 정의된다.

$$
M \\in \\mathbb{C}^{M \\times N} = M(\\Omega, \\Lambda) = (M_{ij})_{0<=i < M,~0<=j<N},\\quad M_{ij} = \\frac{1}{\\omega_i - \\lambda_j}
$$

최종 식에서의 $Q^\\top R(\\Omega, \\Lambda)P$ 부분을 살펴보게 되면(기존의 $z$를 unit $\\Omega$의 각 요소에 대해 생각해볼 수 있다), 이를 계산하는 과정이 **Cauchy matrix-vector multiplication 연산량으로 계산 가능**하다는 것이다. 예컨데 원래대로라면 길이 $L$인 콘볼루션 커널을 총 $N$만큼의 hidden state에 대해 연산하려면 $O(LN)$ 만큼의 연산량이 소모되는데, 약간의 오차를 허용하면 이를 $O((L+N) \\log(L+N) \\log \\frac{1}{\\epsilon})$의 연산량으로 처리가 가능하다. 증명하는 과정은 거의 불필요한데 요약하자면, $Q^\\top R(\\Omega, \\Lambda)P$를 계산하는 것은 $\\Omega$에 속하는 모든 $\\omega$에 대해 우리는 $\\sum_{j} \\frac{q_j^\\top p_j}{\\omega-\\lambda_j}$를 구하고자 하는 것과 같으며, 결국 이 식은 Cauchy kernel의 형태와 같기 때문이다 ($\\omega \\neq \\lambda_j$ 라는 조건은 항상 성립함).

이렇게 알고리즘이 모두 정리가 되었다. 다시 앞서 간단하게 언급했던 알고리즘을 다시 가져오면 다음과 같다.

먼저, $A$가 DPLR이라는 점에서 시작하여 식을 전개하였고, 이때 $\\overline{K}$를 다이렉트로 계산하지 않고 generating function $\\hat{K}$를 계산하기 위해 푸리에 변환을 실시하였다. 이때 나온 식에서 $\\overline{C}$를 reparameterization해준다. 그런 뒤, discretized된 모든 matrix를 state matrix의 closed form으로 표현한 뒤, Woodbury’s Identity를 통해 식을 다시 전개하면,

$$
\\begin{bmatrix}\\tilde{C}^\\top & Q\\end{bmatrix}^\\top \\left(\\frac{2}{\\Delta} \\frac{1-\\omega}{1+\\omega} -\\Lambda \\right)^{-1} \\begin{bmatrix}B&P\\end{bmatrix}
$$

가 된다. 다만 본인의 식 전개와 실제 논문 알고리즘에서 살짝 다른 부분이 있다면 필자는 $\\tilde{C}^\\top = \\tilde{C}$로 쭉 전개해왔다는 사실이다. 같은 구조이기 때문에 큰 문제는 없다고 생각된다. 아무튼 이렇게 계산된 각 요소들로 효율적으로 계산된 $\\hat{K}$에 다시 푸리에 역변환을 적용하면 $\\overline{K}$를 구할 수 있게 된다. 드디어 이 논문의 아이디어가 되는 모든 알고리즘을 이해할 수 있었다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/458610cb-a810-462a-98e3-f524edd4496c" width="">
</div>



# S4로 구성한 Deep layer의 구조

앞에서 본 내용을 통해 S4에 필요한 파라미터는 다음과 같다는 것을 알 수 있다. $A$는 HiPPO 행렬의 어떠한 형태로 초기화가 될 예정이고, 해당 $A$는 그 형태에 맞게끔 특정 Diagonal 및 vector인 $\\Lambda, P, Q, B, C$로 구성된다. Diagonal도 실질적으로 대각 성분 이외에는 다른 파라미터를 저장할 필요가 없기 때문에 S4는 state dimension $N$에 대해 총 $5N$의 학습 가능한 파라미터 수를 가진다. S4 자체는 Linear mapping 이지만 (동일한 길이의 시퀀스를 뽑아내는 구조), 이를 여러층 쌓고 Non-linearity를 더하게 되면 결국 Deep layer로 사용될 수 있다.

# 결론은…

실험 결과를 따로 보여주기보다는 이 상태로 마무리하는게 좋을 것 같다. 실험 결과야 당연히 efficiency를 보여주면서 long-range 효과성을 유지하는 그림이겠거니, 예상했고 논문에 예상한 그대로가 display된 것을 볼 수 있다. 가장 중요하다고 생각한 점은 모델링의 기초가 되는 SSM을 풀어냈던 이전 논문으로부터 개선점을 지속적으로 잡아내고 (예컨데, 행렬곱을 단순화하기 위해 구조를 분해하고 이를 수식으로 증명하는 것) 이러한 과정에서 연속으로 contribution을 낼 수 있다는 점을 배우게 된 것 같다.
`,XO=`---
title: "Mamba modeling의 기초 (3) - Linear-Time Sequence Modeling with Selective State Spaces (Mamba)에 대하여"
category: "ai papers"
publishedAt: "2024-03-01"
thumbnail: "https://github.com/user-attachments/assets/f607dea3-c722-4229-8ecd-d2a9af742312"
---


# 시작하기 전에 ...

처음으로 올렸던 글인 LSSL(Linear State-Space Layer)에서는 연속 시퀀스 데이터셋에 대해 딥러닝 모델이 효과적으로 latent space를 정의할 수 있는 구조의 발달 양상을 살펴보았다.
그 중 가장 주요한 키포인트가 되는 HiPPO 논문의 경우 임의의 길이를 가지는 시퀀스의 hidden state를 모델링할 수 있는 근거로 자리잡았고, 이후 LSSL은 레이어 개념으로 확장시켜 HiPPO 행렬의 학습을 통해 성능을 높일 수 있다는 가능성을 보여주게 된다. 그리고 학습이 진행됨에 따라 떨어질 수 있는 학습 안정성 및 수치 엄밀성을 확보하기 위해, 이에 추가로 이후 scalable(데이터 및 모델 확장 가능성)을 높이기 위해 제안된 S4모델을 두번째 글로 다루었다.
이제 이러한 기존 SSM based modeling을 근거로 하여, 트랜스포머 모델의 불가피한 연산량 증대를 개선하고자 한 Mamba 모델을 소개한다.

# 트랜스포머 모델의 큰 문제

최근 가장 많이 다루게 되는 딥러닝 모델, 혹은 foundation model은 아마 대부분 알고 있겠지만 Transformer에 해당된다. 기계 번역 분야에서 등장한 transformer 구조는 이후 Computer vision, NLP, Audio 등등 모달리티(데이터 형태)의 종류에 무관하게 활발하게 활용되었으며, 가장 중요한 특성인 model scalability(모델 크기와 데이터 크기가 증가함에 따라 성능도 같이 향상됨) 특성이 현존하는 모델링 중 가장 확연하게 드러난 모델이라고 할 수 있다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/355b77fe-80e5-43f6-98c7-a0a2b8bb19bb" width="500">
</div>


그러나 트랜스포머 모델에는 가장 큰 문제점이 있다. 그것은 바로 한정된 길이의 시퀀스를 (보통은 트랜스포머 모델은 토크나이저를 통해 문장을 토큰 단위로 분해하고, 이를 임베딩화하여 사용한다) 받아들이며 이를 병렬 연산(Attention)하기 위해 그만큼의 메모리를 소모하게 되고, 이는 결국 동시에 처리 가능한 데이터의 길이가 어느 정도 한정될 수 밖에 없는 것이다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/f9f1430e-eb9c-4fb1-af5b-e13bf50a6185" width="800">
</div>


만약 짧은 문장 하나를 트랜스포머에 넣게 되면, 적당히 토크나이징하여 연산을 수행하면 되지만 논문과 같이 긴 줄글의 경우 한정된 연산량 때문에 이를 분리해서 넣게 된다. 가장 큰 문제는 트랜스포머 모델은 논문 제목에서도 볼 수 있듯이(Attention is All you Need) RNN모델과 같이 hidden state를 따로 학습하지 않다 보니, 모델에 들어간 시퀀스 내에서 모든 것을 수행하게끔 되어있다. 즉, 논문 전체를 쿼리로 하여 특정 질문에 대한 주요 부분들을 추출하거나 하는 과정에서는 충분한 연산량이 보장되지 않으면 좋은 성능을 보이지 않게 된다. 이러한 한계점은 곧 트랜스포머 모델의 어텐션 연산을 위한 효율화 작업으로 이어지게 된다.

# 그럼 왜 트랜스포머 모델을 쓰는 건데?


<div align="center">
    <img src="https://github.com/user-attachments/assets/948369c9-b2fc-4cd4-a9b6-0a00581c3125" width="550">
</div>


그럼 대체 왜 꼭 “트랜스포머”여야만 하는가??라고 한다면, 그간 연산 효율화를 위해 단순 어텐션을 벗어난 모델링인 **linear attention,** **gated convolution, RNN,**  **structured state space models (SSMs)** 모두 연구되었지만 Language와 같은 데이터에 대해 트랜스포머의 어텐션보다 높은 성능을 보이지 못했다는 단순한 사실이다. 트랜스포머가 보여준 가능성에 대해 수많은 연구가 진행되었기 때문에 하드웨어 친화적인 알고리즘이나 다양한 학습법 등등 많은 연구가 진행되어 이미 상당히 높은 발전을 이루어낸 트랜스포머 시장에서 적당한 연산량으로 애매한 성능을 보이는 다른 모델이 주목받기가 힘든 상황이다. 특히나 최근 long range dependency/reasoning 에 집중했던 SSM의 경우에는 텍스트와 같이 오히려 정보 집약적인 task에 대해서 높은 성능을 보이지 못했다.

# Mamba, 너로 정했다

맘바는 SSM에 집중한다. SSM은 기본적으로 특정 시스템을 모델링하기 위한 구조로, 이 방법에 attention, RNN에서 사용되는 “gate” 개념을 “selection”으로 가져간 것이다. Attention은 사실 대단한 알고리즘은 아니고, 현재 토큰에 대해 참고가 가능한 시퀀스 내에서 집중할 부분과 무시할 부분을 구분하고, 이를 예측에 활용하는 것이다. 결국 SSM에서도 특정 정보만 선택적으로 활용하는 방법을 사용해볼 수 있는 것이다. 그러나 selective SSM은 일반적인 SSM에 비해 효율적 콘볼루션 연산 등 연산 효율화를 위한 장치를 전혀 사용하지 못하게 된다. 고로 가장 기본적인 연산 방식인 recurrent를 기본적으로 사용하게 되는데, 이때 하드웨어 친화적인 알고리즘을 고안하여 연산 비효율성을 보완하게 되는 것이다. 즉 어텐션과 MLP 없이, selective SSM과 이를 효율적으로 연산할 수 있는 방법을 더하여 Mamba를 만들어낸 것이다. 그래서 사실 맘바 논문의 가장 주요한 키포인트는 SSM에 있다기 보다는 FlashAttention과 비슷한 맥락인 GPU 활용력에 있는 것이다.

### Selection 메커니즘

이전의 SSM에서 다뤄지지 않은 내용은 “input”에 대해 선택적 알고리즘이 없다는 사실이다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/b98e9fc3-1996-4438-9c9e-224484e660a0" width="700">
</div>


SSM 모델링을 보게 되면 길이에 무관한 모델링을 할 수 있다는 장점이 있지만, 이를 다르게 해석하면 임의의 시점에 들어오는 입력은 모두 동일하게 모델링된 state를 보기 때문에 관찰 시점 이전의 input 혹은 이후의 input 중 일부 무시해야할 내용을 구분할 수 없다. SSM의 가장 큰 장점은 아무리 긴 길이의 입력이 들어오더라도, 모든 정보를 함축적으로 모델링할 수 있다는 사실이다. 그러나 위와 같은 예시를 보면 어텐션과의 차이점이 크게 드러난다. 예컨데 어떤 텍스트의 초반에 “고양이를 5년 전부터 키워왔다.”라는 정보가 있고, 중간에 그와 무관한 이야기인 “시골집에서 살던 내용”이 포함되어있고, 텍스트 후반부에 고양이 이름이 나와있는 경우를 생각해보자.

해당 쿼리에 대해 “OOO는 몇살 정도로 예상되는가?”라는 질문을 받는다고 가정하면 어텐션 모델은 우선적으로 해당 질문과 가장 큰 연관성을 지니는 “고양이 이름은 OOO이다.” 라는 내용과 “대략 5년 전부터 고양이를 키웠다.”라는 내용을 참고하겠지만, SSM은 중간에 있는 시골집에 살던 내용까지 전부 참고하여 정답을 내놓게될 것이다. 물론 SSM이 잘못된 대답을 내놓지 않고 정답을 내놓을 수 있지만, 결국 말하고자 하는 것은 이처럼 정보가 집약적인 데이터(특정 부분에 집중해야 제대로 된 QnA가 가능한 데이터 구조)에 대해서는 어텐션 만큼 효율적이고 정확한 방법이 없다는 것이다. 따라서 맘바에서는 기존 SSM 구조에 추가로 입력 신호에 대해 SSM 파라미터(이전의 정보들)를 다변화하는 구조를 통해 Selection 매커니즘을 추가하게 된다.

### 하드웨어 친화적 알고리즘


<div align="center">
    <img src="https://github.com/user-attachments/assets/38ffcf27-2b6e-4189-9750-65734e49030d" width="750">
</div>


이전의 모든 SSM을 위한 효율화 알고리즘은 selective SSM에는 적용되지 않는다. 이는 LTI(Invariant)와 같은 모델링 구조에서 기본적으로 입력 및 시간에 무관하게 시스템은 동일하다는 가정을 가지고 있기 때문이다. 따라서 기존 논문에서 제안된 콘볼루션 기반 방법들을 모두 사용할 수 없게 되었고, 오로지 recurrent 연산을 사용할 수 밖에 없으므로, 이를 하드웨어 친화적으로 연산하는 방법을 고안하게 된다. 실제로 구현된 하드웨어 친화적인 알고리즘은 시퀀스 길이에 대해 Linear한 복잡도를 가지게 되어, 이전 콘볼루션 기반 알고리즘이 pseudo-linearity를 가졌던 것에 비해 recurrent 연산을 더욱 효과적으로 수행할 수 있게 되었다.

# 모델링

모델 구조는 간단하다. Selective SSM 구조를 하나의 모듈처럼 취급하여, 기존 트랜스포머 모델을 구성하는 MLP 파트를(Attention 및 Projection 등등) SSM 모듈로 갈아끼워서 사용한다. Selective SSM이 아닌 일반적인 SSM에 대한 내용은 이전에 다루었던 글들을 통해 간단하게 이해하고 오면 좋다. 간단하게 소개하자면 대개 Structured SSM은 4개의 파라미터$(\\Delta, A, B, C)$를 기본으로 적용되며, 이를 discretize한 ($\\overline{A}, \\overline{B}, C$)을 사용한다. SSM은 LTI 시스템을 기반으로 하여 시간에 따른 시스템 함수 불변성을 가정하였으나, 이러한 불변성이 필연적으로 가지는 한계 때문에 맘바에서는 기존에 적용될 수 있었던 연산 효율성을 포기하고 Selective SSM을 채택하게 된다. 이 부분에서는 어떠한 파트가 구체화되어 Selective SSM이 설계되었는지 단계별로 정리하고자 한다.

### Compression(축약)을 위한 Selection

시퀀스를 다루는 모든 모델링은 임의의 길이를 가지는 ‘문맥’을 어떻게 하면 작은 크기의 ‘hidden state’ 혹은 ‘latent’로 함축하는가?를 다루게 된다. 모든 시퀀스 모델링은 해당 관점에서의 trade-off를 고려할 수 밖에 없는데, 대표적인 시퀀스 모델링에 해당되는 ‘트랜스포머(Transformer)’는 문맥을 전혀 압축/함축하지 않는다는 특징을 가지고 있다. 이러한 특징은 autoregressive한 추론 단계에서 Key-Value 문맥 전체를 참조하기 위해 길이에 따라 연산 속도 및 메모리가 증가하게 되며, 이는 트랜스포머의 quadratic-time consuming의 주된 원인으로 작용한다. 반대로 RNN과 같은 recurrent model은 한정된 크기의 state를 가진다는 점에서 학습 효율성을 가지나, 과연 한정된 크기의 state에 얼만큼 context(문맥)이 잘 요약될 수 있는가가 문제점으로 작용된다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/e141efe2-cefb-4d05-b7d9-77f8fcd6be3b" width="800">
</div>


위의 그림에 나타난 생성 task를 보게 되면 이러한 trade-off system을 SSM(LTI system)의 레벨 단에서 이해하기 쉽다.

좌측에 나타난 task는 입력으로 들어온 연속 신호 중 일부분(연속되어 색칠된 부분)을 복사하여 생성하는 과정이다. LTI system이 처리할 수 있는 가장 기본적인 형태의 delay라고 볼 수 있다. 즉 LTI system으로 매핑 가능한 일반적인 모델로 간단하게 수행할 수 있는 과제이다.

우측에 나타난 두 가지의 task 중 위쪽은 입력으로 들어온 연속 신호 중 관련 신호(색칠된 부분)과 무관한 신호(흰색 부분)을 구분하고, 관련 신호를 입력 순서대로 복사하여 생성하는 과제이다. 앞선 복사 task처럼 LTI system으로 수행될 수 없기에 time-varying system 및 non-linear system이 활용되어야하는 것을 볼 수 있다. 아래쪽은 Induction heads라는 과제로, 흔히 요즘 LLM에서의 In-context learning에서 대두되는 task라고 볼 수 있다. 입력으로 넣어준 일련의 신호에 대해 맥락을 파악하고, 이후에 특정 신호(검정색 토큰)를 입력으로 넣어줬을때 문맥에 맞는 정답을 내놓게되는 것이다(파란색 토큰). 이 역시 입력 신호에 대해 어떤 특정 신호가 뒤따를지 모르기 때문에 시스템이 문맥에 대한 추론이 필수적이고, 이를 위한 모델링을 추가로 수행해야한다.

결국 위의 그림으로 이해하고자 한 것은 여러 복잡한 생성 이론을 효과적으로 수행하기 위해 “선택적으로” 문맥을 이해하는 과정을 모델링에 추가해야 한다는 사실은 시퀀스를 처리하는 모든 모델링이 다루는 문제라는 사실이다. 해당 문제를 수행하기 위해 기존 방법론들을 총정리했을때, trade-off로서 context 용량과 효율성 간의 합의점이 필요하고, 현재 다루는 모델에서 이를 어떻게 적용해낼지(Attention으로 일부 특징들을 걸러낼 것인지, Recurrent module로 문맥을 요약한 state를 구축할 것인지) 고민하게 된다. 그렇기 때문에 SSM에서도 비슷한 맥락으로의 구조가 필요하고, 기존 시퀀스 모델에서 개별적으로 적용되던 context compression의 수단으로 selection mechanism을 넣은 것이다.

### SSM에 selection을 넣기


<div align="center">
    <img src="https://github.com/user-attachments/assets/f607dea3-c722-4229-8ecd-d2a9af742312" width="850">
</div>


결국 Mamba는 SSM에 어떻게 selection mechanism을 심느냐는 것인데, 저자는 RNN의 recurrent dynamics나 CNN의 파라미터와 같이 직접적으로 입력에 영향을 주는 “파라미터”를 입력 신호에 따라 바꾸는 방식을 생각해냈다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/013d5753-7f0c-4e73-86fe-ecf4dbaa415e" width="800">
</div>


좌측과 우측을 비교하게되면 그 차이가 나타난다. 주된 차이는 $B, C, \\Delta$ 파라미터가 **더이상 입력(배치) 및 각 입력의 타이밍(길이)에 무관하지 않고**, 입력 및 출력 신호와 동일한 크기를 가지는 것을 볼 수 있고, 이는 더이상 Time-invariant system이 아닌 Time-variant system이 되었다는 것을 의미한다.

또한 $B, C. \\Delta$가 어떻게 정해지는지 우측을 잘 보게되면 $s_B(x), s_C(x), s_\\Delta(x)$와 같은 방식으로 입력 신호에 대한 함수로 표현이 되어있는 것을 볼 수 있다.

$$
s_B(x) = \\text{Linear}_{N}(x),~s_C(x) = \\text{Linear}_{N}(x)
$$

이는 가장 간단한 형태로, dimension $D$인 입력을 받아 $N$인 출력을 해주는 Linear module로 parameterize하여 함수를 구성하고,

$$
s_\\Delta(x) = \\text{Broadcast}_D(\\text{Linear}_1(x))
$$

이산 신호의 간격은 위와 같이 스칼라 값을 dimension에 브로드캐스팅하는 형태로 함수를 구성하였다. 이렇게 SSM을 시간 변화에 따른 함수로 파라미터화 하였다.

### 하드웨어 친화적 알고리즘

Convolution 모델이라던지, Attention 모델들은 하드웨어 친화적으로 설계가 되었다. 콘볼루션 커널은 입력 크기와 무관하게 항상 일정한 receptive field 크기를 가져 메모리를 최적화할 수 있으며, 어텐션은 길이에 따라 메모리 및 시간이 증가하기는 하지만 HBM 대신 [SRAM에서 잘 동작할 수 있는 알고리즘](https://arxiv.org/abs/2205.14135)이 등장했으니까 (실제로 FlashAttention 저자인 Tri Dao가 맘바 저자로 참여했음). Selective SSM도 비록 LTI system을 사용할 수 없게 되어버렸지만 분명 학습 효율화할 수 있는 부분은 있을 것이다. 기존 방법들의 한계점은 다음과 같다.

(1) SSM과 같은 recurrent model은 표현력(state size)과 속도 사이의 합의점이 필요하다. 높은 표현력을 가지면서도 속도 저하가 심하지 않은 방법을 찾는 것이 목적.

(2) Recurrent가 Convolution보다 더 유연하다. 후자가 전자의 확장판이기 때문에 latent state 구축을 위한 연산량이 (B, L, D, N) 만큼 필요한데, 이러한 문제를 해결하려는 방법이 나옴 ([S4 모델](https://arxiv.org/pdf/2111.00396)).

(3) 기존의 LTI state model은 표현력 확보를 위한 state dimension $N$의 넉넉한 확보를 위해 dual recurrent-convolutional form을 고안함.

우리는 이제 selection mechanism을 적용하기 때문에 LTI system을 사용할 수 없다. LTI system이 가지는 한계점을 해결하기 위해 Selective SSM을 고안하였으나 연산 비효율성 문제를 해결해야한다는 점에 직면하게 된다. 저자는 문제를 해결하기 전 두가지 중요한 특징을 활용한다.

- Recurrent를 단순하게 적용할 경우 FLOPs는 $O(BLDN)$, Convolution은 $O(BLD\\log(L))$으로 적용된다. 즉 시퀀스 길이가 길어질수록 적당한 크기의 hidden state dimension $N$에 대해 오히려 Recurrent 연산이 적은 연산량을 가진다.
- 두가지 주된 문제는 recurrent 연산의 순차성과(병렬적 연산이 안됨)과 큰 메모리 사용 문제에 직면한다. 후자의 경우에는 convolution과 같이 굳이 전체 state $h$를 구성하지 않아도 된다는 개선점이 있다.

결국 주된 아이디어는 엄청 특별한 내용은 아니고, hidden state $h$를 GPU에서 효율적으로 연산할 수 있는 방법들(kernel fusion, parallel scan, recomputation)로 빠르게 구해보자는 것이다. 


<div align="center">
    <img src="https://github.com/user-attachments/assets/95e9fdc6-cbaf-4f39-b84f-c307798e0440" width="850">
</div>


SSM의 시스템 주축이 되는 $\\overline{A}, \\overline{B}$를 직접 HBM에서 계산하지 않고, SSM parameter $A, B, C, \\Delta$를 SRAM단으로 로드, 이산화 작업을 거져 다시 HBM에 쓰는 방식을 취한다. 또한 순차성 부분은 스캔할 타이밍에서 parallel scan algorithm을 적용하게 된다. 이로써 적은 메모리 bandwidth를 가지는 SRAM과의 데이터 송수신 관련 코스트를 최소화하여 사용한다. 이외의 backpropagation 시의 recomputation 방식은 FlashAttention과 하드웨어적으로 동일하게 적용된다.

### Neural Network에 Mamba 섞기

Structured SSM(S4)와 마찬가지로 Selective SSM(Mamba) 또한 시퀀스에 대한 변환 모듈에 해당되기 때문에 neural network에 적용할 수 있다. 맘바의 구조를 종합적으로 이해하기 위해서는 H3와 Gated Unit을 이해하는 과정이 필요하다. 속칭 ‘배고픈 하마 ([Hungry Hungry Hippos](https://arxiv.org/pdf/2212.14052))’라 불리는 H3의 경우 트랜스포머의 Attention Algorithm의 효과를 따라갈 수 있는 SSM 구조 모델링을 위해 Shifting SSM과 Recalling SSM을 구별하고, 이를 multiplication으로 엮는 시도를 하게 된다. 이렇게 모델링하게 되면 Q, K, V로 추출되는 입력에 대한 정보가 Shifting SSM에서 이전 입력을 참조하기 위해 옮겨주는 역할을 수행하고, 만약 현재 입력 정보가 기억이 된다면(Shifting $\\odot$ SSM), 그 이후 입력에 대한 출력값(Value $\\odot$ SSM)을 응답으로 내놓는 구조가 된다. Gated MLP의 경우에도 결국 트랜스포머의 Attention 구조를 MLP 구조에 통합하고자 한 구조에 해당된다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/dc376841-3f05-48ab-a77f-057f1b689fd5" width="850">
</div>


즉, 맘바의 경우에도 기본적으로 SSM 구조를 사용하기 때문에 트랜스포머의 Attention 효과를 활용하고자 했던 H3와 근본적으로 문제시하는 부분이 동일하다. 그렇기 때문에 내부적으로 연산되는 SSM 부분은 H3와 동일하다. 그러나 차이가 있는 점은 H3는 Linear Attention의 Q, K, V 구조를 활용하였지만, Mamba에서는 이러한 어텐션 구조를 전혀 사용하지 않고 Gated MLP를 2개의 SSM 시스템을 Wrapping하는 방식으로 구조화하였다.

# 각 요소별 효과

이와 같이 모델링했다. 이때의 각 요소별 효과를 간략하게 서술하면 다음과 같다.

- Variable Spacing : 언어와 같이 Discrete data에 대해서 문맥 해석에 무관한 신호를 무시할 수 있게 된다. 문맥을 생성하는 상황에서 문맥에 무관한 신호를 제외하여($g_t = 0$) 보다 풍부한 문맥을 생성해낼 수 있다.
- Filtering Context : 각 상황에서 문맥의 중요도를 결정한다. 경우에 따라 일부 문맥을 무시해야할 경우가 생기는데, 시간 불변성을 지니는 LTI system에서는 이를 적용할 수 없으나, selective SSM인 맘바에서는 상황에 따라 문맥을 필터링할 수 있게 된다.
- Transformer는 구조상 문맥상에서 독립적인 문구를 어텐션하여 사용할 수 있지만(필요한 부분을 불연속적으로 추출 가능) LTI system에서는 문맥을 하나의 연속적인 형태로 보기 때문에 이러한 특성이 발견되지 않는다. 그러나 Selective SSM에서는 연속된 시간 단위에서의 $\\Delta$, 혹은 이전 hidden state를 무시할 수 있는 gate $g_t$의 값이 유동적으로 학습될 수 있기 때문에 이러한 특성을 찾을 수 있다는 가능성이 있다.
- 일반적으로 문맥의 간격에 해당되는 $\\Delta$는 현재 입력에 얼마나 집중할 지 결정해주게 된다. 바로 위의 꼭지에서 언급했던 것과 같이 $\\Delta \\rightarrow \\infty$가 되는(커지는) 상황이 되면 이전의 state를 무시하고 현재의 입력에 집중하는 형태가 될 것이고, 반대로 작아지는 경우에는 이전의 state를 현재 입력보다 중요시하는 형태가 될 것이다.
- $A$ 파라미터는 기존의 시스템에서는 hidden state를 구축하는 역할을 수행했었다. 마찬가지로 Selective SSM에서도 같은 역할을 수행하지만, 차이점은 유동적인 $\\Delta$와 discretization되어 구축되는 문맥 시스템에 selective 속성을 부여할 수 있다는 것이다. (아래 수식 참고)
- $B, C$ 파라미터는 gated system에서 현재 입력 $x_t$에 대한 정보를 문맥에 추가할 것인지, output $y_t$를 내보내는 과정에서 state 정보를 얼마나 활용할 것인지 결정하는 역할을 수행한다. (아래 수식 참고)

$$
\\begin{aligned}
\\mathbf{h_t}^\\prime = A\\mathbf{h_{t-1}} + B\\mathbf{x_t} \\newline
\\mathbf{y_t} = C\\mathbf{h_t}+D\\mathbf{x_t}
\\end{aligned}
$$

# 실험 결과


<div align="center">
    <img src="https://github.com/user-attachments/assets/a68626a6-74af-472c-94a7-74b0eb0c8ffe" width="350">
    <img src="https://github.com/user-attachments/assets/ec4c9341-ce8f-4d4e-8730-b19314c9e3b8" width="700">
</div>


Selective copying(좌측) 그리고 Induction head(우측) 각각의 성능이 기존 SSM baseline에 비해 월등히 좋아지는 것을 확인할 수 있다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/89bca885-1f2d-4bb3-8103-5746c97e301c" width="1000">
</div>


또한 확연히 좋아지는 부분은 Perplexity인데, 연산량이 늘어날수록(모델의 파라미터 수가 증가할수록) 문맥에 대한 생성 능력이 확연히 올라간 모습을 보여준다. 이전까지는 H3까지도 어텐션에 필적하지 못했던 부분이었는데, 맘바를 통해 꽤나 많이 따라잡은 것을 확인할 수 있다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/98d16282-81dd-4051-8e36-59ca04488d20" width="850">
</div>


여러 downstream task에 대해 zero-shot 성능을 확인하였다. 파라미터 수가 증가할수록 perplexity는 감소하고 average는 증가하였고, 다소 적은 파라미터 수를 가지고도 좋은 성능을 보인다. 이외에 DNA, Audio modeling등 다른 시계열 모달리티에 대해서도 좋은 성능을 보여준다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/d2f29887-d420-4af5-948c-83bf56c95b37" width="1000">
</div>


Mamba의 장점 중 가장 주요한 포인트는 context 길이가 길어질 경우에 연산량 및 추론 시간을 줄일 수 있다는 점인데, 실제로 Attention을 효율화한 FlashAttention과 비교했을 때에도 Mamba의 inference time 및 throughput이 좋아지는 것을 볼 수 있다.
`,QO=`---
title: "AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning 논문 리뷰"
category: "ai papers"
publishedAt: "2025-07-08"
thumbnail: "https://github.com/user-attachments/assets/7ed5a03c-c5f2-4a3e-9091-e7a2ef3522b4"
---

# AnimateDiff

# 논문이 나오게 된 배경

해당 논문이 나오게 된 배경을 정리하면 아래와 같다.

1. 기존의 T2I 모델이 성능이 많이 올라왔다 (Diffusion Prior). 심지어 Personalization이 가능해지면서 (DreamBooth, LoRA 등의 방법) 보다 정교한 생성이라던지 원하는 이미지 생성 방법이 많이 공개되기 시작하였다.
2. 특히 personalize가 가능하다는 관점에서 큰 학습 리소스를 들이지 않고도 많은 개인화된 모델이 huggingface 등의 커뮤니티에 공개되기 시작하였다.
3. 그러나 이러한 객체나 스타일에 대한 personalize 이외에, T2I 모델을 기반으로 특정 모션을 주는 (Video prior, Motion prior) 연구는 여전히 부족하고, 특히 디퓨전의 경우 학습량이 방대하므로 이를 줄일 수 있는 방법이 절실하다.

# T2I Model Personalization (개인화)란.

Text to Image model에서의 **개인화**라는 단어는 곧 **특정 스타일이나 오브젝트에 미세 조정된 모델을 얻고자** 하는 관점에서 처음으로 정의되었다.

## DreamBooth

**특정 오브젝트의 형태는 유지하면서, 해당 오브젝트를 주제로 한 다양한 Scene을 구성하고자 하는 문제가 있다.**

기존의 T2I 모델은 방대한 데이터로부터 프롬프트 - 이미지 쌍을 학습하여 고퀄리티 이미지 생성의 prior를 학습할 수 있었다.

하지만 **‘강아지’**라는 단어의 의미를 공유하는 다양한 형태의 prior를 생성할 수 있다는 장점은 있지만 특정 모습의 ‘강아지’만 집중적으로 생성할 수는 없는 문제가 있다.

아무리 텍스트 Description를 **정밀하게 묘사하더라도** 특정 객체의 모습이 계속 달라지는 문제가 생긴다.

이러한 문제는 Language model의 임베딩이 Vision model의 임베딩과 공유 공간에서 매핑되어있더라도 사라지지 않고 발생한다. (아래와 같은 다양한 모델에서의 실패 사례)


<div align="center">
    <img src="https://github.com/user-attachments/assets/4e66489b-f1bb-4432-90c8-e46d9e0c86e5" width="500">
</div>


따라서 이를 문제점으로 삼아, 특정 단어(강아지나 시계 등)과 생성될 이미지의 특징을 고정시키고자 한 연구가 dreambooth이다.

### 예상되는 문제

객체에 대한 prior를 diffusion model에 심고 (output domain에 원하는 물체 이미지를 추가하고) 싶다.

그러나 만약 Few-shot dataset으로 특정 객체에 대한 미세 조정을 진행한다면? 너무 뻔하게도 Mode collapse / overfitting 위험성이 증대되고, 원래 모델의 성능을 해칠 수 있다.

따라서 기존의 방법들은 보수적인 선택을 한다: 모델이 subject prior를 직접 학습하도록 하지 않고 target distribution에 따라가게끔만 task를 정의하였다. 즉, 객체를 완전히 personalize하진 못하는 문제가 있다.

Diffusion model에 대한 loss (아래 수식)만 잘 세팅하면 객체 정보를 학습시키면서도 기존의 prior를 유지할 수 있다.

$$
\\mathbb{E}_{\\mathbf{x},\\,\\mathbf{c},\\,\\boldsymbol{\\epsilon},\\,t}
\\!\\Bigl[
  w_t \\bigl\\lVert
    \\hat{\\mathbf{x}}_{\\theta}\\!\\bigl(\\alpha_t \\mathbf{x} + \\sigma_t \\boldsymbol{\\epsilon},\\,\\mathbf{c}\\bigr)
    - \\mathbf{x}
  \\bigr\\rVert_2^{2}
\\Bigr]
$$

### Identifier 정의 방법

특정 객체 이미지를 학습시키려면 이를 특별하게 만들어줄 텍스트 묘사가 함께 필요하다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/ed8108a2-7443-44f4-8524-c90d549de4fb" width="500">
</div>


*(e.g.)* [\`바로 그\`] [\`강아지\`] / [\`내가 개인화한\`] [\`고양이\`]

위의 예시에서 \`강아지\` / \`고양이\`는 일반적인 객체를 지칭하는 명사고, 그 앞에 특별한 \`identifier\`를 달아서 text로 컨디션을 줄 때 **내가 원하는 객체가 나오게끔 학습**하고자 한다.

그런데 여기서 문제가 발생하는데, 예컨데 \`unique\` / \`special\` 같이 이미 있는 영어 단어를 무작위로 사용하게 되면 원래 \`unique\` / \`special\`이 기존에 학습되었던 prior에서 disentangle을 수행하고(원래 해당 단어가 생성하던 이미지의 특정 property를 잊어야함) personalize하고자 하는 객체에 달라붙어야하는데, 이러한 구조는 학습이 어렵기 때문에 지양해야한다.

그래서 방법으로 아예 \`xy5syt00\`같은 랜덤한 텍스트를 넣어서 일종의 일련번호처럼 쓰는 방법을 고안해보았다.

근데 이렇게 랜덤하게 뽑아도 이상하게 tokenizer를 통과시켰을때, word 단위가 아니라 토큰 단위로 들어가다보면 strong prior를 가진 애들이 나왔다. 그래서 일반적인 단어를 쓰는 것과 별로 큰 차이가 없는 경우가 생겨서 이 또한 문제가 될 수 있다고 판단하였다.

결론은 **직접 rare token을 찾아서 사용하였다.** 이 부분은 일종의 리버스 엔지니어링으로 수행하게 되는데, token ID를 랜덤으로 샘플링해서 prior와의 entanglement를 측정하고, 이들 중 가장 낮은 entanglement를 가지는 단어를 역으로 decode해서 추리해낸다.

T5-XXL 토크나이저 기준으로 5000번대에서 10000번대 사이의 토큰을 디코딩했을 때의 단어가 가장 괜찮았다고 한다.

### 학습 시 주의할 점

**Language drift :** 언어 모델 함부로 학습시키면 이미 대량의 데이터로 학습한 언어 모델의 (Large Text Corpus로 학습된 결과) representation이 날아간다.

**Reduced output diversity :** 이미지 생성 모델을 잘못 학습시키면 이미지 다양성이 훼손될 수 있는 문제가 있다.

$$
\\mathbb{E}_{\\mathbf{x},\\,\\mathbf{c},\\,\\boldsymbol{\\epsilon},\\,\\boldsymbol{\\epsilon}',\\,t}
\\!\\Bigl[
w_t\\,
\\bigl\\lVert
\\hat{\\mathbf{x}}_{\\theta}\\!\\bigl(\\alpha_t \\mathbf{x} + \\sigma_t \\boldsymbol{\\epsilon},\\,\\mathbf{c}\\bigr)
- \\mathbf{x}
\\bigr\\rVert_2^{2}
\\;+\\;
\\lambda\\,w_{t'}\\,
\\bigl\\lVert
\\hat{\\mathbf{x}}_{\\theta}\\!\\bigl(\\alpha_{t'} \\mathbf{x}_{\\mathrm{pr}} + \\sigma_{t'} \\boldsymbol{\\epsilon}',\\,\\mathbf{c}_{\\mathrm{pr}}\\bigr)
- \\mathbf{x}_{\\mathrm{pr}}
\\bigr\\rVert_2^{2}
\\Bigr]
$$

뒤쪽에 있는 텀이 정규화 텀이다. 사전 학습 모델에서 **미리 샘플링해두고** 학습할 때 source dataset 형태로 사용한다. 흔한 domain adaptation 방법과 동일하다.

## LoRA (Low-Rank Adaptation)


<div align="center">
    <img src="https://github.com/user-attachments/assets/485e39ab-c39e-41f5-9d35-b7a15dfcd530" width="500">
</div>


원래 모델은 건들지 않고 기존 weight보다 낮은 차원에서 conditioning을 주는 방법을 통해 적은 파라미터 수로 기존 모델의 성능을 바꾸지 않고 plug-and play가 가능한 학습법. Low Rank 영역에서의 implicit space를 학습하는 것이 성능에 악영향을 미치지 않는다는 관점에서 나온 논문이다.

$$
h = W_0 x + \\Delta W x = W_0 x + BAx
$$

위의 수식에서 $BA$만 효율적으로 학습하려고 한다. 결국 잔차 학습을 통해 fine-tuning의 효과를 얻고자 한다.

# 학습 프레임워크

AnimateDiff는 기본적으로 Text to Image Diffusion Model을 사용하고, 이러한 T2I 모델 아키텍쳐로 대표적으로 잘 알려진 것들은 GLIDE, DALL-E 등 많지만 그 중 가장 대표적으로 많이 쓰이면서 퀄리티가 높은 Stable Diffusion (LDM) 모델에 대해서 보면 다음과 같다.

기존 DDPM에서는 ResNet 중간에 $16 \\times 16$ 피쳐맵에서 어텐션을 사용하여 타임코드에 대한 정보를 넘겨주는 방식을 사용하는데, ADM에서는 이를 $32 \\times 32$ 부터 $8 \\times 8$로 확장시키고, 어텐션 헤드 수나 dimension 등을 조절하여 최적의 아키텍쳐를 찾았다. ADM부터 본격적으로 GAN의 성능을 넘어서게 되었다.

이후 후속으로 나온 연구들은 이러한 ADM 구조의 어텐션에 text embedding 정보를 잘 넣는 방식을 연구했는데, 이러한 연구들이 대표적으로는 GLIDE, DALL-E 그리고 Imagen 이라고 할 수 있겠다. 

LDM은 이런 아키텍쳐와는 조금 다른 독자노선을 선택했는데, 이는 이미지 단위가 아니라 VQ-GAN과 같은 모델 기반으로 latent 단위로 인코딩을 해서, 노이즈로부터 이미지가 아닌 디코딩 이전의 이미지 임베딩을 예측하도록 하여 연산량을 줄였다. 또한 conditioning 파트에서도, 기존 방법들은 대부분 text embedding을 pooling해서 기존 아키텍쳐에 있는 어텐션에 타임 코드($t$)로부터 온 positional embedding과 함께 잘 추가해주는 방식을 선택하여 학습하는데, 이 모델은 self-attention 구조를 self-attention 과 cross-attention이 혼합된 말 그대로 트랜스포머 구조를 차용하였다. Latent 연산이 기본적으로 연산량을 줄여주면서, 텍스트 임베딩에 대한 conditioning에 더욱 집중할 수 있던 흐름이 되는 것이다.

AnimateDiff에서도 대부분의 T2I 레퍼런스는 personalized model에서 나오게 되었는데, 사실상 거의 모든 T2I 모델은 LDM을 LoRA 튜닝하는 방식으로 생성되는 것이 한때 인기였기 때문에 이 논문에서 적용한 방법론은 ablated UNet이 아닌 transformer architecture로 생각하는 것이 맞는 것 같다. 그 중 Stable Diffusion Model은 이를 보다 확장시켜 모델링을 더욱 진행한 형태다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/3eee8c62-4a38-4a7f-8b97-3ffa6befdcb7" width="500">
</div>


AnimateDiff는 3개의 파이프라인으로 구성되어서 학습된다.

1. 이미지 도메인과 비디오 도메인 사이의 GAP 없애기 (추가 모듈)
2. 움직임에 대한 prior를 학습시키기 (추가 모듈)
3. 원하는 움직임에 대한 personalize 진행하기 (추가 모듈)

여기서 움직임에 대한 정보를 학습시키는 구조는 2번과 3번이 유사하지만, 2번은 일반적인 비디오의 특성 (temporal information)에 대한 학습을 진행하는 한편, 3번은 생성 시 특정 움직임을 위한 모델을 만들고자 한다. 그렇기 때문에 1번과 3번은 미세 조정의 관점으로 LoRA를 적용하며,  2번은 transformer 구조를 차용하여 pretraining하게 된다. 

### Alleviate Negative Effects

사실 이미지와 비디오는 프레임별로 보게 되면 크게 다르지는 않다. 어찌 보면 이미지는 정적인 프레임 하나만 있는 비디오라고 생각하면 된다.

하지만 비디오 프레임은 일반적인 이미지와 다르게 각 프레임별로 보게 되면 객체가 이동하거나 카메라가 이동하게 되면 중간중간 모션 블러가 발생하거나 몇몇 인코딩/디코딩 방식에 따른 compression이 발생해서 일부 픽셀이 뭉개진다던지, 워터마크가 있다던지 등등 깔끔함과는 차이가 있다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/251f6c57-e6ac-42df-a454-d92fbc1004db" width="500">
</div>



<div align="center">
    <img src="https://github.com/user-attachments/assets/990f0fdc-e4fb-4dd0-a41d-b6e84b830e21" width="500">
</div>


이는 곧 이미지와 비디오 도메인 간의 distribution misalignment 문제를 발생시키는데, 이를 조정하지 않고 motion prior를 학습하려하면 다음과 같은 문제가 발생한다.

1. 사전 학습된 DM의 text-image conditioning space가 깨진다.
2. 사전 학습된 DM의 분포에서 벗어난 이미지에 대해 학습을 수행하지 못한다.

따라서 좌측과 같이 ResNet 블록 간에 conditioning을 위한 transformer block (Attention Modules)에 Adapter를 달아서 학습하게 된다. 수식을 보면 알 수 있듯이 LoRA로 학습시킨 것을 알 수 있다.

$$
Q = \\mathcal{W}^Q z + \\operatorname{AdapterLayer}(z) = \\mathcal{W}^Q z + \\alpha \\cdot AB^\\top z
$$

### Learn Motion Priors

Adapter로 DM Block의 출력값에 대한 분포를 맞춰줬기 때문에 이제는 temporal 정보를 학습시키고자 한다.

앞에 학습한 파이프라인은 비디오 데이터셋을 일부 프레임을 샘플링해서 Domain adapter를 비디오 프레임 형태의 이미지에 익숙하게 한거라, 이미지 단위에서 도메인을 맞춘거지 비디오 도메인으로 끌고왔다고 할 수 없기 때문에 따라서 여전히 DM(Diffusion Model)은 단일 이미지에 대해서만 적용이 가능하다는 문제가 있다.

움직임에 대한 특성만 학습하기 위해 해당 학습 파이프라인을 추가하였다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/c0a56b99-b110-4b97-b7d7-ce2bd51fc577" width="500">
</div>


DM을 비디오 영역으로 확장시키는 개념은 Make a video 논문에서 내부분이 구현되었다. 가장 간단한 방식으로는 video frame을 이미지 단위로 처리하는 방식이 있다. 예를 들어 배치 $b$에 프레임 $f$라면, 이미지를 $b \\times f$ 배치만큼 처리한다고 생각하면 된다.

그런데 이 구조에서는 video motion에 대한 prior를 따로 학습할 수 없기 때문에, motion 부분만 학습하고자 temporal Transformer 아키텍쳐를 추가한다. 이때, 모션 prior를 학습하는 모듈은 일반 영상을 처리하는 트랜스포머와 다르게 spatial dimension을 무시해서 처리하게끔 한다 ($b \\times h \\times w$). Conditioning이 되는 파트이므로 zero initialization + residual 연결을 통해 motion에 대한 잔차 정보만 학습시키는 방법을 사용하였고, 전반적으로 모두 Make a video 논문 아이디어에서 차용된 것으로 보인다. 아래는 [깃허브 소스](https://github.com/guoyww/AnimateDiff/blob/main/animatediff/models/motion_module.py)에서 Versatile Attention 부분을 참고한 것이다.

\`\`\`python
def forward(self, hidden_states, encoder_hidden_states=None, attention_mask=None, video_length=None):
        batch_size, sequence_length, _ = hidden_states.shape

        if self.attention_mode == "Temporal":
            d = hidden_states.shape[1]
            hidden_states = rearrange(hidden_states, "(b f) d c -> (b d) f c", f=video_length)
            
            if self.pos_encoder is not None:
                hidden_states = self.pos_encoder(hidden_states)
            
            encoder_hidden_states = repeat(encoder_hidden_states, "b n c -> (b d) n c", d=d) if encoder_hidden_states is not None else encoder_hidden_states
        else:
            raise NotImplementedError

        # ... hidden states 구하는 부분 생략

        # linear proj
        hidden_states = self.to_out[0](hidden_states)

        # dropout
        hidden_states = self.to_out[1](hidden_states)

        if self.attention_mode == "Temporal":
            hidden_states = rearrange(hidden_states, "(b d) f c -> (b f) d c", d=d)

        return hidden_states
\`\`\`

이 부분을 보면 cross attention은 사용되지 않는 것으로 보아, Temporal Attention 파트는 오로지 앞단에서 생성되었을 latent 간의 temporal consistency를 위한 장치로 생각된다.

즉, 이미 앞부분에서 어느 정도 프레임별 이미지가 완성되었다고 가정하고, temporal attention 파트에서는 프레임 간의 temporal information만 학습하고자 한 것.

### Customize motion prior


<div align="center">
    <img src="https://github.com/user-attachments/assets/11ae4835-f794-40eb-9af5-6274606ff8e8" width="500">
</div>


2번까지만 해도 T2I 모델에 T2V prior를 넣을 수 있는데, 만약에 특정 모션 정보에 대해 특화된 모델을 만들고자 한다면(카메라 모션에 따라서) 해당 모션에 대한 레퍼런스 영상 20~50개만 가지고 튜닝할 수 있는 방법을 제안하였다. 앞서 했던 내용은 \`personalize\` T2I 모델을 \`personalize\` T2V 모델로 바꾸는 거였고, 3번은 \`personalize\` T2V를 \`personalize + motion personalize\` T2V로 바꾸는 작업이 되겠다. 특별한 작업은 없고 stage2에 추가했던 motion module에 LoRA를 붙여서 학습시킨다.

# 실험 과정

### 학습 파트

Domain Adapter는 기존의 Stable Diffusion Model 학습 objective로 튜닝이 가능하고, Motion module과 LoRA는 비슷하기는 한데 차이가 있다면 motion을 학습시킬때는 학습 범위 내에서 frame에 대한 정보가 필요하기 때문에

$$
\\mathcal{L}
= \\mathbb{E}_{\\mathcal{E}\\!\\left(x_{0}^{1:f}\\right),\\,y,\\,\\epsilon^{1:f}\\sim\\mathcal{N}(0,I),\\,t}
\\!\\left[
  \\left\\|
    \\epsilon
    - \\epsilon_{\\theta}\\!\\left(z_{t}^{1:f},\\,t,\\,\\tau_{\\theta}(y)\\right)
  \\right\\|_{2}^{2}
\\right].
$$

Objective가 관측하게 되는 empirical도 frame \`1 ~ f\` 를 기준으로 업데이트된다. 물론 파라미터는 학습되는 각 파트를 제외하고는 모두 freeze하고 학습한다.

### 정량 평가

텍스트랑 얼마나 잘 align 되는가에 대한 CLIP metric 그리고 실제 사용감 테스트를 위한 User Study


<div align="center">
    <img src="https://github.com/user-attachments/assets/3169413e-85e7-464b-b622-8793cd26d0ea">
</div>


### 추론 결과


<div align="center">
    <img src="https://github.com/user-attachments/assets/783b2ed7-6f9c-4595-94ff-cd8b86b4376d">
</div>


대부분의 모듈은 LoRA 형태로 학습되었기 때문에 (Motion prior 학습한 temporal Transformer 제외). 이에 대한 $\\alpha$값으로 ablation 수행이 가능하다. 상대적으로 video domain이 이미지 퀄리티가 image domain보다는 확연히 떨어지기 때문에 점진적으로 떨어지는 것을 확인할 수 있다.
`,ZO=`---
title: "Pytorch 2.0에서 어떤 점이 변했을까??"
category: "ai technology"
publishedAt: "2023-01-05"
thumbnail: "https://pytorch.org/wp-content/uploads/2025/01/pytorch_seo.png"
---


이번 게시글에서 소개할 내용은 pytorch 1.13 버전 이후 새롭게 출시된 [pytorch 2.0](https://pytorch.org/get-started/pytorch-2.0/)을 사용해보는 것이다.


# OVERVIEW

본인은 파이토치, 텐서플로우 모두 써본 사람이고, 굳이 비교하자면 <U>파이토치</U>가 <U>텐서플로우</U>보다 사용하기 편하다고 생각된다. 일단 사용자 친화적(클래스 정의부터 시작해서, 학습 함수를 구현하는 과정 등등이 굉장히 깔끔하다)이기도 하고, 가장 큰 이유는 그냥 내가 많이 써와서인듯하다. 암튼 이렇게 어떤 모듈이 더 낫다는 것은 절대적으로 주관적인 생각이고 파이토치, 텐서플로우와 더불어 다양한 딥러닝 모듈이 존재한다.   
파이토치를 개발한 meta(구 facebook)은 tensorflow와는 다르게 굉장히 꾸준한 문법 및 사용법을 유지해오고 있다. 뒤쪽에서 motivation에서 언급하겠지만 <U>파이토치 개발자</U>들도 <U>사용 편의성에 집중한 플랫폼 구축</U>을 가장 최우선의 목표로 삼았다고 한 것을 볼 수 있다. 아무튼 그러한 이전 단계에서 넘어서서, 이제는 next-generation pytorch를 준비하고자 큰 결심을 해서 한 것이 바로 pytorch 1.x에서 pytorch 2.0으로 이름을 달리 지은 것.   
파이토치가 가지는 큰 장점 중 하나는 python API와의 효용성이 크다는 것인데, Pytorch 2.0은 여기에 이어 더 빠른 속도로 학습이 가능하다고 한다. 아마도 Pytorch 2.x 버전에서는 뒤이어 설명할 \`\`\`model.compile\`\`\`이 가장 주요한 변화가 아닐까 싶다.


# Pytorch 2.0에서 등장한 model.compile이란?
아마 tensorflow를 사용해본 사람들은 해당 문구에 익숙할 것이다. Tensorflow에서는 \`\`\`model.compile\`\`\`이 의미하는 바가 메소드의 인자로 최적화(fitting)할 optimizer, loss 그리고 metric을 설정해주는 것이다. 여기서 loss는 직접적으로 최적화에 사용되는 objective function으로 작용하고, metric은 classification과 같은 task에서의 accuracy로 생각하면 된다.   
Pytorch 2.x에서의 \`\`\`model.compile\`\`\`은 조금은 다르게 기존 학습 코드 자체는 그대로 유지하도록 하지만, Pytorch가 기존에 작동하던 C++ 베이스에서 넘어와서 <U>python 상에서 구동하도록</U> 한다는 것이다. 그리고 pytorch 2.0에서 좋다고 말할 수 있는 부분은 완전히 추가적인 기능(optional)이므로, \`\`\`model.compile\`\`\`과 같은 기능을 사용하지 않는다고 해서 <U>pytorch 2.x 버전을 사용하지 못하는 것</U>은 아니다. 즉, 이전 버전의 pytorch 모든 코드가 <U>pytorch 2.x에서도 그대로 호환이 가능하다는 것</U>이다. \`\`\`torch.compile\`\`\` 코드에 적용되는 기반 기술들은 다음과 같다.

- TorchDynamo : Python frame evaluation hooks를 사용하여 pytorch 프로그램 안정성에 기여한다. 정확히는 아직 잘 모르겠지만 백프롭과 같은 graph capture에서 도움이 된다는 듯하다. 
- AOTAutograd : 기존 pytorch의 Autograd 엔진을 오버로딩한다. 미리 연산된 역연산 trace를 생성하기 위해 autodiff를 예측한다. 
- PrimTorch : 완벽한 PyTorch 백엔드를 구축하기 위해 $2000$개에 달하는 pytorch 연산들을 $250$개의 기본 연산들로 canonicalize한다.
- TorchInductor :  여러 액셀러레이터 및 백엔드용 고속 코드를 생성하는 딥러닝 컴파일러. NVIDIA GPU의 경우 OpenAI Triton을 주요 구성 요소로 사용함.

대충 번역해봤는데, 아무튼 아직 데모 단계에 가까워서 조금 고오급 최신 GPU에 대해서는 성능이 확실하게 보장되는 것 같은데, 옛날 GPU 모델에 대해서는 아직까지는 유의미한 발전은 아닌 것 같다. 그래도 아마 조금은 빨라질 것 같은 느낌적인 느낌은 있다. 일단 되게 좋은 점은 <U>기존 파이토치 문법을 그대로 사용해도 된다는 것</U>이다. 사실 번역된 내용만 보면 컨셉을 파악하기가 힘들어서(영어로 봐도...ㅋㅋㅋ), 이 부분은 뒤에서 다시 디테일하게 짚고 넘어갈 것이다.


# Pytorch에서 직접 검증
말만 빠르다 빠르다 하면 검증이 안되니까 실제로 파이토치에서 실험을 해봤다. 내가 한 것은 아니고 파이토치 개발진 분들이 했다고 한다. 앞서 말했던 것처럼 단순히 model을 컴파일만 해주면 되는 부분이라, 163개의 open-source model를 활용(구체적으로 분류해보면 46개의 [HuggingFace Transformer](https://github.com/huggingface/transformers) 모델들, 61개의 [TIMM](https://github.com/rwightman/pytorch-image-models) model들 그리고 56개의 [TorchBench](https://github.com/pytorch/benchmark/) 모델들)하였고, 이는 Image classification부터 시작해서 NLP task나 강화학습과 같이 광범위한 딥러닝 네트워크를 커버한다. 즉, 어떠한 task에 대해서도 속도가 좋아진다는 걸 보여주고 싶었다고 한다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/210737816-cd5c206a-c632-4417-ba8d-f6a3f1a19848.png" width="700"/>
</div>


결론부터 보자면 위와 같다. open-source model의 형태나 코드를 전혀 바꾸지 않고 단순히 \`\`\`torch.compile\`\`\`을 통해 wrapping해주고, 속도 향상과 validate accuracy를 측정하였다. 물론 속도 향상은 data-type에 따라 상이하기 때문에 float32와 Automatic Mixed Precision(AMP)등에 대한 속도 향상을 모두 측정했고, 계산한 두 속도 향상을 $0.75 \\times AMP + 0.25 \\times float32$로 계산했다고 한다. 여기서 AMP에 weight을 좀 더 넣어준 이유는 실질적으로 더 많이 보여서(활용되어서) 그렇다고 한다.   
163개의 open-source model에 대해 \`\`\`torch.compile\`\`\`은 93%, 모델 학습은 43% 빠른 속도로 동작하였다고 한다. 참고로 이 결과 기준은 **서버용 GPU인 A100**에서 측정된 결과고, 로컬 컴퓨터나 desktop에서 사용하는 GPU인 3090과 같은 시리즈에서는 잘 동작하지 않을 수 있고, 심지어는 더 느릴 수도 있다고 언급했다.

> Caveats: On a desktop-class GPU such as a NVIDIA 3090, we’ve measured that speedups are lower than on server-class GPUs such as A100. As of today, our default backend TorchInductor supports CPUs and NVIDIA Volta and Ampere GPUs. It does not (yet) support other GPUs, xPUs or older NVIDIA GPUs.

실제로 위와 같이 써놓으심.. 잔뜩 기대하고 pytorch 2.0 설치했는데 내 컴퓨터에서는 안된다니 너무 슬펐다. 나중에라도 쓸 수 있겠지 기대하면서 존버해야지.


# Direction of Pytorch 2.x
앞서 본 내용은 \`\`\`pytorch 2.0\`\`\` 출시 초창기에 완벽히 셋팅되지 않은 상태에서 사용했기 때문에 이런 저런 문제가 있었던 것 같다. 간만에 파이토치 홈페이지를 들어가보니 내용이 많이 업데이트된 것으로 보이고 보다 안정적인 \`\`\`pytorch 2.0\`\`\`을 사용해볼 수 있지 않을까 하는 기대감에 추가로 글을 작성해본다. 


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/225178361-b159f7d8-06b0-44a0-9807-b2e706bee4b5.png" width="700"/>
</div>


파이토치가 \`\`\`2.x\`\`\` 버전으로 넘어가면서 개발자 분들은 \`\`\`compile\`\`\` 이라는 메소드에 집중한다고 한다. 앞서 언급했던 여러 기반 기술들을 통해 model을 최적화하고, 학습 속도를 빠르게 할 수 있다는 면을 보다 강화하고 이를 점차 scalable하게 키울 생각인 것 같다. 따라서 기존의 파이토치와는 조금 다르게 \`\`\`Pytorch 2.0\`\`\` 시리즈에서는 지속적으로 연구하면서 그와 동시에 <U>사용자들과의 interaction</U>을 통해 인사이트를 얻고, <U>더 좋은 서비스</U>를 이후 버전에서 보여주겠다는 것이 목표인 듯하다.

실제로 파이토치 사용자들 중 유명한 사람들의 몇몇 <U>인사이트에 대해 인용한 부분</U>은 다음과 같다.

#### Sylvain Gugger the primary maintainer of HuggingFace transformers:
> "With just one line of code to add, PyTorch 2.0 gives a speedup between 1.5x and 2.x in training Transformers models. This is the most exciting thing since mixed precision training was introduced!"

간단하게 말하자면 Transformer에 \`\`\`model.compile\`\`\`을 사용하니, 학습 속도가 약 1.5배에서 2.0배 증가했다는 말이다.


#### Ross Wightman the primary maintainer of TIMM (one of the largest vision model hubs within the PyTorch ecosystem):

> “It just works out of the box with majority of TIMM models for inference and train workloads with no code changes”

코드 변화 없이(?) 바로 TIMM에 내장된 대부분의 모델에서 inference나 train 속도가 빨라졌다는 말인 것 같다. ~~정말 그런가...?~~



#### Luca Antiga the CTO of Lightning AI and one of the primary maintainers of PyTorch Lightning
> “PyTorch 2.0 embodies the future of deep learning frameworks. The possibility to capture a PyTorch program with effectively no user intervention and get massive on-device speedups and program manipulation out of the box unlocks a whole new dimension for AI developers.”

유저가 굳이 <U>프로그램을 뜯어볼 필요 없이</U> pytorch의 프레임웤을 효율적으로 사용할 수 있게 되었다는 건, pytorch가 어떤 임베디드 플랫폼 상에서도 효율적으로 작동할 수 있을 것이라는 말인 듯하다.


# Motivation
파이토치 개발자들이 개발에 있어서 가장 중시했던 포인트는 flexibility와 hackability를 보장하는 것이었다. 즉 사용하는 사람들에게 제공하는 시스템은 <U>최대의 자유도</U>를 가지고 있는 것이 바람직하다고 생각한 것 같다. 그리고 여기에 추가적으로 플랫폼 최적화를 통한 performance 향상이 이어진다고 볼 수 있다.

Pytorch가 런칭한 것은 2017년이고, 런칭 이후 GPU와 같은 병렬 연산기/하드웨어 가속기는 당시 스펙보다 <U>메모리 접근 속도</U>나 <U>연산 속도</U> 측면에서 많은 향상이 이루어졌다. 그러면서 자연스레 파이토치는 eager execution의 성능을 높이기 위해 pytorch의 코드 대부분을 C++로 옮기게 되었으며(파이토치 코드의 대부분 source는 C++이 기반에 있는 것을 알 수 있다), 이러한 방식은 사용자들에게 있어 코드 기여도(hackability)를 낮추는 진입 장벽이 되어버렸다. 여기서 eager execution이란, 그래프 생성 없이 <U>연산을 즉시 실행</U>하는 명령형 프로그래밍 환경을 의미한다. 아마도 텐서플로우를 써본 사람이라면 어떤 개념인지 알 것 같은데, 이러한 이유 때문에 pytorch는 tensorflow와 사실상 딥러닝 개발 환경 구축에 있어 <U>방향성이 다른 연구</U>를 했다고 볼 수 있을 것 같다.

아무튼 이렇게 독자적으로 열심히 eager execution 쪽에서의 성능을 높이는 것에는 한계가 있다고 판단하였다. 그렇기에 pytorch에서도 상당히 이른 시기인 2017년 중순(7월)에 pytorch를 위한 compiler를 만들기로 결정하였다. 컴파일러는 <U>pytorch 속도를 빠르게</U> 바꾸면서, 그와 동시에 <U>pytorch experience를 해치면 안된다</U>(복잡하거나 자유도가 낮으면 안됨)라는 목표를 가진다. 이런 유저의 자유도/사용 편의성과 관련된 flexibility를 유지하는 메인 기준으로는 dynamic shapes, programs를 보조하는 것이었다.



<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/225183779-0bc28ba9-1d24-424f-a3b3-f9395f0e3337.gif" width="500"/>
</div>



# Pytorch 2.0 컴파일러
앞서 언급했듯이 파이토치는 2017년을 기준으로 컴파일러 연구를 지속해왔다. 컴파일러의 구성 요소를 총 3가지로 나누어 설명한다. 다음 세 요소 중 가장 어려운 부분이 <U>graph acquisition</U>이라고 한다.

- Graph acquisition
- Graph lowering
- Graph compilation

런칭 이후 파이토치에서는 \`\`\`torch.jit.trace\`\`\`, TorchScript, FX tracing 그리고 Lazy Tensors와 같이 모델 최적화를 위한 방법을 개발했지만, 어떤 것도 개발진들이 원하는 <U>'컴파일러의 느낌'</U>을 주지는 못했다. 몇몇은 자유도가 높지만 그에 비해 속도가 떨어진다거나, 속도가 빨라지면 그만큼 자유도가 떨어지는 trade-off가 발생한 것이다. 사실 본인도 다른 플랫폼 상에서 학습된 모델을 사용할 때 효율성 및 적합성을 위해 몇몇 스크립트를 사용해봤지만 불편한 점이 좀 많았던 기억이 있다. 그나마 TorchScript가 괜찮은 성능을 보였지만 작성한 code에 성능이 많이 의존하기도 하고 사용자가 코드를 많이 손봐야한다는 점에서 단점이 발생하였다. 그러다보니 <U>파이토치가 익숙하지 않은 사람들</U>은 많이 사용하지 않는 경향이 생겨버렸다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/225185127-0875e8fa-bd44-4322-914d-752e9877950b.png" width="700"/>
</div>


그림을 보면 앞서 언급한 \`\`\`compiler\`\`\`의 각 기술적 요소들이 어떠한 역할을 하는지 확인할 수 있다. 먼저 \`\`\`TorchDynamo\`\`\`, \`\`\`AOTAutograd\`\`\`가 eager execution 형태로 작성된 <U>모델 구조를 그래프화</U>하고, gradient 연산이 가능하게끔 도와준다. 여기에 \`\`\`Prim\`\`\`과 같은 연산 단순화 방법을 통해, 파이토치에 있는 수많은 연산들을 간단한 연산들로 바꾸어 <U>그래프를 단순화</U>하는 작업을 도와준다. 이렇게 단순화된 그래프 구조를 <U>TorchInductor가 컴파일</U>하는 것이다.

#### TorchDynamo: Acquiring Graphs reliably and fast

TorchDynamo는 [PEP-0523](https://peps.python.org/pep-0523/)에서 'frame evaluation API'로 소개된 Cpython feature를 사용한다. 컴파일링이 안정적으로 진행되기 위해서는 graph acquire이 필요한데, 이를 도와주는 c언어 기반 프레임 워크라고 볼 수 있을 것 같다. 실제로 해당 방법이 효과적인지 비교해보기 위해 pytorch로 작성된 $7000$개가 넘는 깃허브 소스를 참고하여 코드를 돌려보았고, TorchScript나 기타 방법들은 graph를 획득하는 시간이 $50\\%$에 불과했지만(코드를 손봐야하는 overhead도 큼), TorchDynamo는 $99\\%$의 거의 대부분의 시간동안 graph를 획득했고, 코드를 손봐야하는 번거로움도 없어졌다. 개발자들이 원했던 flexibility(overhead), speed(graph acquisition) 모두 충족한 것이다.

#### TorchInductor: fast codegen using a define-by-run IR

Pytorch 2.0을 위한 새로운 백엔드 컴파일러를 설계하는데 있어서, [Triton language](https://github.com/openai/triton)의 사용량 증대에 영감을 받았다고 한다(트리톤을 사용하게 되면 GPU에서 연산되는 작업을 가속화할 수 있다는 장점이 있다). 따라서 개발자들도 Pytorch eager를 그대로 가져가고, pytorch의 여러 특징들을 <U>그대로 유지할 수 있는 컴파일러 백엔드를 고안</U>하였다. 


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/225198662-229ead11-7a41-4d34-bf0a-8cc28256232c.png" width="400"/>
	<img src="https://user-images.githubusercontent.com/79881119/225198674-6acda856-d06f-48f0-a22b-a63f6dcc7324.png" width="500"/>
</div>


Pytorch는 tensorflow와 다르게 연산 구조를 정의한 후 값을 계산하는 구조가 아닌, 구조와 계산이 동시에 진행되는 방식인 <U>'define by run'</U>을 사용하는데, TorchInductor는 <U>IR 레벨에서의 학습 루프</U>를 사용하여 GPU상의 Triton 코드/CPU 상의 C++/OpenMP 코드로 매핑하게 된다. Core loop가 IR 레벨에서 적은 operator로 구성되고 무엇보다 <U>python으로 구현되어있기 때문</U>에 C++보다 hackable/extensible한 효과를 기대할 수 있다.

#### AOTAutograd: reusing Autograd for ahead-of-time graphs
Pytorch 2.0에서 목표로 삼았던 것 중 하나는 학습 속도를 빠르게 하는 것이다. 그렇기 때문에 단순히 user-level code만 capture하는 것이 아니라, 학습 속도에 큰 영향을 주는 backpropagation 또한 capture하는 것이 중요하다. <U>Capture</U>라는 말이 자주 쓰이는데, 흔히 스마트폰으로 재미있는 사진이나 기록하고 싶은 내용을 찾게 되면 스크린샷(캡처)를 아는 상황을 생각해보면 된다. 학습 과정에 유저가 정의한 모델 연산의 순서나 loss function의 미분을 통한 backpropagation 모두 학습 루프 상에서 지속적으로 유지된다면 <U>일종의 가이드북 역할</U>을 통해 <U>번거로운 연산의 중복</U>을 피할 수 있다는 것이다. 그리고 이미 pytorch에 있는 autograd system을 재활용하고 싶다는 것이 메인 아이디어였고, AOTAutograd는 pytorch의 \`\`\`torch_dispatch\`\`\`를 사용하여 backward pass를 Autograd 엔진을 통해 <U>미리 capture 해둠으로써</U>(Ahead-Of-Time) 속도 향상을 이루어냈다. 

#### PrimTorch: Stable Primitive operators
Pytorch backend를 작성하는건 생각보다 굉장히 어려웠는데, 이는 Pytorch가 operators를 굉장히 많이 지원하기 때문이다(세세하게 분류하자면 거의 $2000$개 이상).

<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/225202341-6c5a5716-2c96-4e09-b6a5-dbf4ab8c04b4.png" width="500"/>
</div>


따라서 <U>각 operator마다</U>의 특징을 잡아서 <U>백엔드를 구성하는 것</U>은 여간 어려운 일이 아닌 것이다. 그렇기에 PrimTorch라는 프로젝트에서 진행한 내용은 수많은 operator를 통합할 수 있는 보다 작고 안정적인 operator set를 구성하는 것이었다. Pytorch에 존재하는 여러 operator는 <U>적은 규모의 operator set를 조합함으로써</U> 모두 구성될 수 있기 때문이다. Prim 프로젝트에서 정의한 두 operator set는 다음과 같다.

- Prim operators($\\sim 250$) : 대부분 Low level operator에 해당된다(컴파일러 레벨에 적합함).
- ATen operators($\\sim 750$) : ATen level의 operator나 level에 적합한 연산이다. Prim operator보다는 high level operator에 해당된다.


# Pytorch 2.0 설치법

우선 자신의 GPU 사양을 확인해준다.

\`\`\`bash
nvidia-smi
\`\`\`

나는 CUDA 버전이 11.7이라 해당 스펙에 맞는 latest nightlies를 설치해주었다.

\`\`\`bash
pip3 install numpy --pre torch torchvision torchaudio --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu117
\`\`\`

본인 컴퓨터에 CUDA 버전이 11.6이면 다음 install 코드를 쓰면 되고,

\`\`\`bash
pip3 install numpy --pre torch torchvision torchaudio --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu116
\`\`\`

그것도 아니고 CPU만 있으면 다음 install 코드를 쓰면 된다.

\`\`\`bash
pip3 install numpy --pre torch torchvision torchaudio --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cpu
\`\`\`


# Pytorch 2.0 사용법

\`\`\`python
import torch
import torchvision.models as models

model = models.resnet18().cuda()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
compiled_model = torch.compile(model)

x = torch.randn(16, 3, 224, 224).cuda()
optimizer.zero_grad()
out = compiled_model(x)
out.sum().backward()
optimizer.step()
\`\`\`

\`\`\`compiled_model\`\`\`은 본인이 정의한 네트워크 구조를 기준으로 \`\`\`forward\`\`\` 메소드를 보다 최적화시켜 속도를 빠르게하는 것이다. Compiling과 관련된 내용을 조금 더 디테일하게 보면 다음과 같다.

\`\`\`python
def torch.compile(model: Callable,
  *,
  mode: Optional[str] = "default",
  dynamic: bool = False,
  fullgraph:bool = False,
  backend: Union[str, Callable] = "inductor",
  # advanced backend options go here as kwargs
  **kwargs
) -> torch._dynamo.NNOptimizedModule
\`\`\`

컴파일 단계에서 model을 제외하고 아무런 옵션을 넣지 않는다면 위에 보이는 것과 같은 <U>default 셋팅이 적용</U>된다. 각각의 arg에 대해 살펴보면 다음과 같다.

- mode
모드는 컴파일 단계에서 <U>어떻게 최적화할지</U> 지정해주는 변수이다. Default mode는 너무 오래걸리지 않으면서 메모리를 많이 잡아먹지 않는 선에서 <U>compile을 효율적으로 진행한다</U>. 다른 방법으로는 \`\`\`reduce-overhead\`\`\`를 통해 메모리는 조금 더 쓰되 overhead를 줄여주는 옵션이 있으며, \`\`\`max-autotune\`\`\`을 통해 최대한 빠르게 학습할 수 있는 코드를 오랜 시간동안 컴파일해서 제공하는 옵션도 있다.

\`\`\`python
# API NOT FINAL
# default: optimizes for large models, low compile-time
#          and no extra memory usage
torch.compile(model)

# reduce-overhead: optimizes to reduce the framework overhead
#                and uses some extra memory. Helps speed up small models
torch.compile(model, mode="reduce-overhead")

# max-autotune: optimizes to produce the fastest model,
#               but takes a very long time to compile
torch.compile(model, mode="max-autotune")
\`\`\`

- dynamic
다이내믹은 dynamic shape에 대해 code path를 enabling할 지 결정하는 boolean 변수이다. Compiler 최적화 과정이 프로그램을 dynamic shape 프로그램에 적용될 수 없게 만드는 경우가 있는데, 이를 조절함으로써 본인이 원하는 방향대로 컴파일을 할 수 있게 해준다. 이 부분은 아직 완벽히 이해가 되지는 않지만 데이터 shape가 변하는 상황에서 <U>graph를 유동적으로 컴파일</U>할 수 있게끔 하는 것과 관련이 있을 것 같다.

- fullgraph
풀그래프는 Numba의 \`\`\`nopython\`\`\`과 유사하다. 전체 프로그램을 하나의 그래프로 컴파일하고, 만약 실패한다면 왜 불가능한지 설명해주는 error 메세지를 띄운다. 굳이 쓰지 않아도 상관없는 옵션.

- backend
백엔드는 어떤 compiler backend를 적용할 지 결정하게 된다. 디폴트로 정해진 값은 앞서 설명했던 \`\`\`TorchInductor\`\`\`가 사용되지만, 다른 옵션들도 존재한다고 한다(제대로 알아보진 않았다).


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/225205790-9ddc3526-28d3-49e9-9859-f00132e0d015.png" width="800"/>
</div>



# Compile 이후 사용할 수 있는 기능들

#### Reading and updating Attributes

Eager mode에서 가장 용이한 점은 학습 도중에 model의 weight에 접근하거나 값을 그대로 읽어올 수 있다는 점이다(예를 들어, \`\`\`model.conv1.weight\`\`\`). TorchDynamo는 이를 인지하고 만약 attribute가 변한 것을 감지하면 자동으로 해당 부분에 대한 변화를 다시 컴파일해주게 된다.

\`\`\`python
# optimized_model works similar to model, feel free to access its attributes and modify them
optimized_model.conv1.weight.fill_(0.01)

# this change is reflected in model
\`\`\`

#### Serialization
\`\`\`optimized model\`\`\`의 state-dict나 \`\`\`model\`\`\`의 state-dict를 serialize할 수 있다. 최적화된 모델과 모델이 서로 같은 parameter를 지칭하기 때문에 <U>아래의 두 코드는 같은 결과</U>를 낸다.

\`\`\`python
torch.save(optimized_model.state_dict(), "foo.pt")
# both these lines of code do the same thing
torch.save(model.state_dict(), "foo.pt")
\`\`\`
 
하지만 \`\`\`model\`\`\` 자체를 serialize할 때는 조금 달라진다. 이럴 경우 \`\`\`optimized model\`\`\`이 아닌 \`\`\`model\`\`\`을 저장해야 오류가 나지 않는다.

\`\`\`python
torch.save(optimized_model, "foo.pt") # Error
torch.save(model, "foo.pt")           # Works
\`\`\`

저장할 때 주의하도록 해야겠다.

#### Model inference
Model inference를 하는 경우, \`\`\`compile\`\`\`을 통해 compiled model을 생성한 뒤 warm-up step을 밟는 것이 초반 latency를 줄여준다고 한다. 아직 pytorch 2.x가 개선되어야할 부분처럼 보이는데, \`\`\`export\`\`\`라는 메소드를 통해 latency에 대한 환경적 변수까지 안정화할 수 있는 방법을 제공할 예정이라고 한다. 이 부분도 차차 개선되는 부분이 생기면 이해를 해봐야겠다.

\`\`\`python
# API Not Final
exported_model = torch._dynamo.export(model, input)
torch.save(exported_model, "foo.pt")
\`\`\`

#### 디버깅
컴파일을 하는 상황에서는 디버깅이 힘들다. 예를 들어 프로그램이 <U>compile mode와 충돌</U>한다던지, eager mode(원래의 pytorch 연산)과 compile된 <U>연산이 정확하게 일치하는지</U>, 혹은 컴파일을 진행했음에도 <U>speedups가 원활하지 않다</U>는 등의 문제가 발생할 수 있다. 만약 컴파일된 코드가 <U>프로그램과 충돌</U>하거나 eager mode로 실행했을 때보다 <U>오차 범위 이상</U>으로 결과가 달라진다면, 코드 자체가 문제라고 보기는 어렵다. 사용자의 디버깅이나 reproduction 등을 지원해주기 위해 **minifier**를 제공한다. Minifier는 마주한 issue를 간단한 code snippet으로 자동으로 줄이게 되고, 해당 코드는 문제를 재현하고 축소된 코드로 github issue를 제시할 수 있게 된다. 즉 이슈를 제시하는 과정에서 파이토치 팀이 굳이 방대한 코드를 보지 않더라도 원인규명을 빠르게 할 수 있게끔 해준다는 것이다. 또한 만약 속도 향상과 관련된 이슈가 있다면, \`\`\`torch._dynamo.explain\`\`\` 툴을 통해 graph breaks라 불리는 속도 저하 요인을 찾아낼 수 있다.

#### Distributed
여러 개의 GPU를 병렬 연산에 활용하는 방법 중 하나가 바로 \`\`\`torch.distributed\`\`\` 방법이다. Pytorch의 대표적인 두 distributed wrapper인 DDP(\`\`\`DistributedDataParallel\`\`\`), FSDP(\`\`\`FullySharedDataParallel\`\`\`) 모두 컴파일 후 제대로 작동하는 것을 알 수 있으며 eager mode(기존 방법)에 비해 <U>성능이나 메모리 효율성</U>이 올라갔다고 한다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/225211378-95021b7f-6639-4903-9315-a3770148e2b7.png" width="500"/>
	<img src="https://user-images.githubusercontent.com/79881119/225211544-3c1be915-c671-4c0c-9c04-7de56dd473fe.png" width="500"/>
</div>



# 요약하자면...


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/225211779-bdfc9e43-2b39-4efd-859c-f5b27d6e5611.png" width="600"/>
</div>


사실 아직 제대로 이해된 부분이 많지는 않지만 요약하자면 \`\`\`compile\`\`\`을 통해 eager mode에서 작동하던 기존 pytorch model을 최적화하였지만, 기존의 편의성(모델 파라미터에 접근이 쉽고, 연산의 병렬 처리 등등 코드 구성하는 과정에서 자유도가 높음)을 그대로 유지했다는 것이다. Pytorch 2.x는 앞으로 \`\`\`compile\`\`\`을 보다 편리하고 확장 가능성 있는 방향으로 발전시키고자 할 것이며, 상대적으로 자유도가 높지 않기도 했고 복잡했던 tensorflow에 비해 <U>보다 대중적인 언어</U>로 자리잡으면서도, 고성능을 요구하는 <U>개발자들의 니즈</U>를 만족할 수 있는 발전 방향이 될 것이라고 생각된다.
`,JO=`---
title: "ChatGPT(챗지피티)가 사람같이 자연스럽게 말할 수 있는 이유는!? RLHF 학습법에 대하여"
category: "ai technology"
publishedAt: "2023-03-25"
thumbnail: "https://user-images.githubusercontent.com/79881119/213901191-b4994362-af6a-4685-bcf0-9a9924ec3b1a.png"
---


이 글은 huggingface의 게시글 중 <U>'Illustrating Reinforcement Learning from Human Feedback (RLHF)'</U>를 번역 및 각색하여 딥러닝을 잘 모르는 사람들도 이해할 수 있게 바꾼 내용이다. 원본 게시글 링크는 다음과 같다([참고 링크](https://huggingface.co/blog/rlhf)).

> Lambert, et al., "Illustrating Reinforcement Learning from Human Feedback (RLHF)", Hugging Face Blog, 2022.

# 지금은 AI의 호황기?
국내에서는 AI에 대한 일반인들의 관심이 <U>알파고 이후</U> 급증했다가 점차 사그라들었다. 그러면서 점점 4차 산업혁명이니 빅데이터 등등의 워딩을 온갖 플랫폼에 가져다놓기 시작했다. (솔직히 여기서 소신발언을 하자면 나는 딥러닝이나 머신러닝에 대해 제대로 공부해보지도 않은 사람들이 사업성 멘트로 인공지능을 넣는 행위를 싫어한다...)

아무튼 그러던 와중 ChatGPT가 이번에는 자연어 처리 쪽으로 다시 불을 지피면서 사람들의 이목을 끌기 시작했다.

<U>컴퓨터 사이언스</U>가 국내에서도 spotlight를 받기 시작하면서 원리에 대한 이해를 배제한 채로 사업 플랫폼을 인공지능이니 메타버스니 있어보이는 말에 맞추는 경향이 있었다. 물론 제대로 된 스타트업이나 사업체도 있었지만 대부분 기술에 대한 <U>명확한 솔루션</U>을 제공하지 못하는 경우가 허다했었다.

그럼에도 불구하고 <U>'인공지능'</U>이라는 키워드는 사람들을 고무시키기엔 충분했다. 하지만 그만큼 정작 딥러닝과는 전혀 관련이 없는 서비스들도(학부생 수준의 toy project로도 가능할 수준) 웹 상에 등장하기 시작했다. 물론 실제로 머신러닝을 적용한 서비스들은 있었지만 다양한 계층/문화의 사람들에게 범용적으로 제공되는 서비스라고 말할 순 없었던 것은 사실이다.

이번 글에서 소개하고자 하는 [ChatGPT](https://openai.com/blog/chatgpt)는 이러한 기존 인공지능 사업들과 비교했을때 시장성/실용성을 극한으로 뽑아낸 대표적인 사례라 볼 수 있다. 알 사람들은 알겠지만 Stability AI 기업에서 발표한 [Stable diffusion](https://stability.ai/blog/stable-diffusion-public-release)가 ChatGPT 이전에 사람들에게 하나의 컨텐츠로서 다가갈 수 있었던 대규모 모델 중 하나이지만, 검색이나 스토리텔링, 질의응답이 가능한 ChatGPT의 효용성/활용성과는 차이가 있다. 과연 챗지피티와 같은 LLM(대용량 언어 모델)이 구글과 같은 <U>검색 엔진에 의존</U>했던 <U>과거의 트렌드</U>를 완전히 뒤엎을 수 있을지는 앞으로 더 지켜봐야할 것이다.


# 들어가며...
딥러닝을 기존에 연구하던 사람들은 알겠지만 <U>자연어 처리</U>(NLP)와 관련된 language model은 최근 몇 년간 급진적인 발전을 이루었다. 단순 기계 번역에서 새로운 모델링을 제시한 [Transformer](https://arxiv.org/abs/1706.03762)(Attention is all you need)라는 논문이 딥러닝을 통한 기계 번역(Machine translation)에서 제시되면서 이를 활용한 GPT, BERT와 같은 모델들이 다양화되고 발전하기 시작했다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213901161-30db60a8-bb44-46c8-9f42-ea2e45de38fe.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/213901191-b4994362-af6a-4685-bcf0-9a9924ec3b1a.png" width="400"/>
</div>


GPT와 BERT에 대해 말하자면, GPT는 <U>문장을 '생성'</U>하는 과정에 집중한 자연어 처리 연구에 해당되고 BERT는 반대로 주어진 <U>문장의 문맥을 '이해'</U>하는 과정에 집중한 자연어 처리 연구라고 볼 수 있다. 따라서 <U>ChatGPT</U>가 기반을 둔 GPT모델은 결국 **'그럴듯한 문장을 생성하는 과정'**에 집중한 연구라고 할 수 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227694380-30f31f95-4ffc-4a30-bc46-768e0566d49d.png" width="600"/>
</div>



# 그럴듯한 문장이란?
컴퓨터 알고리즘은 수학으로 구성된다. 결국 문장을 만들어내는 과정 자체가 복잡하게 연산된 숫자들의 집합을 통해 계산하는 과정이 되는데, 그렇다면 사람이 어쩌면 합리적으로 인식할 수 있는 <U>'좋은 text'</U>를 정의하는 방식 또한 주관적이기도 하고, 문장을 생성하는 상황에 따라 달라질 수 있다는 것을 알 수 있다.

GPT는 창의성을 요구하는 <U>소설</U>을 써야할 수도, <U>수필</U>을 써야할 수도 있고 아니면 <U>논문의 absract</U>를 작성해야할 수도 있으며 심지어는 개발자에게 필요한 <U>코드 알고리즘 일부</U>를 작성해줄 수도 있다. 딥러닝에서 <U>학습의 주체</U>가 되는 심층 신경망은 수많은 파라미터(숫자의 집합)으로 구성되어있는데, 이 파라미터들이 '그럴듯한 문장을 생성하기 위해' setting되기 위해서는 <U>해당 목적성을 대표</U>할 수 있는 **목적 함수**를 설정해야한다. 이를 머신 러닝에서는 'loss function', 혹은 'objective function'을 정하는 문제가 된다.

대부분의 language model에서는 단순히 이전 문장을 참고하여 다음 단어를 예측하는 categorical function인 cross entropy와 같은 <U>trivial solution</U>을 가지는 목적 함수를 사용한다. 여기서 trivial solution이라면 '학습된 데이터를 기준으로, 이러한 문장 구조에서는 이러한 단어가 많이 나오더라'라는 식의 예측이다. 하지만 과연 이게 문맥을 이해해야하는 상황에 <U>제대로 부합할 것인지</U>는 알 수 없다. 이러한 문제들을 완화하기 위해 생성된 문장에 대한 human preferences(사람의 주관적 평가)가 어느 정도 반영된 metric(측정 지표)인 [BLEU](https://en.wikipedia.org/wiki/BLEU)나 [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric))가 최적화에 활용되기 시작했다.

$$
    \\begin{cases}
        \\text{ROUGE}, &(w_{ref} \\in S_{gen} \\vert w_{ref} \\in S_{ref}) / \\vert S_{ref} \\vert \\newline
        \\text{BLEU}, &(w_{gen} \\in S_{ref} \\vert w_{gen} \\in S_{gen}) / \\vert S_{gen} \\vert
    \\end{cases}
$$

위의 수식이 표현하는 바를 간단하게 요약하자면 'ROUGE'는 reference(기준)가 되는 문장($S_{ref}$)에 있는 단어($w_{ref}$)가 generated(생성된) 문장($S_{gen}$)에 포함된 정도를 나타낸 것이고 'BLEU'는 generated(생성된) 문장($S_{gen}$)에 있는 단어($w_{gen}$)가 reference(기준)가 되는 문장에 포함된 정도를 비율로 표현한 것이다. 따라서 해당 metric을 주관적 지표로 사용하더라도 실질적으로 <U>단어가 포함된 정도</U>를 나타낼 뿐이지, 생성된 각 단어가 적합한 문맥/위치에서 생성되었는지 판단할 수 없다는 문제가 생긴다. 최적화하는 과정에서 <U>한계점</U>이 드러난 것이다.


# RLHF의 두두등장

그렇다면 조금은 귀찮긴하지만, 만약 생성된 문장에 대해 <U>사람이 직접 feedback</U>을 해주고 이를 기반으로 <U>performance를 측정</U>(수치화)할 수 있는 방법이 있다면 언어 생성 모델의 파라미터 보다 <U>섬세하게 조작</U> 및 최적화 할 수 있지 않을까?라는 생각이 스멀스멀 피어오르게 된다. 바로 이러한 아이디어에서 나온 것이 ChatGPT를 학습한 방법인 <U>Reinforcement Learning from Human Feedback</U>(RLHF)이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227696698-d5dc083b-e0c1-4b6d-9ab2-abb3bc6bd255.png" width="400"/>
</div>


RLHF란 결국 사람의 feedback을 reinforcement learning(강화 학습)에서 사용하는 일종의 penalty/reward로 간주하여 최적화를 진행하겠다는 것이다. RLHF를 사용함으로 인해 LLM(Large Language Model)이 생성하는 문장이 <U>보다 사람이 내뱉은 문장에 가깝게</U> 학습이 될 수 있었다. 과연 챗지피티는 본인이 학습된 RLHF에 대해서 제대로 설명할 수 있을까?


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227696407-8e1db0e8-1a20-49a8-9516-e2ccc910198f.png" width="600"/>
</div>


아무래도 챗지피티는 <U>느낌표를 붙이는 것</U>이 어린 아이한테 설명하는 문맥상 포인트라고 생각했나보다. 내용은 말한 것과 같이 강화학습을 통해 학습하는데 보다 <U>빠르고 정확한 방향</U>으로 학습하고자 하는 것이 RLHF의 기본 방법이다. 지금까지는 간단하게 설명하기 위해 빌드업을 했지만, 지금부터는 구체적으로 학습하는 방법에 대해서 언급하도록 하겠다.


# RLHF : 단계별로 이해해보기
사실 본인도 reinforcement learning에 대해 제대로 이해하고 있는 상황은 아니기 때문에 글을 적고 있으면서도 ~~양심에 찔리지만~~.. hugging face에 있는 게시글 내용을 <U>보다 친절한 형식</U>으로 번역해보기 위해 작성하게 되었다. RLHF 학습법은 다음과 같이 여러 pipeline으로 구성된 training process를 가진다.

- Language Model(LM)을 사전 학습(Pre-training)하기
- Data를 획득하고 reward model을 학습시키기
- LM을 강화 학습으로 fine-tuning하기

요 각각의 단계들에 대해서 자세히 살펴보도록 하자.

### Pretraining language models
RLHF를 진행하려면 feedback을 받을 language model이 필요하다. [Language model을 학습](https://huggingface.co/blog/how-to-train)하는 것은 기존 <U>classical pretraining objective</U>로 학습한다(앞서 설명했던 Cross-entropy loss나 BLEU, ROUGE 등등 모두 가능). 예를 들어 OpenAI에서는 GPT-3 시리즈 중 가벼운 모델을 활용하여 첫 RLHF 방법으로 학습한 모델인 [InstructGPT](https://openai.com/research/instruction-following)를 만들었다. 다른 서비스인 [CLAUDE](https://www.anthropic.com/)를 런칭한 Anthropic의 경우에는 $10 \\times 10^6 \\sim 52 \\times 10^9$의 parameter를 가지는 사전 학습된 transformer를 기준으로 RLHF에 적용하였고, DeepMind에서는 본인들의 모델인 [GoPher](https://arxiv.org/abs/2112.11446)를 사전 학습된 LM으로 사용하였다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227706651-afae5325-30f4-40a4-9c69-312578fbec5b.png" width="600"/>
</div>


이렇게 상정한 초기 모델(Initial model)을 fine-tuning할 수도 있다. 예컨데 OpenAI는 <U>사람이 직접 생성</U>한 "보다 선호되는(preferable)" prompt를 fine-tuning에 사용하였고, Anthropic에서는 RLHF에 활용할 LM의 초기 상태를 그들의 기준인 "helpful, honest, and harmless"라는 <U>기준에 대한 문맥</U>을 가이드를 해주는 방식을 사용하여 학습하였다. 두 방식 모두 일종의 '잘 정제된' 데이터셋을 사용한 것과 같으며, 일반적으로 고퀄리티의 데이터셋을 구성하는 과정 자체가 <U>cost가 높기 때문에</U> RLHF 방식을 사용하는데 있어 위와 같은 fine tuning 과정이 필수적인 것은 아니다.

어떠한 모델을 강화학습(RLHP)의 <U>starting point</U>로 사용하는 것이 가장 바람직한지 명확한 답은 정해진 것이 없다. 사실상 Large language model이 여러 기업들로부터 다양하게 학습되고 있기는 하지만 RLHF 방식을 사용하는데 있어서 design space를 구성하는 모든 네트워크 구조에 대해 성능비교를 한 연구는 아직 전무하기 때문이다. 즉 RLHF에서 '언어 모델을 강화하는 방법'은 아직 <U>구조에 따른 성능 차이</U>가 규명되지 않은 채로 설명된다고 생각하면 된다.

### Reward model training
언어 모델을 평가하기 위해서는 모델이 만들어내는 데이터를 얻어야한다. 사람이 실제로 선호하는 text(document)를 만들어내는 언어 모델을 만들기 위해서는, 사람이 주는 피드백을 모델에게 reward(문장을 잘 만든 정도에 따라 보상을 주는 방식)로 반영시킬 수 있는 reward model(RM), 이른바 <U>보상 모델이 필요</U>하다.

조금 더 구체적인 목표를 설정하자면, LM이 추출하는 연속된 텍스트를 받아서, 생성한 텍스트에 대한 <U>사람의 선호도</U>를 <U>scalar</U> reward(점수표 형태로 생각하면 된다)로 리턴할 수 있는 <U>보상 시스템</U>이나 <U>모델 구조</U>를 구성하고 싶은 것이다. 예컨데 보상 모델이 특정 텍스트를 보고 나름의 랭킹을 매겨주면, 이에 따른 scalar 값을 reward로 주는 것이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227707301-41fcb50f-6542-42c7-8908-500aed82f700.png" width="600"/>
</div>


RM의 output이 scalar가 되는 것이 RL 알고리즘에 있어 가장 필수적인 부분이라고 생각하면 된다. 간단하게 표현하자면 <U>사람의 선호도를 점수화</U>하는 기준을 잘 정하는 것이 RL 알고리즘이 RLHF(사람의 선호도를 네트워크의 강화 학습에 사용하는 방법)에 자연스럽게 녹아들게하는 주요 요소가 된다고 표현할 수 있다.

Reward modeling 결국 '텍스트'를 기준으로 ranking을 수치화하는 것이기 때문에 또다른 언어 모델을 preference data로 학습(처음부터 학습하거나, 사전 학습된 LM을 fine-tuning하는 두 경우를 생각해볼 수 있음)한다고 생각해볼 수 있다. 실제로 Anthropic에서는 사전 학습된 LM을 기준으로 PMP(preference model pretraining)이라는 특별한 fine-tuning 방법을 사용하였고, 이 방법이 sample efficient(상대적으로 적은 샘플로도 원하는 기능을 구현할 수 있음)하다고 밝혔다. 하지만 앞서 RLHF에서 <U>가장 효과적인 LM 구조에 대한 연구가 부족</U>하다고 말했던 것과 같이 여전히 이 부분<U>(RM의 효과적 학습)</U> 또한 확실한 정답은 밝혀지지 않은 상태이다.

RM(Reward model)을 학습하기 위해서는 학습된 네트워크가 특정 prompt를 받아서 생성한 텍스트 데이터셋이 필요한데(prompt-generation pair), 이때는 사전 데이터셋에서 샘플링하여 추출하게 된다. Anthropic의 경우 Amazoon Mechanical Turk의 <U>chat tool</U>로부터 획득한 prompt를 기준으로 생성하게 되고, Open AI는 본인들이 풀어놓은 GPT API를 <U>유저들이 사용</U>한 <U>prompt submission</U>을 사용했다고 한다(역시 공짜는 없음 ㄷㄷ).

이렇게 생성한 LM의 text output를 human annotator(랭킹을 매기는 인력 시장이라고 보면 된다)에 추면, 생성된 text에 나름대로 기준을 적용하여 줄세우기를 시작한다. 이런 세팅에서 '<U>굳이 줄세울 필요없이</U> 바로 텍스트를 보고 scalar value로 <U>점수를 매겨버리면</U> reward model이 굳이 필요없는 거 아님?'라고 생각할 수도 있는데 사실상 이게 어려운 부분이 있다. 만약 당신이 human annotator고, prompt에 대해 생성된 문장을 봤을때 한 문장이 다른 문장보다 '얼마나' 좋은지 측정하는 것이 굉장히 어렵다는 것을 느끼게 된다. 확연히 차이가 보일 정도로 문장 퀄리티에 차이가 있다면 물론 점수를 부여하는 것이 어렵지 않을 수 있지만, 실질적으로 점수를 주는 기준 범위(For example, $0 \\sim 10$) 내에서 구분해서 점수를 주는 것이 어렵기 때문에 사람이 직접 score를 측정하는 것은 <U>calibration</U>이나 <U>noise</U> 문제가 발생한다. 따라서 직접 점수를 주는 방식이 아니라 여러 모델로부터 나온 text를 비교하는 식으로 상대적 점수표를 구성함으로써 <U>dataset에 대한 정규화를 진행</U>한다. 

Text에 대해 ranking을 주는 방식은 다양하다. 가장 효과적이었던 방법은 같은 prompt에 대해 두 개의 다른 LM의 output을 비교하는 것이다. 1대 1로 승부하여 승자와 패자를 결정하는 방식은 마치 체스 게임을 생각해볼 수 있는데, 이런 식으로 다른 LM과의 기준으로 output을 비교하는 것을 통해 각자의 상대적인 위치를 [Elo system](https://en.wikipedia.org/wiki/Elo_rating_system)으로 나타낼 수 있다. Elo system은 체스 뿐만이 아니라 대부분의 스포츠에서 사용되는 레이팅 방식이다. 간단하게 요약하자면 <U>'예측된 승률'</U>에 따라 점수 등락이 결정되는 형태가 된다. 예를 들어 <U>약자가 강자를 이겼다면</U> 그만큼의 추가 <U>보상이 주어지는 것</U>과 같다. 물론 이 방법만 존재하는 것은 아니고 여러 ranking 방식이 존재한다. 이렇게 정해진(라벨링된) ranking을 기준으로 학습에 활용될 점수표가 normalizing된다.

Reward model이 가지는 <U>내재적 한계점</U>은 다음과 같다. RLHF 시스템에서 학습하고자 하는 언어 모델의 크기를 어느 정도 Reward model도 따라가야한다는 점이다.
예컨데 OpenAI의 $175 \\times 10^9$ 파라미터 크기를 가지는 LM은 $6 \\times 10^9$의 파라미터 크기를 가지는 RM을 사용하고, Anthropic과 DeepMind는 LM과 RM의 크기가 서로 비례하면서 증가하는 경향을 보인다. 결국 특정 언어 모델의 성능을 효과적으로 피드백해주기 위한 reward model도 언어 모델에 필적하는 capacity를 가져야한다는 점이다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227710112-caa2fb85-638c-4e77-9995-b7637a36487b.png" width="600"/>
</div>


즉 학습하고자 하는 LM이 달라질 때마다 RM을 다시 학습시켜야 한다는 어려움이 존재하고, 이는 학습 process 단계에서 사전 학습된 RM을 여러 LM에 사용할 수 없다는 문제점과 동시에 모델 구조에 대한 엄밀한 비교(연구 방향)를 불가능하게 하는 요소가 된다. 사실 직관적으로 이해하려 해도, 더 큰 파라미터 수를 가지는 language model이 가지는 representation을 이해하기 위해서는 그만큼 더 큰 representation 수용력을 가진 RM을 사용하는 것이 바람직하다고 볼 수 있다.

이를 <U>최상위권 학생</U>과 <U>중하위권 학생</U>을 담당하는 <U>과외 선생님의 실력</U>에 빗대어 볼 수 있다. 상대적으로 중하위권 정도의 실력을 가지는 학생(적은 parameter 수를 가지는 네트워크)은 이를 평가하고 가르치는 과외 선생님의 실력이 그다지 기대되지는 않지만, 최상위권 실력을 가지는 학생(많은 parameter 수를 가지는 네트워크)을 평가하고 가르치는 과외 선생님의 실력은 뛰어나야할 것이다. 이렇듯 공부 실력/ 티칭 실력을 일종의 representation에 빗대어 생각해보면, 더욱 <U>큰 모델일수록</U> 보상 시스템/모델의 크기도 <U>함께 커져야 한다</U>는 점을 받아들일 수 있게 된다.

지금까지 <U>LM을 사전 학습하는 과정</U>과 <U>RM을 구성하는 방법</U>에 대해 알아보았고, 이제 특정 prompt에 텍스트를 생성하는 모델(LM)과, 생성한 텍스트에 대해 인간의 선호도를 scalar 값으로 매핑해주는 모델(RM)을 모두 얻을 수 있었다. 


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227710523-b2e06ced-140e-4f22-bb28-994df461cd98.png" width="600"/>
</div>


### Fine-tuning with Reinforcement Learning
<U>강화학습</U>(Reinforcement Learning)을 기반으로 Large language model을 학습하는 것은 사실 알고리즘 상으로는 불가능의 영역에 가까웠다. 미분 연산을 통해 직접 최적화가 가능한 backpropagation과는 다르게 강화학습은 agent가 주어진 상황에서 내리는 결정에 대해 '좋음'을 수치화하기 때문이다. 따라서 딥러닝에서 사용되는 <U>심층 신경망</U>을 학습하는 것은 <U>복잡한 함수</U>에 접근하는 작업이기 때문에 학습 pipeline을 설계하는 engineering 측면에서나, algorithm 측면에서나 쉽지 않은 일이었다.  

이에 대해 여러 기관에서는 RL의 <U>policy gradient algorithm</U>인 Proximal Policy Optimization(PPO) 방법을 사용([참고 링크](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/))하여 **initial LM의 복사본**(사전 학습된 언어 모델을 copy한 객체)의 파라미터 <U>일부/전체를 fine-tuning</U>하는 방법을 사용하게 된다. 또한 LM의 파라미터 수는 적게는 $10B$ 부터 $100B$ 이상에 이르기까지 heavy하기 때문에, 이를 모두 fine-tuning하는 것이 비효율적이다. 따라서 Low-Renk Adaptation(LoRA) 논문([참고 링크](https://arxiv.org/pdf/2106.09685.pdf))에서는 <U>고차원의 parameter</U>가 사실은 <U>low intrinsic dimension</U>의 <U>manifold</U>를 대표하게 된다는 연구에서 영감을 받아 연산 효율적인 fine-tuning 방법을 입증하였다. 


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227755020-bb0b3979-16f0-4ae8-ac7b-2f3a52e03e32.png" width="400"/>
</div>


예컨데 DeepMind의 Sparrow model([참고 링크](https://arxiv.org/pdf/2209.14375.pdf))는 선호되는 prompt answering/부정적인 답변에 대한 data bootstrapping을 기반으로 점차 <U>RL 프로세스의 policy를 개선</U>하는 pipeline을 구성하였다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227755264-f5cbfdfe-4c98-4a4b-aaff-e432c96a296e.png" width="600"/>
</div>


보다 자세하게 각 내용을 다루지는 않겠지만 궁금한 사람들은 링크를 타고 한번씩 읽어봐도 좋은 내용들인 것 같다.

PPO는 RL에서는 널리 사용되는 알고리즘이기도 하고 많은 가이드라인과 튜토리얼이 나와있을 정도로 이미 mature한 연구라고 볼 수 있다. 이렇듯 비교적 널리 알려진 연구이기 때문에 RLHF에 scalable한 approach를 통해 쉽게 적용할 수 있었다고 한다. RLHF에서 사용되는 RL은 결국 <U>large language model</U>(LM)을 어떻게 하면 <U>익숙한 알고리즘</U>을 기반으로 최적화할 수 있을지에 대한 설명으로 이어진다. 


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227756620-e29ff2c1-f9a3-4b10-8565-6b64a238ebef.png" width="550"/>
</div>


위의 그림을 참고하여 <U>Fine-tuning task</U>를 RL 문제로 나타내면 다음과 같다. 먼저, **policy**는 language model로, 받아들인 prompt에 대해 sequence of text(문장)을 만들어낸다. LM을 fine tuning하는 과정에서 parameter의 집합으로 표현된 네트워크가 결국 policy의 주체가 되고, 해당 네트워크의 output인 <U>sequence of text</U> 혹은 <U>text들의 probability distribution</U>을 **policy**로 생각해볼 수 있다. 그리고 이 policy의 **action space**는 LM의 vocabulary의 모든 token에 해당된다고 볼 수 있다.
**Observation space**는 결국 agent가 내린 결정에 대한 environment를 의미하는데, 사실상 RL 학습에 대한 LM이 마주하고 있는 환경이 정의될 수는 없기 때문에 단순히 input으로 들어올 수 있는 <U>다양한 token sequence의 집합</U>이라고 생각할 수 있다. 보다 정확하게 말하자면 input token sequence에 대한 분포가 observation space에 해당된다. 일반적인 RL에 비해 observation space가 가지는 크기가 상당히 큰데, 예컨데 모든 단어 조합을 기준으로 생각한다면 대략
$$
    \\text{단어 수}^\\text{입력 시퀀스의 토큰 길이}
$$
라고 볼 수 있다. 보다 다양한 observation이 가능하다는 것은 생성할 수 있는 text의 다양성이 증가한다는 의미가 된다. **Reward function**은 preference model(output에 대한 평가 지표)과 policy shift에 대한 constraint(업데이트 방식)로 구성된다.

Reward function은 결국 지금까지 언급한 모든 model이나 학습 pipeline을 RLHF process로 통합해주는 과정이 된다. 만약 dataset으로부터 prompt $x$가 주어졌을 때 초기 상태의 language model에 의해 $y_1$ text가, 현재 iteration을 기준으로 update된 policy에 의한 $y_2$ text가 각각 생성되었다고 생각해보자. 현재 policy에 대해 생성된 text인 $y_2$가 preference model을 통과하게 되면, 앞서 설명했던 것과 같이 "선호도"를 반영한 scalar value($r_{\\theta}$)를 추출하게 된다. 해당 text는 initial model에 의한 text인 $y_1$과 비교되고, <U>둘의 차이</U>를 통해 <U>penalty를 연산</U>하게 된다. OpenAI, Anthropic 그리고 DeepMind의 대부분의 paper에서는 <U>분포 간 포함 정도</U>를 통해 <U>distribution 거리</U>를 나타내는 대표적 메트릭 중 하나인 Kullback-Leibler(KL) divergence를 scale한 값을 penalty 연산에 활용하였다. KL divergence($r_{\\text{KL}}$) 연산은 연속된 token에 걸쳐 계산된 분포 사이에 계산된다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227758349-88896b92-f828-4dec-a931-c4d2ac94d7f0.png" width="700"/>
</div>


<U>KL divergence 메트릭</U>은 이 글에서 언급하고자 하는 RLHF 뿐만 아니라 분포를 다루는 task나 네트워크 구조에서 자주 사용된다. 그러한 방식들과 차이가 있다면, 대부분 분포를 최적화하고자 하는 task에서는 이상적인 분포에 encoder나 decoder의 output 혹은 feature map을 align하기 위해 <U>KL divergence를 최소화</U>하는 방향으로 학습시키지만, RLHF의 경우에는 초기 모델과 policy optimized 모델이 생성하는 텍스트가 어느 정도 적절한 간격을 두고 차이를 보여야하므로 <U>정규화 작업에 사용된다</U>는 점이다. 만약 해당 정규화 term이 없다면 policy는 단순히 preference model을 속이기 위해 <U>기상천외한 text를 생성하더라도</U> 높은 reward를 받을 수 있게 된다(위의 그림 참고). 따라서 RL update rule에 적용될 reward는 다음과 같은 규칙을 따르게 된다.

$$
    r = r_{\\theta} - \\gamma r_{\\text{KL}}    
$$

Reward를 최대화하긴 하는데, initial LM이 추출하는 sequence of text의 분포와 너무 멀어진다면 reward를 깎아버리는 것이다.

몇몇 RLHF framework에서는 해당 reward function에 추가로 term을 추가하기도 하였다. 예컨데 OpenAI에서 RLHF를 적용한 [InstructGPT](https://arxiv.org/pdf/2203.02155.pdf)에서는 다음과 같이 pretraining gradients를 섞어주는 방식을 사용하였다.

$$
    L(\\phi) = E_{(x, y)\\sim D_{\\pi_\\phi^{\\text{RL}}}} (r_\\theta (x, y) - \\beta \\log (\\pi_\\phi^\\text{RL}(y \\vert x) / \\pi^\\text{SFT} (y \\vert x))) + \\gamma E_{x \\sim D_\\text{pretrain}} (\\log (\\pi_\\phi^\\text{RL} (x)))
$$

물론 여기서 언급하는 pretraining이란 human annotated set에 대한 gradient를 의미한다. 이처럼 RLHF의 <U>reward function</U>은 아직 <U>많은 연구가 필요</U>한 부분이라고 할 수 있다.

최종적으로는 **update rule**에 따라 현재 data batch의 reward metric을 최대화하는 방향으로 parameter update가 진행된다. PPO는 on-policy라고 부르는데, 이는 parameter가 오직 <U>현재의 batch만을 기준</U>으로 업데이트되는 프로세스를 일컫는다. PPO는 trust region optimization이라는 알고리즘을 따르는데, 간단하게 설명하자면 gradient가 업데이트될 때 중구난방으로 된다면 learning process에 노이즈가 발생하기 때문에 의도적으로 constraints를 주어 안정적인 학습을 할 수 있게 도와준다고 보면 된다.DeepMind도 Gopher에 대해 유사한 reward setup을 사용했지만 gradient를 최적화할 때 synchronous advantage actor-critic(A2C) 방법([참고 링크](http://proceedings.mlr.press/v48/mniha16.html?ref=https://githubhelp.com))을 사용했다는 점에서 차이가 있다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227759528-b3d978c7-7ed3-44db-a51e-796b75b2d0cf.png" width="700"/>
</div>


추가로, RLHF에서 reward model과 policy를 <U>같이 업데이트하는 방식</U>도 고려해볼 수 있다. RL(Reinforcement learning) policy가 업데이트되면 user는 해당 output에 대해서 초기 모델과 비교하는 방식을 통해 ranking(줄세우기)가 가능해진다. 유저가 지속적으로 관여해야한다는 점 때문에 너무 수고스럽기도 해서 아직 이런 operation에 대해서 다루는 paper는 많지 않다.

Anthropic은 이런 optional한 학습법에 대해 Iterated Online RLHF라고 명명하였고([참고 링크](https://arxiv.org/abs/2204.05862)), 이 경우에는 각 iteration에서의 policy가 ELO ranking system의 경쟁 대상이 된다.


# Open-source tools for RLHF(주로 이용되는 오픈 소스)
위에서 본 내용이 사실상 RLHF에 대한 모든 설명이었고, 이 부분에서는 RLHF 학습을 위한 tool을 소개하려고 한다. RLHF를 LM에 적용한 첫번째 코드는 2019년에 tensorflow 언어를 기반으로 OpenAI에서 릴리즈한 버전이 있다([lm-human-preferences 깃허브 링크](https://github.com/openai/lm-human-preferences)).

눈물을 흘릴 Pytorch 사용자들도 쓸 수 있는 repository도 왕왕 등장했다. Transformers Reinforcement Learning([TRL](https://github.com/lvwerra/trl), [TRLX](https://github.com/CarperAI/trlx)) 그리고 Reinforcement Learning for Language models([RL4LMs](https://github.com/allenai/RL4LMs))가 대표적인 깃헙 링크이다.

TRL이 만들어진 목적은 Huggingface 생태계에 잘 서식하고 있는 사전 학습된 LM을 PPO 방식을 통해 강화학습으로 fine-tuning하기 위함이다. TRL을 fork하여 만들어진 TRLX는 LLM에 PPO/ILQL([Implicit Q-Learning](https://sea-snell.github.io/ILQL_site/)) 기반의 RLHF을 적용하기 위해 만들어진 API이다. 아직 TRLX는 parameter 수가 많은 LLM에 대해서는 적용할 수는 없지만, 앞으로 scaling up된 사전 학습 모델을 적용할 수 있게끔 업데이트될 예정이라고 한다.

RL4LMs는 보다 다양한 RL 알고리즘(PPO, NLPO, A2C 그리고 TRPO)과 reward function 및 metric을 LLM의 fine-tuning에 적용할 수 있는 서비스를 제공한다. 또한 해당 서비스의 장점은 customizing이 간편하여, trasformer 기반 구조를 가지는 어떠한 encoder/decoder 에서도 유저가 reward function을 정의하기만 한다면 사용할 수 있다는 것이다. TRLX, RL4LM 모두 활발하게 experiment에 활용되고 있기 때문에, 보다 다양한 기능이 추가된 서비스로 개편될 것으로 전망되고 있다.


# 마무리하며...
위에서 소개된 RLHF는 상당히 좋은 성능을 보여주면서 <U>인공지능 연구</U>에 많은 영향력을 행사하기 시작했지만, 그럼에도 불구하고 명확히 존재하는 한계점이 있다. 먼저 model의 성능이 좋아지긴 했지만 여전히 <U>harmful, factually inaccurate text</U>를 생성한다는 점이다. 예컨데 다음과 같은 멍멍사운드를 매우 빠르게 생성해낸다.


<div align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227761255-0373f966-3961-4200-bab8-7bf2473ad398.png" width="700"/>
</div>


아무래도 이러한 결점을 해결하는 것이 앞으로 LM을 발전시킬 방향이 될 것이고, RLHF가 완벽하지 않다는 사실, 앞으로 <U>ChatGPT</U>와 같은 서비스가 발전해야할 방향이 된다.

RLHF에 의한 시스템을 사용할 때 <U>사람의 preference data</U>를 얻는 과정은 training loop와 별개로 진행되기 때문에 상당히 수고스러우며, 또한 RLHF의 퍼포먼스가 가지는 upper-bound가 사람의 annotation quality에 한정될 수 밖에 없다는 점도 짚을 수 있다(사람이 생성한 text, 사람이 측정한 preference).

특정 prompt에 대해 잘 쓰여진 human-text를 생성하는 것은 쉬운 일이 아니고 돈이 드는 작업이다. 그나마 다행인 것은 reward model을 학습할 때 사용되는 data scale은 지나치게 비싸진 않다는 점이다. Image dataset의 annotation에 비해 그닥 전문성이 요구되지는 않기 때문일 수도 있다. 그럼에도 불구하고 일반적인 연구실에서 감당하기는 <U>무시할 수 없는 액수</U>를 자랑하며, 큰 기업이 아니고서는 RLHF를 연구하기 힘든 실정이다. 현재 공개된 large scale 데이터셋은 Anthropic 이외에는 없는 것으로 알고 있다. 돈을 들여서 데이터셋을 획득하더라도 생기는 또다른 문제는 <U>human annotator 끼리의 주관적 의견이 충돌</U>하는 것이며, 정해진 답이 없는 preference의 경우에는 ranking variance가 존재할 수 밖에 없는 noisy labeling이 진행된다.

결국 RLHF는 다양한 네트워크 구조 및 loss function에 대한 ablation이 진행되기 힘들고, 다양한 연구실 환경에서 searching될 수 없는 분야라는 한계가 명확하게 존재한다. 가장 주요하게 발전해야할 점은 오래된 RL optimizer 알고리즘인 PPO를 사용하는 것인데, 결국 PPO보다 다른 알고리즘이 더 효율적이라는 것을 보이기 위해서는 현재의 RLHF 연구 흐름을 가속화할 수 있는 대안이 필요하다는 점에서 <U>여러 문제가 복합적으로 얽힌 상태</U>라고 볼 수 있다. 또한 RL framework에서 environment가 reward model의 연산 결과에 해당되므로, 이에 대한 연산 cost도 피할 수 없다는 점도 또다른 한계로 볼 수 있다. 이러한 online RL의 cost를 줄이기 위해서 최근에는 <U>ILQL과 같은 offline RL 방식</U>이 최적화 도구로 연구되는 중이다. 이외에도 RL의 exploration-exploitation balance(다양성과 효율 사이의 trade-off) 등등 메커니즘이 적용되지 않았고, 이러한 강화 학습의 알고리즘들을 RLHF에 잘 적용하는 것이 LLM의 성능 향상에 어떤 영향을 미칠 수 있을지 지켜봐야할 것이다.
`,ez=`---
title: "딥시크(Deepseek)-R1에 대한 고찰. Thinking about Deepseek-R1 with Reinforcement Learning(RL)."
category: "personal insights"
publishedAt: "2025-02-02"
thumbnail: "https://github.com/user-attachments/assets/6a26b68f-b36b-4c1b-9ce6-46be674a1e9b"
---

# A Little Bit Personal

### 들어가기전 극히 개인적인 주저리

나는 아직도 공부를 한다. 대부분의 개발자는 커리어를 쌓으며 끊임없는 공부가 필요한데, 왜냐하면 지금까지 익혀온 기술 스택이 더이상 트렌디하지 않아지는 경우가 많기 때문이다.
특히나 AI의 경우에는 빠른 변화를 보이는데, 그래서 그런지 실제로 논문을 쓰다가 미친 일반화 성능을 보이는 파운데이션 모델이 나와버리면 해당 task가 아예 날아가버리는 경우도 발생하기 시작했다. 이렇듯 AI 연구자의 숙명은 어쩔 수 없이 기술의 최전선에서 그 누구보다 빠르게 현황을 파악해야만 살아남는다는 사실이다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/dd47382a-4d38-4641-91a5-e4ad2bfb7619" width="372" />
    <img src="https://github.com/user-attachments/assets/6a26b68f-b36b-4c1b-9ce6-46be674a1e9b" width="278" />
</div>


본인도 학부 그리고 대학원 시절을 거치며 개인적으로 새로운 인공지능 논문들이 나올 때마다 거의 즉각 읽는 습관을 들였다. 처음에는 일주일에 한 편 읽는 것도 어려웠는데 이제는 완전 디테일하게 읽지는 못해도 하루에 논문 두세편 정도는 읽어낼 수 있게 되었다. 인공지능을 연구하는 사람이기도 하고 나에게 있어 논문은 일종의 뉴스, 신문같은 존재이며, 급변하는 기술 환경에 보다 빠르게 적응할 수 있도록 도움을 주는 방법이다. 딥시크라는 모델도 테크니컬 레포트로 처음 접하여 알고는 있었지만 R1 모델의 등장이 이렇게나 파격적인 행보를 보일 줄은 몰랐다.

나름 챗지피티 이후로도 인공지능 모델은 끊임없이 학계나 산업계에서 연구되었으나, 그 모든 연구들이 딱히 크게 이슈화되는 경우는 거의 없었다. 여기서의 이슈화는 사람마다 기준이 다르겠지만 적어도 본인은 지금의 scaling law를 깨부술 무언가를 원했던 것일수도 있다. 물론 다양한 연구들이 진행되었기 때문에 현존하는 수많은 closed/opened source AI의 성능 향상에 큰 영향을 끼친 것은 사실이다.  그러나 실제 서비스에 활용되는 모델을 기준으로 대부분의 연구는 덩치 키우기에 집중했다. 거대한 자본이 수많은 리소스와 컴퓨팅 파워, 리소스 그리고 Human Resource(AI Engineer)을 끌어모았고,  매우 빠른 연구 및 개발이 시작되었다. Explainability와 scalability의 줄다리기가 어느새 scalability의 승리로 마무리되는 양상이 되어버렸다. 그로 인해 인공지능 연구 방향이 바뀌기도 했으며, 기존에는 생각하지도 못했던 task가 새롭게 제안되는 일도 생겼다.

사실 테크니컬 레포트를 읽어본 후기로는 딥시크는 <u>기술적으로 그렇게까지 독보적이거나 유니크한 모델이라고 볼 수 있을까?</u>였다. 그럼에도 대단하다고 여긴 것은 현재의 기술 시장을 흔들 정도의 파급력을 가져왔다는 사실이고, 기존에 소스코드를 공개하지 않았던 OpenAI의 기술력을 따라잡기 위해 정말 많은 노력을 했다는 사실이다. 적어도 적당히 타협해서 오픈 소스 튜닝하던 대부분의 방법들보단 훨씬 유의미한 결과를 낸 것은 사실이다.

아무튼 이제 글을 시작해보고자 한다. 이번 글은 테크니컬 라이팅이라기 보다는 개인적인 고찰, 혹은 일기장 정도 될 것 같다. 

# Preliminary

### 강화학습에 대한 고찰

우리는 로봇에게 어떤 일을 수행하게 시키고 싶다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/6550fd4d-bd7d-4d99-93f8-6733bb71f204" width="500" />
</div>


지금은 딥시크, 즉 LLM 모델에 대해 얘기하고 있으니 LLM 모델을 기준으로 말하면 <u>‘대답 잘 하게 만들기’</u> 쯤 되겠다. 옛말에 미운놈 떡 하나 준다는 말이 있는데 인공지능의 세상은 그리 호락호락하지 않다.

앞으로 우리는 <u>예쁜놈한테 떡 하나를 더 줄거다</u>. 이러한 개념을 “Reward(보상)”이라고 한다. 적절히 잘 학습된 모델을 가지고 이런저런 질의응답 Environment(환경)에서 대답을 잘하면 칭찬해주는 프로세스를 반복해서 <u>더욱 대답을 잘하는 모델을 만들고자 하는 것</u>이다.

### Policy based method의 발전

기존의 강화학습 방식이었던 Value based method는 인공지능 모델이 취할 모든 행동에 대해 가치평가하는 모델을 학습시킨다. 그래서 인공지능 모델이 현재 상태에서 어떤 Action(동작)을 취할지에 따른 Value(가치)를 판단하고, 가치에 따라 greedy 알고리즘으로 다음 동작을 결정하는 방식을 학습하게 된다. 이때 각 상황에서 어떤 동작을 취할지에 대한 기준이 바로 Policy(정책)이다. 이때 <u>가치를 판단해주는 모델을 딥러닝으로 학습</u>시키자는 관점이 바로 <u>DQN(Deep Q-Network)</u>이다.

이때 Value function에 해당되는 Q-Network의 개념이 나온 이유는 여러 동작에 대한 Reward가 가지는 불안정성을 어느 정도 <u>action-value function Q</u>가 해소해줄 수 있다는 관점이었다 (아래는 알고리즘 코드).


<div align="center">
    <img src="https://github.com/user-attachments/assets/e1aabcad-72fd-4197-a0dd-9a67c71b7441" width="700" />
</div>


그러나 사실 생각해보면 결국 가치 판단을 위한 모델을 학습하는 것의 최종 목적은 <u>‘각 동작의 가치를 잘 판단하자’</u>가 아니라 <u>‘가장 높은 가치를 지니는 동작을 취하게 하자’</u> 라는 것이다. 그렇다면, 그냥 모델이 취할 모든 행동에 대한 가치 판단 없이 바로 모델의 현재 상태에서 바람직한 다음 동작을 예측하는 Policy, 그 자체를 학습하는 것이 낫지 않겠는가에 대한 근본적 의문이 생긴다. 심지어 동작이 속한 공간을 Discrete하게 만들 필요도 없다. 정책 모델이 내뱉은 확률 분포에 근거해서 다음 동작을 예측하고, 만약 이러한 흐름이 좋은 결과를 가져왔을때 보상을 주어 Policy 모델을 업데이트한다. 이러한 학습법에 대한 이론이 바로 <u>‘Policy Gradient’</u>이다.

강화학습에서는 다양한 시뮬레이션  환경을 가정한다. 데이터셋 전체를 objective function에 empirical하게 근사시키는 deep learning의 개념과 유사하게, 강화학습에서는 각 시뮬레이션 단계를 ‘Episode’라고 부르고 하나의 에피소드 내에서 시간의 흐름에 따라 강화학습을 구성하는 각 모델의 입출력이 나오게 된다. 결국 강화학습의 가장 큰 목적은 다양한 에피소드로부터 모델의 다양한 의사결정방식을 받고, 해당 모델이 <u>가장 이상적인 의사결정방식을 취했을 때</u> 모든 에피소드로부터의 리워드는 가장 큰 기댓값을 가지게 된다. 흔히 정책은 $\\theta$라는 매개변수를 가지는 함수 $\\pi_\\theta(\\cdot)$로 주로 표현하게 된다. 이 정책 함수는 각 state $s$에서 수행할 수 있는 모든 액션 $a$에 대한 확률 분포를 추출할 수 있다.

따라서 우리는 특정 매개변수를 가지는 정책(Policy)모델이 특정 액션을 수행했을때 관측 가능한 입/출력 결과에 대해 최대의 reward를 얻을 수 있는 방향으로 모델을 학습하고자 하며, 이 방향이 곧 <u>policy gradient</u>에 해당된다.

사실 policy의 gradient를 계산하는 과정은 순탄치 않다. 그 이유는 reward function에 대한 gradient는 정책 함수($\\pi_\\theta$)가 어떤 액션을 수행하였는가 뿐만 아니라, 실제로 마르코프 프로세스에서 정책 함수에 의하여 불가피하게 조정된 현재 state $s$에 대한 확률 분포 또한 고려되어야하기 때문이다. 즉, 정책 함수에 추가로, 정책 함수로 하여금 지속적으로 변해온 state $s$의 확률 분포에도 $\\theta$가 기여한 바가 있기 때문에 gradient를 직접 구하기 어렵다는 문제가 발생한다. 그러나 이를 단순히 무시할 수 있다는 이론이 바로 <u>policy gradient theorem</u>이고 한 문장으로 다음과 같이 정리할 수 있다.

$$
\\text{보상 함수의 gradient는 정책 함수의 gradient에 비례한다.}
$$
엄밀한 증명은 아래에 간단하게 요약해보겠다.

$$
\\nabla_{\\theta} J(\\theta) = \\nabla_{\\theta} \\sum_{s \\in \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in \\mathcal{A}} Q^{\\pi}(s,a) \\pi_{\\theta}(a|s) \\
\\propto \\sum_{s \\in \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in \\mathcal{A}} Q^{\\pi}(s,a) \\nabla_{\\theta} \\pi_{\\theta}(a|s)
$$

보상함수는 각 state에 머물 확률과, 그 확률에서의 모든 액션 수행에 대한 Value의 총합(혹은 기댓값)을 의미한다. 위의 식에서 <u>뒷쪽에 있는 state value function</u>에 대한 gradient를 구하면 다음과 같다.

$$
\\begin{aligned}    \\nabla_{\\theta} V^{\\pi}(s) &= \\nabla_{\\theta} \\left( \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta}(a|s) Q^{\\pi}(s, a) \\right) \\newline    &= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s, a) + \\pi_{\\theta}(a|s) \\nabla_{\\theta} Q^{\\pi}(s, a) \\right) \\newline    &= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s, a) + \\pi_{\\theta}(a|s) \\nabla_{\\theta} \\sum_{s', r} P(s', r | s, a) (r + V^{\\pi}(s')) \\right) \\newline    &= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s, a) + \\pi_{\\theta}(a|s) \\sum_{s', r} P(s', r | s, a) \\nabla_{\\theta} V^{\\pi}(s') \\right) \\newline    &= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s, a) + \\pi_{\\theta}(a|s) \\sum_{s'} P(s' | s, a) \\nabla_{\\theta} V^{\\pi}(s') \\right)\\end{aligned}
$$

우선, 전체 식에 대한 gradient는 product rule에 따라 분배된다. 이때 뒤쪽에 있는 $Q^\\pi$는 <u>현재 가치와 미래 가치의 합</u>으로 표현 가능하다.  이때 뒤에 발생하는 $P$는 Markov decision process(MDP)에 따른 확률 분포로 생각하면 되고, 이전 state가 $s$일때 모든 다음 state에 대한 확률을 정의할 수 있다. 이는 어떠한 정책 함수에 따른 결과가 아니기 때문에 $\\theta$라는 변수와 독립이다. 따라서 뒤쪽 term에 있는 gradient는 시그마의 안쪽으로 들어갈 수 있게 되며, 최종적으로는 $P(s^\\prime, r \\vert s, a)$를 $r$에 대해 marginalize하면서 수식이 완성되는 구조다.이 수식에서 주목할 점은, <u>다음 state value function의 gradient가 현재 state value function에 recursive하게 들어간다는 사실</u>이다. 그렇다면 뒤쪽에 들어있는 $\\nabla_\\theta V^\\pi (s^\\prime)$ 또한 다음 state인 $\\nabla_\\theta V^\\pi (s^{\\prime\\prime})$의 recursive한 수식으로 표현된다.  이렇게 계속 unrolling(recursive하게 $s^\\infty$까지 전개)한다고 생각해보자. 

아래의 식에서 $\\rho$는 policy function $\\pi$에 의해 $k$번의 step 이후 특정 state로 바뀔 확률을 간소화하여 표현한 식이다.

$$
\\begin{aligned}
    \\nabla_{\\theta} V^{\\pi}(s) &= \\phi(s) + \\sum_{a} \\pi_{\\theta}(a|s) \\sum_{s'} P(s'|s,a) \\nabla_{\\theta} V^{\\pi}(s') \\newline
    &= \\phi(s) + \\sum_{s'} \\sum_{a} \\pi_{\\theta}(a|s) P(s'|s,a) \\nabla_{\\theta} V^{\\pi}(s') \\newline
    &= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\nabla_{\\theta} V^{\\pi}(s') \\newline
    &= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s') Q^{\\pi}(s', a) + \\pi_{\\theta}(a|s') \\sum_{s''} P(s''|s', a) \\nabla_{\\theta} V^{\\pi}(s'') \\right) \\newline
    &= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\left( \\phi(s') + \\sum_{s''} \\rho^{\\pi}(s' \\to s'', 1) \\nabla_{\\theta} V^{\\pi}(s'') \\right) \\newline
    &= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\phi(s') + \\sum_{s''} \\rho^{\\pi}(s \\to s'', 2) \\nabla_{\\theta} V^{\\pi}(s'') \\quad \\newline
    &= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\phi(s') + \\sum_{s''} \\rho^{\\pi}(s \\to s'', 2) \\phi(s'') + \\sum_{s'''} \\rho^{\\pi}(s \\to s''', 3) \\nabla_{\\theta} V^{\\pi}(s''') \\newline
    &= \\dots \\newline
    &= \\sum_{x \\in \\mathcal{S}} \\sum_{k=0}^{\\infty} \\rho^{\\pi}(s \\to x, k) \\phi(x)
\\end{aligned}
$$

우리는 현재 state에서 미래 state $x \\in \\mathcal{S}$로 가는 모든 MDP를 전개했다. 이제 전개된 식에 state의 초기 상태를 대입하고 여러 전개 과정을 거치면 다음과 같다.

$$
\\begin{aligned}    \\nabla_{\\theta} J(\\theta) &= \\nabla_{\\theta} V^{\\pi}(s_0) \\newline    &= \\sum_s \\sum_{k=0}^{\\infty} \\rho^{\\pi}(s_0 \\to s, k) \\phi(s) \\newline    &= \\sum_s \\eta(s) \\phi(s) \\newline    &= \\left( \\sum_s \\eta(s) \\right) \\sum_s \\frac{\\eta(s)}{\\sum_s \\eta(s)} \\phi(s) \\newline    &\\propto \\sum_s \\frac{\\eta(s)}{\\sum_s \\eta(s)} \\phi(s) \\newline    &= \\sum_s d^{\\pi}(s) \\sum_a \\nabla_{\\theta} \\pi_{\\theta} (a | s) Q^{\\pi}(s, a)\\end{aligned}
$$

이를 통해 <u>정책 함수의 gradient의 방향</u>이 곧 <u>보상 함수의 gradient의 방향</u>과 일치한다는 점을 알아낸 것이다.

### TRPO to PPO

앞서 본 내용은 policy gradient에 대한 기본적인 내용이었다. 그러나 이 이후 사실 LLM에 적용되기까지 강화학습 분야에서 policy gradient에 대한 다양한 접근법이 있었으며, 지금부터 살펴볼 내용이 현존하는 딥시크-R1의 아이디어 근간이라고 할 수 있다. TRPO와 PPO 중 [PPO](https://arxiv.org/pdf/1707.06347)가 조금 더 중요한데(**OpenAI에서 쓴 논문**), TRPO를 꼭 짚고 넘어가야하는 이유는 <u>TRPO</u>(**피터 아벨 연구소 논문**)가 가장 기본 틀이 되기 때문이라고 생각했기 때문이다. TRPO에는 수학적으로 어려운 내용이 포함된다. 가장 간단하게 설명하자면 TRPO는 정책 모델을 학습할 때 “신뢰 가능한 구간 내에서 업데이트한다”라는 개념이다. 그렇다면 대체 왜 신뢰 가능한 구간이 중요한 것일까?

Policy gradient은 다음과 같이 진행된다. 초기 상태는 확률 분포 속에서 샘플링되었다고 가정하자. 예를 들어 우리가 걷기 시작하거나 뛰기 시작할때 항상 같은 자세에서 시작하지는 않는 것과 같다.  우리는 시시각각 지금 행동에 기반하여 다음 행동을 결정하고 이를 수행한다. 이러한 과정을 우리는 자연스럽게 처리하지만 로봇에서 시키는 경우를 생각해봐야 한다.

로봇이 지금 현재 어떤 행동을 수행했을때, 그 행동이 가져오는 가치 그리고 리워드를 수치화하고 이를 누적해서 더해간다. 그리고 그 행동에 기인하는 Policy가 지속적으로 좋은 방향으로 학습된다면, 이론상 업데이트되는 정책에 따른 누적된 리워드는 계속 우상향할 것이다.

Optimal solution에 수렴할 때까지 지속 학습을 진행한다고 가정해보자. 모든 state에서 평가된 가치가 0보다 크거나 같다면 결국 모든 미래가치가 0이 되어 더이상 발전할 수 없는 상황이 올 때의 Policy가  최적의 해가 될 것이다. 하지만 업데이트된 policy($\\tilde{\\pi}$)는 모든 다음 state, action($s, a$)에 따른 advantage($Q^\\pi$)를 0보다 같거나 크게 만든다는 보장이 없다. 이를 다소 단순화하기 위한 방법으로 학습하고자 하는 Objective는 다음과 같이 표현된다.

$$
L(\\tilde{\\pi}) = \\eta (\\pi) + \\sum_s d^\\pi (s) \\sum_a \\tilde{\\pi}(a \\vert s) Q^\\pi(s, a).
$$

이제는 state에 대한 density 변화($d^{\\tilde{\\pi}}$)는 무시할 수 있다. 좌측 함수와 우측 함수의 초기값(파라미터 : $\\theta_0$)이 동일하기 때문에 Locally 아주 작은 변화량 $\\Delta \\pi$에 대해서 gradient를 같은 방향으로 유지할 수 있지만, step size를 키울 수 없다는 문제가 생긴다. 만약 gradient가 이상적으로 생기지 않았다면, step size를 잘못 지정해주면 학습이 불안정해질 수 있다. 실제로 많은 강화학습에서 가장 큰 문제가 이러한 학습 불안정성에서 기인하며, 이를 해결하기 위해 수많은 방법론이 제안되었다.

이를 해결하는 방법은 새로운 policy를 기존의 policy와 $\\alpha$ mixing하게 되면 lower bound를 가진다. 근데 이 $\\alpha$값도 매 task마다 새롭게 정의하기 힘드니, $\\pi$와 $\\tilde{\\pi}$ 간의 거리 메트릭으로 정하자는 것 ($\\alpha =D_{\\text{metric}}(\\pi \\vert \\tilde{\\pi})$ )이 솔루션이 된다. 해당 논문에서는 KL divergence를 확률 분포상의 거리 메트릭 기준으로 진행하였다. 이때의 가장 큰 문제점은 $\\alpha$에 대한 constraints를 가지는 $L$의 최대화 수식(Objective)에서, 최적의 $\\theta$값을 찾기 위해 Approximation을 진행하고, 이를 위해 <u>Hessian(2차 미분)을 수행해야한다는 점</u>이다. 이를 우회해서 풀 수 있는 방법으로 <u>Fisher Information Matrix</u>가 있는데, Gradient의 공분산을 Empirical하게 평균내면 Hessian에 근사할 수 있는 방법이다. TRPO에서는 $k=10$ 정도에서 타협을 했고, 이를 통해 Policy gradient의 안정적인 강화학습을 제안했던 논문이다.

PPO에서는 TRPO의 이런 타협점을 개선할 수 있는 방향을 제시한다. TRPO에서 constraint인 KL divergence에 적용되던 2차 미분(이를 surrogate objective로 표현한다)대신 1차 미분에 근사할 수 있는 방법을 찾고자 했다. PPO에서 적용한 방법은 기존의 surrogate objective에서 old/new policy가 크게 벗어나는 지점을 비율로 조절하게 된다($1-\\epsilon$, $1+\\epsilon$). 이러한 개념은 TRPO에서의 constraints의 $2^{nd}$ order differentiation에서 $\\pi \\simeq \\tilde{\\pi}$ 인 지점을 찾고자 하는 주목적을 Hessian에서 단순 clipping으로 간소화시켰다고 이해할 수 있다.

사실 PPO/TRPO에 쓰이는 Q-function은 Advantage function $A$로 사용하는데, 이에 대한 보다 자세한 내용은 다음 섹터인 GRPO에서 언급하며 시작하도록 하겠다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/7d8f485e-6b43-496b-a4a1-06e97adec7fe" width="700">
</div>


### GRPO

이전에 올렸던 글 중 챗지피티는 어떤 식으로 학습되었을까에 대해 간단하게 소개했던 내용 중, RLHF와 PPO에 대해 간단하게 넘어갔던 부분이 있다. 딥시크는 PPO가 아닌 GRPO라는 자체적으로 연구한 방법을 통해 강화학습을 수행하였고, 강화학습을 통해 학습된 가장 고성능의 모델을 여러 작은 모델에 distillation하는 과정을 수행하였다. GRPO는 [“DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models”](https://arxiv.org/pdf/2402.03300)라는 논문에서 제안된 방법인 듯하다. 강화학습 논문이라기보다 <u>Deepseek에서 Language model의 수학적 추론 능력을 강화하기 위한 방법론</u>으로 제안하였다.

$$
\\mathcal{J}_{PPO}(\\theta) = \\mathbb{E} \\left( q \\sim P(Q), o \\sim \\pi_{\\theta_{\\text{old}}} (O | q) \\right) \\frac{1}{|o|} \\sum_{t=1}^{|o|} \\min \\left( \\frac{\\pi_{\\theta} (o_t | q, o_{<t})}{\\pi_{\\theta_{\\text{old}}} (o_t | q, o_{<t})} A_t, \\text{clip} \\left( \\frac{\\pi_{\\theta} (o_t | q, o_{<t})}{\\pi_{\\theta_{\\text{old}}} (o_t | q, o_{<t})}, 1 - \\epsilon, 1 + \\epsilon \\right) A_t \\right).
$$

LLM에 위와 같은 PPO가 적용되며 다음과 같은 문제점이 생긴다. 우선 갑자기 등장한 수식에 맛있게 먹었던 점심이 다시 올라올 것 같기 때문에 천천히 보고 넘어가면 다음과 같다. 앞서 주구장창 언급했기 때문에 policy model에 대한 notation은 어느 정도 유추가 된다. 업데이트되기 전 파라미터가 $\\theta_\\text{old}$이고 업데이트된 파라미터는 $\\theta$이다. $q, o$는 question dataset으로부터 샘플링된 질문, old policy로부터의 output을 의미한다(성능 비교의 베이스라인이 되는 LLM이 내뱉는 답변). 그리고 PPO에서의 clipping을 통해 학습 안정화하는 과정 또한 동일하다. 앞서 단순하게 넘어갔던 Advantage가 여기서 등장하는데, <u>기존의 Advantage function estimator들이 가지고 있는 문제점</u>(특히 state의 변화 갯수 $k$에 따른 bias/variance trade-off, $k$가  크면 variance가 커지고, $k$를 줄이게 되면 bias가 커지는 문제)를 보다 일반화하는 방법으로 소개된 Generalized Advantage Estimator(GAE)을 짚고자 한다. 우선 시작하기 전, estimator에 대한 정의부터 알아야한다.

$$
\\begin{aligned}    \\hat{A}_t^{(1)} &= r_t + \\gamma V(s_{t+1}) - V(s_t) \\newline    \\hat{A}_t^{(2)} &= r_t + \\gamma r_{t+1} + \\gamma^2 V(s_{t+2}) - V(s_t) \\newline    &\\quad \\vdots = \\vdots \\newline    \\hat{A}_t^{(\\infty)} &= r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\cdots - V(s_t)\\end{aligned}
$$

현재의 policy가 앞으로의 state 변화에 끼칠 영향력을 생각한다. Value를 판단할 function $V$ 또한 학습 가능한 하나의 뉴럴 네트워크이다. 이렇듯 policy 모델과 함께 value 평가 모델이 함께 학습되는 구조를 <u>actor-critic model</u>이라 부른다.그런데 이때, time step+1인 시점과 time step+$k$인 시점의 가치는 서로 다르다. 당연하게도 만약 discounting value $\\gamma$가 적용되지 않는다면 estimator가 보는 시점이 길어지면 길어질수록 그만큼 보다 가까운 다음 state에 대한 미래 가치를 상실하게 되고, 결국 <u>policy 및 value에 대한 공정 평가가 어렵기 때문</u>이다. 하지만 이러한 Advantage Function에 문제가 있다.

작은 $k$값을 갖는 추정량 $A_t^{(k)}$는 분산이 낮지만 편향이 크고, 큰 $k$값을 갖는 경우 편향이 낮지만 분산이 크다는 점이다. 이는 <u>항의 개수를 보면 직관화가 가능</u>하다. 합산해야 할 항이 적을 경우(state의 변화가 많지 않기 때문에) 분산이 낮아지지만, 상대적으로 $r_k$에 대한 정확한 정보를 활용하지 않기 때문에 편향이 상대적으로 커진다. 합산해야 할 양이 큰 경우는 반대로 생각하면 된다. 또한, $V(s_t)$ 가 추정 클래스 내에서는 상수로 생각될 수 있기 때문에, 추정량 간 차이는 오직  $k-step$ return에서만 발생하게 된다(길게 표현했지만 trade-off가 존재한다는 사실).

그렇기 때문에 GAE는 <u>추정량 집합 전체에 대해 싸그리 어셈블하는 전략을 사용</u>하였다. 각 estimator에 존재하는 trade-off를 <u>$\\lambda$로 조절하겠다는 생각</u>이다.

$$
\\begin{aligned}    \\hat{A}_t^{GAE(\\gamma, \\lambda)}    &= (1 - \\lambda) \\left( \\hat{A}_t^{(1)} + \\lambda \\hat{A}_t^{(2)} + \\lambda^2 \\hat{A}_t^{(3)} + \\cdots \\right) \\newline    &= (1 - \\lambda) \\left( \\delta_t^V + \\lambda (\\delta_t^V + \\gamma \\delta_{t+1}^V) + \\lambda^2 (\\delta_t^V + \\gamma \\delta_{t+1}^V + \\gamma^2 \\delta_{t+2}^V) + \\cdots \\right) \\newline    &= (1 - \\lambda) \\left( \\delta_t^V (1 + \\lambda + \\lambda^2 + \\cdots) + \\gamma \\delta_{t+1}^V (\\lambda + \\lambda^2 + \\cdots) + \\cdots \\right) \\newline    &= (1 - \\lambda) \\left( \\delta_t^V \\frac{1}{1 - \\lambda} + \\gamma \\delta_{t+1}^V \\frac{\\lambda}{1 - \\lambda} + \\cdots \\right) \\newline    &= \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l}^V\\end{aligned}
$$

바로 이 개념이 GAE이고, PPO 또한 이를 적용한 모델이라고 볼 수 있다. 여기서의 문제점을 드러내면 다음과 같다. Value model은 Policy model이 학습됨에 따라 내뱉는 <u>output에 대한 성능 평가를 진행</u>하는데, 이때 Policy model과 별개로 학습이 진행되어있어야한다는 점이다.  PPO에서는 reward model에 과적합되는 문제를 직접적으로 피하기 위해 <u>KL penalty</u>를 도입하게 된다. Reference model은 주로 초기 언어 모델이나 SFT(Supervised Fine-tuned) 모델로 사용하게 되는데, 학습되는 정책 모델이 내뱉는 output이 SFT 모델과 지나치게 달라지지 않도록 각 토큰 단위에서 제약을 가하게 되어 생성되는 텍스트의 <u>각 토큰이 기준 모델에서 기대되는 분포에서 크게 벗어나지 않도록</u> 한다. 

$$
r_t = r_{\\varphi} (q, o_{\\leq t}) - \\beta \\log \\frac{\\pi_{\\theta} (o_t | q, o_{<t})}{\\pi_{\\text{ref}} (o_t | q, o_{<t})}
$$


<div align="center">
    <img src="https://github.com/user-attachments/assets/d856b251-e24a-4a8b-a5bb-f9d5cad93d63" width="800"/>
</div>


그런데 실제 GRPO 논문의 그림을 보면 살짝 이해하기 어려운 부분이 있을텐데, 바로 KL divergence의 방향성이다. 그런데 실질적으로 KL penalty가 적용되는 구조는 동일하다. PPO 기반으로 동작하는 것은 거의 유사하지만, 바뀐 점은 다음과 같다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/bab0ba17-d682-4ba3-88dd-d4b9b237a8f0" width="800"/>
</div>


- PPO와 달리 GRPO는 더이상 Value Model을 사용하지 않는다. 그렇기 때문에 메모리를 절약할 수 있다.
- 그러나 Value Model을 사용할 수 없다는 것은 GAE를 사용할 수 없다는 뜻이고, 결국 기존에 연산되던 advantage function $A$를 새롭게 정의해야한다.
- 두 정책 모델의 TR 내에서 강화학습을 진행하는 구조는 동일하다. 그러나 이번에는 동일 질문($q$)에 대해 여러 output $o_{1 \\ldots G}$를 답변으로 추출한다.
- 그리고 이 각각의 답변에 대해 예측된 이득의 평균을 정책 강화 학습의 Objective로 사용할 것이다.
- 학습된 reward 모델로부터 각각의 output을 평가한다.
- Reward model의 결과 $\\{r_1, r_2, \\cdots, r_G\\}$를 그룹 전체의 reward로 normalize해서 사용한다. 이는 특정 문맥 $q/o$에 대한 bias/variance(GAE에서 문제가 된 부분)를 줄이기 위한 노력이다.
- KL divergence는 기존의 식이 아닌 Unbiased Estimator를 사용하였다.

요약하자면, Value function을 없앴기 때문에 <u>불가능한 GAE 대신 Reward의 grouped output을 정규화</u>하여 trade-off 효과를 상쇄하였으며, KL divergence 식이 바뀐 정도가 되겠다.

$$
\\mathcal{J}_{GRPO}(\\theta) = \\mathbb{E} \\left( q \\sim P(Q), \\{o_i\\}_{i=1}^{G} \\sim \\pi_{\\theta_{\\text{old}}} (O | q) \\right) \\
\\frac{1}{G} \\sum_{i=1}^{G} \\left( \\min \\left( \\frac{\\pi_{\\theta} (o_i | q)}{\\pi_{\\theta_{\\text{old}}} (o_i | q)} A_i,
\\text{clip} \\left( \\frac{\\pi_{\\theta} (o_i | q)}{\\pi_{\\theta_{\\text{old}}} (o_i | q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta \\mathbb{D}_{KL} (\\pi{\\theta} \\| \\pi_{\\text{ref}}) \\right)
$$

여기서 쓰인 KL divergence 식은:

$$
\\mathbb{D}_{KL} (\\pi_{\\theta} \\| \\pi_{\\text{ref}}) = 
    \\frac{\\pi_{\\text{ref}} (o_i | q)}{\\pi_{\\theta} (o_i | q)} - \\log \\frac{\\pi_{\\text{ref}} (o_i | q)}{\\pi_{\\theta} (o_i | q)} - 1.
$$

이며 각각의 리워드는 다음과 같이 정규화하여 연산된다:

$$
A_i = \\frac{r_i - \\operatorname{mean} (\\{r_1, r_2, \\dots, r_G\\})}{\\operatorname{std} (\\{r_1, r_2, \\dots, r_G\\})}.
$$

# Methods / Approaches

### Deepseek-R1 Zero의 학습법

딥시크는 <u>DeepSeek-V3-Base를 베이스 모델</u>로 활용, <u>GRPO를 가장 메인인 강화 학습법</u>으로 적용한 연구이다. 그 중 가장 메인이 되는 학습법에는 크게 리워드 모델링과 학습 템플릿이 있다. 이외의 방법론은 직접 학습을 통해 demonstration 과정을 거쳤다. 딥시크의 가장 큰 주장은, LLM 베이스 모델을 강화학습 만으로도 o1 만큼의 코딩/수학적 능력까지 키울 수 있다는 사실로, 강화학습이 LLM 학습에 큰 도움이 된다는 사실을 입증한다. Deepseek-R1은 Ollama에서 경량화 버전을 바로 사용해볼 수 있는데, 이때의 output을 확인해보면 실제 답변을 하기 전 ‘생각하는 부분’이 추가된 것을 알 수 있다. 개인적으로 맥북 프로 (M3)를 사용중인데, 속도 면에서 deepseek-r1:14b 모델 정도의 distillation 버전 정도면 편하게 돌릴 수 있는 것 같다. Deepseek-R1:14B 버전에게 LLM system에 대해 질문한 예시는 다음과 같다. 물론 이렇게 사용한 모델은 R1-Zero는 아니다. 그냥 예시를 보여주기 위해 사용하였다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/90bec77e-aa06-414c-b27d-d284155c3017" width="962"/>
</div>


딥시크는 보는 바와 같이 추론 과정을 <think> ~ </think> 태그로 넣고, 그 이후에 실제 답변을 내놓는 것을 볼 수 있다(근데 distill 버전을 쓰다보면 가끔 생각을 멈추기도 함). 이러한 답변 특성은 <u>학습 템플릿 구성</u>에 있다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/4138fb52-5867-4c76-b1d0-a7a9b57393a7" width="962"/>
</div>


결국 o1이고 o3고 어떤 방식으로 추론해서 정답을 내놓는지는 모르겠지만, 이 <u>일련의 과정을 강화학습으로 자동 학습하게 하는 것이 주된 아이디어인 듯</u>하다. 이에 따라 R1-Zero의 Reward modeling 또한 두 분류로 나뉘게 되었다. 리워드 모델을 따로 학습하여 사용할 수도 있지만, Deepseek-R1-Zero 경우 Rule-based method (blackbox가 아닌 평가가 예측 가능한 경우 사용)를 적용하였다. 주요 보상 유형은 두 가지로 구성된다. 개발하는 과정에서 결과 기반 또는 과정 기반 신경망 보상 모델은 적용하지 않았다. 

- **정확성 보상:** 정확성 보상 모델은 응답이 올바른지 평가한다. 예를 들어, 결정적인 결과를 가진 수학 문제의 경우, 모델이 특정한 형식으로 정답을 제시해야 하며, 이를 통해 신뢰할 수 있는 규칙 기반 검증이 가능하다. 마찬가지로, LeetCode (코딩) 문제의 경우, 사전에 정의된 테스트 케이스를 기반으로 컴파일러를 활용하여 피드백을 생성할 수 있다.
- **형식 보상:** 정확성 보상 모델 외에도, 형식 보상 모델을 적용하여 모델이 사고 과정을 <think> 및 </think> 태그 사이에 넣도록 강제한다.

### 흥미로운 점

학습 도중 발견한 흥미로운 내용으로 <u>‘아하 모먼트’</u>를 언급한다. 처음엔 잘못 본 줄 알았는데 정말 아하 모먼트였다. 학습 중간의 정책 모델에서 아하 모멘트가 등장하고 난 후, 사고하는 시간에 보다 시간을 할애하고(스스로 생성한 think가 보다 풍부해진다는 뜻 같음) 본인의 초반 접근법(아하 포인트 이전의 의식의 흐름)을 자체 평가하고 개선하는 과정이 추가되었다는 것이다 (아래 예시).


<div align="center">
    <img src="https://github.com/user-attachments/assets/6d402966-6b49-41f9-84c7-85ff98a1cdd1" width="400">
    <img src="https://github.com/user-attachments/assets/d12ecda9-385d-4cc7-8088-115948d2feb8" width="400">
</div>


답변을 내놓는 LLM은 하나의 의사 결정을 하는 정책 모델이다. 이 정책 모델을 학습하기 위해 기준선인 reward를 세우고 템플릿에 맞춰 자체적으로 발전할 수 있게 했더니 o1의 성능을 넘었다는 연구가 되었다.

### Deepseek-R1의 학습법

흥미롭지만 일단 여기까지는 <u>수학문제나 코딩에 대한 부분</u>이다. LLM은 보다 다양한 task에 적용되어야하는데, 다음 step으로 어떤 방법을 썼을까?
Deepseek-R1-Zero로부터 알아낸 결과는 템플릿과 강화 학습의 조합으로 충분히 좋은 추론 능력을 키울 수 있다는 점이었다. 그렇다면 다음과 같은 의문이 들게 된다.

1. 강화학습 과정은 초반 학습이 불안정하다는 단점이 있다. 이때 만약 적은 양의 high-quality data(사전 학습 데이터)를 통해 수렴 속도를 줄일 수 있는가? (R1-Zero는 오로지 강화학습만으로 학습함)
2. 수학이랑 코딩 능력 말고 일반화 능력이 강화됨과 동시에, 앞서 했던 것과 같이 CoT 과정이 보다 사용자 친화적이면서 명확해질 수 있는 방법이 있는가? (R1-Zero는 수학 문제랑 코딩만 함)

이를 해소하기 위해 Cold Start 방식을 사용하였다. RL을 통해 DeepSeek-V3-Base 모델을 처음부터 학습하는 것이 아닌, 어느 정도 길이의 CoT를 포함한 question/answer 쌍을 답변 데이터로 구성하고 이를 사용하여 fine-tuning한 모델을 시작 포인트로 삼는다. 해당 데이터는 기존 모델의 성능을 해치지 않고 사후의 RL 학습에 도움이 되어야하기 때문에 <u>최대한 깔끔하게 curating하는 과정</u>을 거치며, 모집한 샘플은 대략 $8\\times 10^5$개다.

그러나 Cold Start로 미세 조정한 모델을 RL로 학습했을때 language mixing 문제가 발생하였는데, 이는 추론 과정이나 답변 과정에 하나의 언어만 포함되지 않고 여러 언어가 포함되는 경우(한글, 중국어, 영어 혼용 등등) 가 발생한다는 사실이다.

실제로 DeepSeek 사용자들의 리뷰를 봤을 때 <u>한국어를 사용할 경우 종종 경량화 모델에서 언어가 섞여서 출몰한다는 경험담</u>이 있었는데, 아마 이 문제를 완벽하게 해결하지는 못했나보다. 아무튼 이를 그나마 좀 해소하기 위해 선택한 방법은 RL training 과정에 추가 reward로 language consistency를 준다는 것이다. 해당 리워드 때문에 성능이 좀 저하는 되지만 그래도 일단 LLM이라면 알아듣게는 말하는게 선호도가 더 높다는 것이 DeepSeek의 판단.

### Distilled Model

Distillation 과정을 통해 원본 모델에 비해 상대적으로 경량화된 버전을 내놓았다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/ff9a3f7a-6fed-4b27-a4db-387c70fbe13a" width="700"/>
</div>


앞서 Cold Start 때문에 모아놨던 샘플을 똑같이 Llama, Qwen 미세 조정에 적용하고, 이후에는 RL 학습을 하지 않고 SFT 방식을 적용했다. 이유는 <u>아래와 같이 RL로 학습하는 것보다, R1 모델로부터 증여받는 것이 훨씬 이득이었기 때문</u>이다. **이래서 금수저 집안이 좋은가보다ㅠㅠ**. 그렇다해서 RL 성능이 너무 낮은 것은 또 아니다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/fb023b85-5ec8-41c2-bf82-79eae2310a2e" width="700"/>
</div>


근데 사실 LLM 써보면 느끼는 점인데 추론 문제에 대한 정량평가는 수치일 뿐 실제 사용했을때 유저 평가 지표가 좀 더 중요할 듯 하다.

# 내 생각
DeepSeek는 오픈소스이다. OpenAI는 클로즈소스이다. 이게 가장 큰 장점이라고 할 순 있겠다. 그런데 메모리 사양이 딸려서 원본 모델을 직접 올려볼 수는 없지만 정말 필요하다면 서버를 확보하고 사용할 수는 있을 것 같다.
`,tz=`---
title: "Limitations in AI"
category: "personal insights"
publishedAt: "2025-10-19"
thumbnail: "https://github.com/user-attachments/assets/52bf9796-774f-480a-ae44-1265c553f43c"
---

# 탁상공론

AI 연구를 수행할 당시에는 크게 신경쓰지 않았지만 서비스 단계에서 고려해야할 것들이 은근히 많다. 프론트나 백엔드 단계에서의 이슈가 실제 모델 성능보다 더 우선시되는 경우가 많고, 서비스 단계에서는 생각보다는 복잡한 알고리즘을 원하지 않는다.
단순 개발 측면에서 보면 AI 모델이 차지하는 비중보다 실제 서빙 환경에서의 사용감이 더 중요하며, 그런 문제 때문에 AI 기술 자체에만 투자를 했을때 투자 대비 효율이 떨어지는 경우가 많다.
AI 서비스가 웹이나 어플리케이션 단계로 나오고, 여러 기능들을 사용하기 위해서는 가장 심플한 질문은 다음과 같다.

> "그래서 대체 어떻게 사용해야하는데?", "대체 어느 기능까지 쓸 수 있는건데?", "내가 사용하던 기존 서비스를 얼마나 대체할 수 있는건데?"
> 

실제로 일을 수행하며 느꼈던 점은 여전히 엔지니어와 실수요자 사이에는 감히 넘을 수 없는 벽이 존재하고, 그 벽이 존재하는 한 끊임없이 유지보수가 필요하다는 점이다.
사실 이따금씩 어쩌면 영원히 그 벽을 극복할 수 없을 수 있다는 부정적인 생각도 하게 된다.

# AI로 돈벌기의 딜레마

### 수익화가 어려워요

서비스를 기획하고 판매하는 입장에서는 서비스의 필요성을 강조해야한다.
이번 글에서 주제로 다루는 AI 기술이 메인 주제가 되는 AI 서비스는, 이를 도입했을 때의 기대되는 효과를 강조할 수 밖에 없다.
개발하는 입장에서 고려하게될 수요자는 일반적으로 다음 두 가지에 주로 속하기 마련이다: (1) 현재 AI 기술로는 어림도 없을 정도로의 자동화를 원하거나. (2) 현재 AI 기술력도 필요하지 않을 정도로 단순하거나.

<div align="center">
    <img src="https://github.com/user-attachments/assets/de882a82-84e4-4f3c-932f-b2b33647b2f5" width="">
</div>

그렇기에 기획 단계에서 개발자와 실수요자 사이에서 충분히 가치 판단이 전제되어야 안정적이면서도 효과 좋은 서비스를 만들 수 있다.
결국 이런 상황에서 서비스는 다음과 같은 딜레마에 직면한다;

너무 복잡할 경우 AI 기술력으로는 불가능하기 때문에 rule-based가 덕지덕지 붙기 시작하게 되고 범용성이 떨어지며, 반대로 너무 심플한 경우 투자 대비 수익화가 어렵다.

### 산업계와 학계는 여전히 괴리 상태

그렇기 때문에 투자하는 입장에서도 골치아픈 것이 AI라는 키워드이다. 대체 어느 정도까지 효율화를 했을때 연구 및 개발에 대한 수지가 맞는가를 판단하기 어렵기 때문이다.
특히 최근에 AI 기술에 대한 최종 결정권은 대개 애매한 비전문가에게 있다. 공학자/기술자라고 해서 크게 다르지 않다. 인공지능을 연구했던 사람이 아니라면 AI 기술에 대해 제대로 이해하고 있는 사람은 극히 드물다.

그렇다면 인공지능을 연구했던 전문가가 기획을 하게 되면 어떤 문제가 발생하는가? 우선 경우에 따라 다르겠지만, 대부분 백엔드나 프론트 지식이 부족한 경우가 많다. 특히나 AI 모델 설계나 학습 이외의 서비스 플랫폼, 프레임워크에 대한 지식이 부재한 경우가 많다.
그리고 가장 큰 문제는 인공지능 서비스가 실제로 동작할 도메인에 대한 지식이 현저히 부족하여 실수요자의 니즈와 문제점을 제대로 파악하기 어렵다.
아무래도 최근에는 단순히 박사 학위나 논문 갯수보다는 회사/스타트업 경력이 있거나 실질 서비스 구현 경험이 있는 연구자들을 선호하는 이유가 바로 여기에서 드러난다.

그렇기에 회사 입장에서는 어떤 결정을 하든 불가피한 리스크를 감당하게 된다. 돈을 많이 들여서 AI 자회사를 설립하거나 AI TF 등을 구성해보는 시도를 하지만, 결국 들인 돈에 비해 이를 활용할 서비스를 만들지 못하는 경우가 많다.
무엇보다 그 서비스가 실제로 기업이나 개인에게 매력적으로 다가갈 수 있는가?라고 생각하면 그렇지도 않다.

### Small / Big AI 서비스의 딜레마

최근에 [AI Festa](https://aifesta.kr/)를 방문해서 다녀왔었다. 대개 여러 AI 스타트업들과 대기업에서의 리서치 부스들로 구성되어 있었는데, 크게 보아 두 가지 유형으로 나뉘는 것을 볼 수 있다:

(1) 특정 도메인에 특화된 Small AI 서비스 (e.g. 법률, 의료, 부동산, 금융, ... 등) (2) 범용적으로 사용 가능한 Big AI 서비스 (e.g. 챗봇, 문서 작성, 이미지/비디오 생성, ... 등)
(1)번과 같이 규모가 크기 않은 서비스의 경우, B2C보다 본인들이 기존에 가지고 있던 도메인 지식과 관계를 활용하여 B2B로 접근하는 경우가 많았다. 예를들어 금융 도메인에 특화된 AI 서비스를 제공하는 스타트업의 경우, 기존에 금융 관련 자동화 서비스를 제공하던 기업들을 대상으로 AI 모델을 도입하는 형태로 사업을 확장하는 경우가 많았다.
반대로 (2)번과 같이 규모가 큰 서비스의 경우에는 대부분 대기업에서 제공하는 서비스가 많았다. 국가대표 LLM/VLM이 주된 서비스였고, 대부분 AI SaaS 형태로 서비스하거나 API 형태로 다른 자동화 서비스에 확장하는 형태였다.
여기서 딜레마/문제가 되는 점은 바로 다음에 있다.

(1)번의 Small AI 서비스는 도메인 특화가 되어있기 때문에 실제로 도입하는 기업 입장에서는 매력적일 수 있지만, AI 모델이 해결할 수 있는 문제의 범위가 좁기 때문에 결국 투자 대비 수익화가 어렵다. 게다가 도메인 특화가 되어있기 때문에 해당 도메인에 대한 전문 지식이 없는 경우에는 아예 서비스를 활용할 수 없는 경우도 많다.
그렇기에 단순 스타트업보다는 기존에 해당 도메인에서 사업을 영위하던 기업들이 AI 기술을 도입하는 형태로 사업을 확장하는 경우가 많았고, 실제로 이런 케이스에서는 ROI가 구체적으로 드러나는 것을 볼 수 있었다. 그러나 이런 케이스도, 결국 기술 도입이 완료된 이후에는 추가적인 수익화가 어렵다는 점에서 장기적으로 지속 가능한 사업 모델을 구축하기 어렵다.

(2)번의 Big AI 서비스는 도메인 특화가 되어있지 않다. AI 모델이 해결할 수 있는 문제의 범위가 넓기 때문에 결국 투자 대비 수익화가 가능하다. 게다가 도메인 특화가 되어있지 않기 때문에 해당 도메인에 대한 전문 지식이 없는 경우에도 충분히 서비스를 활용할 수 있다.
그러나 결국 한계점은 외부 수익화가 어렵다는 점이다. 만약 B2B로 접근한다고 하면 AI 모델링 기술이 비용 대비 뛰어난 성능을 보장해야하는데, 현재로서는 API 비용에서 성능대비 경쟁력을 갖추기 어렵다. 게다가 B2C로 접근한다고 하면 결국 기존에 시장을 선점한 글로벌 빅테크 기업들과 경쟁해야하는데, 이들 기업들과 경쟁하기에는 자본력에서 밀릴 수 밖에 없다.
예컨데 당장의 간단한 챗봇 서비스의 경우에도, ChatGPT, Claude, Gemini 등 이미 시장을 선점한 고성능 모델이 존재하며 가격 경쟁력도 뛰어나다. 심지어 아직 LLM/VLM에서도 허덕이는 국내와는 다르게 미국/중국의 여러 기업에서는 이미 그 이상의 성능을 보이는 모델을 오픈 소스로 출시하며 그 이상의 task에 도전하고 있다. 
그렇기에 당장의 이슈 메이킹을 통해 정부 지원금을 받거나 초기 투자를 유도하는 형태는 가능하겠지만, 장기적으로는 결국 지속 가능한 수익 모델을 구축하지 못할 가능성이 크다.
이럴 경우 국내 잡마켓에서는 이미 AI 연구자의 몸값이 지나치게 올라온 상태에서, 그만큼의 추가 투자와 수익화를 실현하지 못해 결국 인력 감축으로 이어지는 악순환이 반복될 수 밖에 없다.

### 연구 속도를 역전해버린 투자

AI 연구의 가장 큰 패착은 연구 속도에 비해 투자를 초기에 너무 많이 받았다는 점이다.
결국 정확한 답을 찾지 못한 채로 산업계에서는 개발부터 시작했지만, 학계에서 문제들을 정의하고 해결하기도 전에 대규모의 학습을 진행해버렸다.
물론 당장 눈앞에 보이는 문제는 꽤나 해결이 되었을지 모르지만, 결국 AI 연구의 본질적인 문제들은 여전히 해결되지 않은 채로 남아있다.
그리고 그런 문제들은 아직 해결되지 못한 채로 LLM/VLM의 일반화 성능의 그늘에 가려져 있는 상황이다.
Transformer 구조와 현재 학습 방식은 절대 완벽한 정답이 될 수 없다. 그러나 이걸 증명할 시간적 여유를 주기도 전에 이미 모든 자본이 투입되어버렸다.
이를 단번에 부정할 수 있는 노력에 대해서는 가치를 인정해주지 않을 뿐더러, 오히려 이러한 시도가 버블이 된 기술 업계에 위험한 시도가 될 수 있다.

# AI는 절대로 완벽해질 수 없다.

> 본인은 강철의 연금술사라는 애니메이션을 좋아한다. 개인적으로는 현재 나오는 그 어떤 애니메이션보다 명작이라고 생각하는데, 그 이유는 단순히 스토리나 작화가 뛰어나서가 아니다. 이 애니메이션이 가지는 철학적 메시지 때문이다.
> 인간의 생명을 인위적으로 창조하려는 시도는 결국 실패할 수 밖에 없다는 메시지, 그리고 인간이 가지는 한계에 대한 이야기는 AI 연구를 하면서도 계속 떠오르는 생각이다.
>

<div align="center">
    <img src="https://github.com/user-attachments/assets/5e10fce5-4f0b-47be-b86b-238a544e8bd6" width="400">
</div>

AI의 근본적 한계점은 데이터를 기반으로 empirical(경험적) 학습을 진행하기 때문에, 아직 관측되지 못한 수많은 Task에 대해 일반화된 높은 성능의 범위를 절대로 정의내릴 수 없다는 점이다.
그렇기에 딥러닝의 이론 본질 자체가 정확도 100%의 AI를 개발할 수 없는 구조가 된다.
가장 쉽게 예로 들 수 있는 것이 자율주행이다. 예를 들어 테슬라가 막대한 자본력을 투자해서 현재 인간이 주행했던 모든 상황을 데이터로 수집했다고 하자.
해당 데이터로 AI 모델을 학습하여 모든 상황에 대해 가장 법적 리스크를 줄일 수 있는 방향 / 가장 사고 확률이 적은 방향으로 주행할 수 있는 AI 모델을 개발했다고 하자.
그렇다면 이 AI 모델은 과연 완벽할 수 있는가? 그렇지 않다. 그 이유는 다음과 같이 정리할 수 있다.

1) 아직 관측되지 못한 상황이 존재할 수 있다.
2) 만약 모든 상황에 관측되었더라도, 인과 관계에 대한 모델링이 불완전할 수 있다.
3) 모든 상황이 관측되었고 인과 관계에 대한 모델링이 완전하더라도, ML 근본적 한계로 인해 함수 예측이 불완전할 수 있다.

2025년 10월까지 발생한 모든 교통사고를 피할 수 있는 데이터를 수집했다고 하더라도, 그 모든 교통사고가 단순히 특정 차의 주행 판단이나 패턴의 문제로 발생한 것만은 아니다. 외적 요인을 고려하지 못한 경우도 많다.
결국 주행이라는 것은 주행 이외의 발생할 수 있는 모든 인과성(Causality)을 고려해야하는데, 현실적으로 어렵다는 것이다. 그리고 막말로 2025년 10월까지의 모든 인과관계를 다 파악했다고 하더라도, 2025년 11월에 발생하는 새로운 상황에 대해서는 여전히 대응할 수 없다.
최종적으로는 ML 알고리즘의 근본적 한계로 인해, 해당 전체 데이터에 대해서 최적의 과적합이 가능한 모델이 존재하지 않을 수 있다 (일단 규모가 장난이 아닐테니).
정답에 가까운 형태의 파라미터는 찾을 수 있어도, 결국 확률적 예측 모델이기 때문에 100%의 정확도를 가지는 모델은 존재할 수 없다.

물론 AI 모델이 어느 정도 이상의 성능을 보장해야 사용감을 높이는 것은 맞다. 하지만 그 상관관계가 모든 분야에서 일맥상통한다고 보기는 어렵다. 커버하는 기능은 많은 정확도 95%의 AI 모델보다는, 제공하는 기능은 매우 제한적이면서 업무 효율화에 큰 기여는 하지 못하지만, 정확도 100%를 가지는 rule-based 알고리즘을 선호하는 경우가 많다.
현재 LLM이 자료조사나 문서작성, 그리고 이를 넘어서 자소서에 활용될 수 있는 것은 어느 정도 성능이 보장되면서 생성 프로세스 상에서 사람이 QC에 리스크가 없이 관여할 수 있기 때문이다. 그렇기 때문에 연구자들이 아직 성능에 더욱 집착하고 (e.g. 문제 풀이 정답률 82%의 성능에서 90%의 성능으로 끌어올리는) 성과를 낼 수 있는 것이다.
**AI가 세상의 모든 일들을 대체할 것이라고 생각한다면 이는 큰 오산이다.** 다만 우리가 경계해야하는 부분은 AI에 의존적이 되면서 생기는 부수적인 문제들이다. 오히려 AI는 이론상 절대 완벽해질 수 없기 때문에 리스크가 크지 않는 문제들에 대해 우선적으로 적용되는 중이고, 리스크가 큰 문제들에서는 AI가 차지하는 비중이 작다는 것이다.

# 맺음말

요즘 생각하는 LLM/VLM 발전의 가장 이상한 점은, 벤치마크 성능을 아무리 높인다고 해도 실제 고차원의 개발 환경에서는 별로 쓸모가 없다는 점이다.
박사급(?)의 GPT-5를 출시했다고는 하지만, GPT-4o나 GPT-o3를 사용했을때 발생하지 않았던 짜잘한 문제들이 추가로 발생하기도 했다.
이외에는 말투가 부자연스럽게 바뀌는 경우가 생기거나 (갑자기 반존대함), 특정 프롬프트에 대해서 에러가 발생하는 부분이 바로 그러하다.
본인이 생각했을때 현재 AI가 가지고 있는 근본적 한계점이 가장 직관적으로 드러나는 부분이라고 생각한다.

<div align="center">
    <img src="https://github.com/user-attachments/assets/525714e3-0c91-428c-b9aa-cb726ffce5f6" width="800">
</div>

AI agent를 사용했을때 생각보다 AI가 생성한 코드가 쓸만하지 않은 경우가 많다. 직접 사용해봤을때 문제가 되는 부분은 보통 기능 수정을 할때, 코드 버전이나 구현 가능성에 대해 염두하고 수정을 해야하는데, AI가 생성한 코드는 그런 부분을 전혀 고려하지 못한다는 점이다.
그 다음으로 문제가 되는 점은, 전체 구조를 구성하는 부분은 꽤나 쓸만하지만, 내부에서 세부적인 로직을 구현하거나 수정하는 부분에서는 자꾸 오류가 발생한다는 점이다. 어떤 경우에는 30번 정도 시도를 하다가 지쳐서 그냥 직접 코딩을 하게 되는 경우도 있었다. 오히려 직접 코딩을 하니까 금방 끝나버림.
마지막으로는 케이스에 따라 차이가 있기는 하지만, 특정 기능을 수정하거나 추가하는 부분에서 사용자가 인지하지 못하는 부분의 코드 로직을 수정하거나 건드리는 경우가 많았다. 이는 특히나 프론트 구현과 같은 task에서 자주 발생했는데, 로직이 바뀌는 경우 로깅 메시지로 확인이 불가능하기 때문에 기능이 바뀐 경우 사용자가 인지하지 못하는 경우가 많았다.
최종적으로는 프로젝트의 규모가 커질수록 바이브 코딩은 한계점이 명확해지고, 그 말인즉슨 어느 정도 AI가 생성해낸 코드를 중간부터는 사람이 직접 이해하고 수정하는 것이 더 효과적이라는 점이다. 그런데 따지고 보면 남이 짜준 코드를 이해하고 수정하는 시간이 내가 직접 짜는 시간이랑 크게 다르지 않다.

끝으로 안드레 카파시(Andrej Karpathy)의 [최근 인터뷰에서의 비판](https://www.youtube.com/watch?v=lXUZvyajciY)을 언급하며 글을 마무리하고자 한다.

<div align="center">
    <img src="https://github.com/user-attachments/assets/0c4f9f83-f927-48dd-9459-a5150afe5f92" width="800">
</div>

해당 영상에서 카파시는 여전히 AGI는 몇십년 이후는 걸릴 것으로 예측하고, 현재 대부분의 추론 모델의 학습의 기반이 되는 강화학습을 강하게 비판한다.
예를 들어 수학 문제를 풀리는데, AI 모델은 추론 과정에 수백 가지의 시도를 하고 정답을 도출하지만 사실 현재 학습하는 방식에서는 각각의 시도에서 어떤 추론 과정이 있었고, 각 추론 방식이 실제로 정답에 도출하기까지 논리적이었는지 판단할 수 있는 기준이 없다.
그렇기 때문에 RL은 모델이 출력한 결과에만 의존하는 학습을 수행하므로 완벽하지 않다는 것이다.
여기에 추가로 언급한 내용이 더 중요하다고 생각되는데, 바로 다음과 같다:

<div align="center">
    <img src="https://github.com/user-attachments/assets/6204c5ba-9267-4ef6-81a9-a232e22ad1ee" width="800">
</div>

바로 RL이 지금 가장 효과적이라고 주목받고 있는 이유는 이전의 방식이 더 별로였다는 것 (RL is terrible. It just so happens that everything that we had before it is much worse).

`,nz=`---
title: "가우시안 분포 공간에 대한 특징자의 Disentanglement 가능성에 대한 아이디어 및 일반론적 고찰"
category: "personal insights"
publishedAt: "2025-07-10"
thumbnail: "https://github.com/user-attachments/assets/eba54253-b1d3-4323-9b7e-a3dc34df85e8"
---

# Disentanglement의 필요성

보통의 딥러닝 모델을 학습한다고 하면 representation space에서 임베딩을 어떻게 학습할 것이냐, 다시금 말해서 latent vector의 각 축이 의미하는 바를 어떻게 효과적으로 학습시킬 것인가로 귀결된다.

그러나 대부분의 딥러닝 학습법들은 데이터에 의존하여 학습하기 때문에 실제 그 특성이 latent에 어떻게 반영되는지 확인하기도 어렵고, 무엇보다 데이터에서 정말 유의미한 특성들을 위주로 학습하기 어렵다는 단점이 있다.

가장 간단한 예시로 들었을때, 만약 이미지를 객체 분류하는 모델을 학습한다고 했을때, 실제 데이터 특성에 따라 정말 그 모델이 물체를 인지하기 위한 중요한 부분에 집중하고 있는지, 그게 아니라면 전혀 엉뚱한 곳에 집중하고 있는지 확인할 수 없다.


<div align="center">
    <img src="https://github.com/user-attachments/assets/d020085c-ae56-4a76-8da8-d01659ffda73", alt="Image" width="500">
</div>


예를 들어, 위와 같은 이미지에서 ‘\`강아지\`’라는 정답을 맞추기 위해서는 강아지가 입고 있는 옷을 제외하고 강아지라고 생각되는 부분에 집중해야하는 것을 넘어 왜 이 털복숭이 생물이 고양이나 다른 포유류가 아니라 ‘\`귀여운 강아지\`’인지 명시할 수 있어야하고, 이때 이렇게 강아지라 명시할 수 있는 특성이 정말 **latent에 오롯이 담길 수 있냐는 것**이다.

그렇기에 Attention module이나 Activation function 등등 non-linearity에 의존하여 학습하는 방식에서는 굳이 고려할 필요가 없었으며 사실상 포기한 것이나 다름없는 disentanglement(원하는 특성을 분리해서 학습하고자 함)이 representation learning에서 더우 중요해지고, 이는 신뢰도가 중요한 영향을 미치는 다양한 task에서 더욱 강조된다.

# 어디 괜찮은 공간 없나?

그렇지만 disentanglement를 완전히 수행하는 모델링을 구축하기는 어렵기 때문에, 우리는 어떻게든 latent space에서 독립된 축을 학습시킬 방법을 찾아야 한다. 그런데 생각보다 가장 가까운 곳에, 우리가 원하는 이 모든 조건을 충족하는 변수 공간이 있다. 바로 Multivariate Gaussian이다.

### Multivariate Gaussian의 정의

$k$차원을 가지는 정규분포 $\\mathcal{N}(\\mu, \\Sigma)$의 확률밀도함수는

$$
f(\\rm{x}) = \\frac{1}{(2\\pi)^{\\it{k}/2} \\vert \\Sigma \\vert^{1/2}} \\exp \\left[ -\\frac{1}{2} (\\rm{x}-\\mu)^\\top \\Sigma^{-1} (\\rm{x}-\\mu)\\right].
$$

이고, $\\Sigma$는 positive-definite 공분산 행렬이다.

### 공분산이 대각이면 독립

공분산이 $\\Sigma = diag(\\sigma_1^2, \\ldots, \\sigma_k^2)$ 이면 밀도함수는 곱으로 분리된다.

$$
f(\\rm{x}) = \\prod_{\\it{i}=1}^{\\it{k}} \\frac{1}{\\sqrt{2\\pi}\\sigma_{\\it{i}}} \\exp \\left[ -\\frac{(x_{\\it{i}} -\\mu_{\\it{i}})^2}{2\\sigma_{\\it{i}}^2} \\right] = \\prod_{\\it{i}=1}^{\\it{k}} \\it{f}(x_{\\it{i}}).
$$

공분산이 대각 행렬인 경우 다변량 정규분포는 1차원 정규분포의 곱으로 인수분해가 가능하기 때문에, 다변수의 모든 차원에 대해 독립성이 보장된다.

### 그래서 이 얘기를 왜 꺼냈냐면

그래서 돌고 돌아 디퓨전 모델로 돌아갈 것이다. 디퓨전 모델의 경우 데이터에 매번 가우시안 노이즈를 중첩하여 더함으로써 완전한 가우시안 분포를 만드는 과정을 구축하게 된다. 이때, 최종 목적지이자 출발점이기도 한 이 다변수 가우시안이 바로 우리가 찾던 ‘공분산이 대각이라’ 독립인 분포인 것이다. 물론 아닌 경우도 있긴 하지만 가장 베이스 모델링 자체는 $\\mathcal{N}(0, I)$에 기반을 두고 있기 때문에(대각 모든 요소가 1인 Identity Matrix), 우리는 임베딩 공간에서 바로 이 디퓨전 모델을 중심으로 분석할 것이다.

# Multivariate Gaussian을 구성할 수 있는 모델링

물론 가우시안 모델링을 할 수 있는 방법은 디퓨전 이외에도 다양하게 존재할 수 있다.

### VAE 기반 모델링

인코더가 $q_\\phi(z\\vert x)$를 추정하고, Multivariate Gaussian의 prior와 KL(Kullback–Leibler divergence) divergence로 정규화해 ELBO 극대화하는 방식.

### **GAN (implicit) + Gaussian $z$**

Generator $G_\\theta(z)$,  $z\\sim \\mathcal{N}(0,I)$, discriminator로 분포 정렬 (다소 간접적인 방식)

### **Normalizing Flow (NF)**

가우시안 base distribution $u\\sim \\mathcal{N}(0,I)$를 다중 함수인 $f_\\theta$로 역변환하는 방식.

### **Diffusion / Score-based**

(Forward Process) 데이터 to 가우시안 노이즈 $q( x_t \\vert x_0 )$, (Reverse process) 모델 $p_\\theta( x_{t-1}\\vert x_t )$ 학습해 denoise. Diffusion stochastic differential equation(SDE)에 기반.

### **Diffusion Prior (Cross-modal)**

텍스트(CLIP text $t$) → (가우시안 노이즈) → CLIP image $z$ 로 매핑하는 조건부 diffusion이다. 이미지 디코더가 $z$로부터 직접 픽셀을 생성하는 방식. Multimodal latent mapping (e.g. DALL-E 2 diffusion prior). 결국 비슷한 개념으로 3D(Voxel)이나 타 도메인의 latent를 mapping하는 방향의 연구도 동일하게 적용된다.

### Flow matching

연속 $t \\in [0,1]$ SDE 대신 ODE를 학습해 $\\mathcal{N}$ to data 매핑을 한 패스에 근사 (e.g. Consistency Models)

# 왜 Diffusion Prior가 중요할까?

오늘은 조금 일반적인 이론과 더불어, 내가 지금부터 하고자 하는 실험과 중요히 관계된 논문을 리뷰하도록 하겠다. 바로 최근 2025 인터스피치에 억셉된 논문인 SEED가 바로 그에 속한다. 해당 학회는 4쪽짜리 짧은 논문을 받는 바람에 디테일한 내용을 다 담지는 못한 논문이지만, 읽으면서 하나의 가설이 떠오르기 시작했다. 만약 디퓨전 forward process로 임베딩 공간을 완전 독립된 또다른 공간으로 매핑이 가능하다면, disentanglement와 동시에 many to many mapping / one to many mapping 학습이 가능할 것이다.


<div align="center">
    <img width="2198" height="1120" alt="Image" src="https://github.com/user-attachments/assets/64bbe1c3-1177-48cf-83f3-7f4c8ba03f8c" />
</div>


논문의 가장 메인 아이디어는 기존의 화자 embedding 시스템은 화자 분리에 **그다지 robust하지 않은 모습**을 보였는데, 그 이유는 바로 얼굴과는 다르게 화자가 놓인 다양한 상황이나 화자의 다양한 형태의 목소리에 따라 학습된 도메인과 추론 도메인의 분포 차이가 심하다는 것이었다. 그렇기에 그림에서 보이는 바와 같이 clean speaker에 해당되는 임베딩과 noisy speaker에 해당되는 임베딩 간의 consistency가 떨어지는 문제가 발생하는데, 이때 이러한 분포 차이를 diffusion model을 통해 줄일 수 있다는 것이 주된 아이디어.

### 오디오는 다루기 어려운 데이터

일반적인 이미지나 글과는 다르게 오디오는 그 특성이 굉장히 다양하게 존재할 수 있다. 사람의 목소리, 악기, 자연 소리, 효과음 등등 분명 청각이라는 동일한 감각기관으로 수용하는 데이터로 인식되지만, 그 형태는 상당히 다르기 때문에 전처리 단계에서부터 정해진 정답이 없을 정도로 상당히 어렵다.

텍스트는 토크나이징해서 일단 숫자로 매핑하게 되면 장르가 다른 글이어도 동일한 토큰으로 인식된다. 그러나 음성은 토크나이징해서 숫자로 매핑하더라도 매 상황에 따라 다른 값이 된다. 즉 데이터 전처리 단계에서부터 원치 않은 여러 특성들이 학습 및 추론에 방해가 되는 것이다.

### 딥러닝이 불가능한 영역?

그래서 그런지 오디오는 딥러닝이 해결하기 어려운 영역에 속한다. STT는 아무리 발전해도 클린한 음성이 나오지 않으면 오류율이 높게 나온다. 이런 상황에서 단순히 또다른 생성형 모델이나 데이터 증강 형태로 일반화 능력을 높여보고자 해도 큰 소용이 없다. 애초에 데이터가 \`entangled\` 된 상태에서 시작하는데, 딥러닝이 이걸 다시 풀어줄 수 있는 능력도 보장이 안되며 우리는 그러한 목적 함수를 절대 찾을 수 없다.


<div align="center">
    <img width="415" height="658" alt="Image" src="https://github.com/user-attachments/assets/eff664c1-0910-4470-92cf-32f771b1ff61" />
</div>


### 공간에 집중하고자

즉, 목적 함수 자체에서 explicit한 정보를 줄 수 없기 때문에 간접적으로나마 이를 확보할 수 있는 임베딩 공간을 통해 원하는 정보만 필터링하고자 하는 것이다.

결국 이 모델링에서 디퓨전 모델에게 기대하는 것은 다양한 augmentation이 가해지게 되는 입력을 다시 깨끗한 상태로 되돌리게 되면서, 신호와 노이즈가 무작위하게 섞여있을 때 노이즈 부분만 잘 걸러내기 위해 disentanglement를 진행한다고 생각할 수 있다.

Diffusion 이전에는 모든 축에 대한 정보가 서로 어떠한 특성을 지니지 않고 존재했다면, 이를 디퓨전 forward process를 거치며 다변수 가우시안 영역으로 가져갔을때 디퓨전 reverse process는 필터링 네트워크로서 특정 노이즈를 인지하고 걸러낼 수 있도록 학습하게 된다.


<div align="center">
    <img width="767" height="834" alt="Image" src="https://github.com/user-attachments/assets/9e8c00d1-5c92-4267-9e77-0f6bca0cba4b" />
</div>


# 그러나 사실은 완전한 disentanglement는 불가능하다.

> 일반적인 embedding system에서 왜 생성형 모델링이 그다지 이점을 갖지는 못하는가? 그에 대한 나름의 이유를 찾아 일부 개념들을 정리해보았다.
> 

앞서 열심히 설명했지만 사실은 완전한 disentanglement는 불가능하다. 그렇다면 왜 “항상 다변수 가우시안”이  자동으로 disentanglement를 주지 못하는가? 그리고 왜 이러한 학습 구조가 일반적으로 높은 성능을 보여주지 못하고 실패할 수 밖에 없을까?

### 통계적 독립과 의미적 독립은 다르다

Isotropic 가우시안의 경우에 축(차원) 간 상관계수를 $0$으로 만들 수 있지만, 정말로 각 축이 데이터의 **해석 가능한** 요인(각도, 성별, 배경음 등)을 담당한다는 보장은 없다.

$\\beta-$VAE, Factor VAE 연구에서도 KL 항을 늘려 $q(z|x)$를 $\\mathcal{N}(0, I)$에 가깝게 만들면 정보량이 줄어들어 모델이 가장 쉽게 압축할 수 있는 요인(e.g. 배경색)만 남고 나머지는 서로 얽힌 채 남는 현상이 드러났다.

### 정보-선호(Information preference) 문제

KL divergence를 정규화로 강하게 조건부를 걸어 가우시안에 분포를 붙이게 되면 재구성(reconstruction) 오류를 줄이려는 목적 함수와 충돌하여 쉽게 예측되는 부분만 남기고 어려운 요인은 버리는 현상이 생긴다.

결과적으로 중요한 요인들은 latent space에 전혀 포함되지 않거나(\`entanglement\`가 아니라 \`omission\`) 특징이 뭉개지는 \`collapse\` 가 발생할 수 있다.

### 고차원 가우시안의 쉘(shell) 현상

차원 수 $d$가 100 단위로 넘어가는 다변수 가우시안에서 $\\mathcal{N}(0, I)$ 샘플은 대부분  $\\vert\\vert z \\vert\\vert = \\sqrt{d}$  근방의 얇은 껍질에 몰린다. 이는 근사적으로 각 방향이 동일하지만, 거리 기반 분류나 검색 시스템에서 불리해지므로 얼굴, 화자 임베딩 등 클러스터 구조가 중요한 응용엔 맞지 않는다.

### 단위 구면(hypersphere) 정규화

얼굴, 화자, 음성 등 여러 모달리티에 대한 임베딩은 보통 마지막 벡터를 \`L2\`-정규화해 **단위 구면**에 올린 뒤 각도/코사인 거리 + 마진 손실(ArcFace, SphereFace 등)을 쓴다.
구면은 벡터 크기를 제거해 학습을 안정화하고, Angular margin을 직접 제어해 클래스 간 분리도를 높이기 쉽기 때문이다.

### 다중 모달리티 & 조건부 표현

멀티모달 모델(CLIP, 음성-텍스트 등)은 주로 대조 학습(Contrastive Learning)으로 ‘같은 의미를 갖는 서로 다른 modality 벡터를 가깝게’ 만든다.
여기서 가장 중요한 건 **상대적 위치**(positive vs. negative)이지, 분포가 가우시안인지 아닌지가 아니다. 따라서 Divergence에 대한 정규화는 복잡성을 늘리고 성능 향상의 효과가 불확실하기에 실제 학습에서 쓰이지 않는다.

### 다중 요소의 Alignment의 효과에 대한 불확실성

Disentanglement를 효과적으로 진행한다고 하더라도, 실제로 disentangle된 latent가 controllability와 설명 가능성(explainability)를 가진다는 것은 명확해지나, 이 latent가 실제로 여러 downstream task에서 효과적인 모습을 보여줄 지는 미지수이다.

# Disentanglement

DRL에서의 가장 흔한 가정은 숨은 요인 $z_1,\\dots,z_k$ 사이가 통계적으로 독립이라는 전제다. 예를 들어 $\\beta-$VAE, **FactorVAE** 등은 total correlation을 억제하는 과정을 통해 latent distribution이 axis 별로 독립하도록 강제한다. 그러나 Locatello et al.의 [대규모 실험](https://arxiv.org/abs/1811.12359?utm_source=chatgpt.com)은 독립을 강제해도 감독(label) 없이는 **진짜 의미적 요인**을 찾지 못한다는 점을 보여주었다.

또한 많은 VAE 계열은 관측 $x$가 잠복 $z$의 **각 축에 단조(혹은 선형)로 대응된다**(연관성이 있다)는 암묵적 가정을 두는데, 이 전제가 깨지면 설령 $z$들이 통계적으로 독립이라도 축과 해석 요인 대응이 뒤틀려 설명 가능성을 잃는다. Locatello의 불가능성 정리 역시 축이 일치하지 않으면 Unsupervised는 어렵다는 논지를 그대로 포함한다.

이론적 식별성 증명은 보통 **인코더-디코더가** almost invertible, **학습이** Global Optimal**에 수렴**한다는 가정을 전제한다. 하지만 실제 딥러닝에서는 네트워크 용량, optimizer, data bias 때문에 이 조건이 잘 성립하지 않아, 실전 성능이 제대로 나오지 않게 된다. Non-linear ICA **이론**은 요인들이 조건부 독립이어야만 식별 가능하다고 본다. 이처럼 여러 연구로부터 Supervising 없이 (이에 대한 explicit한 데이터 구축 등) Disentangle은 불가능하다는 증명이 나오게 되었다.

최근 CausalVAE, ICM-VAE 류의 연구는 잠복 요인들 사이에 방향성 $z_i\\rightarrow z_j$ 이 존재한다는 **구조적 인과 모델**을 VAE 안에 넣어 식별성을 확보한다. 이때도 Weakly Supervision이 필요하며, 관측 노이즈는 독립 가우시안(additive noise)으로 단순화하는 전제를 둔다.

비디오나 시계열 연구에서는 짧은 간격의 샘플은 동일 요인을 공유한다는 Temporal Coherence를 이용한다.
한 프레임에서 다음 프레임으로 변하지 않는 부분과 변하는 부분을 분리해 latent를 disentangle하는 식이다.

# 결론

결국 이론상 데이터의 특성에 의존하는 문제점이 있기에 완벽한 disentangle과 성능 향상을 동시에 이루는 것은 아무래도 불가능해보인다.

특히 표준 가우시안과 같이 흔히 가정할 수 있는 여러 분포들의 경우 계산과 샘플링을 편리하게 만들지만, 그 자체가 의미적 요인 분리를 보장하지 않는다. 오히려 과도한 정규화는 의미 손실이나 피상적인 정보만 압축한다는 문제점이 제시되었다.

그리고 생각보다는 disentanglement보다 성능에 크게 영향을 미치는 요인이 따로 존재할 수 있다. 얼굴, 화자, 멀티모달 임베딩에서 성능을 결정짓는 것은 벡터 분포의 모양보다 **양극성(**positive/negative**) 간 임베딩 구조**다. $L2-$정규화된 구면 임베딩이나 대조 손실이 널리 쓰이는 이유가 여기에 있다.

성능을 향상시킬 것이냐, controllability를 얻을 것이냐에 대한 문제에서 만약 제어 가능성을 염두해두고 학습하고자 한다면 AI의 학습 방식이나 해결 가능한 논제는 지금보다는 좁아질 것이다. 무엇이든 얻는게 있다면 잃는 것도 있는 법이니 ..
`,rz={"cs231n-lecture-review-01":iO,"cs231n-lecture-review-02":aO,"cs231n-lecture-review-03":sO,"cs231n-lecture-review-04":oO,"cs231n-lecture-review-05":lO,"cs231n-lecture-review-06":cO,"cs231n-lecture-review-07":uO,"cs231n-lecture-review-08":dO,"cs231n-lecture-review-09":mO,"cs231n-lecture-review-10":hO,"cs231n-lecture-review-11":pO,"ddpm-proof":fO,"diff-llm":gO,"ddpm-review":bO,"infonce-review":vO,"nerf-review":xO,"nsvf-review":$O,"mixmatch-review":yO,"efficient-review":_O,"light-review":wO,"gan-review":TO,"gan-review2":EO,"imagemanipulate-review":kO,"graph-review":SO,"transfer-review":NO,"lowshot-review":CO,"transformer-multimodal-review":AO,"gangeal-review":LO,"clip-review":PO,"maskclip-review":IO,"style-review":UO,"dreamfield-review":DO,"blip-review":MO,"coop-review":RO,"coca-review":OO,"ddim-review":zO,"scoreds-review":BO,"consistency-review":FO,"diff-review":jO,"controlnet-review":VO,"glide-review":HO,"meru-review":qO,"dino-review":GO,"dino2-review":WO,"lssl-review":KO,"s4-review":YO,"mamba-review":XO,"animatediff-review":QO,"torch2-review":ZO,"rlhf-review":JO,"deepseekr1-insight":ez,"ai-truth":tz,"disentangle-gaussian":nz};function iz(e){const t=/^---\s*\n([\s\S]*?)\n---\s*\n([\s\S]*)$/,n=e.match(t);if(!n)throw new Error("Invalid frontmatter format");const r=n[1],i=n[2],a={},s=r.split(`
`);for(const o of s){const l=o.indexOf(":");if(l===-1)continue;const c=o.substring(0,l).trim();let u=o.substring(l+1).trim();(u.startsWith('"')&&u.endsWith('"')||u.startsWith("'")&&u.endsWith("'"))&&(u=u.slice(1,-1)),a[c]=u}return{metadata:a,content:i}}function az(e){const n=e.trim().split(/\s+/).length,r=Math.ceil(n/200);return r>0?r:1}function sz(e,t=150){const n=e.replace(/#{1,6}\s+/g,"").replace(/\*\*(.*?)\*\*/g,"$1").replace(/\*(.*?)\*/g,"$1").replace(/`(.*?)`/g,"$1").replace(/\[([^\]]+)\]\([^)]+\)/g,"$1").replace(/^\s*[-*+]\s+/gm,"").replace(/^\s*\d+\.\s+/gm,"").replace(/\$\$[\s\S]*?\$\$/g,"[Math Formula]").replace(/\$.*?\$/g,"[Math]").replace(/\n+/g," ").trim();if(n.length<=t)return n;const r=n.substring(0,t),i=Math.max(r.lastIndexOf("."),r.lastIndexOf("!"),r.lastIndexOf("?"));if(i>t*.6)return r.substring(0,i+1);const a=r.lastIndexOf(" ");return a>0?r.substring(0,a)+"...":r+"..."}function oz(){return"Junyoung Park"}async function lz(){const e=Object.entries(rz),t=[];for(const[n,r]of e)try{const{metadata:i,content:a}=iz(r),s={title:i.title,category:i.category,publishedAt:i.publishedAt,slug:n,content:a,excerpt:sz(a),author:oz(),readTime:az(a),thumbnail:i.thumbnail};t.push(s)}catch(i){console.error(`Failed to load post ${n}:`,i)}return t.sort((n,r)=>new Date(r.publishedAt).getTime()-new Date(n.publishedAt).getTime())}const d2=()=>{const[e,t]=T.useState([]),[n,r]=T.useState(!0),[i,a]=T.useState(null);return T.useEffect(()=>{(async()=>{try{r(!0);const o=await lz();t(o)}catch(o){a(o instanceof Error?o.message:"Failed to load posts"),console.error("Error loading blog posts:",o)}finally{r(!1)}})()},[]),{posts:e,loading:n,error:i}},Br=T.forwardRef(({className:e,...t},n)=>h.jsx("div",{ref:n,className:Ue("rounded-lg border bg-card text-card-foreground shadow-sm",e),...t}));Br.displayName="Card";const Fr=T.forwardRef(({className:e,...t},n)=>h.jsx("div",{ref:n,className:Ue("flex flex-col space-y-1.5 p-6",e),...t}));Fr.displayName="CardHeader";const jr=T.forwardRef(({className:e,...t},n)=>h.jsx("h3",{ref:n,className:Ue("text-2xl font-semibold leading-none tracking-tight",e),...t}));jr.displayName="CardTitle";const cz=T.forwardRef(({className:e,...t},n)=>h.jsx("p",{ref:n,className:Ue("text-sm text-muted-foreground",e),...t}));cz.displayName="CardDescription";const Vr=T.forwardRef(({className:e,...t},n)=>h.jsx("div",{ref:n,className:Ue("p-6 pt-0",e),...t}));Vr.displayName="CardContent";const uz=T.forwardRef(({className:e,...t},n)=>h.jsx("div",{ref:n,className:Ue("flex items-center p-6 pt-0",e),...t}));uz.displayName="CardFooter";const dz=p1("inline-flex items-center rounded-full border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2",{variants:{variant:{default:"border-transparent bg-primary text-primary-foreground hover:bg-primary/80",secondary:"border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80",destructive:"border-transparent bg-destructive text-destructive-foreground hover:bg-destructive/80",outline:"text-foreground",category:"border-0 text-white font-bold shadow-lg hover:shadow-xl transition-all duration-300 transform hover:scale-105 backdrop-blur-sm"}},defaultVariants:{variant:"default"}});function m2({className:e,variant:t,...n}){return h.jsx("div",{className:Ue(dz({variant:t}),e),...n})}const h2=T.forwardRef(({className:e,type:t,...n},r)=>h.jsx("input",{type:t,className:Ue("flex h-10 w-full rounded-md border border-input bg-background px-3 py-2 text-base ring-offset-background file:border-0 file:bg-transparent file:text-sm file:font-medium file:text-foreground placeholder:text-muted-foreground focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:cursor-not-allowed disabled:opacity-50 md:text-sm",e),ref:r,...n}));h2.displayName="Input";function mz(e,t=[]){let n=[];function r(a,s){const o=T.createContext(s),l=n.length;n=[...n,s];function c(d){const{scope:m,children:p,...x}=d,g=(m==null?void 0:m[e][l])||o,w=T.useMemo(()=>x,Object.values(x));return h.jsx(g.Provider,{value:w,children:p})}function u(d,m){const p=(m==null?void 0:m[e][l])||o,x=T.useContext(p);if(x)return x;if(s!==void 0)return s;throw new Error(`\`${d}\` must be used within \`${a}\``)}return c.displayName=a+"Provider",[c,u]}const i=()=>{const a=n.map(s=>T.createContext(s));return function(o){const l=(o==null?void 0:o[e])||a;return T.useMemo(()=>({[`__scope${e}`]:{...o,[e]:l}}),[o,l])}};return i.scopeName=e,[r,hz(i,...t)]}function hz(...e){const t=e[0];if(e.length===1)return t;const n=()=>{const r=e.map(i=>({useScope:i(),scopeName:i.scopeName}));return function(a){const s=r.reduce((o,{useScope:l,scopeName:c})=>{const d=l(a)[`__scope${c}`];return{...o,...d}},{});return T.useMemo(()=>({[`__scope${t.scopeName}`]:s}),[s])}};return n.scopeName=t.scopeName,n}var pz=T.createContext(void 0);function p2(e){const t=T.useContext(pz);return e||t||"ltr"}var wh="rovingFocusGroup.onEntryFocus",fz={bubbles:!1,cancelable:!0},am="RovingFocusGroup",[Df,x7,gz]=c1(am),[bz,$7]=mz(am,[gz]),[vz,xz]=bz(am),y7=T.forwardRef((e,t)=>h.jsx(Df.Provider,{scope:e.__scopeRovingFocusGroup,children:h.jsx(Df.Slot,{scope:e.__scopeRovingFocusGroup,children:h.jsx($z,{...e,ref:t})})}));y7.displayName=am;var $z=T.forwardRef((e,t)=>{const{__scopeRovingFocusGroup:n,orientation:r,loop:i=!1,dir:a,currentTabStopId:s,defaultCurrentTabStopId:o,onCurrentTabStopIdChange:l,onEntryFocus:c,preventScrollOnEntryFocus:u=!1,...d}=e,m=T.useRef(null),p=yt(t,m),x=p2(a),[g=null,w]=pc({prop:s,defaultProp:o,onChange:l}),[v,$]=T.useState(!1),_=Pn(c),C=x7(n),k=T.useRef(!1),[S,L]=T.useState(0);return T.useEffect(()=>{const U=m.current;if(U)return U.addEventListener(wh,_),()=>U.removeEventListener(wh,_)},[_]),h.jsx(vz,{scope:n,orientation:r,dir:x,loop:i,currentTabStopId:g,onItemFocus:T.useCallback(U=>w(U),[w]),onItemShiftTab:T.useCallback(()=>$(!0),[]),onFocusableItemAdd:T.useCallback(()=>L(U=>U+1),[]),onFocusableItemRemove:T.useCallback(()=>L(U=>U-1),[]),children:h.jsx(We.div,{tabIndex:v||S===0?-1:0,"data-orientation":r,...d,ref:p,style:{outline:"none",...e.style},onMouseDown:Ae(e.onMouseDown,()=>{k.current=!0}),onFocus:Ae(e.onFocus,U=>{const F=!k.current;if(U.target===U.currentTarget&&F&&!v){const q=new CustomEvent(wh,fz);if(U.currentTarget.dispatchEvent(q),!q.defaultPrevented){const G=C().filter(J=>J.focusable),H=G.find(J=>J.active),ne=G.find(J=>J.id===g),te=[H,ne,...G].filter(Boolean).map(J=>J.ref.current);T7(te,u)}}k.current=!1}),onBlur:Ae(e.onBlur,()=>$(!1))})})}),_7="RovingFocusGroupItem",w7=T.forwardRef((e,t)=>{const{__scopeRovingFocusGroup:n,focusable:r=!0,active:i=!1,tabStopId:a,...s}=e,o=Vc(),l=a||o,c=xz(_7,n),u=c.currentTabStopId===l,d=x7(n),{onFocusableItemAdd:m,onFocusableItemRemove:p}=c;return T.useEffect(()=>{if(r)return m(),()=>p()},[r,m,p]),h.jsx(Df.ItemSlot,{scope:n,id:l,focusable:r,active:i,children:h.jsx(We.span,{tabIndex:u?0:-1,"data-orientation":c.orientation,...s,ref:t,onMouseDown:Ae(e.onMouseDown,x=>{r?c.onItemFocus(l):x.preventDefault()}),onFocus:Ae(e.onFocus,()=>c.onItemFocus(l)),onKeyDown:Ae(e.onKeyDown,x=>{if(x.key==="Tab"&&x.shiftKey){c.onItemShiftTab();return}if(x.target!==x.currentTarget)return;const g=wz(x,c.orientation,c.dir);if(g!==void 0){if(x.metaKey||x.ctrlKey||x.altKey||x.shiftKey)return;x.preventDefault();let v=d().filter($=>$.focusable).map($=>$.ref.current);if(g==="last")v.reverse();else if(g==="prev"||g==="next"){g==="prev"&&v.reverse();const $=v.indexOf(x.currentTarget);v=c.loop?Tz(v,$+1):v.slice($+1)}setTimeout(()=>T7(v))}})})})});w7.displayName=_7;var yz={ArrowLeft:"prev",ArrowUp:"prev",ArrowRight:"next",ArrowDown:"next",PageUp:"first",Home:"first",PageDown:"last",End:"last"};function _z(e,t){return t!=="rtl"?e:e==="ArrowLeft"?"ArrowRight":e==="ArrowRight"?"ArrowLeft":e}function wz(e,t,n){const r=_z(e.key,n);if(!(t==="vertical"&&["ArrowLeft","ArrowRight"].includes(r))&&!(t==="horizontal"&&["ArrowUp","ArrowDown"].includes(r)))return yz[r]}function T7(e,t=!1){const n=document.activeElement;for(const r of e)if(r===n||(r.focus({preventScroll:t}),document.activeElement!==n))return}function Tz(e,t){return e.map((n,r)=>e[(t+r)%e.length])}var Ez=y7,kz=w7,f2="Tabs",[Sz,gZ]=M0(f2,[$7]),E7=$7(),[Nz,g2]=Sz(f2),k7=T.forwardRef((e,t)=>{const{__scopeTabs:n,value:r,onValueChange:i,defaultValue:a,orientation:s="horizontal",dir:o,activationMode:l="automatic",...c}=e,u=p2(o),[d,m]=pc({prop:r,onChange:i,defaultProp:a});return h.jsx(Nz,{scope:n,baseId:Vc(),value:d,onValueChange:m,orientation:s,dir:u,activationMode:l,children:h.jsx(We.div,{dir:u,"data-orientation":s,...c,ref:t})})});k7.displayName=f2;var S7="TabsList",N7=T.forwardRef((e,t)=>{const{__scopeTabs:n,loop:r=!0,...i}=e,a=g2(S7,n),s=E7(n);return h.jsx(Ez,{asChild:!0,...s,orientation:a.orientation,dir:a.dir,loop:r,children:h.jsx(We.div,{role:"tablist","aria-orientation":a.orientation,...i,ref:t})})});N7.displayName=S7;var C7="TabsTrigger",A7=T.forwardRef((e,t)=>{const{__scopeTabs:n,value:r,disabled:i=!1,...a}=e,s=g2(C7,n),o=E7(n),l=I7(s.baseId,r),c=U7(s.baseId,r),u=r===s.value;return h.jsx(kz,{asChild:!0,...o,focusable:!i,active:u,children:h.jsx(We.button,{type:"button",role:"tab","aria-selected":u,"aria-controls":c,"data-state":u?"active":"inactive","data-disabled":i?"":void 0,disabled:i,id:l,...a,ref:t,onMouseDown:Ae(e.onMouseDown,d=>{!i&&d.button===0&&d.ctrlKey===!1?s.onValueChange(r):d.preventDefault()}),onKeyDown:Ae(e.onKeyDown,d=>{[" ","Enter"].includes(d.key)&&s.onValueChange(r)}),onFocus:Ae(e.onFocus,()=>{const d=s.activationMode!=="manual";!u&&!i&&d&&s.onValueChange(r)})})})});A7.displayName=C7;var L7="TabsContent",P7=T.forwardRef((e,t)=>{const{__scopeTabs:n,value:r,forceMount:i,children:a,...s}=e,o=g2(L7,n),l=I7(o.baseId,r),c=U7(o.baseId,r),u=r===o.value,d=T.useRef(u);return T.useEffect(()=>{const m=requestAnimationFrame(()=>d.current=!1);return()=>cancelAnimationFrame(m)},[]),h.jsx(O0,{present:i||u,children:({present:m})=>h.jsx(We.div,{"data-state":u?"active":"inactive","data-orientation":o.orientation,role:"tabpanel","aria-labelledby":l,hidden:!m,id:c,tabIndex:0,...s,ref:t,style:{...e.style,animationDuration:d.current?"0s":void 0},children:m&&a})})});P7.displayName=L7;function I7(e,t){return`${e}-trigger-${t}`}function U7(e,t){return`${e}-content-${t}`}var Cz=k7,D7=N7,M7=A7,R7=P7;const O7=Cz,b2=T.forwardRef(({className:e,...t},n)=>h.jsx(D7,{ref:n,className:Ue("inline-flex h-10 items-center justify-center rounded-md bg-muted p-1 text-muted-foreground",e),...t}));b2.displayName=D7.displayName;const Nc=T.forwardRef(({className:e,...t},n)=>h.jsx(M7,{ref:n,className:Ue("inline-flex items-center justify-center whitespace-nowrap rounded-sm px-3 py-1.5 text-sm font-medium ring-offset-background transition-all focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 data-[state=active]:bg-background data-[state=active]:text-foreground data-[state=active]:shadow-sm",e),...t}));Nc.displayName=M7.displayName;const Az=T.forwardRef(({className:e,...t},n)=>h.jsx(R7,{ref:n,className:Ue("mt-2 ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2",e),...t}));Az.displayName=R7.displayName;function wx(e,[t,n]){return Math.min(n,Math.max(t,e))}var Th=0;function Lz(){T.useEffect(()=>{const e=document.querySelectorAll("[data-radix-focus-guard]");return document.body.insertAdjacentElement("afterbegin",e[0]??Tx()),document.body.insertAdjacentElement("beforeend",e[1]??Tx()),Th++,()=>{Th===1&&document.querySelectorAll("[data-radix-focus-guard]").forEach(t=>t.remove()),Th--}},[])}function Tx(){const e=document.createElement("span");return e.setAttribute("data-radix-focus-guard",""),e.tabIndex=0,e.style.outline="none",e.style.opacity="0",e.style.position="fixed",e.style.pointerEvents="none",e}var Eh="focusScope.autoFocusOnMount",kh="focusScope.autoFocusOnUnmount",Ex={bubbles:!1,cancelable:!0},Pz="FocusScope",z7=T.forwardRef((e,t)=>{const{loop:n=!1,trapped:r=!1,onMountAutoFocus:i,onUnmountAutoFocus:a,...s}=e,[o,l]=T.useState(null),c=Pn(i),u=Pn(a),d=T.useRef(null),m=yt(t,g=>l(g)),p=T.useRef({paused:!1,pause(){this.paused=!0},resume(){this.paused=!1}}).current;T.useEffect(()=>{if(r){let g=function(_){if(p.paused||!o)return;const C=_.target;o.contains(C)?d.current=C:Qi(d.current,{select:!0})},w=function(_){if(p.paused||!o)return;const C=_.relatedTarget;C!==null&&(o.contains(C)||Qi(d.current,{select:!0}))},v=function(_){if(document.activeElement===document.body)for(const k of _)k.removedNodes.length>0&&Qi(o)};document.addEventListener("focusin",g),document.addEventListener("focusout",w);const $=new MutationObserver(v);return o&&$.observe(o,{childList:!0,subtree:!0}),()=>{document.removeEventListener("focusin",g),document.removeEventListener("focusout",w),$.disconnect()}}},[r,o,p.paused]),T.useEffect(()=>{if(o){Sx.add(p);const g=document.activeElement;if(!o.contains(g)){const v=new CustomEvent(Eh,Ex);o.addEventListener(Eh,c),o.dispatchEvent(v),v.defaultPrevented||(Iz(Oz(B7(o)),{select:!0}),document.activeElement===g&&Qi(o))}return()=>{o.removeEventListener(Eh,c),setTimeout(()=>{const v=new CustomEvent(kh,Ex);o.addEventListener(kh,u),o.dispatchEvent(v),v.defaultPrevented||Qi(g??document.body,{select:!0}),o.removeEventListener(kh,u),Sx.remove(p)},0)}}},[o,c,u,p]);const x=T.useCallback(g=>{if(!n&&!r||p.paused)return;const w=g.key==="Tab"&&!g.altKey&&!g.ctrlKey&&!g.metaKey,v=document.activeElement;if(w&&v){const $=g.currentTarget,[_,C]=Uz($);_&&C?!g.shiftKey&&v===C?(g.preventDefault(),n&&Qi(_,{select:!0})):g.shiftKey&&v===_&&(g.preventDefault(),n&&Qi(C,{select:!0})):v===$&&g.preventDefault()}},[n,r,p.paused]);return h.jsx(We.div,{tabIndex:-1,...s,ref:m,onKeyDown:x})});z7.displayName=Pz;function Iz(e,{select:t=!1}={}){const n=document.activeElement;for(const r of e)if(Qi(r,{select:t}),document.activeElement!==n)return}function Uz(e){const t=B7(e),n=kx(t,e),r=kx(t.reverse(),e);return[n,r]}function B7(e){const t=[],n=document.createTreeWalker(e,NodeFilter.SHOW_ELEMENT,{acceptNode:r=>{const i=r.tagName==="INPUT"&&r.type==="hidden";return r.disabled||r.hidden||i?NodeFilter.FILTER_SKIP:r.tabIndex>=0?NodeFilter.FILTER_ACCEPT:NodeFilter.FILTER_SKIP}});for(;n.nextNode();)t.push(n.currentNode);return t}function kx(e,t){for(const n of e)if(!Dz(n,{upTo:t}))return n}function Dz(e,{upTo:t}){if(getComputedStyle(e).visibility==="hidden")return!0;for(;e;){if(t!==void 0&&e===t)return!1;if(getComputedStyle(e).display==="none")return!0;e=e.parentElement}return!1}function Mz(e){return e instanceof HTMLInputElement&&"select"in e}function Qi(e,{select:t=!1}={}){if(e&&e.focus){const n=document.activeElement;e.focus({preventScroll:!0}),e!==n&&Mz(e)&&t&&e.select()}}var Sx=Rz();function Rz(){let e=[];return{add(t){const n=e[0];t!==n&&(n==null||n.pause()),e=Nx(e,t),e.unshift(t)},remove(t){var n;e=Nx(e,t),(n=e[0])==null||n.resume()}}}function Nx(e,t){const n=[...e],r=n.indexOf(t);return r!==-1&&n.splice(r,1),n}function Oz(e){return e.filter(t=>t.tagName!=="A")}function zz(e){const t=T.useRef({value:e,previous:e});return T.useMemo(()=>(t.current.value!==e&&(t.current.previous=t.current.value,t.current.value=e),t.current.previous),[e])}var Bz=function(e){if(typeof document>"u")return null;var t=Array.isArray(e)?e[0]:e;return t.ownerDocument.body},Cs=new WeakMap,Du=new WeakMap,Mu={},Sh=0,F7=function(e){return e&&(e.host||F7(e.parentNode))},Fz=function(e,t){return t.map(function(n){if(e.contains(n))return n;var r=F7(n);return r&&e.contains(r)?r:(console.error("aria-hidden",n,"in not contained inside",e,". Doing nothing"),null)}).filter(function(n){return!!n})},jz=function(e,t,n,r){var i=Fz(t,Array.isArray(e)?e:[e]);Mu[n]||(Mu[n]=new WeakMap);var a=Mu[n],s=[],o=new Set,l=new Set(i),c=function(d){!d||o.has(d)||(o.add(d),c(d.parentNode))};i.forEach(c);var u=function(d){!d||l.has(d)||Array.prototype.forEach.call(d.children,function(m){if(o.has(m))u(m);else try{var p=m.getAttribute(r),x=p!==null&&p!=="false",g=(Cs.get(m)||0)+1,w=(a.get(m)||0)+1;Cs.set(m,g),a.set(m,w),s.push(m),g===1&&x&&Du.set(m,!0),w===1&&m.setAttribute(n,"true"),x||m.setAttribute(r,"true")}catch(v){console.error("aria-hidden: cannot operate on ",m,v)}})};return u(t),o.clear(),Sh++,function(){s.forEach(function(d){var m=Cs.get(d)-1,p=a.get(d)-1;Cs.set(d,m),a.set(d,p),m||(Du.has(d)||d.removeAttribute(r),Du.delete(d)),p||d.removeAttribute(n)}),Sh--,Sh||(Cs=new WeakMap,Cs=new WeakMap,Du=new WeakMap,Mu={})}},Vz=function(e,t,n){n===void 0&&(n="data-aria-hidden");var r=Array.from(Array.isArray(e)?e:[e]),i=Bz(e);return i?(r.push.apply(r,Array.from(i.querySelectorAll("[aria-live]"))),jz(r,i,n,"aria-hidden")):function(){return null}},Qr=function(){return Qr=Object.assign||function(t){for(var n,r=1,i=arguments.length;r<i;r++){n=arguments[r];for(var a in n)Object.prototype.hasOwnProperty.call(n,a)&&(t[a]=n[a])}return t},Qr.apply(this,arguments)};function j7(e,t){var n={};for(var r in e)Object.prototype.hasOwnProperty.call(e,r)&&t.indexOf(r)<0&&(n[r]=e[r]);if(e!=null&&typeof Object.getOwnPropertySymbols=="function")for(var i=0,r=Object.getOwnPropertySymbols(e);i<r.length;i++)t.indexOf(r[i])<0&&Object.prototype.propertyIsEnumerable.call(e,r[i])&&(n[r[i]]=e[r[i]]);return n}function Hz(e,t,n){if(n||arguments.length===2)for(var r=0,i=t.length,a;r<i;r++)(a||!(r in t))&&(a||(a=Array.prototype.slice.call(t,0,r)),a[r]=t[r]);return e.concat(a||Array.prototype.slice.call(t))}var vd="right-scroll-bar-position",xd="width-before-scroll-bar",qz="with-scroll-bars-hidden",Gz="--removed-body-scroll-bar-size";function Nh(e,t){return typeof e=="function"?e(t):e&&(e.current=t),e}function Wz(e,t){var n=T.useState(function(){return{value:e,callback:t,facade:{get current(){return n.value},set current(r){var i=n.value;i!==r&&(n.value=r,n.callback(r,i))}}}})[0];return n.callback=t,n.facade}var Kz=typeof window<"u"?T.useLayoutEffect:T.useEffect,Cx=new WeakMap;function Yz(e,t){var n=Wz(null,function(r){return e.forEach(function(i){return Nh(i,r)})});return Kz(function(){var r=Cx.get(n);if(r){var i=new Set(r),a=new Set(e),s=n.current;i.forEach(function(o){a.has(o)||Nh(o,null)}),a.forEach(function(o){i.has(o)||Nh(o,s)})}Cx.set(n,e)},[e]),n}function Xz(e){return e}function Qz(e,t){t===void 0&&(t=Xz);var n=[],r=!1,i={read:function(){if(r)throw new Error("Sidecar: could not `read` from an `assigned` medium. `read` could be used only with `useMedium`.");return n.length?n[n.length-1]:e},useMedium:function(a){var s=t(a,r);return n.push(s),function(){n=n.filter(function(o){return o!==s})}},assignSyncMedium:function(a){for(r=!0;n.length;){var s=n;n=[],s.forEach(a)}n={push:function(o){return a(o)},filter:function(){return n}}},assignMedium:function(a){r=!0;var s=[];if(n.length){var o=n;n=[],o.forEach(a),s=n}var l=function(){var u=s;s=[],u.forEach(a)},c=function(){return Promise.resolve().then(l)};c(),n={push:function(u){s.push(u),c()},filter:function(u){return s=s.filter(u),n}}}};return i}function Zz(e){e===void 0&&(e={});var t=Qz(null);return t.options=Qr({async:!0,ssr:!1},e),t}var V7=function(e){var t=e.sideCar,n=j7(e,["sideCar"]);if(!t)throw new Error("Sidecar: please provide `sideCar` property to import the right car");var r=t.read();if(!r)throw new Error("Sidecar medium not found");return T.createElement(r,Qr({},n))};V7.isSideCarExport=!0;function Jz(e,t){return e.useMedium(t),V7}var H7=Zz(),Ch=function(){},sm=T.forwardRef(function(e,t){var n=T.useRef(null),r=T.useState({onScrollCapture:Ch,onWheelCapture:Ch,onTouchMoveCapture:Ch}),i=r[0],a=r[1],s=e.forwardProps,o=e.children,l=e.className,c=e.removeScrollBar,u=e.enabled,d=e.shards,m=e.sideCar,p=e.noIsolation,x=e.inert,g=e.allowPinchZoom,w=e.as,v=w===void 0?"div":w,$=e.gapMode,_=j7(e,["forwardProps","children","className","removeScrollBar","enabled","shards","sideCar","noIsolation","inert","allowPinchZoom","as","gapMode"]),C=m,k=Yz([n,t]),S=Qr(Qr({},_),i);return T.createElement(T.Fragment,null,u&&T.createElement(C,{sideCar:H7,removeScrollBar:c,shards:d,noIsolation:p,inert:x,setCallbacks:a,allowPinchZoom:!!g,lockRef:n,gapMode:$}),s?T.cloneElement(T.Children.only(o),Qr(Qr({},S),{ref:k})):T.createElement(v,Qr({},S,{className:l,ref:k}),o))});sm.defaultProps={enabled:!0,removeScrollBar:!0,inert:!1};sm.classNames={fullWidth:xd,zeroRight:vd};var eB=function(){if(typeof __webpack_nonce__<"u")return __webpack_nonce__};function tB(){if(!document)return null;var e=document.createElement("style");e.type="text/css";var t=eB();return t&&e.setAttribute("nonce",t),e}function nB(e,t){e.styleSheet?e.styleSheet.cssText=t:e.appendChild(document.createTextNode(t))}function rB(e){var t=document.head||document.getElementsByTagName("head")[0];t.appendChild(e)}var iB=function(){var e=0,t=null;return{add:function(n){e==0&&(t=tB())&&(nB(t,n),rB(t)),e++},remove:function(){e--,!e&&t&&(t.parentNode&&t.parentNode.removeChild(t),t=null)}}},aB=function(){var e=iB();return function(t,n){T.useEffect(function(){return e.add(t),function(){e.remove()}},[t&&n])}},q7=function(){var e=aB(),t=function(n){var r=n.styles,i=n.dynamic;return e(r,i),null};return t},sB={left:0,top:0,right:0,gap:0},Ah=function(e){return parseInt(e||"",10)||0},oB=function(e){var t=window.getComputedStyle(document.body),n=t[e==="padding"?"paddingLeft":"marginLeft"],r=t[e==="padding"?"paddingTop":"marginTop"],i=t[e==="padding"?"paddingRight":"marginRight"];return[Ah(n),Ah(r),Ah(i)]},lB=function(e){if(e===void 0&&(e="margin"),typeof window>"u")return sB;var t=oB(e),n=document.documentElement.clientWidth,r=window.innerWidth;return{left:t[0],top:t[1],right:t[2],gap:Math.max(0,r-n+t[2]-t[0])}},cB=q7(),oo="data-scroll-locked",uB=function(e,t,n,r){var i=e.left,a=e.top,s=e.right,o=e.gap;return n===void 0&&(n="margin"),`
  .`.concat(qz,` {
   overflow: hidden `).concat(r,`;
   padding-right: `).concat(o,"px ").concat(r,`;
  }
  body[`).concat(oo,`] {
    overflow: hidden `).concat(r,`;
    overscroll-behavior: contain;
    `).concat([t&&"position: relative ".concat(r,";"),n==="margin"&&`
    padding-left: `.concat(i,`px;
    padding-top: `).concat(a,`px;
    padding-right: `).concat(s,`px;
    margin-left:0;
    margin-top:0;
    margin-right: `).concat(o,"px ").concat(r,`;
    `),n==="padding"&&"padding-right: ".concat(o,"px ").concat(r,";")].filter(Boolean).join(""),`
  }
  
  .`).concat(vd,` {
    right: `).concat(o,"px ").concat(r,`;
  }
  
  .`).concat(xd,` {
    margin-right: `).concat(o,"px ").concat(r,`;
  }
  
  .`).concat(vd," .").concat(vd,` {
    right: 0 `).concat(r,`;
  }
  
  .`).concat(xd," .").concat(xd,` {
    margin-right: 0 `).concat(r,`;
  }
  
  body[`).concat(oo,`] {
    `).concat(Gz,": ").concat(o,`px;
  }
`)},Ax=function(){var e=parseInt(document.body.getAttribute(oo)||"0",10);return isFinite(e)?e:0},dB=function(){T.useEffect(function(){return document.body.setAttribute(oo,(Ax()+1).toString()),function(){var e=Ax()-1;e<=0?document.body.removeAttribute(oo):document.body.setAttribute(oo,e.toString())}},[])},mB=function(e){var t=e.noRelative,n=e.noImportant,r=e.gapMode,i=r===void 0?"margin":r;dB();var a=T.useMemo(function(){return lB(i)},[i]);return T.createElement(cB,{styles:uB(a,!t,i,n?"":"!important")})},Mf=!1;if(typeof window<"u")try{var Ru=Object.defineProperty({},"passive",{get:function(){return Mf=!0,!0}});window.addEventListener("test",Ru,Ru),window.removeEventListener("test",Ru,Ru)}catch{Mf=!1}var As=Mf?{passive:!1}:!1,hB=function(e){return e.tagName==="TEXTAREA"},G7=function(e,t){if(!(e instanceof Element))return!1;var n=window.getComputedStyle(e);return n[t]!=="hidden"&&!(n.overflowY===n.overflowX&&!hB(e)&&n[t]==="visible")},pB=function(e){return G7(e,"overflowY")},fB=function(e){return G7(e,"overflowX")},Lx=function(e,t){var n=t.ownerDocument,r=t;do{typeof ShadowRoot<"u"&&r instanceof ShadowRoot&&(r=r.host);var i=W7(e,r);if(i){var a=K7(e,r),s=a[1],o=a[2];if(s>o)return!0}r=r.parentNode}while(r&&r!==n.body);return!1},gB=function(e){var t=e.scrollTop,n=e.scrollHeight,r=e.clientHeight;return[t,n,r]},bB=function(e){var t=e.scrollLeft,n=e.scrollWidth,r=e.clientWidth;return[t,n,r]},W7=function(e,t){return e==="v"?pB(t):fB(t)},K7=function(e,t){return e==="v"?gB(t):bB(t)},vB=function(e,t){return e==="h"&&t==="rtl"?-1:1},xB=function(e,t,n,r,i){var a=vB(e,window.getComputedStyle(t).direction),s=a*r,o=n.target,l=t.contains(o),c=!1,u=s>0,d=0,m=0;do{var p=K7(e,o),x=p[0],g=p[1],w=p[2],v=g-w-a*x;(x||v)&&W7(e,o)&&(d+=v,m+=x),o instanceof ShadowRoot?o=o.host:o=o.parentNode}while(!l&&o!==document.body||l&&(t.contains(o)||t===o));return(u&&(Math.abs(d)<1||!i)||!u&&(Math.abs(m)<1||!i))&&(c=!0),c},Ou=function(e){return"changedTouches"in e?[e.changedTouches[0].clientX,e.changedTouches[0].clientY]:[0,0]},Px=function(e){return[e.deltaX,e.deltaY]},Ix=function(e){return e&&"current"in e?e.current:e},$B=function(e,t){return e[0]===t[0]&&e[1]===t[1]},yB=function(e){return`
  .block-interactivity-`.concat(e,` {pointer-events: none;}
  .allow-interactivity-`).concat(e,` {pointer-events: all;}
`)},_B=0,Ls=[];function wB(e){var t=T.useRef([]),n=T.useRef([0,0]),r=T.useRef(),i=T.useState(_B++)[0],a=T.useState(q7)[0],s=T.useRef(e);T.useEffect(function(){s.current=e},[e]),T.useEffect(function(){if(e.inert){document.body.classList.add("block-interactivity-".concat(i));var g=Hz([e.lockRef.current],(e.shards||[]).map(Ix),!0).filter(Boolean);return g.forEach(function(w){return w.classList.add("allow-interactivity-".concat(i))}),function(){document.body.classList.remove("block-interactivity-".concat(i)),g.forEach(function(w){return w.classList.remove("allow-interactivity-".concat(i))})}}},[e.inert,e.lockRef.current,e.shards]);var o=T.useCallback(function(g,w){if("touches"in g&&g.touches.length===2||g.type==="wheel"&&g.ctrlKey)return!s.current.allowPinchZoom;var v=Ou(g),$=n.current,_="deltaX"in g?g.deltaX:$[0]-v[0],C="deltaY"in g?g.deltaY:$[1]-v[1],k,S=g.target,L=Math.abs(_)>Math.abs(C)?"h":"v";if("touches"in g&&L==="h"&&S.type==="range")return!1;var U=Lx(L,S);if(!U)return!0;if(U?k=L:(k=L==="v"?"h":"v",U=Lx(L,S)),!U)return!1;if(!r.current&&"changedTouches"in g&&(_||C)&&(r.current=k),!k)return!0;var F=r.current||k;return xB(F,w,g,F==="h"?_:C,!0)},[]),l=T.useCallback(function(g){var w=g;if(!(!Ls.length||Ls[Ls.length-1]!==a)){var v="deltaY"in w?Px(w):Ou(w),$=t.current.filter(function(k){return k.name===w.type&&(k.target===w.target||w.target===k.shadowParent)&&$B(k.delta,v)})[0];if($&&$.should){w.cancelable&&w.preventDefault();return}if(!$){var _=(s.current.shards||[]).map(Ix).filter(Boolean).filter(function(k){return k.contains(w.target)}),C=_.length>0?o(w,_[0]):!s.current.noIsolation;C&&w.cancelable&&w.preventDefault()}}},[]),c=T.useCallback(function(g,w,v,$){var _={name:g,delta:w,target:v,should:$,shadowParent:TB(v)};t.current.push(_),setTimeout(function(){t.current=t.current.filter(function(C){return C!==_})},1)},[]),u=T.useCallback(function(g){n.current=Ou(g),r.current=void 0},[]),d=T.useCallback(function(g){c(g.type,Px(g),g.target,o(g,e.lockRef.current))},[]),m=T.useCallback(function(g){c(g.type,Ou(g),g.target,o(g,e.lockRef.current))},[]);T.useEffect(function(){return Ls.push(a),e.setCallbacks({onScrollCapture:d,onWheelCapture:d,onTouchMoveCapture:m}),document.addEventListener("wheel",l,As),document.addEventListener("touchmove",l,As),document.addEventListener("touchstart",u,As),function(){Ls=Ls.filter(function(g){return g!==a}),document.removeEventListener("wheel",l,As),document.removeEventListener("touchmove",l,As),document.removeEventListener("touchstart",u,As)}},[]);var p=e.removeScrollBar,x=e.inert;return T.createElement(T.Fragment,null,x?T.createElement(a,{styles:yB(i)}):null,p?T.createElement(mB,{gapMode:e.gapMode}):null)}function TB(e){for(var t=null;e!==null;)e instanceof ShadowRoot&&(t=e.host,e=e.host),e=e.parentNode;return t}const EB=Jz(H7,wB);var Y7=T.forwardRef(function(e,t){return T.createElement(sm,Qr({},e,{ref:t,sideCar:EB}))});Y7.classNames=sm.classNames;var kB=[" ","Enter","ArrowUp","ArrowDown"],SB=[" ","Enter"],Yc="Select",[om,lm,NB]=c1(Yc),[Wo,bZ]=M0(Yc,[NB,q0]),cm=q0(),[CB,Oa]=Wo(Yc),[AB,LB]=Wo(Yc),X7=e=>{const{__scopeSelect:t,children:n,open:r,defaultOpen:i,onOpenChange:a,value:s,defaultValue:o,onValueChange:l,dir:c,name:u,autoComplete:d,disabled:m,required:p,form:x}=e,g=cm(t),[w,v]=T.useState(null),[$,_]=T.useState(null),[C,k]=T.useState(!1),S=p2(c),[L=!1,U]=pc({prop:r,defaultProp:i,onChange:a}),[F,q]=pc({prop:s,defaultProp:o,onChange:l}),G=T.useRef(null),H=w?x||!!w.closest("form"):!0,[ne,K]=T.useState(new Set),te=Array.from(ne).map(J=>J.props.value).join(";");return h.jsx(AL,{...g,children:h.jsxs(CB,{required:p,scope:t,trigger:w,onTriggerChange:v,valueNode:$,onValueNodeChange:_,valueNodeHasChildren:C,onValueNodeHasChildrenChange:k,contentId:Vc(),value:F,onValueChange:q,open:L,onOpenChange:U,dir:S,triggerPointerDownPosRef:G,disabled:m,children:[h.jsx(om.Provider,{scope:t,children:h.jsx(AB,{scope:e.__scopeSelect,onNativeOptionAdd:T.useCallback(J=>{K(ae=>new Set(ae).add(J))},[]),onNativeOptionRemove:T.useCallback(J=>{K(ae=>{const B=new Set(ae);return B.delete(J),B})},[]),children:n})}),H?h.jsxs(y_,{"aria-hidden":!0,required:p,tabIndex:-1,name:u,autoComplete:d,value:F,onChange:J=>q(J.target.value),disabled:m,form:x,children:[F===void 0?h.jsx("option",{value:""}):null,Array.from(ne)]},te):null]})})};X7.displayName=Yc;var Q7="SelectTrigger",Z7=T.forwardRef((e,t)=>{const{__scopeSelect:n,disabled:r=!1,...i}=e,a=cm(n),s=Oa(Q7,n),o=s.disabled||r,l=yt(t,s.onTriggerChange),c=lm(n),u=T.useRef("touch"),[d,m,p]=__(g=>{const w=c().filter(_=>!_.disabled),v=w.find(_=>_.value===s.value),$=w_(w,g,v);$!==void 0&&s.onValueChange($.value)}),x=g=>{o||(s.onOpenChange(!0),p()),g&&(s.triggerPointerDownPosRef.current={x:Math.round(g.pageX),y:Math.round(g.pageY)})};return h.jsx(h5,{asChild:!0,...a,children:h.jsx(We.button,{type:"button",role:"combobox","aria-controls":s.contentId,"aria-expanded":s.open,"aria-required":s.required,"aria-autocomplete":"none",dir:s.dir,"data-state":s.open?"open":"closed",disabled:o,"data-disabled":o?"":void 0,"data-placeholder":$_(s.value)?"":void 0,...i,ref:l,onClick:Ae(i.onClick,g=>{g.currentTarget.focus(),u.current!=="mouse"&&x(g)}),onPointerDown:Ae(i.onPointerDown,g=>{u.current=g.pointerType;const w=g.target;w.hasPointerCapture(g.pointerId)&&w.releasePointerCapture(g.pointerId),g.button===0&&g.ctrlKey===!1&&g.pointerType==="mouse"&&(x(g),g.preventDefault())}),onKeyDown:Ae(i.onKeyDown,g=>{const w=d.current!=="";!(g.ctrlKey||g.altKey||g.metaKey)&&g.key.length===1&&m(g.key),!(w&&g.key===" ")&&kB.includes(g.key)&&(x(),g.preventDefault())})})})});Z7.displayName=Q7;var J7="SelectValue",e_=T.forwardRef((e,t)=>{const{__scopeSelect:n,className:r,style:i,children:a,placeholder:s="",...o}=e,l=Oa(J7,n),{onValueNodeHasChildrenChange:c}=l,u=a!==void 0,d=yt(t,l.onValueNodeChange);return sn(()=>{c(u)},[c,u]),h.jsx(We.span,{...o,ref:d,style:{pointerEvents:"none"},children:$_(l.value)?h.jsx(h.Fragment,{children:s}):a})});e_.displayName=J7;var PB="SelectIcon",t_=T.forwardRef((e,t)=>{const{__scopeSelect:n,children:r,...i}=e;return h.jsx(We.span,{"aria-hidden":!0,...i,ref:t,children:r||"▼"})});t_.displayName=PB;var IB="SelectPortal",n_=e=>h.jsx(u1,{asChild:!0,...e});n_.displayName=IB;var bs="SelectContent",r_=T.forwardRef((e,t)=>{const n=Oa(bs,e.__scopeSelect),[r,i]=T.useState();if(sn(()=>{i(new DocumentFragment)},[]),!n.open){const a=r;return a?$s.createPortal(h.jsx(i_,{scope:e.__scopeSelect,children:h.jsx(om.Slot,{scope:e.__scopeSelect,children:h.jsx("div",{children:e.children})})}),a):null}return h.jsx(a_,{...e,ref:t})});r_.displayName=bs;var $r=10,[i_,za]=Wo(bs),UB="SelectContentImpl",a_=T.forwardRef((e,t)=>{const{__scopeSelect:n,position:r="item-aligned",onCloseAutoFocus:i,onEscapeKeyDown:a,onPointerDownOutside:s,side:o,sideOffset:l,align:c,alignOffset:u,arrowPadding:d,collisionBoundary:m,collisionPadding:p,sticky:x,hideWhenDetached:g,avoidCollisions:w,...v}=e,$=Oa(bs,n),[_,C]=T.useState(null),[k,S]=T.useState(null),L=yt(t,be=>C(be)),[U,F]=T.useState(null),[q,G]=T.useState(null),H=lm(n),[ne,K]=T.useState(!1),te=T.useRef(!1);T.useEffect(()=>{if(_)return Vz(_)},[_]),Lz();const J=T.useCallback(be=>{const[Ne,...He]=H().map(Ve=>Ve.ref.current),[Le]=He.slice(-1),Be=document.activeElement;for(const Ve of be)if(Ve===Be||(Ve==null||Ve.scrollIntoView({block:"nearest"}),Ve===Ne&&k&&(k.scrollTop=0),Ve===Le&&k&&(k.scrollTop=k.scrollHeight),Ve==null||Ve.focus(),document.activeElement!==Be))return},[H,k]),ae=T.useCallback(()=>J([U,_]),[J,U,_]);T.useEffect(()=>{ne&&ae()},[ne,ae]);const{onOpenChange:B,triggerPointerDownPosRef:X}=$;T.useEffect(()=>{if(_){let be={x:0,y:0};const Ne=Le=>{var Be,Ve;be={x:Math.abs(Math.round(Le.pageX)-(((Be=X.current)==null?void 0:Be.x)??0)),y:Math.abs(Math.round(Le.pageY)-(((Ve=X.current)==null?void 0:Ve.y)??0))}},He=Le=>{be.x<=10&&be.y<=10?Le.preventDefault():_.contains(Le.target)||B(!1),document.removeEventListener("pointermove",Ne),X.current=null};return X.current!==null&&(document.addEventListener("pointermove",Ne),document.addEventListener("pointerup",He,{capture:!0,once:!0})),()=>{document.removeEventListener("pointermove",Ne),document.removeEventListener("pointerup",He,{capture:!0})}}},[_,B,X]),T.useEffect(()=>{const be=()=>B(!1);return window.addEventListener("blur",be),window.addEventListener("resize",be),()=>{window.removeEventListener("blur",be),window.removeEventListener("resize",be)}},[B]);const[P,se]=__(be=>{const Ne=H().filter(Be=>!Be.disabled),He=Ne.find(Be=>Be.ref.current===document.activeElement),Le=w_(Ne,be,He);Le&&setTimeout(()=>Le.ref.current.focus())}),he=T.useCallback((be,Ne,He)=>{const Le=!te.current&&!He;($.value!==void 0&&$.value===Ne||Le)&&(F(be),Le&&(te.current=!0))},[$.value]),D=T.useCallback(()=>_==null?void 0:_.focus(),[_]),Ee=T.useCallback((be,Ne,He)=>{const Le=!te.current&&!He;($.value!==void 0&&$.value===Ne||Le)&&G(be)},[$.value]),je=r==="popper"?Rf:s_,xe=je===Rf?{side:o,sideOffset:l,align:c,alignOffset:u,arrowPadding:d,collisionBoundary:m,collisionPadding:p,sticky:x,hideWhenDetached:g,avoidCollisions:w}:{};return h.jsx(i_,{scope:n,content:_,viewport:k,onViewportChange:S,itemRefCallback:he,selectedItem:U,onItemLeave:D,itemTextRefCallback:Ee,focusSelectedItem:ae,selectedItemText:q,position:r,isPositioned:ne,searchRef:P,children:h.jsx(Y7,{as:Co,allowPinchZoom:!0,children:h.jsx(z7,{asChild:!0,trapped:$.open,onMountAutoFocus:be=>{be.preventDefault()},onUnmountAutoFocus:Ae(i,be=>{var Ne;(Ne=$.trigger)==null||Ne.focus({preventScroll:!0}),be.preventDefault()}),children:h.jsx(R0,{asChild:!0,disableOutsidePointerEvents:!0,onEscapeKeyDown:a,onPointerDownOutside:s,onFocusOutside:be=>be.preventDefault(),onDismiss:()=>$.onOpenChange(!1),children:h.jsx(je,{role:"listbox",id:$.contentId,"data-state":$.open?"open":"closed",dir:$.dir,onContextMenu:be=>be.preventDefault(),...v,...xe,onPlaced:()=>K(!0),ref:L,style:{display:"flex",flexDirection:"column",outline:"none",...v.style},onKeyDown:Ae(v.onKeyDown,be=>{const Ne=be.ctrlKey||be.altKey||be.metaKey;if(be.key==="Tab"&&be.preventDefault(),!Ne&&be.key.length===1&&se(be.key),["ArrowUp","ArrowDown","Home","End"].includes(be.key)){let Le=H().filter(Be=>!Be.disabled).map(Be=>Be.ref.current);if(["ArrowUp","End"].includes(be.key)&&(Le=Le.slice().reverse()),["ArrowUp","ArrowDown"].includes(be.key)){const Be=be.target,Ve=Le.indexOf(Be);Le=Le.slice(Ve+1)}setTimeout(()=>J(Le)),be.preventDefault()}})})})})})})});a_.displayName=UB;var DB="SelectItemAlignedPosition",s_=T.forwardRef((e,t)=>{const{__scopeSelect:n,onPlaced:r,...i}=e,a=Oa(bs,n),s=za(bs,n),[o,l]=T.useState(null),[c,u]=T.useState(null),d=yt(t,L=>u(L)),m=lm(n),p=T.useRef(!1),x=T.useRef(!0),{viewport:g,selectedItem:w,selectedItemText:v,focusSelectedItem:$}=s,_=T.useCallback(()=>{if(a.trigger&&a.valueNode&&o&&c&&g&&w&&v){const L=a.trigger.getBoundingClientRect(),U=c.getBoundingClientRect(),F=a.valueNode.getBoundingClientRect(),q=v.getBoundingClientRect();if(a.dir!=="rtl"){const Be=q.left-U.left,Ve=F.left-Be,Dt=L.left-Ve,Mt=L.width+Dt,xn=Math.max(Mt,U.width),Ft=window.innerWidth-$r,$n=wx(Ve,[$r,Math.max($r,Ft-xn)]);o.style.minWidth=Mt+"px",o.style.left=$n+"px"}else{const Be=U.right-q.right,Ve=window.innerWidth-F.right-Be,Dt=window.innerWidth-L.right-Ve,Mt=L.width+Dt,xn=Math.max(Mt,U.width),Ft=window.innerWidth-$r,$n=wx(Ve,[$r,Math.max($r,Ft-xn)]);o.style.minWidth=Mt+"px",o.style.right=$n+"px"}const G=m(),H=window.innerHeight-$r*2,ne=g.scrollHeight,K=window.getComputedStyle(c),te=parseInt(K.borderTopWidth,10),J=parseInt(K.paddingTop,10),ae=parseInt(K.borderBottomWidth,10),B=parseInt(K.paddingBottom,10),X=te+J+ne+B+ae,P=Math.min(w.offsetHeight*5,X),se=window.getComputedStyle(g),he=parseInt(se.paddingTop,10),D=parseInt(se.paddingBottom,10),Ee=L.top+L.height/2-$r,je=H-Ee,xe=w.offsetHeight/2,be=w.offsetTop+xe,Ne=te+J+be,He=X-Ne;if(Ne<=Ee){const Be=G.length>0&&w===G[G.length-1].ref.current;o.style.bottom="0px";const Ve=c.clientHeight-g.offsetTop-g.offsetHeight,Dt=Math.max(je,xe+(Be?D:0)+Ve+ae),Mt=Ne+Dt;o.style.height=Mt+"px"}else{const Be=G.length>0&&w===G[0].ref.current;o.style.top="0px";const Dt=Math.max(Ee,te+g.offsetTop+(Be?he:0)+xe)+He;o.style.height=Dt+"px",g.scrollTop=Ne-Ee+g.offsetTop}o.style.margin=`${$r}px 0`,o.style.minHeight=P+"px",o.style.maxHeight=H+"px",r==null||r(),requestAnimationFrame(()=>p.current=!0)}},[m,a.trigger,a.valueNode,o,c,g,w,v,a.dir,r]);sn(()=>_(),[_]);const[C,k]=T.useState();sn(()=>{c&&k(window.getComputedStyle(c).zIndex)},[c]);const S=T.useCallback(L=>{L&&x.current===!0&&(_(),$==null||$(),x.current=!1)},[_,$]);return h.jsx(RB,{scope:n,contentWrapper:o,shouldExpandOnScrollRef:p,onScrollButtonChange:S,children:h.jsx("div",{ref:l,style:{display:"flex",flexDirection:"column",position:"fixed",zIndex:C},children:h.jsx(We.div,{...i,ref:d,style:{boxSizing:"border-box",maxHeight:"100%",...i.style}})})})});s_.displayName=DB;var MB="SelectPopperPosition",Rf=T.forwardRef((e,t)=>{const{__scopeSelect:n,align:r="start",collisionPadding:i=$r,...a}=e,s=cm(n);return h.jsx(p5,{...s,...a,ref:t,align:r,collisionPadding:i,style:{boxSizing:"border-box",...a.style,"--radix-select-content-transform-origin":"var(--radix-popper-transform-origin)","--radix-select-content-available-width":"var(--radix-popper-available-width)","--radix-select-content-available-height":"var(--radix-popper-available-height)","--radix-select-trigger-width":"var(--radix-popper-anchor-width)","--radix-select-trigger-height":"var(--radix-popper-anchor-height)"}})});Rf.displayName=MB;var[RB,v2]=Wo(bs,{}),Of="SelectViewport",o_=T.forwardRef((e,t)=>{const{__scopeSelect:n,nonce:r,...i}=e,a=za(Of,n),s=v2(Of,n),o=yt(t,a.onViewportChange),l=T.useRef(0);return h.jsxs(h.Fragment,{children:[h.jsx("style",{dangerouslySetInnerHTML:{__html:"[data-radix-select-viewport]{scrollbar-width:none;-ms-overflow-style:none;-webkit-overflow-scrolling:touch;}[data-radix-select-viewport]::-webkit-scrollbar{display:none}"},nonce:r}),h.jsx(om.Slot,{scope:n,children:h.jsx(We.div,{"data-radix-select-viewport":"",role:"presentation",...i,ref:o,style:{position:"relative",flex:1,overflow:"hidden auto",...i.style},onScroll:Ae(i.onScroll,c=>{const u=c.currentTarget,{contentWrapper:d,shouldExpandOnScrollRef:m}=s;if(m!=null&&m.current&&d){const p=Math.abs(l.current-u.scrollTop);if(p>0){const x=window.innerHeight-$r*2,g=parseFloat(d.style.minHeight),w=parseFloat(d.style.height),v=Math.max(g,w);if(v<x){const $=v+p,_=Math.min(x,$),C=$-_;d.style.height=_+"px",d.style.bottom==="0px"&&(u.scrollTop=C>0?C:0,d.style.justifyContent="flex-end")}}}l.current=u.scrollTop})})})]})});o_.displayName=Of;var l_="SelectGroup",[OB,zB]=Wo(l_),BB=T.forwardRef((e,t)=>{const{__scopeSelect:n,...r}=e,i=Vc();return h.jsx(OB,{scope:n,id:i,children:h.jsx(We.div,{role:"group","aria-labelledby":i,...r,ref:t})})});BB.displayName=l_;var c_="SelectLabel",u_=T.forwardRef((e,t)=>{const{__scopeSelect:n,...r}=e,i=zB(c_,n);return h.jsx(We.div,{id:i.id,...r,ref:t})});u_.displayName=c_;var c0="SelectItem",[FB,d_]=Wo(c0),m_=T.forwardRef((e,t)=>{const{__scopeSelect:n,value:r,disabled:i=!1,textValue:a,...s}=e,o=Oa(c0,n),l=za(c0,n),c=o.value===r,[u,d]=T.useState(a??""),[m,p]=T.useState(!1),x=yt(t,$=>{var _;return(_=l.itemRefCallback)==null?void 0:_.call(l,$,r,i)}),g=Vc(),w=T.useRef("touch"),v=()=>{i||(o.onValueChange(r),o.onOpenChange(!1))};if(r==="")throw new Error("A <Select.Item /> must have a value prop that is not an empty string. This is because the Select value can be set to an empty string to clear the selection and show the placeholder.");return h.jsx(FB,{scope:n,value:r,disabled:i,textId:g,isSelected:c,onItemTextChange:T.useCallback($=>{d(_=>_||(($==null?void 0:$.textContent)??"").trim())},[]),children:h.jsx(om.ItemSlot,{scope:n,value:r,disabled:i,textValue:u,children:h.jsx(We.div,{role:"option","aria-labelledby":g,"data-highlighted":m?"":void 0,"aria-selected":c&&m,"data-state":c?"checked":"unchecked","aria-disabled":i||void 0,"data-disabled":i?"":void 0,tabIndex:i?void 0:-1,...s,ref:x,onFocus:Ae(s.onFocus,()=>p(!0)),onBlur:Ae(s.onBlur,()=>p(!1)),onClick:Ae(s.onClick,()=>{w.current!=="mouse"&&v()}),onPointerUp:Ae(s.onPointerUp,()=>{w.current==="mouse"&&v()}),onPointerDown:Ae(s.onPointerDown,$=>{w.current=$.pointerType}),onPointerMove:Ae(s.onPointerMove,$=>{var _;w.current=$.pointerType,i?(_=l.onItemLeave)==null||_.call(l):w.current==="mouse"&&$.currentTarget.focus({preventScroll:!0})}),onPointerLeave:Ae(s.onPointerLeave,$=>{var _;$.currentTarget===document.activeElement&&((_=l.onItemLeave)==null||_.call(l))}),onKeyDown:Ae(s.onKeyDown,$=>{var C;((C=l.searchRef)==null?void 0:C.current)!==""&&$.key===" "||(SB.includes($.key)&&v(),$.key===" "&&$.preventDefault())})})})})});m_.displayName=c0;var wl="SelectItemText",h_=T.forwardRef((e,t)=>{const{__scopeSelect:n,className:r,style:i,...a}=e,s=Oa(wl,n),o=za(wl,n),l=d_(wl,n),c=LB(wl,n),[u,d]=T.useState(null),m=yt(t,v=>d(v),l.onItemTextChange,v=>{var $;return($=o.itemTextRefCallback)==null?void 0:$.call(o,v,l.value,l.disabled)}),p=u==null?void 0:u.textContent,x=T.useMemo(()=>h.jsx("option",{value:l.value,disabled:l.disabled,children:p},l.value),[l.disabled,l.value,p]),{onNativeOptionAdd:g,onNativeOptionRemove:w}=c;return sn(()=>(g(x),()=>w(x)),[g,w,x]),h.jsxs(h.Fragment,{children:[h.jsx(We.span,{id:l.textId,...a,ref:m}),l.isSelected&&s.valueNode&&!s.valueNodeHasChildren?$s.createPortal(a.children,s.valueNode):null]})});h_.displayName=wl;var p_="SelectItemIndicator",f_=T.forwardRef((e,t)=>{const{__scopeSelect:n,...r}=e;return d_(p_,n).isSelected?h.jsx(We.span,{"aria-hidden":!0,...r,ref:t}):null});f_.displayName=p_;var zf="SelectScrollUpButton",g_=T.forwardRef((e,t)=>{const n=za(zf,e.__scopeSelect),r=v2(zf,e.__scopeSelect),[i,a]=T.useState(!1),s=yt(t,r.onScrollButtonChange);return sn(()=>{if(n.viewport&&n.isPositioned){let o=function(){const c=l.scrollTop>0;a(c)};const l=n.viewport;return o(),l.addEventListener("scroll",o),()=>l.removeEventListener("scroll",o)}},[n.viewport,n.isPositioned]),i?h.jsx(v_,{...e,ref:s,onAutoScroll:()=>{const{viewport:o,selectedItem:l}=n;o&&l&&(o.scrollTop=o.scrollTop-l.offsetHeight)}}):null});g_.displayName=zf;var Bf="SelectScrollDownButton",b_=T.forwardRef((e,t)=>{const n=za(Bf,e.__scopeSelect),r=v2(Bf,e.__scopeSelect),[i,a]=T.useState(!1),s=yt(t,r.onScrollButtonChange);return sn(()=>{if(n.viewport&&n.isPositioned){let o=function(){const c=l.scrollHeight-l.clientHeight,u=Math.ceil(l.scrollTop)<c;a(u)};const l=n.viewport;return o(),l.addEventListener("scroll",o),()=>l.removeEventListener("scroll",o)}},[n.viewport,n.isPositioned]),i?h.jsx(v_,{...e,ref:s,onAutoScroll:()=>{const{viewport:o,selectedItem:l}=n;o&&l&&(o.scrollTop=o.scrollTop+l.offsetHeight)}}):null});b_.displayName=Bf;var v_=T.forwardRef((e,t)=>{const{__scopeSelect:n,onAutoScroll:r,...i}=e,a=za("SelectScrollButton",n),s=T.useRef(null),o=lm(n),l=T.useCallback(()=>{s.current!==null&&(window.clearInterval(s.current),s.current=null)},[]);return T.useEffect(()=>()=>l(),[l]),sn(()=>{var u;const c=o().find(d=>d.ref.current===document.activeElement);(u=c==null?void 0:c.ref.current)==null||u.scrollIntoView({block:"nearest"})},[o]),h.jsx(We.div,{"aria-hidden":!0,...i,ref:t,style:{flexShrink:0,...i.style},onPointerDown:Ae(i.onPointerDown,()=>{s.current===null&&(s.current=window.setInterval(r,50))}),onPointerMove:Ae(i.onPointerMove,()=>{var c;(c=a.onItemLeave)==null||c.call(a),s.current===null&&(s.current=window.setInterval(r,50))}),onPointerLeave:Ae(i.onPointerLeave,()=>{l()})})}),jB="SelectSeparator",x_=T.forwardRef((e,t)=>{const{__scopeSelect:n,...r}=e;return h.jsx(We.div,{"aria-hidden":!0,...r,ref:t})});x_.displayName=jB;var Ff="SelectArrow",VB=T.forwardRef((e,t)=>{const{__scopeSelect:n,...r}=e,i=cm(n),a=Oa(Ff,n),s=za(Ff,n);return a.open&&s.position==="popper"?h.jsx(f5,{...i,...r,ref:t}):null});VB.displayName=Ff;function $_(e){return e===""||e===void 0}var y_=T.forwardRef((e,t)=>{const{value:n,...r}=e,i=T.useRef(null),a=yt(t,i),s=zz(n);return T.useEffect(()=>{const o=i.current,l=window.HTMLSelectElement.prototype,u=Object.getOwnPropertyDescriptor(l,"value").set;if(s!==n&&u){const d=new Event("change",{bubbles:!0});u.call(o,n),o.dispatchEvent(d)}},[s,n]),h.jsx(jc,{asChild:!0,children:h.jsx("select",{...r,ref:a,defaultValue:n})})});y_.displayName="BubbleSelect";function __(e){const t=Pn(e),n=T.useRef(""),r=T.useRef(0),i=T.useCallback(s=>{const o=n.current+s;t(o),function l(c){n.current=c,window.clearTimeout(r.current),c!==""&&(r.current=window.setTimeout(()=>l(""),1e3))}(o)},[t]),a=T.useCallback(()=>{n.current="",window.clearTimeout(r.current)},[]);return T.useEffect(()=>()=>window.clearTimeout(r.current),[]),[n,i,a]}function w_(e,t,n){const i=t.length>1&&Array.from(t).every(c=>c===t[0])?t[0]:t,a=n?e.indexOf(n):-1;let s=HB(e,Math.max(a,0));i.length===1&&(s=s.filter(c=>c!==n));const l=s.find(c=>c.textValue.toLowerCase().startsWith(i.toLowerCase()));return l!==n?l:void 0}function HB(e,t){return e.map((n,r)=>e[(t+r)%e.length])}var qB=X7,T_=Z7,GB=e_,WB=t_,KB=n_,E_=r_,YB=o_,k_=u_,S_=m_,XB=h_,QB=f_,N_=g_,C_=b_,A_=x_;const L_=qB,P_=GB,x2=T.forwardRef(({className:e,children:t,...n},r)=>h.jsxs(T_,{ref:r,className:Ue("flex h-10 w-full items-center justify-between rounded-md border border-input bg-background px-3 py-2 text-sm ring-offset-background placeholder:text-muted-foreground focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 disabled:cursor-not-allowed disabled:opacity-50 [&>span]:line-clamp-1",e),...n,children:[t,h.jsx(WB,{asChild:!0,children:h.jsx(I6,{className:"h-4 w-4 opacity-50"})})]}));x2.displayName=T_.displayName;const I_=T.forwardRef(({className:e,...t},n)=>h.jsx(N_,{ref:n,className:Ue("flex cursor-default items-center justify-center py-1",e),...t,children:h.jsx(tC,{className:"h-4 w-4"})}));I_.displayName=N_.displayName;const U_=T.forwardRef(({className:e,...t},n)=>h.jsx(C_,{ref:n,className:Ue("flex cursor-default items-center justify-center py-1",e),...t,children:h.jsx(I6,{className:"h-4 w-4"})}));U_.displayName=C_.displayName;const $2=T.forwardRef(({className:e,children:t,position:n="popper",...r},i)=>h.jsx(KB,{children:h.jsxs(E_,{ref:i,className:Ue("relative z-50 max-h-96 min-w-[8rem] overflow-hidden rounded-md border bg-popover text-popover-foreground shadow-md data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",n==="popper"&&"data-[side=bottom]:translate-y-1 data-[side=left]:-translate-x-1 data-[side=right]:translate-x-1 data-[side=top]:-translate-y-1",e),position:n,...r,children:[h.jsx(I_,{}),h.jsx(YB,{className:Ue("p-1",n==="popper"&&"h-[var(--radix-select-trigger-height)] w-full min-w-[var(--radix-select-trigger-width)]"),children:t}),h.jsx(U_,{})]})}));$2.displayName=E_.displayName;const ZB=T.forwardRef(({className:e,...t},n)=>h.jsx(k_,{ref:n,className:Ue("py-1.5 pl-8 pr-2 text-sm font-semibold",e),...t}));ZB.displayName=k_.displayName;const Cc=T.forwardRef(({className:e,children:t,...n},r)=>h.jsxs(S_,{ref:r,className:Ue("relative flex w-full cursor-default select-none items-center rounded-sm py-1.5 pl-8 pr-2 text-sm outline-none focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",e),...n,children:[h.jsx("span",{className:"absolute left-2 flex h-3.5 w-3.5 items-center justify-center",children:h.jsx(QB,{children:h.jsx(eC,{className:"h-4 w-4"})})}),h.jsx(XB,{children:t})]}));Cc.displayName=S_.displayName;const JB=T.forwardRef(({className:e,...t},n)=>h.jsx(A_,{ref:n,className:Ue("-mx-1 my-1 h-px bg-muted",e),...t}));JB.displayName=A_.displayName;const eF=()=>{const[e,t]=T.useState("all"),[n,r]=T.useState(""),i=em(),{posts:a,loading:s,error:o}=d2();if(s)return h.jsx("section",{id:"blog",className:"py-20 section-gradient",children:h.jsx("div",{className:"container mx-auto px-6",children:h.jsxs("div",{className:"text-center",children:[h.jsx("div",{className:"animate-spin rounded-full h-12 w-12 border-b-2 border-research-500 mx-auto"}),h.jsx("p",{className:"mt-4 text-foreground/60",children:"Loading blog posts..."})]})})});if(o)return h.jsx("section",{id:"blog",className:"py-20 section-gradient",children:h.jsx("div",{className:"container mx-auto px-6",children:h.jsx("div",{className:"text-center",children:h.jsxs("p",{className:"text-red-500",children:["Error loading blog posts: ",o]})})})});const l=a.filter(m=>{const p=e==="all"||m.category===e,x=n===""||m.title.toLowerCase().includes(n.toLowerCase())||m.excerpt.toLowerCase().includes(n.toLowerCase());return p&&x}).slice(0,6),c=m=>{i(`/blog/${m.slug}`)},u=()=>{i("/blog")},d=m=>({"ai theory":"bg-gradient-to-r from-sky-500 to-sky-600","ai papers":"bg-gradient-to-r from-purple-500 to-purple-600","ai technology":"bg-gradient-to-r from-emerald-500 to-emerald-600","personal insights":"bg-gradient-to-r from-rose-500 to-rose-600"})[m]||"bg-gradient-to-r from-gray-500 to-gray-600";return h.jsx("section",{id:"blog",className:"py-20 section-gradient",children:h.jsxs("div",{className:"container mx-auto px-6",children:[h.jsxs("div",{className:"text-center mb-16",children:[h.jsx("h2",{className:"text-4xl md:text-5xl font-bold mb-6 text-foreground",children:"Latest Posts"}),h.jsx("div",{className:"w-24 h-1 bg-gradient-to-r from-research-500 to-purple-500 mx-auto mb-8"}),h.jsx("p",{className:"text-lg text-foreground/80 max-w-3xl mx-auto font-medium",children:"Recent insights, research findings, and technical deep-dives into the world of AI and machine learning."})]}),h.jsx("div",{className:"max-w-md mx-auto mb-8",children:h.jsxs("div",{className:"relative",children:[h.jsx(R6,{className:"absolute left-3 top-1/2 transform -translate-y-1/2 text-foreground/50 w-4 h-4"}),h.jsx(h2,{type:"text",placeholder:"Search latest posts...",value:n,onChange:m=>r(m.target.value),className:"pl-10 pr-4 py-2 w-full"})]})}),h.jsx("div",{className:"md:hidden mb-8",children:h.jsxs(L_,{value:e,onValueChange:t,children:[h.jsx(x2,{className:"w-full",children:h.jsx(P_,{placeholder:"Select category"})}),h.jsxs($2,{className:"z-[60]",children:[h.jsx(Cc,{value:"all",children:"Latest Posts"}),ma.map(m=>h.jsx(Cc,{value:m.slug,children:m.name},m.slug))]})]})}),h.jsx(O7,{value:e,onValueChange:t,className:"w-full hidden md:block",children:h.jsxs(b2,{className:"grid w-full grid-cols-5 mb-8",children:[h.jsx(Nc,{value:"all",children:"Latest Posts"}),ma.map(m=>h.jsx(Nc,{value:m.slug,children:m.name},m.slug))]})}),h.jsx("div",{className:"grid md:grid-cols-2 lg:grid-cols-3 gap-8 mb-12",children:l.length===0?h.jsx("div",{className:"col-span-full text-center py-8",children:h.jsx("p",{className:"text-foreground/60",children:n?`No posts found matching "${n}"`:"No posts found in this category"})}):l.map(m=>{const p=ma.find(x=>x.slug===m.category);return h.jsxs(Br,{className:"group liquid-glass-card rounded-xl overflow-hidden shadow-lg hover:shadow-xl transition-all duration-300 hover:scale-[1.02] shimmer cursor-pointer relative",onClick:()=>c(m),children:[h.jsx("div",{className:"absolute top-4 left-4 z-10",children:h.jsx(m2,{variant:"category",className:`${d(m.category)} px-3 py-1.5 text-xs`,children:p==null?void 0:p.name})}),m.thumbnail?h.jsx("img",{src:m.thumbnail,alt:m.title,className:"w-full h-48 object-cover bg-white dark:bg-white",style:{backgroundColor:"white"}}):h.jsx("div",{className:"h-48 relative overflow-hidden bg-gradient-to-br from-research-500/20 to-purple-500/20 flex items-center justify-center",children:h.jsx("div",{className:"text-4xl font-bold text-white/80",children:m.title.charAt(0)})}),h.jsx(Fr,{className:"pb-2",children:h.jsx(jr,{className:"text-lg font-semibold text-foreground group-hover:text-transparent group-hover:bg-clip-text group-hover:bg-gradient-to-r group-hover:from-research-600 group-hover:to-purple-600 transition-all line-clamp-2",children:m.title})}),h.jsxs(Vr,{className:"space-y-4",children:[h.jsx("p",{className:"text-foreground/80 text-sm leading-relaxed line-clamp-3",children:m.excerpt}),h.jsxs("div",{className:"flex items-center justify-between text-xs text-foreground/60",children:[h.jsxs("div",{className:"flex items-center gap-4",children:[h.jsxs("div",{className:"flex items-center gap-1",children:[h.jsx(g1,{className:"w-3 h-3"}),h.jsx("span",{children:m.author})]}),h.jsxs("div",{className:"flex items-center gap-1",children:[h.jsx(f1,{className:"w-3 h-3"}),h.jsxs("span",{children:[m.readTime," min read"]})]})]}),h.jsx("span",{children:new Date(m.publishedAt).toLocaleDateString()})]}),h.jsx(sr,{className:"liquid-glass-button text-foreground w-full font-medium transition-all transform",onClick:x=>{x.stopPropagation(),c(m)},children:"Read More"})]})]},m.slug)})}),h.jsx("div",{className:"text-center mt-12",children:h.jsxs("button",{onClick:u,className:"liquid-glass-button inline-flex items-center text-foreground px-6 py-3 rounded-lg font-semibold transition-all transform hover:scale-105 shadow-lg",children:["View All Posts",h.jsx(F0,{className:"w-4 h-4 ml-2"})]})})]})})};class Xc{constructor(t=0,n="Network Error"){this.status=t,this.text=n}}const tF=()=>{if(!(typeof localStorage>"u"))return{get:e=>Promise.resolve(localStorage.getItem(e)),set:(e,t)=>Promise.resolve(localStorage.setItem(e,t)),remove:e=>Promise.resolve(localStorage.removeItem(e))}},Kt={origin:"https://api.emailjs.com",blockHeadless:!1,storageProvider:tF()},y2=e=>e?typeof e=="string"?{publicKey:e}:e.toString()==="[object Object]"?e:{}:{},nF=(e,t="https://api.emailjs.com")=>{if(!e)return;const n=y2(e);Kt.publicKey=n.publicKey,Kt.blockHeadless=n.blockHeadless,Kt.storageProvider=n.storageProvider,Kt.blockList=n.blockList,Kt.limitRate=n.limitRate,Kt.origin=n.origin||t},D_=async(e,t,n={})=>{const r=await fetch(Kt.origin+e,{method:"POST",headers:n,body:t}),i=await r.text(),a=new Xc(r.status,i);if(r.ok)return a;throw a},M_=(e,t,n)=>{if(!e||typeof e!="string")throw"The public key is required. Visit https://dashboard.emailjs.com/admin/account";if(!t||typeof t!="string")throw"The service ID is required. Visit https://dashboard.emailjs.com/admin";if(!n||typeof n!="string")throw"The template ID is required. Visit https://dashboard.emailjs.com/admin/templates"},rF=e=>{if(e&&e.toString()!=="[object Object]")throw"The template params have to be the object. Visit https://www.emailjs.com/docs/sdk/send/"},R_=e=>e.webdriver||!e.languages||e.languages.length===0,O_=()=>new Xc(451,"Unavailable For Headless Browser"),iF=(e,t)=>{if(!Array.isArray(e))throw"The BlockList list has to be an array";if(typeof t!="string")throw"The BlockList watchVariable has to be a string"},aF=e=>{var t;return!((t=e.list)!=null&&t.length)||!e.watchVariable},sF=(e,t)=>e instanceof FormData?e.get(t):e[t],z_=(e,t)=>{if(aF(e))return!1;iF(e.list,e.watchVariable);const n=sF(t,e.watchVariable);return typeof n!="string"?!1:e.list.includes(n)},B_=()=>new Xc(403,"Forbidden"),oF=(e,t)=>{if(typeof e!="number"||e<0)throw"The LimitRate throttle has to be a positive number";if(t&&typeof t!="string")throw"The LimitRate ID has to be a non-empty string"},lF=async(e,t,n)=>{const r=Number(await n.get(e)||0);return t-Date.now()+r},F_=async(e,t,n)=>{if(!t.throttle||!n)return!1;oF(t.throttle,t.id);const r=t.id||e;return await lF(r,t.throttle,n)>0?!0:(await n.set(r,Date.now().toString()),!1)},j_=()=>new Xc(429,"Too Many Requests"),cF=async(e,t,n,r)=>{const i=y2(r),a=i.publicKey||Kt.publicKey,s=i.blockHeadless||Kt.blockHeadless,o=i.storageProvider||Kt.storageProvider,l={...Kt.blockList,...i.blockList},c={...Kt.limitRate,...i.limitRate};return s&&R_(navigator)?Promise.reject(O_()):(M_(a,e,t),rF(n),n&&z_(l,n)?Promise.reject(B_()):await F_(location.pathname,c,o)?Promise.reject(j_()):D_("/api/v1.0/email/send",JSON.stringify({lib_version:"4.4.1",user_id:a,service_id:e,template_id:t,template_params:n}),{"Content-type":"application/json"}))},uF=e=>{if(!e||e.nodeName!=="FORM")throw"The 3rd parameter is expected to be the HTML form element or the style selector of the form"},dF=e=>typeof e=="string"?document.querySelector(e):e,mF=async(e,t,n,r)=>{const i=y2(r),a=i.publicKey||Kt.publicKey,s=i.blockHeadless||Kt.blockHeadless,o=Kt.storageProvider||i.storageProvider,l={...Kt.blockList,...i.blockList},c={...Kt.limitRate,...i.limitRate};if(s&&R_(navigator))return Promise.reject(O_());const u=dF(n);M_(a,e,t),uF(u);const d=new FormData(u);return z_(l,d)?Promise.reject(B_()):await F_(location.pathname,c,o)?Promise.reject(j_()):(d.append("lib_version","4.4.1"),d.append("service_id",e),d.append("template_id",t),d.append("user_id",a),D_("/api/v1.0/email/send-form",d))},hF={init:nF,send:cF,sendForm:mF,EmailJSResponseStatus:Xc},pF=()=>{const[e,t]=T.useState(!1),[n,r]=T.useState(!1),i=T.useRef(null),{toast:a}=r6(),s=async o=>{if(o.preventDefault(),!!i.current){r(!0);try{(await hF.sendForm("service_fk1pspj","template_dsd991k",i.current,"3gJ7P8w1YQ1RtdX6T")).status===200&&(a({title:"Message sent successfully!",description:"Thank you for your message. I'll get back to you soon."}),i.current.reset())}catch(l){console.error("EmailJS Error:",l),a({title:"Failed to send message",description:"Please try again or contact me directly at junia3@naver.com",variant:"destructive"})}finally{r(!1)}}};return h.jsx("section",{id:"contact",className:"py-20 section-gradient",children:h.jsxs("div",{className:"container mx-auto px-6",children:[h.jsxs("div",{className:"text-center mb-16",children:[h.jsx("h2",{className:"text-4xl md:text-5xl font-bold mb-6 text-foreground",children:"Get In Touch"}),h.jsx("div",{className:"w-24 h-1 bg-gradient-to-r from-research-500 to-purple-500 mx-auto mb-8"}),h.jsx("p",{className:"text-lg text-foreground/80 max-w-3xl mx-auto",children:"Interested in collaboration, paper co-works, research opportunities, or discussing deep learning innovations? I'd love to hear from you."})]}),h.jsx("div",{className:"max-w-6xl mx-auto",children:h.jsxs("div",{className:"grid md:grid-cols-3 gap-12",children:[h.jsxs("div",{className:"space-y-8",children:[h.jsxs("div",{children:[h.jsx("h3",{className:"text-2xl font-semibold mb-6 text-foreground",children:"Contact Information"}),h.jsxs("div",{className:"space-y-4",children:[h.jsxs("div",{className:"flex items-center gap-4",children:[h.jsx("div",{className:"w-12 h-12 bg-primary/20 rounded-lg flex items-center justify-center",children:h.jsx(M6,{className:"w-6 h-6"})}),h.jsxs("div",{children:[h.jsx("p",{className:"font-medium text-foreground",children:"Email"}),h.jsx("p",{className:"text-foreground/70",children:"junia3@naver.com"})]})]}),h.jsxs("div",{className:"flex items-center gap-4",children:[h.jsx("div",{className:"w-12 h-12 bg-primary/20 rounded-lg flex items-center justify-center",children:h.jsx(md,{className:"w-6 h-6"})}),h.jsxs("div",{children:[h.jsx("p",{className:"font-medium text-foreground",children:"Location"}),h.jsx("p",{className:"text-foreground/70",children:"Seoul, Korea"})]})]})]})]}),h.jsxs("div",{children:[h.jsx("h3",{className:"text-xl font-semibold mb-4 text-foreground",children:"Connect With Me"}),h.jsxs("div",{className:"flex gap-4",children:[h.jsx("a",{href:"https://www.linkedin.com/in/junyoung-park-490597344/",className:"w-12 h-12 bg-muted rounded-lg flex items-center justify-center hover:bg-primary hover:text-primary-foreground transition-colors",children:h.jsx("svg",{className:"w-6 h-6",fill:"currentColor",viewBox:"0 0 24 24",children:h.jsx("path",{d:"M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"})})}),h.jsx("a",{href:"https://github.com/6unoyunr",className:"w-12 h-12 bg-muted rounded-lg flex items-center justify-center hover:bg-primary hover:text-primary-foreground transition-colors",children:h.jsx("svg",{className:"w-6 h-6",fill:"currentColor",viewBox:"0 0 24 24",children:h.jsx("path",{d:"M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"})})}),h.jsx("a",{href:"https://www.instagram.com/6unoyunr",className:"w-12 h-12 bg-muted rounded-lg flex items-center justify-center hover:bg-primary hover:text-primary-foreground transition-colors",target:"_blank",rel:"noopener noreferrer",children:h.jsx("svg",{className:"w-6 h-6",viewBox:"0 0 24 24",fill:"currentColor",children:h.jsx("path",{d:"M12 2.2c3.2 0 3.6 0 4.9.1 1.2.1 2 .2 2.5.4.6.2 1 .5 1.5 1 .5.5.8.9 1 1.5.2.5.3 1.3.4 2.5.1 1.3.1 1.7.1 4.9s0 3.6-.1 4.9c-.1 1.2-.2 2-.4 2.5-.2.6-.5 1-1 1.5-.5.5-.9.8-1.5 1-.5.2-1.3.3-2.5.4-1.3.1-1.7.1-4.9.1s-3.6 0-4.9-.1c-1.2-.1-2-.2-2.5-.4-.6-.2-1-.5-1.5-1-.5-.5-.8-.9-1-1.5-.2-.5-.3-1.3-.4-2.5C2.2 15.6 2.2 15.2 2.2 12s0-3.6.1-4.9c.1-1.2.2-2 .4-2.5.2-.6.5-1 1-1.5.5-.5.9-.8 1.5-1 .5-.2 1.3-.3 2.5-.4C8.4 2.2 8.8 2.2 12 2.2zm0-2.2C8.7 0 8.3 0 7 .1 5.7.1 4.8.3 4.1.6 3.3.9 2.7 1.3 2.1 2 .5 3.6.1 5.5.1 7.9 0 9.3 0 9.7 0 12s0 2.7.1 4.1c0 2.4.4 4.3 1.9 5.9s3.5 1.8 5.9 1.9c1.4.1 1.8.1 4.1.1s2.7 0 4.1-.1c2.4 0 4.3-.4 5.9-1.9s1.8-3.5 1.9-5.9c.1-1.4.1-1.8.1-4.1s0-2.7-.1-4.1c0-2.4-.4-4.3-1.9-5.9S18.6.1 16.2.1C14.8 0 14.4 0 12 0zm0 5.8a6.2 6.2 0 100 12.4 6.2 6.2 0 000-12.4zm0 10.2a4 4 0 110-8 4 4 0 010 8zm6.4-11.6a1.4 1.4 0 11-2.8 0 1.4 1.4 0 012.8 0z"})})})]})]}),h.jsxs("div",{className:"space-y-4",children:[h.jsx("h3",{className:"text-xl font-semibold text-foreground",children:"Business Card"}),h.jsx("div",{className:"flex flex-col items-start space-y-4",children:h.jsx("div",{className:"business-card-container w-64 h-80 cursor-pointer",onClick:()=>t(!e),children:h.jsxs("div",{className:`business-card-flip ${e?"rotate-y-180":""}`,children:[h.jsxs("div",{className:`business-card-front liquid-glass-card p-8 flex flex-col justify-between text-foreground transition-all duration-500 ${e?"z-10 business-card-blur":"z-20"}`,style:{transition:"filter 0.4s"},children:[h.jsxs("div",{children:[h.jsx("h4",{className:"text-2xl font-bold mb-2",children:"Junyoung Park"}),h.jsx("p",{className:"text-sm opacity-90",children:"AI Researcher / Manager"})]}),h.jsxs("div",{className:"space-y-2",children:[h.jsx("p",{className:"text-sm",children:"Technology R&D Center"}),h.jsx("p",{className:"text-sm",children:"Seoul Broadcasting System (SBS) Co., Ltd."})]}),h.jsx("div",{className:"space-y-1 text-xs",children:h.jsx("p",{children:"junia3@naver.com"})}),h.jsxs("div",{className:"mt-4 flex justify-left pr-2",children:[h.jsx("img",{src:"https://github.com/user-attachments/assets/df4fe686-1b90-428c-bb7e-0804bf27d7a9",alt:"Junyoung Park",style:{width:"30%",height:"auto"},className:"rounded-md dark:hidden"}),h.jsx("img",{src:"https://github.com/user-attachments/assets/a812c9a3-827c-462b-833b-cc8f86078c84",alt:"Junyoung Park (dark)",style:{width:"30%",height:"auto"},className:"rounded-md hidden dark:block"})]})]}),h.jsx("div",{className:`business-card-back liquid-glass-card p-8 flex flex-col justify-center text-foreground transition-all duration-500 ${e?"z-20":"z-10 business-card-blur"}`,style:{transform:"rotateY(180deg)",transition:"filter 0.4s"},children:h.jsxs("div",{className:"text-center space-y-4",children:[h.jsx("div",{className:"w-16 h-16 bg-white/20 rounded-full mx-auto flex items-center justify-center overflow-hidden",children:h.jsx("img",{src:"https://avatars.githubusercontent.com/u/201962047?v=4",alt:"Research Icon",className:"w-full h-full object-cover"})}),h.jsxs("div",{children:[h.jsx("p",{className:"text-sm font-semibold",children:"Research Interests"}),h.jsx("p",{className:"text-xs mt-2 opacity-90",children:"Domain Adaptation"}),h.jsx("p",{className:"text-xs opacity-90",children:"Multimodal AI"}),h.jsx("p",{className:"text-xs opacity-90",children:"Test-Time Training / Post-Training"}),h.jsx("p",{className:"text-xs opacity-90",children:"Representation Alignment for Foundation Models"})]}),h.jsxs("div",{className:"mt-4 flex justify-center pr-2 pt-5",children:[h.jsx("img",{src:"https://github.com/user-attachments/assets/df4fe686-1b90-428c-bb7e-0804bf27d7a9",alt:"Junyoung Park",style:{width:"30%",height:"auto"},className:"rounded-md dark:hidden"}),h.jsx("img",{src:"https://github.com/user-attachments/assets/a812c9a3-827c-462b-833b-cc8f86078c84",alt:"Junyoung Park (dark)",style:{width:"30%",height:"auto"},className:"rounded-md hidden dark:block"})]})]})})]})})})]})]}),h.jsxs("div",{className:"md:col-span-2 liquid-glass-card rounded-xl p-8 shadow-lg border border-border",children:[h.jsx("h3",{className:"text-2xl font-semibold mb-6 text-foreground",children:"Send a Message"}),h.jsxs("form",{ref:i,onSubmit:s,className:"space-y-6",children:[h.jsxs("div",{children:[h.jsx("label",{className:"block text-sm font-medium text-foreground mb-2",children:"Name"}),h.jsx("input",{type:"text",name:"from_name",required:!0,className:"w-full px-4 py-3 border border-border rounded-lg focus:ring-2 focus:ring-primary focus:border-transparent transition-colors bg-background",placeholder:"Your name"})]}),h.jsxs("div",{children:[h.jsx("label",{className:"block text-sm font-medium text-foreground mb-2",children:"Email"}),h.jsx("input",{type:"email",name:"from_email",required:!0,className:"w-full px-4 py-3 border border-border rounded-lg focus:ring-2 focus:ring-primary focus:border-transparent transition-colors bg-background",placeholder:"your.email@example.com"})]}),h.jsxs("div",{children:[h.jsx("label",{className:"block text-sm font-medium text-foreground mb-2",children:"Subject"}),h.jsx("input",{type:"text",name:"subject",required:!0,className:"w-full px-4 py-3 border border-border rounded-lg focus:ring-2 focus:ring-primary focus:border-transparent transition-colors bg-background",placeholder:"Research collaboration opportunity"})]}),h.jsxs("div",{children:[h.jsx("label",{className:"block text-sm font-medium text-foreground mb-2",children:"Message"}),h.jsx("textarea",{rows:5,name:"message",required:!0,className:"w-full px-4 py-3 border border-border rounded-lg focus:ring-2 focus:ring-primary focus:border-transparent transition-colors bg-background resize-none",placeholder:"Your message..."})]}),h.jsx("button",{type:"submit",disabled:n,className:"w-full py-3 rounded-lg font-semibold liquid-glass-button transition-all transform hover:shadow-lg disabled:opacity-50 disabled:cursor-not-allowed",children:n?"Sending...":"Send Message"})]})]})]})}),h.jsx("div",{className:"text-center mt-16 pt-8 border-t border-border",children:h.jsx("p",{className:"text-foreground/60",children:"© AI Researcher Junyoung Park"})})]})})},fF=()=>h.jsxs("div",{className:"min-h-screen bg-background",children:[h.jsx(Ei,{}),h.jsx(hI,{}),h.jsx(tO,{}),h.jsx(nO,{}),h.jsx(eO,{}),h.jsx(rO,{}),h.jsx(eF,{}),h.jsx(pF,{})]}),gF=()=>{const e=J0();return T.useEffect(()=>{console.error("404 Error: User attempted to access non-existent route:",e.pathname)},[e.pathname]),h.jsx("div",{className:"min-h-screen flex items-center justify-center bg-gray-100",children:h.jsxs("div",{className:"text-center",children:[h.jsx("h1",{className:"text-4xl font-bold mb-4",children:"404"}),h.jsx("p",{className:"text-xl text-gray-600 mb-4",children:"Oops! Page not found"}),h.jsx("a",{href:"/",className:"text-blue-500 hover:text-blue-700 underline",children:"Return to Home"})]})})};function Ux(e){const t=[],n=String(e||"");let r=n.indexOf(","),i=0,a=!1;for(;!a;){r===-1&&(r=n.length,a=!0);const s=n.slice(i,r).trim();(s||!a)&&t.push(s),i=r+1,r=n.indexOf(",",i)}return t}function V_(e,t){const n={};return(e[e.length-1]===""?[...e,""]:e).join((n.padRight?" ":"")+","+(n.padLeft===!1?"":" ")).trim()}const bF=/^[$_\p{ID_Start}][$_\u{200C}\u{200D}\p{ID_Continue}]*$/u,vF=/^[$_\p{ID_Start}][-$_\u{200C}\u{200D}\p{ID_Continue}]*$/u,xF={};function Dx(e,t){return(xF.jsx?vF:bF).test(e)}const $F=/[ \t\n\f\r]/g;function yF(e){return typeof e=="object"?e.type==="text"?Mx(e.value):!1:Mx(e)}function Mx(e){return e.replace($F,"")===""}let Qc=class{constructor(t,n,r){this.normal=n,this.property=t,r&&(this.space=r)}};Qc.prototype.normal={};Qc.prototype.property={};Qc.prototype.space=void 0;function H_(e,t){const n={},r={};for(const i of e)Object.assign(n,i.property),Object.assign(r,i.normal);return new Qc(n,r,t)}function Ac(e){return e.toLowerCase()}let In=class{constructor(t,n){this.attribute=n,this.property=t}};In.prototype.attribute="";In.prototype.booleanish=!1;In.prototype.boolean=!1;In.prototype.commaOrSpaceSeparated=!1;In.prototype.commaSeparated=!1;In.prototype.defined=!1;In.prototype.mustUseProperty=!1;In.prototype.number=!1;In.prototype.overloadedBoolean=!1;In.prototype.property="";In.prototype.spaceSeparated=!1;In.prototype.space=void 0;let _F=0;const Me=_s(),Lt=_s(),jf=_s(),ce=_s(),nt=_s(),lo=_s(),Mn=_s();function _s(){return 2**++_F}const Vf=Object.freeze(Object.defineProperty({__proto__:null,boolean:Me,booleanish:Lt,commaOrSpaceSeparated:Mn,commaSeparated:lo,number:ce,overloadedBoolean:jf,spaceSeparated:nt},Symbol.toStringTag,{value:"Module"})),Lh=Object.keys(Vf);let _2=class extends In{constructor(t,n,r,i){let a=-1;if(super(t,n),Rx(this,"space",i),typeof r=="number")for(;++a<Lh.length;){const s=Lh[a];Rx(this,Lh[a],(r&Vf[s])===Vf[s])}}};_2.prototype.defined=!0;function Rx(e,t,n){n&&(e[t]=n)}function Ko(e){const t={},n={};for(const[r,i]of Object.entries(e.properties)){const a=new _2(r,e.transform(e.attributes||{},r),i,e.space);e.mustUseProperty&&e.mustUseProperty.includes(r)&&(a.mustUseProperty=!0),t[r]=a,n[Ac(r)]=r,n[Ac(a.attribute)]=r}return new Qc(t,n,e.space)}const q_=Ko({properties:{ariaActiveDescendant:null,ariaAtomic:Lt,ariaAutoComplete:null,ariaBusy:Lt,ariaChecked:Lt,ariaColCount:ce,ariaColIndex:ce,ariaColSpan:ce,ariaControls:nt,ariaCurrent:null,ariaDescribedBy:nt,ariaDetails:null,ariaDisabled:Lt,ariaDropEffect:nt,ariaErrorMessage:null,ariaExpanded:Lt,ariaFlowTo:nt,ariaGrabbed:Lt,ariaHasPopup:null,ariaHidden:Lt,ariaInvalid:null,ariaKeyShortcuts:null,ariaLabel:null,ariaLabelledBy:nt,ariaLevel:ce,ariaLive:null,ariaModal:Lt,ariaMultiLine:Lt,ariaMultiSelectable:Lt,ariaOrientation:null,ariaOwns:nt,ariaPlaceholder:null,ariaPosInSet:ce,ariaPressed:Lt,ariaReadOnly:Lt,ariaRelevant:null,ariaRequired:Lt,ariaRoleDescription:nt,ariaRowCount:ce,ariaRowIndex:ce,ariaRowSpan:ce,ariaSelected:Lt,ariaSetSize:ce,ariaSort:null,ariaValueMax:ce,ariaValueMin:ce,ariaValueNow:ce,ariaValueText:null,role:null},transform(e,t){return t==="role"?t:"aria-"+t.slice(4).toLowerCase()}});function G_(e,t){return t in e?e[t]:t}function W_(e,t){return G_(e,t.toLowerCase())}const wF=Ko({attributes:{acceptcharset:"accept-charset",classname:"class",htmlfor:"for",httpequiv:"http-equiv"},mustUseProperty:["checked","multiple","muted","selected"],properties:{abbr:null,accept:lo,acceptCharset:nt,accessKey:nt,action:null,allow:null,allowFullScreen:Me,allowPaymentRequest:Me,allowUserMedia:Me,alt:null,as:null,async:Me,autoCapitalize:null,autoComplete:nt,autoFocus:Me,autoPlay:Me,blocking:nt,capture:null,charSet:null,checked:Me,cite:null,className:nt,cols:ce,colSpan:null,content:null,contentEditable:Lt,controls:Me,controlsList:nt,coords:ce|lo,crossOrigin:null,data:null,dateTime:null,decoding:null,default:Me,defer:Me,dir:null,dirName:null,disabled:Me,download:jf,draggable:Lt,encType:null,enterKeyHint:null,fetchPriority:null,form:null,formAction:null,formEncType:null,formMethod:null,formNoValidate:Me,formTarget:null,headers:nt,height:ce,hidden:jf,high:ce,href:null,hrefLang:null,htmlFor:nt,httpEquiv:nt,id:null,imageSizes:null,imageSrcSet:null,inert:Me,inputMode:null,integrity:null,is:null,isMap:Me,itemId:null,itemProp:nt,itemRef:nt,itemScope:Me,itemType:nt,kind:null,label:null,lang:null,language:null,list:null,loading:null,loop:Me,low:ce,manifest:null,max:null,maxLength:ce,media:null,method:null,min:null,minLength:ce,multiple:Me,muted:Me,name:null,nonce:null,noModule:Me,noValidate:Me,onAbort:null,onAfterPrint:null,onAuxClick:null,onBeforeMatch:null,onBeforePrint:null,onBeforeToggle:null,onBeforeUnload:null,onBlur:null,onCancel:null,onCanPlay:null,onCanPlayThrough:null,onChange:null,onClick:null,onClose:null,onContextLost:null,onContextMenu:null,onContextRestored:null,onCopy:null,onCueChange:null,onCut:null,onDblClick:null,onDrag:null,onDragEnd:null,onDragEnter:null,onDragExit:null,onDragLeave:null,onDragOver:null,onDragStart:null,onDrop:null,onDurationChange:null,onEmptied:null,onEnded:null,onError:null,onFocus:null,onFormData:null,onHashChange:null,onInput:null,onInvalid:null,onKeyDown:null,onKeyPress:null,onKeyUp:null,onLanguageChange:null,onLoad:null,onLoadedData:null,onLoadedMetadata:null,onLoadEnd:null,onLoadStart:null,onMessage:null,onMessageError:null,onMouseDown:null,onMouseEnter:null,onMouseLeave:null,onMouseMove:null,onMouseOut:null,onMouseOver:null,onMouseUp:null,onOffline:null,onOnline:null,onPageHide:null,onPageShow:null,onPaste:null,onPause:null,onPlay:null,onPlaying:null,onPopState:null,onProgress:null,onRateChange:null,onRejectionHandled:null,onReset:null,onResize:null,onScroll:null,onScrollEnd:null,onSecurityPolicyViolation:null,onSeeked:null,onSeeking:null,onSelect:null,onSlotChange:null,onStalled:null,onStorage:null,onSubmit:null,onSuspend:null,onTimeUpdate:null,onToggle:null,onUnhandledRejection:null,onUnload:null,onVolumeChange:null,onWaiting:null,onWheel:null,open:Me,optimum:ce,pattern:null,ping:nt,placeholder:null,playsInline:Me,popover:null,popoverTarget:null,popoverTargetAction:null,poster:null,preload:null,readOnly:Me,referrerPolicy:null,rel:nt,required:Me,reversed:Me,rows:ce,rowSpan:ce,sandbox:nt,scope:null,scoped:Me,seamless:Me,selected:Me,shadowRootClonable:Me,shadowRootDelegatesFocus:Me,shadowRootMode:null,shape:null,size:ce,sizes:null,slot:null,span:ce,spellCheck:Lt,src:null,srcDoc:null,srcLang:null,srcSet:null,start:ce,step:null,style:null,tabIndex:ce,target:null,title:null,translate:null,type:null,typeMustMatch:Me,useMap:null,value:Lt,width:ce,wrap:null,writingSuggestions:null,align:null,aLink:null,archive:nt,axis:null,background:null,bgColor:null,border:ce,borderColor:null,bottomMargin:ce,cellPadding:null,cellSpacing:null,char:null,charOff:null,classId:null,clear:null,code:null,codeBase:null,codeType:null,color:null,compact:Me,declare:Me,event:null,face:null,frame:null,frameBorder:null,hSpace:ce,leftMargin:ce,link:null,longDesc:null,lowSrc:null,marginHeight:ce,marginWidth:ce,noResize:Me,noHref:Me,noShade:Me,noWrap:Me,object:null,profile:null,prompt:null,rev:null,rightMargin:ce,rules:null,scheme:null,scrolling:Lt,standby:null,summary:null,text:null,topMargin:ce,valueType:null,version:null,vAlign:null,vLink:null,vSpace:ce,allowTransparency:null,autoCorrect:null,autoSave:null,disablePictureInPicture:Me,disableRemotePlayback:Me,prefix:null,property:null,results:ce,security:null,unselectable:null},space:"html",transform:W_}),TF=Ko({attributes:{accentHeight:"accent-height",alignmentBaseline:"alignment-baseline",arabicForm:"arabic-form",baselineShift:"baseline-shift",capHeight:"cap-height",className:"class",clipPath:"clip-path",clipRule:"clip-rule",colorInterpolation:"color-interpolation",colorInterpolationFilters:"color-interpolation-filters",colorProfile:"color-profile",colorRendering:"color-rendering",crossOrigin:"crossorigin",dataType:"datatype",dominantBaseline:"dominant-baseline",enableBackground:"enable-background",fillOpacity:"fill-opacity",fillRule:"fill-rule",floodColor:"flood-color",floodOpacity:"flood-opacity",fontFamily:"font-family",fontSize:"font-size",fontSizeAdjust:"font-size-adjust",fontStretch:"font-stretch",fontStyle:"font-style",fontVariant:"font-variant",fontWeight:"font-weight",glyphName:"glyph-name",glyphOrientationHorizontal:"glyph-orientation-horizontal",glyphOrientationVertical:"glyph-orientation-vertical",hrefLang:"hreflang",horizAdvX:"horiz-adv-x",horizOriginX:"horiz-origin-x",horizOriginY:"horiz-origin-y",imageRendering:"image-rendering",letterSpacing:"letter-spacing",lightingColor:"lighting-color",markerEnd:"marker-end",markerMid:"marker-mid",markerStart:"marker-start",navDown:"nav-down",navDownLeft:"nav-down-left",navDownRight:"nav-down-right",navLeft:"nav-left",navNext:"nav-next",navPrev:"nav-prev",navRight:"nav-right",navUp:"nav-up",navUpLeft:"nav-up-left",navUpRight:"nav-up-right",onAbort:"onabort",onActivate:"onactivate",onAfterPrint:"onafterprint",onBeforePrint:"onbeforeprint",onBegin:"onbegin",onCancel:"oncancel",onCanPlay:"oncanplay",onCanPlayThrough:"oncanplaythrough",onChange:"onchange",onClick:"onclick",onClose:"onclose",onCopy:"oncopy",onCueChange:"oncuechange",onCut:"oncut",onDblClick:"ondblclick",onDrag:"ondrag",onDragEnd:"ondragend",onDragEnter:"ondragenter",onDragExit:"ondragexit",onDragLeave:"ondragleave",onDragOver:"ondragover",onDragStart:"ondragstart",onDrop:"ondrop",onDurationChange:"ondurationchange",onEmptied:"onemptied",onEnd:"onend",onEnded:"onended",onError:"onerror",onFocus:"onfocus",onFocusIn:"onfocusin",onFocusOut:"onfocusout",onHashChange:"onhashchange",onInput:"oninput",onInvalid:"oninvalid",onKeyDown:"onkeydown",onKeyPress:"onkeypress",onKeyUp:"onkeyup",onLoad:"onload",onLoadedData:"onloadeddata",onLoadedMetadata:"onloadedmetadata",onLoadStart:"onloadstart",onMessage:"onmessage",onMouseDown:"onmousedown",onMouseEnter:"onmouseenter",onMouseLeave:"onmouseleave",onMouseMove:"onmousemove",onMouseOut:"onmouseout",onMouseOver:"onmouseover",onMouseUp:"onmouseup",onMouseWheel:"onmousewheel",onOffline:"onoffline",onOnline:"ononline",onPageHide:"onpagehide",onPageShow:"onpageshow",onPaste:"onpaste",onPause:"onpause",onPlay:"onplay",onPlaying:"onplaying",onPopState:"onpopstate",onProgress:"onprogress",onRateChange:"onratechange",onRepeat:"onrepeat",onReset:"onreset",onResize:"onresize",onScroll:"onscroll",onSeeked:"onseeked",onSeeking:"onseeking",onSelect:"onselect",onShow:"onshow",onStalled:"onstalled",onStorage:"onstorage",onSubmit:"onsubmit",onSuspend:"onsuspend",onTimeUpdate:"ontimeupdate",onToggle:"ontoggle",onUnload:"onunload",onVolumeChange:"onvolumechange",onWaiting:"onwaiting",onZoom:"onzoom",overlinePosition:"overline-position",overlineThickness:"overline-thickness",paintOrder:"paint-order",panose1:"panose-1",pointerEvents:"pointer-events",referrerPolicy:"referrerpolicy",renderingIntent:"rendering-intent",shapeRendering:"shape-rendering",stopColor:"stop-color",stopOpacity:"stop-opacity",strikethroughPosition:"strikethrough-position",strikethroughThickness:"strikethrough-thickness",strokeDashArray:"stroke-dasharray",strokeDashOffset:"stroke-dashoffset",strokeLineCap:"stroke-linecap",strokeLineJoin:"stroke-linejoin",strokeMiterLimit:"stroke-miterlimit",strokeOpacity:"stroke-opacity",strokeWidth:"stroke-width",tabIndex:"tabindex",textAnchor:"text-anchor",textDecoration:"text-decoration",textRendering:"text-rendering",transformOrigin:"transform-origin",typeOf:"typeof",underlinePosition:"underline-position",underlineThickness:"underline-thickness",unicodeBidi:"unicode-bidi",unicodeRange:"unicode-range",unitsPerEm:"units-per-em",vAlphabetic:"v-alphabetic",vHanging:"v-hanging",vIdeographic:"v-ideographic",vMathematical:"v-mathematical",vectorEffect:"vector-effect",vertAdvY:"vert-adv-y",vertOriginX:"vert-origin-x",vertOriginY:"vert-origin-y",wordSpacing:"word-spacing",writingMode:"writing-mode",xHeight:"x-height",playbackOrder:"playbackorder",timelineBegin:"timelinebegin"},properties:{about:Mn,accentHeight:ce,accumulate:null,additive:null,alignmentBaseline:null,alphabetic:ce,amplitude:ce,arabicForm:null,ascent:ce,attributeName:null,attributeType:null,azimuth:ce,bandwidth:null,baselineShift:null,baseFrequency:null,baseProfile:null,bbox:null,begin:null,bias:ce,by:null,calcMode:null,capHeight:ce,className:nt,clip:null,clipPath:null,clipPathUnits:null,clipRule:null,color:null,colorInterpolation:null,colorInterpolationFilters:null,colorProfile:null,colorRendering:null,content:null,contentScriptType:null,contentStyleType:null,crossOrigin:null,cursor:null,cx:null,cy:null,d:null,dataType:null,defaultAction:null,descent:ce,diffuseConstant:ce,direction:null,display:null,dur:null,divisor:ce,dominantBaseline:null,download:Me,dx:null,dy:null,edgeMode:null,editable:null,elevation:ce,enableBackground:null,end:null,event:null,exponent:ce,externalResourcesRequired:null,fill:null,fillOpacity:ce,fillRule:null,filter:null,filterRes:null,filterUnits:null,floodColor:null,floodOpacity:null,focusable:null,focusHighlight:null,fontFamily:null,fontSize:null,fontSizeAdjust:null,fontStretch:null,fontStyle:null,fontVariant:null,fontWeight:null,format:null,fr:null,from:null,fx:null,fy:null,g1:lo,g2:lo,glyphName:lo,glyphOrientationHorizontal:null,glyphOrientationVertical:null,glyphRef:null,gradientTransform:null,gradientUnits:null,handler:null,hanging:ce,hatchContentUnits:null,hatchUnits:null,height:null,href:null,hrefLang:null,horizAdvX:ce,horizOriginX:ce,horizOriginY:ce,id:null,ideographic:ce,imageRendering:null,initialVisibility:null,in:null,in2:null,intercept:ce,k:ce,k1:ce,k2:ce,k3:ce,k4:ce,kernelMatrix:Mn,kernelUnitLength:null,keyPoints:null,keySplines:null,keyTimes:null,kerning:null,lang:null,lengthAdjust:null,letterSpacing:null,lightingColor:null,limitingConeAngle:ce,local:null,markerEnd:null,markerMid:null,markerStart:null,markerHeight:null,markerUnits:null,markerWidth:null,mask:null,maskContentUnits:null,maskUnits:null,mathematical:null,max:null,media:null,mediaCharacterEncoding:null,mediaContentEncodings:null,mediaSize:ce,mediaTime:null,method:null,min:null,mode:null,name:null,navDown:null,navDownLeft:null,navDownRight:null,navLeft:null,navNext:null,navPrev:null,navRight:null,navUp:null,navUpLeft:null,navUpRight:null,numOctaves:null,observer:null,offset:null,onAbort:null,onActivate:null,onAfterPrint:null,onBeforePrint:null,onBegin:null,onCancel:null,onCanPlay:null,onCanPlayThrough:null,onChange:null,onClick:null,onClose:null,onCopy:null,onCueChange:null,onCut:null,onDblClick:null,onDrag:null,onDragEnd:null,onDragEnter:null,onDragExit:null,onDragLeave:null,onDragOver:null,onDragStart:null,onDrop:null,onDurationChange:null,onEmptied:null,onEnd:null,onEnded:null,onError:null,onFocus:null,onFocusIn:null,onFocusOut:null,onHashChange:null,onInput:null,onInvalid:null,onKeyDown:null,onKeyPress:null,onKeyUp:null,onLoad:null,onLoadedData:null,onLoadedMetadata:null,onLoadStart:null,onMessage:null,onMouseDown:null,onMouseEnter:null,onMouseLeave:null,onMouseMove:null,onMouseOut:null,onMouseOver:null,onMouseUp:null,onMouseWheel:null,onOffline:null,onOnline:null,onPageHide:null,onPageShow:null,onPaste:null,onPause:null,onPlay:null,onPlaying:null,onPopState:null,onProgress:null,onRateChange:null,onRepeat:null,onReset:null,onResize:null,onScroll:null,onSeeked:null,onSeeking:null,onSelect:null,onShow:null,onStalled:null,onStorage:null,onSubmit:null,onSuspend:null,onTimeUpdate:null,onToggle:null,onUnload:null,onVolumeChange:null,onWaiting:null,onZoom:null,opacity:null,operator:null,order:null,orient:null,orientation:null,origin:null,overflow:null,overlay:null,overlinePosition:ce,overlineThickness:ce,paintOrder:null,panose1:null,path:null,pathLength:ce,patternContentUnits:null,patternTransform:null,patternUnits:null,phase:null,ping:nt,pitch:null,playbackOrder:null,pointerEvents:null,points:null,pointsAtX:ce,pointsAtY:ce,pointsAtZ:ce,preserveAlpha:null,preserveAspectRatio:null,primitiveUnits:null,propagate:null,property:Mn,r:null,radius:null,referrerPolicy:null,refX:null,refY:null,rel:Mn,rev:Mn,renderingIntent:null,repeatCount:null,repeatDur:null,requiredExtensions:Mn,requiredFeatures:Mn,requiredFonts:Mn,requiredFormats:Mn,resource:null,restart:null,result:null,rotate:null,rx:null,ry:null,scale:null,seed:null,shapeRendering:null,side:null,slope:null,snapshotTime:null,specularConstant:ce,specularExponent:ce,spreadMethod:null,spacing:null,startOffset:null,stdDeviation:null,stemh:null,stemv:null,stitchTiles:null,stopColor:null,stopOpacity:null,strikethroughPosition:ce,strikethroughThickness:ce,string:null,stroke:null,strokeDashArray:Mn,strokeDashOffset:null,strokeLineCap:null,strokeLineJoin:null,strokeMiterLimit:ce,strokeOpacity:ce,strokeWidth:null,style:null,surfaceScale:ce,syncBehavior:null,syncBehaviorDefault:null,syncMaster:null,syncTolerance:null,syncToleranceDefault:null,systemLanguage:Mn,tabIndex:ce,tableValues:null,target:null,targetX:ce,targetY:ce,textAnchor:null,textDecoration:null,textRendering:null,textLength:null,timelineBegin:null,title:null,transformBehavior:null,type:null,typeOf:Mn,to:null,transform:null,transformOrigin:null,u1:null,u2:null,underlinePosition:ce,underlineThickness:ce,unicode:null,unicodeBidi:null,unicodeRange:null,unitsPerEm:ce,values:null,vAlphabetic:ce,vMathematical:ce,vectorEffect:null,vHanging:ce,vIdeographic:ce,version:null,vertAdvY:ce,vertOriginX:ce,vertOriginY:ce,viewBox:null,viewTarget:null,visibility:null,width:null,widths:null,wordSpacing:null,writingMode:null,x:null,x1:null,x2:null,xChannelSelector:null,xHeight:ce,y:null,y1:null,y2:null,yChannelSelector:null,z:null,zoomAndPan:null},space:"svg",transform:G_}),K_=Ko({properties:{xLinkActuate:null,xLinkArcRole:null,xLinkHref:null,xLinkRole:null,xLinkShow:null,xLinkTitle:null,xLinkType:null},space:"xlink",transform(e,t){return"xlink:"+t.slice(5).toLowerCase()}}),Y_=Ko({attributes:{xmlnsxlink:"xmlns:xlink"},properties:{xmlnsXLink:null,xmlns:null},space:"xmlns",transform:W_}),X_=Ko({properties:{xmlBase:null,xmlLang:null,xmlSpace:null},space:"xml",transform(e,t){return"xml:"+t.slice(3).toLowerCase()}}),EF={classId:"classID",dataType:"datatype",itemId:"itemID",strokeDashArray:"strokeDasharray",strokeDashOffset:"strokeDashoffset",strokeLineCap:"strokeLinecap",strokeLineJoin:"strokeLinejoin",strokeMiterLimit:"strokeMiterlimit",typeOf:"typeof",xLinkActuate:"xlinkActuate",xLinkArcRole:"xlinkArcrole",xLinkHref:"xlinkHref",xLinkRole:"xlinkRole",xLinkShow:"xlinkShow",xLinkTitle:"xlinkTitle",xLinkType:"xlinkType",xmlnsXLink:"xmlnsXlink"},kF=/[A-Z]/g,Ox=/-[a-z]/g,SF=/^data[-\w.:]+$/i;function w2(e,t){const n=Ac(t);let r=t,i=In;if(n in e.normal)return e.property[e.normal[n]];if(n.length>4&&n.slice(0,4)==="data"&&SF.test(t)){if(t.charAt(4)==="-"){const a=t.slice(5).replace(Ox,CF);r="data"+a.charAt(0).toUpperCase()+a.slice(1)}else{const a=t.slice(4);if(!Ox.test(a)){let s=a.replace(kF,NF);s.charAt(0)!=="-"&&(s="-"+s),t="data"+s}}i=_2}return new i(r,t)}function NF(e){return"-"+e.toLowerCase()}function CF(e){return e.charAt(1).toUpperCase()}const um=H_([q_,wF,K_,Y_,X_],"html"),Yo=H_([q_,TF,K_,Y_,X_],"svg");function zx(e){const t=String(e||"").trim();return t?t.split(/[ \t\n\r\f]+/g):[]}function Q_(e){return e.join(" ").trim()}var T2={},Bx=/\/\*[^*]*\*+([^/*][^*]*\*+)*\//g,AF=/\n/g,LF=/^\s*/,PF=/^(\*?[-#/*\\\w]+(\[[0-9a-z_-]+\])?)\s*/,IF=/^:\s*/,UF=/^((?:'(?:\\'|.)*?'|"(?:\\"|.)*?"|\([^)]*?\)|[^};])+)/,DF=/^[;\s]*/,MF=/^\s+|\s+$/g,RF=`
`,Fx="/",jx="*",Ya="",OF="comment",zF="declaration",BF=function(e,t){if(typeof e!="string")throw new TypeError("First argument must be a string");if(!e)return[];t=t||{};var n=1,r=1;function i(x){var g=x.match(AF);g&&(n+=g.length);var w=x.lastIndexOf(RF);r=~w?x.length-w:r+x.length}function a(){var x={line:n,column:r};return function(g){return g.position=new s(x),c(),g}}function s(x){this.start=x,this.end={line:n,column:r},this.source=t.source}s.prototype.content=e;function o(x){var g=new Error(t.source+":"+n+":"+r+": "+x);if(g.reason=x,g.filename=t.source,g.line=n,g.column=r,g.source=e,!t.silent)throw g}function l(x){var g=x.exec(e);if(g){var w=g[0];return i(w),e=e.slice(w.length),g}}function c(){l(LF)}function u(x){var g;for(x=x||[];g=d();)g!==!1&&x.push(g);return x}function d(){var x=a();if(!(Fx!=e.charAt(0)||jx!=e.charAt(1))){for(var g=2;Ya!=e.charAt(g)&&(jx!=e.charAt(g)||Fx!=e.charAt(g+1));)++g;if(g+=2,Ya===e.charAt(g-1))return o("End of comment missing");var w=e.slice(2,g-2);return r+=2,i(w),e=e.slice(g),r+=2,x({type:OF,comment:w})}}function m(){var x=a(),g=l(PF);if(g){if(d(),!l(IF))return o("property missing ':'");var w=l(UF),v=x({type:zF,property:Vx(g[0].replace(Bx,Ya)),value:w?Vx(w[0].replace(Bx,Ya)):Ya});return l(DF),v}}function p(){var x=[];u(x);for(var g;g=m();)g!==!1&&(x.push(g),u(x));return x}return c(),p()};function Vx(e){return e?e.replace(MF,Ya):Ya}var FF=kd&&kd.__importDefault||function(e){return e&&e.__esModule?e:{default:e}};Object.defineProperty(T2,"__esModule",{value:!0});T2.default=VF;var jF=FF(BF);function VF(e,t){var n=null;if(!e||typeof e!="string")return n;var r=(0,jF.default)(e),i=typeof t=="function";return r.forEach(function(a){if(a.type==="declaration"){var s=a.property,o=a.value;i?t(s,o,a):o&&(n=n||{},n[s]=o)}}),n}var dm={};Object.defineProperty(dm,"__esModule",{value:!0});dm.camelCase=void 0;var HF=/^--[a-zA-Z0-9_-]+$/,qF=/-([a-z])/g,GF=/^[^-]+$/,WF=/^-(webkit|moz|ms|o|khtml)-/,KF=/^-(ms)-/,YF=function(e){return!e||GF.test(e)||HF.test(e)},XF=function(e,t){return t.toUpperCase()},Hx=function(e,t){return"".concat(t,"-")},QF=function(e,t){return t===void 0&&(t={}),YF(e)?e:(e=e.toLowerCase(),t.reactCompat?e=e.replace(KF,Hx):e=e.replace(WF,Hx),e.replace(qF,XF))};dm.camelCase=QF;var ZF=kd&&kd.__importDefault||function(e){return e&&e.__esModule?e:{default:e}},JF=ZF(T2),ej=dm;function Hf(e,t){var n={};return!e||typeof e!="string"||(0,JF.default)(e,function(r,i){r&&i&&(n[(0,ej.camelCase)(r,t)]=i)}),n}Hf.default=Hf;var tj=Hf;const nj=x0(tj),mm=Z_("end"),ui=Z_("start");function Z_(e){return t;function t(n){const r=n&&n.position&&n.position[e]||{};if(typeof r.line=="number"&&r.line>0&&typeof r.column=="number"&&r.column>0)return{line:r.line,column:r.column,offset:typeof r.offset=="number"&&r.offset>-1?r.offset:void 0}}}function rj(e){const t=ui(e),n=mm(e);if(t&&n)return{start:t,end:n}}function jl(e){return!e||typeof e!="object"?"":"position"in e||"type"in e?qx(e.position):"start"in e||"end"in e?qx(e):"line"in e||"column"in e?qf(e):""}function qf(e){return Gx(e&&e.line)+":"+Gx(e&&e.column)}function qx(e){return qf(e&&e.start)+"-"+qf(e&&e.end)}function Gx(e){return e&&typeof e=="number"?e:1}class on extends Error{constructor(t,n,r){super(),typeof n=="string"&&(r=n,n=void 0);let i="",a={},s=!1;if(n&&("line"in n&&"column"in n?a={place:n}:"start"in n&&"end"in n?a={place:n}:"type"in n?a={ancestors:[n],place:n.position}:a={...n}),typeof t=="string"?i=t:!a.cause&&t&&(s=!0,i=t.message,a.cause=t),!a.ruleId&&!a.source&&typeof r=="string"){const l=r.indexOf(":");l===-1?a.ruleId=r:(a.source=r.slice(0,l),a.ruleId=r.slice(l+1))}if(!a.place&&a.ancestors&&a.ancestors){const l=a.ancestors[a.ancestors.length-1];l&&(a.place=l.position)}const o=a.place&&"start"in a.place?a.place.start:a.place;this.ancestors=a.ancestors||void 0,this.cause=a.cause||void 0,this.column=o?o.column:void 0,this.fatal=void 0,this.file,this.message=i,this.line=o?o.line:void 0,this.name=jl(a.place)||"1:1",this.place=a.place||void 0,this.reason=this.message,this.ruleId=a.ruleId||void 0,this.source=a.source||void 0,this.stack=s&&a.cause&&typeof a.cause.stack=="string"?a.cause.stack:"",this.actual,this.expected,this.note,this.url}}on.prototype.file="";on.prototype.name="";on.prototype.reason="";on.prototype.message="";on.prototype.stack="";on.prototype.column=void 0;on.prototype.line=void 0;on.prototype.ancestors=void 0;on.prototype.cause=void 0;on.prototype.fatal=void 0;on.prototype.place=void 0;on.prototype.ruleId=void 0;on.prototype.source=void 0;const E2={}.hasOwnProperty,ij=new Map,aj=/[A-Z]/g,sj=new Set(["table","tbody","thead","tfoot","tr"]),oj=new Set(["td","th"]),J_="https://github.com/syntax-tree/hast-util-to-jsx-runtime";function lj(e,t){if(!t||t.Fragment===void 0)throw new TypeError("Expected `Fragment` in options");const n=t.filePath||void 0;let r;if(t.development){if(typeof t.jsxDEV!="function")throw new TypeError("Expected `jsxDEV` in options when `development: true`");r=gj(n,t.jsxDEV)}else{if(typeof t.jsx!="function")throw new TypeError("Expected `jsx` in production options");if(typeof t.jsxs!="function")throw new TypeError("Expected `jsxs` in production options");r=fj(n,t.jsx,t.jsxs)}const i={Fragment:t.Fragment,ancestors:[],components:t.components||{},create:r,elementAttributeNameCase:t.elementAttributeNameCase||"react",evaluater:t.createEvaluater?t.createEvaluater():void 0,filePath:n,ignoreInvalidStyle:t.ignoreInvalidStyle||!1,passKeys:t.passKeys!==!1,passNode:t.passNode||!1,schema:t.space==="svg"?Yo:um,stylePropertyNameCase:t.stylePropertyNameCase||"dom",tableCellAlignToStyle:t.tableCellAlignToStyle!==!1},a=ew(i,e,void 0);return a&&typeof a!="string"?a:i.create(e,i.Fragment,{children:a||void 0},void 0)}function ew(e,t,n){if(t.type==="element")return cj(e,t,n);if(t.type==="mdxFlowExpression"||t.type==="mdxTextExpression")return uj(e,t);if(t.type==="mdxJsxFlowElement"||t.type==="mdxJsxTextElement")return mj(e,t,n);if(t.type==="mdxjsEsm")return dj(e,t);if(t.type==="root")return hj(e,t,n);if(t.type==="text")return pj(e,t)}function cj(e,t,n){const r=e.schema;let i=r;t.tagName.toLowerCase()==="svg"&&r.space==="html"&&(i=Yo,e.schema=i),e.ancestors.push(t);const a=nw(e,t.tagName,!1),s=bj(e,t);let o=S2(e,t);return sj.has(t.tagName)&&(o=o.filter(function(l){return typeof l=="string"?!yF(l):!0})),tw(e,s,a,t),k2(s,o),e.ancestors.pop(),e.schema=r,e.create(t,a,s,n)}function uj(e,t){if(t.data&&t.data.estree&&e.evaluater){const r=t.data.estree.body[0];return r.type,e.evaluater.evaluateExpression(r.expression)}Lc(e,t.position)}function dj(e,t){if(t.data&&t.data.estree&&e.evaluater)return e.evaluater.evaluateProgram(t.data.estree);Lc(e,t.position)}function mj(e,t,n){const r=e.schema;let i=r;t.name==="svg"&&r.space==="html"&&(i=Yo,e.schema=i),e.ancestors.push(t);const a=t.name===null?e.Fragment:nw(e,t.name,!0),s=vj(e,t),o=S2(e,t);return tw(e,s,a,t),k2(s,o),e.ancestors.pop(),e.schema=r,e.create(t,a,s,n)}function hj(e,t,n){const r={};return k2(r,S2(e,t)),e.create(t,e.Fragment,r,n)}function pj(e,t){return t.value}function tw(e,t,n,r){typeof n!="string"&&n!==e.Fragment&&e.passNode&&(t.node=r)}function k2(e,t){if(t.length>0){const n=t.length>1?t:t[0];n&&(e.children=n)}}function fj(e,t,n){return r;function r(i,a,s,o){const c=Array.isArray(s.children)?n:t;return o?c(a,s,o):c(a,s)}}function gj(e,t){return n;function n(r,i,a,s){const o=Array.isArray(a.children),l=ui(r);return t(i,a,s,o,{columnNumber:l?l.column-1:void 0,fileName:e,lineNumber:l?l.line:void 0},void 0)}}function bj(e,t){const n={};let r,i;for(i in t.properties)if(i!=="children"&&E2.call(t.properties,i)){const a=xj(e,i,t.properties[i]);if(a){const[s,o]=a;e.tableCellAlignToStyle&&s==="align"&&typeof o=="string"&&oj.has(t.tagName)?r=o:n[s]=o}}if(r){const a=n.style||(n.style={});a[e.stylePropertyNameCase==="css"?"text-align":"textAlign"]=r}return n}function vj(e,t){const n={};for(const r of t.attributes)if(r.type==="mdxJsxExpressionAttribute")if(r.data&&r.data.estree&&e.evaluater){const a=r.data.estree.body[0];a.type;const s=a.expression;s.type;const o=s.properties[0];o.type,Object.assign(n,e.evaluater.evaluateExpression(o.argument))}else Lc(e,t.position);else{const i=r.name;let a;if(r.value&&typeof r.value=="object")if(r.value.data&&r.value.data.estree&&e.evaluater){const o=r.value.data.estree.body[0];o.type,a=e.evaluater.evaluateExpression(o.expression)}else Lc(e,t.position);else a=r.value===null?!0:r.value;n[i]=a}return n}function S2(e,t){const n=[];let r=-1;const i=e.passKeys?new Map:ij;for(;++r<t.children.length;){const a=t.children[r];let s;if(e.passKeys){const l=a.type==="element"?a.tagName:a.type==="mdxJsxFlowElement"||a.type==="mdxJsxTextElement"?a.name:void 0;if(l){const c=i.get(l)||0;s=l+"-"+c,i.set(l,c+1)}}const o=ew(e,a,s);o!==void 0&&n.push(o)}return n}function xj(e,t,n){const r=w2(e.schema,t);if(!(n==null||typeof n=="number"&&Number.isNaN(n))){if(Array.isArray(n)&&(n=r.commaSeparated?V_(n):Q_(n)),r.property==="style"){let i=typeof n=="object"?n:$j(e,String(n));return e.stylePropertyNameCase==="css"&&(i=yj(i)),["style",i]}return[e.elementAttributeNameCase==="react"&&r.space?EF[r.property]||r.property:r.attribute,n]}}function $j(e,t){try{return nj(t,{reactCompat:!0})}catch(n){if(e.ignoreInvalidStyle)return{};const r=n,i=new on("Cannot parse `style` attribute",{ancestors:e.ancestors,cause:r,ruleId:"style",source:"hast-util-to-jsx-runtime"});throw i.file=e.filePath||void 0,i.url=J_+"#cannot-parse-style-attribute",i}}function nw(e,t,n){let r;if(!n)r={type:"Literal",value:t};else if(t.includes(".")){const i=t.split(".");let a=-1,s;for(;++a<i.length;){const o=Dx(i[a])?{type:"Identifier",name:i[a]}:{type:"Literal",value:i[a]};s=s?{type:"MemberExpression",object:s,property:o,computed:!!(a&&o.type==="Literal"),optional:!1}:o}r=s}else r=Dx(t)&&!/^[a-z]/.test(t)?{type:"Identifier",name:t}:{type:"Literal",value:t};if(r.type==="Literal"){const i=r.value;return E2.call(e.components,i)?e.components[i]:i}if(e.evaluater)return e.evaluater.evaluateExpression(r);Lc(e)}function Lc(e,t){const n=new on("Cannot handle MDX estrees without `createEvaluater`",{ancestors:e.ancestors,place:t,ruleId:"mdx-estree",source:"hast-util-to-jsx-runtime"});throw n.file=e.filePath||void 0,n.url=J_+"#cannot-handle-mdx-estrees-without-createevaluater",n}function yj(e){const t={};let n;for(n in e)E2.call(e,n)&&(t[_j(n)]=e[n]);return t}function _j(e){let t=e.replace(aj,wj);return t.slice(0,3)==="ms-"&&(t="-"+t),t}function wj(e){return"-"+e.toLowerCase()}const Ph={action:["form"],cite:["blockquote","del","ins","q"],data:["object"],formAction:["button","input"],href:["a","area","base","link"],icon:["menuitem"],itemId:null,manifest:["html"],ping:["a","area"],poster:["video"],src:["audio","embed","iframe","img","input","script","source","track","video"]},Tj={};function Ej(e,t){const n=Tj,r=typeof n.includeImageAlt=="boolean"?n.includeImageAlt:!0,i=typeof n.includeHtml=="boolean"?n.includeHtml:!0;return rw(e,r,i)}function rw(e,t,n){if(kj(e)){if("value"in e)return e.type==="html"&&!n?"":e.value;if(t&&"alt"in e&&e.alt)return e.alt;if("children"in e)return Wx(e.children,t,n)}return Array.isArray(e)?Wx(e,t,n):""}function Wx(e,t,n){const r=[];let i=-1;for(;++i<e.length;)r[i]=rw(e[i],t,n);return r.join("")}function kj(e){return!!(e&&typeof e=="object")}const Kx=document.createElement("i");function N2(e){const t="&"+e+";";Kx.innerHTML=t;const n=Kx.textContent;return n.charCodeAt(n.length-1)===59&&e!=="semi"||n===t?!1:n}function li(e,t,n,r){const i=e.length;let a=0,s;if(t<0?t=-t>i?0:i+t:t=t>i?i:t,n=n>0?n:0,r.length<1e4)s=Array.from(r),s.unshift(t,n),e.splice(...s);else for(n&&e.splice(t,n);a<r.length;)s=r.slice(a,a+1e4),s.unshift(t,0),e.splice(...s),a+=1e4,t+=1e4}function ir(e,t){return e.length>0?(li(e,e.length,0,t),e):t}const Yx={}.hasOwnProperty;function Sj(e){const t={};let n=-1;for(;++n<e.length;)Nj(t,e[n]);return t}function Nj(e,t){let n;for(n in t){const i=(Yx.call(e,n)?e[n]:void 0)||(e[n]={}),a=t[n];let s;if(a)for(s in a){Yx.call(i,s)||(i[s]=[]);const o=a[s];Cj(i[s],Array.isArray(o)?o:o?[o]:[])}}}function Cj(e,t){let n=-1;const r=[];for(;++n<t.length;)(t[n].add==="after"?e:r).push(t[n]);li(e,0,0,r)}function iw(e,t){const n=Number.parseInt(e,t);return n<9||n===11||n>13&&n<32||n>126&&n<160||n>55295&&n<57344||n>64975&&n<65008||(n&65535)===65535||(n&65535)===65534||n>1114111?"�":String.fromCodePoint(n)}function co(e){return e.replace(/[\t\n\r ]+/g," ").replace(/^ | $/g,"").toLowerCase().toUpperCase()}const Zr=Ba(/[A-Za-z]/),qn=Ba(/[\dA-Za-z]/),Aj=Ba(/[#-'*+\--9=?A-Z^-~]/);function Gf(e){return e!==null&&(e<32||e===127)}const Wf=Ba(/\d/),Lj=Ba(/[\dA-Fa-f]/),Pj=Ba(/[!-/:-@[-`{-~]/);function $e(e){return e!==null&&e<-2}function Ln(e){return e!==null&&(e<0||e===32)}function Ke(e){return e===-2||e===-1||e===32}const Ij=Ba(new RegExp("\\p{P}|\\p{S}","u")),Uj=Ba(/\s/);function Ba(e){return t;function t(n){return n!==null&&n>-1&&e.test(String.fromCharCode(n))}}function Xo(e){const t=[];let n=-1,r=0,i=0;for(;++n<e.length;){const a=e.charCodeAt(n);let s="";if(a===37&&qn(e.charCodeAt(n+1))&&qn(e.charCodeAt(n+2)))i=2;else if(a<128)/[!#$&-;=?-Z_a-z~]/.test(String.fromCharCode(a))||(s=String.fromCharCode(a));else if(a>55295&&a<57344){const o=e.charCodeAt(n+1);a<56320&&o>56319&&o<57344?(s=String.fromCharCode(a,o),i=1):s="�"}else s=String.fromCharCode(a);s&&(t.push(e.slice(r,n),encodeURIComponent(s)),r=n+i+1,s=""),i&&(n+=i,i=0)}return t.join("")+e.slice(r)}function Ze(e,t,n,r){const i=r?r-1:Number.POSITIVE_INFINITY;let a=0;return s;function s(l){return Ke(l)?(e.enter(n),o(l)):t(l)}function o(l){return Ke(l)&&a++<i?(e.consume(l),o):(e.exit(n),t(l))}}const Dj={tokenize:Mj};function Mj(e){const t=e.attempt(this.parser.constructs.contentInitial,r,i);let n;return t;function r(o){if(o===null){e.consume(o);return}return e.enter("lineEnding"),e.consume(o),e.exit("lineEnding"),Ze(e,t,"linePrefix")}function i(o){return e.enter("paragraph"),a(o)}function a(o){const l=e.enter("chunkText",{contentType:"text",previous:n});return n&&(n.next=l),n=l,s(o)}function s(o){if(o===null){e.exit("chunkText"),e.exit("paragraph"),e.consume(o);return}return $e(o)?(e.consume(o),e.exit("chunkText"),a):(e.consume(o),s)}}const Rj={tokenize:Oj},Xx={tokenize:zj};function Oj(e){const t=this,n=[];let r=0,i,a,s;return o;function o(_){if(r<n.length){const C=n[r];return t.containerState=C[1],e.attempt(C[0].continuation,l,c)(_)}return c(_)}function l(_){if(r++,t.containerState._closeFlow){t.containerState._closeFlow=void 0,i&&$();const C=t.events.length;let k=C,S;for(;k--;)if(t.events[k][0]==="exit"&&t.events[k][1].type==="chunkFlow"){S=t.events[k][1].end;break}v(r);let L=C;for(;L<t.events.length;)t.events[L][1].end={...S},L++;return li(t.events,k+1,0,t.events.slice(C)),t.events.length=L,c(_)}return o(_)}function c(_){if(r===n.length){if(!i)return m(_);if(i.currentConstruct&&i.currentConstruct.concrete)return x(_);t.interrupt=!!(i.currentConstruct&&!i._gfmTableDynamicInterruptHack)}return t.containerState={},e.check(Xx,u,d)(_)}function u(_){return i&&$(),v(r),m(_)}function d(_){return t.parser.lazy[t.now().line]=r!==n.length,s=t.now().offset,x(_)}function m(_){return t.containerState={},e.attempt(Xx,p,x)(_)}function p(_){return r++,n.push([t.currentConstruct,t.containerState]),m(_)}function x(_){if(_===null){i&&$(),v(0),e.consume(_);return}return i=i||t.parser.flow(t.now()),e.enter("chunkFlow",{_tokenizer:i,contentType:"flow",previous:a}),g(_)}function g(_){if(_===null){w(e.exit("chunkFlow"),!0),v(0),e.consume(_);return}return $e(_)?(e.consume(_),w(e.exit("chunkFlow")),r=0,t.interrupt=void 0,o):(e.consume(_),g)}function w(_,C){const k=t.sliceStream(_);if(C&&k.push(null),_.previous=a,a&&(a.next=_),a=_,i.defineSkip(_.start),i.write(k),t.parser.lazy[_.start.line]){let S=i.events.length;for(;S--;)if(i.events[S][1].start.offset<s&&(!i.events[S][1].end||i.events[S][1].end.offset>s))return;const L=t.events.length;let U=L,F,q;for(;U--;)if(t.events[U][0]==="exit"&&t.events[U][1].type==="chunkFlow"){if(F){q=t.events[U][1].end;break}F=!0}for(v(r),S=L;S<t.events.length;)t.events[S][1].end={...q},S++;li(t.events,U+1,0,t.events.slice(L)),t.events.length=S}}function v(_){let C=n.length;for(;C-- >_;){const k=n[C];t.containerState=k[1],k[0].exit.call(t,e)}n.length=_}function $(){i.write([null]),a=void 0,i=void 0,t.containerState._closeFlow=void 0}}function zj(e,t,n){return Ze(e,e.attempt(this.parser.constructs.document,t,n),"linePrefix",this.parser.constructs.disable.null.includes("codeIndented")?void 0:4)}function Qx(e){if(e===null||Ln(e)||Uj(e))return 1;if(Ij(e))return 2}function C2(e,t,n){const r=[];let i=-1;for(;++i<e.length;){const a=e[i].resolveAll;a&&!r.includes(a)&&(t=a(t,n),r.push(a))}return t}const Kf={name:"attention",resolveAll:Bj,tokenize:Fj};function Bj(e,t){let n=-1,r,i,a,s,o,l,c,u;for(;++n<e.length;)if(e[n][0]==="enter"&&e[n][1].type==="attentionSequence"&&e[n][1]._close){for(r=n;r--;)if(e[r][0]==="exit"&&e[r][1].type==="attentionSequence"&&e[r][1]._open&&t.sliceSerialize(e[r][1]).charCodeAt(0)===t.sliceSerialize(e[n][1]).charCodeAt(0)){if((e[r][1]._close||e[n][1]._open)&&(e[n][1].end.offset-e[n][1].start.offset)%3&&!((e[r][1].end.offset-e[r][1].start.offset+e[n][1].end.offset-e[n][1].start.offset)%3))continue;l=e[r][1].end.offset-e[r][1].start.offset>1&&e[n][1].end.offset-e[n][1].start.offset>1?2:1;const d={...e[r][1].end},m={...e[n][1].start};Zx(d,-l),Zx(m,l),s={type:l>1?"strongSequence":"emphasisSequence",start:d,end:{...e[r][1].end}},o={type:l>1?"strongSequence":"emphasisSequence",start:{...e[n][1].start},end:m},a={type:l>1?"strongText":"emphasisText",start:{...e[r][1].end},end:{...e[n][1].start}},i={type:l>1?"strong":"emphasis",start:{...s.start},end:{...o.end}},e[r][1].end={...s.start},e[n][1].start={...o.end},c=[],e[r][1].end.offset-e[r][1].start.offset&&(c=ir(c,[["enter",e[r][1],t],["exit",e[r][1],t]])),c=ir(c,[["enter",i,t],["enter",s,t],["exit",s,t],["enter",a,t]]),c=ir(c,C2(t.parser.constructs.insideSpan.null,e.slice(r+1,n),t)),c=ir(c,[["exit",a,t],["enter",o,t],["exit",o,t],["exit",i,t]]),e[n][1].end.offset-e[n][1].start.offset?(u=2,c=ir(c,[["enter",e[n][1],t],["exit",e[n][1],t]])):u=0,li(e,r-1,n-r+3,c),n=r+c.length-u-2;break}}for(n=-1;++n<e.length;)e[n][1].type==="attentionSequence"&&(e[n][1].type="data");return e}function Fj(e,t){const n=this.parser.constructs.attentionMarkers.null,r=this.previous,i=Qx(r);let a;return s;function s(l){return a=l,e.enter("attentionSequence"),o(l)}function o(l){if(l===a)return e.consume(l),o;const c=e.exit("attentionSequence"),u=Qx(l),d=!u||u===2&&i||n.includes(l),m=!i||i===2&&u||n.includes(r);return c._open=!!(a===42?d:d&&(i||!m)),c._close=!!(a===42?m:m&&(u||!d)),t(l)}}function Zx(e,t){e.column+=t,e.offset+=t,e._bufferIndex+=t}const jj={name:"autolink",tokenize:Vj};function Vj(e,t,n){let r=0;return i;function i(p){return e.enter("autolink"),e.enter("autolinkMarker"),e.consume(p),e.exit("autolinkMarker"),e.enter("autolinkProtocol"),a}function a(p){return Zr(p)?(e.consume(p),s):p===64?n(p):c(p)}function s(p){return p===43||p===45||p===46||qn(p)?(r=1,o(p)):c(p)}function o(p){return p===58?(e.consume(p),r=0,l):(p===43||p===45||p===46||qn(p))&&r++<32?(e.consume(p),o):(r=0,c(p))}function l(p){return p===62?(e.exit("autolinkProtocol"),e.enter("autolinkMarker"),e.consume(p),e.exit("autolinkMarker"),e.exit("autolink"),t):p===null||p===32||p===60||Gf(p)?n(p):(e.consume(p),l)}function c(p){return p===64?(e.consume(p),u):Aj(p)?(e.consume(p),c):n(p)}function u(p){return qn(p)?d(p):n(p)}function d(p){return p===46?(e.consume(p),r=0,u):p===62?(e.exit("autolinkProtocol").type="autolinkEmail",e.enter("autolinkMarker"),e.consume(p),e.exit("autolinkMarker"),e.exit("autolink"),t):m(p)}function m(p){if((p===45||qn(p))&&r++<63){const x=p===45?m:d;return e.consume(p),x}return n(p)}}const hm={partial:!0,tokenize:Hj};function Hj(e,t,n){return r;function r(a){return Ke(a)?Ze(e,i,"linePrefix")(a):i(a)}function i(a){return a===null||$e(a)?t(a):n(a)}}const aw={continuation:{tokenize:Gj},exit:Wj,name:"blockQuote",tokenize:qj};function qj(e,t,n){const r=this;return i;function i(s){if(s===62){const o=r.containerState;return o.open||(e.enter("blockQuote",{_container:!0}),o.open=!0),e.enter("blockQuotePrefix"),e.enter("blockQuoteMarker"),e.consume(s),e.exit("blockQuoteMarker"),a}return n(s)}function a(s){return Ke(s)?(e.enter("blockQuotePrefixWhitespace"),e.consume(s),e.exit("blockQuotePrefixWhitespace"),e.exit("blockQuotePrefix"),t):(e.exit("blockQuotePrefix"),t(s))}}function Gj(e,t,n){const r=this;return i;function i(s){return Ke(s)?Ze(e,a,"linePrefix",r.parser.constructs.disable.null.includes("codeIndented")?void 0:4)(s):a(s)}function a(s){return e.attempt(aw,t,n)(s)}}function Wj(e){e.exit("blockQuote")}const sw={name:"characterEscape",tokenize:Kj};function Kj(e,t,n){return r;function r(a){return e.enter("characterEscape"),e.enter("escapeMarker"),e.consume(a),e.exit("escapeMarker"),i}function i(a){return Pj(a)?(e.enter("characterEscapeValue"),e.consume(a),e.exit("characterEscapeValue"),e.exit("characterEscape"),t):n(a)}}const ow={name:"characterReference",tokenize:Yj};function Yj(e,t,n){const r=this;let i=0,a,s;return o;function o(d){return e.enter("characterReference"),e.enter("characterReferenceMarker"),e.consume(d),e.exit("characterReferenceMarker"),l}function l(d){return d===35?(e.enter("characterReferenceMarkerNumeric"),e.consume(d),e.exit("characterReferenceMarkerNumeric"),c):(e.enter("characterReferenceValue"),a=31,s=qn,u(d))}function c(d){return d===88||d===120?(e.enter("characterReferenceMarkerHexadecimal"),e.consume(d),e.exit("characterReferenceMarkerHexadecimal"),e.enter("characterReferenceValue"),a=6,s=Lj,u):(e.enter("characterReferenceValue"),a=7,s=Wf,u(d))}function u(d){if(d===59&&i){const m=e.exit("characterReferenceValue");return s===qn&&!N2(r.sliceSerialize(m))?n(d):(e.enter("characterReferenceMarker"),e.consume(d),e.exit("characterReferenceMarker"),e.exit("characterReference"),t)}return s(d)&&i++<a?(e.consume(d),u):n(d)}}const Jx={partial:!0,tokenize:Qj},e3={concrete:!0,name:"codeFenced",tokenize:Xj};function Xj(e,t,n){const r=this,i={partial:!0,tokenize:k};let a=0,s=0,o;return l;function l(S){return c(S)}function c(S){const L=r.events[r.events.length-1];return a=L&&L[1].type==="linePrefix"?L[2].sliceSerialize(L[1],!0).length:0,o=S,e.enter("codeFenced"),e.enter("codeFencedFence"),e.enter("codeFencedFenceSequence"),u(S)}function u(S){return S===o?(s++,e.consume(S),u):s<3?n(S):(e.exit("codeFencedFenceSequence"),Ke(S)?Ze(e,d,"whitespace")(S):d(S))}function d(S){return S===null||$e(S)?(e.exit("codeFencedFence"),r.interrupt?t(S):e.check(Jx,g,C)(S)):(e.enter("codeFencedFenceInfo"),e.enter("chunkString",{contentType:"string"}),m(S))}function m(S){return S===null||$e(S)?(e.exit("chunkString"),e.exit("codeFencedFenceInfo"),d(S)):Ke(S)?(e.exit("chunkString"),e.exit("codeFencedFenceInfo"),Ze(e,p,"whitespace")(S)):S===96&&S===o?n(S):(e.consume(S),m)}function p(S){return S===null||$e(S)?d(S):(e.enter("codeFencedFenceMeta"),e.enter("chunkString",{contentType:"string"}),x(S))}function x(S){return S===null||$e(S)?(e.exit("chunkString"),e.exit("codeFencedFenceMeta"),d(S)):S===96&&S===o?n(S):(e.consume(S),x)}function g(S){return e.attempt(i,C,w)(S)}function w(S){return e.enter("lineEnding"),e.consume(S),e.exit("lineEnding"),v}function v(S){return a>0&&Ke(S)?Ze(e,$,"linePrefix",a+1)(S):$(S)}function $(S){return S===null||$e(S)?e.check(Jx,g,C)(S):(e.enter("codeFlowValue"),_(S))}function _(S){return S===null||$e(S)?(e.exit("codeFlowValue"),$(S)):(e.consume(S),_)}function C(S){return e.exit("codeFenced"),t(S)}function k(S,L,U){let F=0;return q;function q(te){return S.enter("lineEnding"),S.consume(te),S.exit("lineEnding"),G}function G(te){return S.enter("codeFencedFence"),Ke(te)?Ze(S,H,"linePrefix",r.parser.constructs.disable.null.includes("codeIndented")?void 0:4)(te):H(te)}function H(te){return te===o?(S.enter("codeFencedFenceSequence"),ne(te)):U(te)}function ne(te){return te===o?(F++,S.consume(te),ne):F>=s?(S.exit("codeFencedFenceSequence"),Ke(te)?Ze(S,K,"whitespace")(te):K(te)):U(te)}function K(te){return te===null||$e(te)?(S.exit("codeFencedFence"),L(te)):U(te)}}}function Qj(e,t,n){const r=this;return i;function i(s){return s===null?n(s):(e.enter("lineEnding"),e.consume(s),e.exit("lineEnding"),a)}function a(s){return r.parser.lazy[r.now().line]?n(s):t(s)}}const Ih={name:"codeIndented",tokenize:Jj},Zj={partial:!0,tokenize:eV};function Jj(e,t,n){const r=this;return i;function i(c){return e.enter("codeIndented"),Ze(e,a,"linePrefix",5)(c)}function a(c){const u=r.events[r.events.length-1];return u&&u[1].type==="linePrefix"&&u[2].sliceSerialize(u[1],!0).length>=4?s(c):n(c)}function s(c){return c===null?l(c):$e(c)?e.attempt(Zj,s,l)(c):(e.enter("codeFlowValue"),o(c))}function o(c){return c===null||$e(c)?(e.exit("codeFlowValue"),s(c)):(e.consume(c),o)}function l(c){return e.exit("codeIndented"),t(c)}}function eV(e,t,n){const r=this;return i;function i(s){return r.parser.lazy[r.now().line]?n(s):$e(s)?(e.enter("lineEnding"),e.consume(s),e.exit("lineEnding"),i):Ze(e,a,"linePrefix",5)(s)}function a(s){const o=r.events[r.events.length-1];return o&&o[1].type==="linePrefix"&&o[2].sliceSerialize(o[1],!0).length>=4?t(s):$e(s)?i(s):n(s)}}const tV={name:"codeText",previous:rV,resolve:nV,tokenize:iV};function nV(e){let t=e.length-4,n=3,r,i;if((e[n][1].type==="lineEnding"||e[n][1].type==="space")&&(e[t][1].type==="lineEnding"||e[t][1].type==="space")){for(r=n;++r<t;)if(e[r][1].type==="codeTextData"){e[n][1].type="codeTextPadding",e[t][1].type="codeTextPadding",n+=2,t-=2;break}}for(r=n-1,t++;++r<=t;)i===void 0?r!==t&&e[r][1].type!=="lineEnding"&&(i=r):(r===t||e[r][1].type==="lineEnding")&&(e[i][1].type="codeTextData",r!==i+2&&(e[i][1].end=e[r-1][1].end,e.splice(i+2,r-i-2),t-=r-i-2,r=i+2),i=void 0);return e}function rV(e){return e!==96||this.events[this.events.length-1][1].type==="characterEscape"}function iV(e,t,n){let r=0,i,a;return s;function s(d){return e.enter("codeText"),e.enter("codeTextSequence"),o(d)}function o(d){return d===96?(e.consume(d),r++,o):(e.exit("codeTextSequence"),l(d))}function l(d){return d===null?n(d):d===32?(e.enter("space"),e.consume(d),e.exit("space"),l):d===96?(a=e.enter("codeTextSequence"),i=0,u(d)):$e(d)?(e.enter("lineEnding"),e.consume(d),e.exit("lineEnding"),l):(e.enter("codeTextData"),c(d))}function c(d){return d===null||d===32||d===96||$e(d)?(e.exit("codeTextData"),l(d)):(e.consume(d),c)}function u(d){return d===96?(e.consume(d),i++,u):i===r?(e.exit("codeTextSequence"),e.exit("codeText"),t(d)):(a.type="codeTextData",c(d))}}class aV{constructor(t){this.left=t?[...t]:[],this.right=[]}get(t){if(t<0||t>=this.left.length+this.right.length)throw new RangeError("Cannot access index `"+t+"` in a splice buffer of size `"+(this.left.length+this.right.length)+"`");return t<this.left.length?this.left[t]:this.right[this.right.length-t+this.left.length-1]}get length(){return this.left.length+this.right.length}shift(){return this.setCursor(0),this.right.pop()}slice(t,n){const r=n??Number.POSITIVE_INFINITY;return r<this.left.length?this.left.slice(t,r):t>this.left.length?this.right.slice(this.right.length-r+this.left.length,this.right.length-t+this.left.length).reverse():this.left.slice(t).concat(this.right.slice(this.right.length-r+this.left.length).reverse())}splice(t,n,r){const i=n||0;this.setCursor(Math.trunc(t));const a=this.right.splice(this.right.length-i,Number.POSITIVE_INFINITY);return r&&pl(this.left,r),a.reverse()}pop(){return this.setCursor(Number.POSITIVE_INFINITY),this.left.pop()}push(t){this.setCursor(Number.POSITIVE_INFINITY),this.left.push(t)}pushMany(t){this.setCursor(Number.POSITIVE_INFINITY),pl(this.left,t)}unshift(t){this.setCursor(0),this.right.push(t)}unshiftMany(t){this.setCursor(0),pl(this.right,t.reverse())}setCursor(t){if(!(t===this.left.length||t>this.left.length&&this.right.length===0||t<0&&this.left.length===0))if(t<this.left.length){const n=this.left.splice(t,Number.POSITIVE_INFINITY);pl(this.right,n.reverse())}else{const n=this.right.splice(this.left.length+this.right.length-t,Number.POSITIVE_INFINITY);pl(this.left,n.reverse())}}}function pl(e,t){let n=0;if(t.length<1e4)e.push(...t);else for(;n<t.length;)e.push(...t.slice(n,n+1e4)),n+=1e4}function lw(e){const t={};let n=-1,r,i,a,s,o,l,c;const u=new aV(e);for(;++n<u.length;){for(;n in t;)n=t[n];if(r=u.get(n),n&&r[1].type==="chunkFlow"&&u.get(n-1)[1].type==="listItemPrefix"&&(l=r[1]._tokenizer.events,a=0,a<l.length&&l[a][1].type==="lineEndingBlank"&&(a+=2),a<l.length&&l[a][1].type==="content"))for(;++a<l.length&&l[a][1].type!=="content";)l[a][1].type==="chunkText"&&(l[a][1]._isInFirstContentOfListItem=!0,a++);if(r[0]==="enter")r[1].contentType&&(Object.assign(t,sV(u,n)),n=t[n],c=!0);else if(r[1]._container){for(a=n,i=void 0;a--;)if(s=u.get(a),s[1].type==="lineEnding"||s[1].type==="lineEndingBlank")s[0]==="enter"&&(i&&(u.get(i)[1].type="lineEndingBlank"),s[1].type="lineEnding",i=a);else if(!(s[1].type==="linePrefix"||s[1].type==="listItemIndent"))break;i&&(r[1].end={...u.get(i)[1].start},o=u.slice(i,n),o.unshift(r),u.splice(i,n-i+1,o))}}return li(e,0,Number.POSITIVE_INFINITY,u.slice(0)),!c}function sV(e,t){const n=e.get(t)[1],r=e.get(t)[2];let i=t-1;const a=[];let s=n._tokenizer;s||(s=r.parser[n.contentType](n.start),n._contentTypeTextTrailing&&(s._contentTypeTextTrailing=!0));const o=s.events,l=[],c={};let u,d,m=-1,p=n,x=0,g=0;const w=[g];for(;p;){for(;e.get(++i)[1]!==p;);a.push(i),p._tokenizer||(u=r.sliceStream(p),p.next||u.push(null),d&&s.defineSkip(p.start),p._isInFirstContentOfListItem&&(s._gfmTasklistFirstContentOfListItem=!0),s.write(u),p._isInFirstContentOfListItem&&(s._gfmTasklistFirstContentOfListItem=void 0)),d=p,p=p.next}for(p=n;++m<o.length;)o[m][0]==="exit"&&o[m-1][0]==="enter"&&o[m][1].type===o[m-1][1].type&&o[m][1].start.line!==o[m][1].end.line&&(g=m+1,w.push(g),p._tokenizer=void 0,p.previous=void 0,p=p.next);for(s.events=[],p?(p._tokenizer=void 0,p.previous=void 0):w.pop(),m=w.length;m--;){const v=o.slice(w[m],w[m+1]),$=a.pop();l.push([$,$+v.length-1]),e.splice($,2,v)}for(l.reverse(),m=-1;++m<l.length;)c[x+l[m][0]]=x+l[m][1],x+=l[m][1]-l[m][0]-1;return c}const oV={resolve:cV,tokenize:uV},lV={partial:!0,tokenize:dV};function cV(e){return lw(e),e}function uV(e,t){let n;return r;function r(o){return e.enter("content"),n=e.enter("chunkContent",{contentType:"content"}),i(o)}function i(o){return o===null?a(o):$e(o)?e.check(lV,s,a)(o):(e.consume(o),i)}function a(o){return e.exit("chunkContent"),e.exit("content"),t(o)}function s(o){return e.consume(o),e.exit("chunkContent"),n.next=e.enter("chunkContent",{contentType:"content",previous:n}),n=n.next,i}}function dV(e,t,n){const r=this;return i;function i(s){return e.exit("chunkContent"),e.enter("lineEnding"),e.consume(s),e.exit("lineEnding"),Ze(e,a,"linePrefix")}function a(s){if(s===null||$e(s))return n(s);const o=r.events[r.events.length-1];return!r.parser.constructs.disable.null.includes("codeIndented")&&o&&o[1].type==="linePrefix"&&o[2].sliceSerialize(o[1],!0).length>=4?t(s):e.interrupt(r.parser.constructs.flow,n,t)(s)}}function cw(e,t,n,r,i,a,s,o,l){const c=l||Number.POSITIVE_INFINITY;let u=0;return d;function d(v){return v===60?(e.enter(r),e.enter(i),e.enter(a),e.consume(v),e.exit(a),m):v===null||v===32||v===41||Gf(v)?n(v):(e.enter(r),e.enter(s),e.enter(o),e.enter("chunkString",{contentType:"string"}),g(v))}function m(v){return v===62?(e.enter(a),e.consume(v),e.exit(a),e.exit(i),e.exit(r),t):(e.enter(o),e.enter("chunkString",{contentType:"string"}),p(v))}function p(v){return v===62?(e.exit("chunkString"),e.exit(o),m(v)):v===null||v===60||$e(v)?n(v):(e.consume(v),v===92?x:p)}function x(v){return v===60||v===62||v===92?(e.consume(v),p):p(v)}function g(v){return!u&&(v===null||v===41||Ln(v))?(e.exit("chunkString"),e.exit(o),e.exit(s),e.exit(r),t(v)):u<c&&v===40?(e.consume(v),u++,g):v===41?(e.consume(v),u--,g):v===null||v===32||v===40||Gf(v)?n(v):(e.consume(v),v===92?w:g)}function w(v){return v===40||v===41||v===92?(e.consume(v),g):g(v)}}function uw(e,t,n,r,i,a){const s=this;let o=0,l;return c;function c(p){return e.enter(r),e.enter(i),e.consume(p),e.exit(i),e.enter(a),u}function u(p){return o>999||p===null||p===91||p===93&&!l||p===94&&!o&&"_hiddenFootnoteSupport"in s.parser.constructs?n(p):p===93?(e.exit(a),e.enter(i),e.consume(p),e.exit(i),e.exit(r),t):$e(p)?(e.enter("lineEnding"),e.consume(p),e.exit("lineEnding"),u):(e.enter("chunkString",{contentType:"string"}),d(p))}function d(p){return p===null||p===91||p===93||$e(p)||o++>999?(e.exit("chunkString"),u(p)):(e.consume(p),l||(l=!Ke(p)),p===92?m:d)}function m(p){return p===91||p===92||p===93?(e.consume(p),o++,d):d(p)}}function dw(e,t,n,r,i,a){let s;return o;function o(m){return m===34||m===39||m===40?(e.enter(r),e.enter(i),e.consume(m),e.exit(i),s=m===40?41:m,l):n(m)}function l(m){return m===s?(e.enter(i),e.consume(m),e.exit(i),e.exit(r),t):(e.enter(a),c(m))}function c(m){return m===s?(e.exit(a),l(s)):m===null?n(m):$e(m)?(e.enter("lineEnding"),e.consume(m),e.exit("lineEnding"),Ze(e,c,"linePrefix")):(e.enter("chunkString",{contentType:"string"}),u(m))}function u(m){return m===s||m===null||$e(m)?(e.exit("chunkString"),c(m)):(e.consume(m),m===92?d:u)}function d(m){return m===s||m===92?(e.consume(m),u):u(m)}}function Vl(e,t){let n;return r;function r(i){return $e(i)?(e.enter("lineEnding"),e.consume(i),e.exit("lineEnding"),n=!0,r):Ke(i)?Ze(e,r,n?"linePrefix":"lineSuffix")(i):t(i)}}const mV={name:"definition",tokenize:pV},hV={partial:!0,tokenize:fV};function pV(e,t,n){const r=this;let i;return a;function a(p){return e.enter("definition"),s(p)}function s(p){return uw.call(r,e,o,n,"definitionLabel","definitionLabelMarker","definitionLabelString")(p)}function o(p){return i=co(r.sliceSerialize(r.events[r.events.length-1][1]).slice(1,-1)),p===58?(e.enter("definitionMarker"),e.consume(p),e.exit("definitionMarker"),l):n(p)}function l(p){return Ln(p)?Vl(e,c)(p):c(p)}function c(p){return cw(e,u,n,"definitionDestination","definitionDestinationLiteral","definitionDestinationLiteralMarker","definitionDestinationRaw","definitionDestinationString")(p)}function u(p){return e.attempt(hV,d,d)(p)}function d(p){return Ke(p)?Ze(e,m,"whitespace")(p):m(p)}function m(p){return p===null||$e(p)?(e.exit("definition"),r.parser.defined.push(i),t(p)):n(p)}}function fV(e,t,n){return r;function r(o){return Ln(o)?Vl(e,i)(o):n(o)}function i(o){return dw(e,a,n,"definitionTitle","definitionTitleMarker","definitionTitleString")(o)}function a(o){return Ke(o)?Ze(e,s,"whitespace")(o):s(o)}function s(o){return o===null||$e(o)?t(o):n(o)}}const gV={name:"hardBreakEscape",tokenize:bV};function bV(e,t,n){return r;function r(a){return e.enter("hardBreakEscape"),e.consume(a),i}function i(a){return $e(a)?(e.exit("hardBreakEscape"),t(a)):n(a)}}const vV={name:"headingAtx",resolve:xV,tokenize:$V};function xV(e,t){let n=e.length-2,r=3,i,a;return e[r][1].type==="whitespace"&&(r+=2),n-2>r&&e[n][1].type==="whitespace"&&(n-=2),e[n][1].type==="atxHeadingSequence"&&(r===n-1||n-4>r&&e[n-2][1].type==="whitespace")&&(n-=r+1===n?2:4),n>r&&(i={type:"atxHeadingText",start:e[r][1].start,end:e[n][1].end},a={type:"chunkText",start:e[r][1].start,end:e[n][1].end,contentType:"text"},li(e,r,n-r+1,[["enter",i,t],["enter",a,t],["exit",a,t],["exit",i,t]])),e}function $V(e,t,n){let r=0;return i;function i(u){return e.enter("atxHeading"),a(u)}function a(u){return e.enter("atxHeadingSequence"),s(u)}function s(u){return u===35&&r++<6?(e.consume(u),s):u===null||Ln(u)?(e.exit("atxHeadingSequence"),o(u)):n(u)}function o(u){return u===35?(e.enter("atxHeadingSequence"),l(u)):u===null||$e(u)?(e.exit("atxHeading"),t(u)):Ke(u)?Ze(e,o,"whitespace")(u):(e.enter("atxHeadingText"),c(u))}function l(u){return u===35?(e.consume(u),l):(e.exit("atxHeadingSequence"),o(u))}function c(u){return u===null||u===35||Ln(u)?(e.exit("atxHeadingText"),o(u)):(e.consume(u),c)}}const yV=["address","article","aside","base","basefont","blockquote","body","caption","center","col","colgroup","dd","details","dialog","dir","div","dl","dt","fieldset","figcaption","figure","footer","form","frame","frameset","h1","h2","h3","h4","h5","h6","head","header","hr","html","iframe","legend","li","link","main","menu","menuitem","nav","noframes","ol","optgroup","option","p","param","search","section","summary","table","tbody","td","tfoot","th","thead","title","tr","track","ul"],t3=["pre","script","style","textarea"],_V={concrete:!0,name:"htmlFlow",resolveTo:EV,tokenize:kV},wV={partial:!0,tokenize:NV},TV={partial:!0,tokenize:SV};function EV(e){let t=e.length;for(;t--&&!(e[t][0]==="enter"&&e[t][1].type==="htmlFlow"););return t>1&&e[t-2][1].type==="linePrefix"&&(e[t][1].start=e[t-2][1].start,e[t+1][1].start=e[t-2][1].start,e.splice(t-2,2)),e}function kV(e,t,n){const r=this;let i,a,s,o,l;return c;function c(D){return u(D)}function u(D){return e.enter("htmlFlow"),e.enter("htmlFlowData"),e.consume(D),d}function d(D){return D===33?(e.consume(D),m):D===47?(e.consume(D),a=!0,g):D===63?(e.consume(D),i=3,r.interrupt?t:P):Zr(D)?(e.consume(D),s=String.fromCharCode(D),w):n(D)}function m(D){return D===45?(e.consume(D),i=2,p):D===91?(e.consume(D),i=5,o=0,x):Zr(D)?(e.consume(D),i=4,r.interrupt?t:P):n(D)}function p(D){return D===45?(e.consume(D),r.interrupt?t:P):n(D)}function x(D){const Ee="CDATA[";return D===Ee.charCodeAt(o++)?(e.consume(D),o===Ee.length?r.interrupt?t:H:x):n(D)}function g(D){return Zr(D)?(e.consume(D),s=String.fromCharCode(D),w):n(D)}function w(D){if(D===null||D===47||D===62||Ln(D)){const Ee=D===47,je=s.toLowerCase();return!Ee&&!a&&t3.includes(je)?(i=1,r.interrupt?t(D):H(D)):yV.includes(s.toLowerCase())?(i=6,Ee?(e.consume(D),v):r.interrupt?t(D):H(D)):(i=7,r.interrupt&&!r.parser.lazy[r.now().line]?n(D):a?$(D):_(D))}return D===45||qn(D)?(e.consume(D),s+=String.fromCharCode(D),w):n(D)}function v(D){return D===62?(e.consume(D),r.interrupt?t:H):n(D)}function $(D){return Ke(D)?(e.consume(D),$):q(D)}function _(D){return D===47?(e.consume(D),q):D===58||D===95||Zr(D)?(e.consume(D),C):Ke(D)?(e.consume(D),_):q(D)}function C(D){return D===45||D===46||D===58||D===95||qn(D)?(e.consume(D),C):k(D)}function k(D){return D===61?(e.consume(D),S):Ke(D)?(e.consume(D),k):_(D)}function S(D){return D===null||D===60||D===61||D===62||D===96?n(D):D===34||D===39?(e.consume(D),l=D,L):Ke(D)?(e.consume(D),S):U(D)}function L(D){return D===l?(e.consume(D),l=null,F):D===null||$e(D)?n(D):(e.consume(D),L)}function U(D){return D===null||D===34||D===39||D===47||D===60||D===61||D===62||D===96||Ln(D)?k(D):(e.consume(D),U)}function F(D){return D===47||D===62||Ke(D)?_(D):n(D)}function q(D){return D===62?(e.consume(D),G):n(D)}function G(D){return D===null||$e(D)?H(D):Ke(D)?(e.consume(D),G):n(D)}function H(D){return D===45&&i===2?(e.consume(D),J):D===60&&i===1?(e.consume(D),ae):D===62&&i===4?(e.consume(D),se):D===63&&i===3?(e.consume(D),P):D===93&&i===5?(e.consume(D),X):$e(D)&&(i===6||i===7)?(e.exit("htmlFlowData"),e.check(wV,he,ne)(D)):D===null||$e(D)?(e.exit("htmlFlowData"),ne(D)):(e.consume(D),H)}function ne(D){return e.check(TV,K,he)(D)}function K(D){return e.enter("lineEnding"),e.consume(D),e.exit("lineEnding"),te}function te(D){return D===null||$e(D)?ne(D):(e.enter("htmlFlowData"),H(D))}function J(D){return D===45?(e.consume(D),P):H(D)}function ae(D){return D===47?(e.consume(D),s="",B):H(D)}function B(D){if(D===62){const Ee=s.toLowerCase();return t3.includes(Ee)?(e.consume(D),se):H(D)}return Zr(D)&&s.length<8?(e.consume(D),s+=String.fromCharCode(D),B):H(D)}function X(D){return D===93?(e.consume(D),P):H(D)}function P(D){return D===62?(e.consume(D),se):D===45&&i===2?(e.consume(D),P):H(D)}function se(D){return D===null||$e(D)?(e.exit("htmlFlowData"),he(D)):(e.consume(D),se)}function he(D){return e.exit("htmlFlow"),t(D)}}function SV(e,t,n){const r=this;return i;function i(s){return $e(s)?(e.enter("lineEnding"),e.consume(s),e.exit("lineEnding"),a):n(s)}function a(s){return r.parser.lazy[r.now().line]?n(s):t(s)}}function NV(e,t,n){return r;function r(i){return e.enter("lineEnding"),e.consume(i),e.exit("lineEnding"),e.attempt(hm,t,n)}}const CV={name:"htmlText",tokenize:AV};function AV(e,t,n){const r=this;let i,a,s;return o;function o(P){return e.enter("htmlText"),e.enter("htmlTextData"),e.consume(P),l}function l(P){return P===33?(e.consume(P),c):P===47?(e.consume(P),k):P===63?(e.consume(P),_):Zr(P)?(e.consume(P),U):n(P)}function c(P){return P===45?(e.consume(P),u):P===91?(e.consume(P),a=0,x):Zr(P)?(e.consume(P),$):n(P)}function u(P){return P===45?(e.consume(P),p):n(P)}function d(P){return P===null?n(P):P===45?(e.consume(P),m):$e(P)?(s=d,ae(P)):(e.consume(P),d)}function m(P){return P===45?(e.consume(P),p):d(P)}function p(P){return P===62?J(P):P===45?m(P):d(P)}function x(P){const se="CDATA[";return P===se.charCodeAt(a++)?(e.consume(P),a===se.length?g:x):n(P)}function g(P){return P===null?n(P):P===93?(e.consume(P),w):$e(P)?(s=g,ae(P)):(e.consume(P),g)}function w(P){return P===93?(e.consume(P),v):g(P)}function v(P){return P===62?J(P):P===93?(e.consume(P),v):g(P)}function $(P){return P===null||P===62?J(P):$e(P)?(s=$,ae(P)):(e.consume(P),$)}function _(P){return P===null?n(P):P===63?(e.consume(P),C):$e(P)?(s=_,ae(P)):(e.consume(P),_)}function C(P){return P===62?J(P):_(P)}function k(P){return Zr(P)?(e.consume(P),S):n(P)}function S(P){return P===45||qn(P)?(e.consume(P),S):L(P)}function L(P){return $e(P)?(s=L,ae(P)):Ke(P)?(e.consume(P),L):J(P)}function U(P){return P===45||qn(P)?(e.consume(P),U):P===47||P===62||Ln(P)?F(P):n(P)}function F(P){return P===47?(e.consume(P),J):P===58||P===95||Zr(P)?(e.consume(P),q):$e(P)?(s=F,ae(P)):Ke(P)?(e.consume(P),F):J(P)}function q(P){return P===45||P===46||P===58||P===95||qn(P)?(e.consume(P),q):G(P)}function G(P){return P===61?(e.consume(P),H):$e(P)?(s=G,ae(P)):Ke(P)?(e.consume(P),G):F(P)}function H(P){return P===null||P===60||P===61||P===62||P===96?n(P):P===34||P===39?(e.consume(P),i=P,ne):$e(P)?(s=H,ae(P)):Ke(P)?(e.consume(P),H):(e.consume(P),K)}function ne(P){return P===i?(e.consume(P),i=void 0,te):P===null?n(P):$e(P)?(s=ne,ae(P)):(e.consume(P),ne)}function K(P){return P===null||P===34||P===39||P===60||P===61||P===96?n(P):P===47||P===62||Ln(P)?F(P):(e.consume(P),K)}function te(P){return P===47||P===62||Ln(P)?F(P):n(P)}function J(P){return P===62?(e.consume(P),e.exit("htmlTextData"),e.exit("htmlText"),t):n(P)}function ae(P){return e.exit("htmlTextData"),e.enter("lineEnding"),e.consume(P),e.exit("lineEnding"),B}function B(P){return Ke(P)?Ze(e,X,"linePrefix",r.parser.constructs.disable.null.includes("codeIndented")?void 0:4)(P):X(P)}function X(P){return e.enter("htmlTextData"),s(P)}}const A2={name:"labelEnd",resolveAll:UV,resolveTo:DV,tokenize:MV},LV={tokenize:RV},PV={tokenize:OV},IV={tokenize:zV};function UV(e){let t=-1;const n=[];for(;++t<e.length;){const r=e[t][1];if(n.push(e[t]),r.type==="labelImage"||r.type==="labelLink"||r.type==="labelEnd"){const i=r.type==="labelImage"?4:2;r.type="data",t+=i}}return e.length!==n.length&&li(e,0,e.length,n),e}function DV(e,t){let n=e.length,r=0,i,a,s,o;for(;n--;)if(i=e[n][1],a){if(i.type==="link"||i.type==="labelLink"&&i._inactive)break;e[n][0]==="enter"&&i.type==="labelLink"&&(i._inactive=!0)}else if(s){if(e[n][0]==="enter"&&(i.type==="labelImage"||i.type==="labelLink")&&!i._balanced&&(a=n,i.type!=="labelLink")){r=2;break}}else i.type==="labelEnd"&&(s=n);const l={type:e[a][1].type==="labelLink"?"link":"image",start:{...e[a][1].start},end:{...e[e.length-1][1].end}},c={type:"label",start:{...e[a][1].start},end:{...e[s][1].end}},u={type:"labelText",start:{...e[a+r+2][1].end},end:{...e[s-2][1].start}};return o=[["enter",l,t],["enter",c,t]],o=ir(o,e.slice(a+1,a+r+3)),o=ir(o,[["enter",u,t]]),o=ir(o,C2(t.parser.constructs.insideSpan.null,e.slice(a+r+4,s-3),t)),o=ir(o,[["exit",u,t],e[s-2],e[s-1],["exit",c,t]]),o=ir(o,e.slice(s+1)),o=ir(o,[["exit",l,t]]),li(e,a,e.length,o),e}function MV(e,t,n){const r=this;let i=r.events.length,a,s;for(;i--;)if((r.events[i][1].type==="labelImage"||r.events[i][1].type==="labelLink")&&!r.events[i][1]._balanced){a=r.events[i][1];break}return o;function o(m){return a?a._inactive?d(m):(s=r.parser.defined.includes(co(r.sliceSerialize({start:a.end,end:r.now()}))),e.enter("labelEnd"),e.enter("labelMarker"),e.consume(m),e.exit("labelMarker"),e.exit("labelEnd"),l):n(m)}function l(m){return m===40?e.attempt(LV,u,s?u:d)(m):m===91?e.attempt(PV,u,s?c:d)(m):s?u(m):d(m)}function c(m){return e.attempt(IV,u,d)(m)}function u(m){return t(m)}function d(m){return a._balanced=!0,n(m)}}function RV(e,t,n){return r;function r(d){return e.enter("resource"),e.enter("resourceMarker"),e.consume(d),e.exit("resourceMarker"),i}function i(d){return Ln(d)?Vl(e,a)(d):a(d)}function a(d){return d===41?u(d):cw(e,s,o,"resourceDestination","resourceDestinationLiteral","resourceDestinationLiteralMarker","resourceDestinationRaw","resourceDestinationString",32)(d)}function s(d){return Ln(d)?Vl(e,l)(d):u(d)}function o(d){return n(d)}function l(d){return d===34||d===39||d===40?dw(e,c,n,"resourceTitle","resourceTitleMarker","resourceTitleString")(d):u(d)}function c(d){return Ln(d)?Vl(e,u)(d):u(d)}function u(d){return d===41?(e.enter("resourceMarker"),e.consume(d),e.exit("resourceMarker"),e.exit("resource"),t):n(d)}}function OV(e,t,n){const r=this;return i;function i(o){return uw.call(r,e,a,s,"reference","referenceMarker","referenceString")(o)}function a(o){return r.parser.defined.includes(co(r.sliceSerialize(r.events[r.events.length-1][1]).slice(1,-1)))?t(o):n(o)}function s(o){return n(o)}}function zV(e,t,n){return r;function r(a){return e.enter("reference"),e.enter("referenceMarker"),e.consume(a),e.exit("referenceMarker"),i}function i(a){return a===93?(e.enter("referenceMarker"),e.consume(a),e.exit("referenceMarker"),e.exit("reference"),t):n(a)}}const BV={name:"labelStartImage",resolveAll:A2.resolveAll,tokenize:FV};function FV(e,t,n){const r=this;return i;function i(o){return e.enter("labelImage"),e.enter("labelImageMarker"),e.consume(o),e.exit("labelImageMarker"),a}function a(o){return o===91?(e.enter("labelMarker"),e.consume(o),e.exit("labelMarker"),e.exit("labelImage"),s):n(o)}function s(o){return o===94&&"_hiddenFootnoteSupport"in r.parser.constructs?n(o):t(o)}}const jV={name:"labelStartLink",resolveAll:A2.resolveAll,tokenize:VV};function VV(e,t,n){const r=this;return i;function i(s){return e.enter("labelLink"),e.enter("labelMarker"),e.consume(s),e.exit("labelMarker"),e.exit("labelLink"),a}function a(s){return s===94&&"_hiddenFootnoteSupport"in r.parser.constructs?n(s):t(s)}}const Uh={name:"lineEnding",tokenize:HV};function HV(e,t){return n;function n(r){return e.enter("lineEnding"),e.consume(r),e.exit("lineEnding"),Ze(e,t,"linePrefix")}}const $d={name:"thematicBreak",tokenize:qV};function qV(e,t,n){let r=0,i;return a;function a(c){return e.enter("thematicBreak"),s(c)}function s(c){return i=c,o(c)}function o(c){return c===i?(e.enter("thematicBreakSequence"),l(c)):r>=3&&(c===null||$e(c))?(e.exit("thematicBreak"),t(c)):n(c)}function l(c){return c===i?(e.consume(c),r++,l):(e.exit("thematicBreakSequence"),Ke(c)?Ze(e,o,"whitespace")(c):o(c))}}const wn={continuation:{tokenize:YV},exit:QV,name:"list",tokenize:KV},GV={partial:!0,tokenize:ZV},WV={partial:!0,tokenize:XV};function KV(e,t,n){const r=this,i=r.events[r.events.length-1];let a=i&&i[1].type==="linePrefix"?i[2].sliceSerialize(i[1],!0).length:0,s=0;return o;function o(p){const x=r.containerState.type||(p===42||p===43||p===45?"listUnordered":"listOrdered");if(x==="listUnordered"?!r.containerState.marker||p===r.containerState.marker:Wf(p)){if(r.containerState.type||(r.containerState.type=x,e.enter(x,{_container:!0})),x==="listUnordered")return e.enter("listItemPrefix"),p===42||p===45?e.check($d,n,c)(p):c(p);if(!r.interrupt||p===49)return e.enter("listItemPrefix"),e.enter("listItemValue"),l(p)}return n(p)}function l(p){return Wf(p)&&++s<10?(e.consume(p),l):(!r.interrupt||s<2)&&(r.containerState.marker?p===r.containerState.marker:p===41||p===46)?(e.exit("listItemValue"),c(p)):n(p)}function c(p){return e.enter("listItemMarker"),e.consume(p),e.exit("listItemMarker"),r.containerState.marker=r.containerState.marker||p,e.check(hm,r.interrupt?n:u,e.attempt(GV,m,d))}function u(p){return r.containerState.initialBlankLine=!0,a++,m(p)}function d(p){return Ke(p)?(e.enter("listItemPrefixWhitespace"),e.consume(p),e.exit("listItemPrefixWhitespace"),m):n(p)}function m(p){return r.containerState.size=a+r.sliceSerialize(e.exit("listItemPrefix"),!0).length,t(p)}}function YV(e,t,n){const r=this;return r.containerState._closeFlow=void 0,e.check(hm,i,a);function i(o){return r.containerState.furtherBlankLines=r.containerState.furtherBlankLines||r.containerState.initialBlankLine,Ze(e,t,"listItemIndent",r.containerState.size+1)(o)}function a(o){return r.containerState.furtherBlankLines||!Ke(o)?(r.containerState.furtherBlankLines=void 0,r.containerState.initialBlankLine=void 0,s(o)):(r.containerState.furtherBlankLines=void 0,r.containerState.initialBlankLine=void 0,e.attempt(WV,t,s)(o))}function s(o){return r.containerState._closeFlow=!0,r.interrupt=void 0,Ze(e,e.attempt(wn,t,n),"linePrefix",r.parser.constructs.disable.null.includes("codeIndented")?void 0:4)(o)}}function XV(e,t,n){const r=this;return Ze(e,i,"listItemIndent",r.containerState.size+1);function i(a){const s=r.events[r.events.length-1];return s&&s[1].type==="listItemIndent"&&s[2].sliceSerialize(s[1],!0).length===r.containerState.size?t(a):n(a)}}function QV(e){e.exit(this.containerState.type)}function ZV(e,t,n){const r=this;return Ze(e,i,"listItemPrefixWhitespace",r.parser.constructs.disable.null.includes("codeIndented")?void 0:5);function i(a){const s=r.events[r.events.length-1];return!Ke(a)&&s&&s[1].type==="listItemPrefixWhitespace"?t(a):n(a)}}const n3={name:"setextUnderline",resolveTo:JV,tokenize:eH};function JV(e,t){let n=e.length,r,i,a;for(;n--;)if(e[n][0]==="enter"){if(e[n][1].type==="content"){r=n;break}e[n][1].type==="paragraph"&&(i=n)}else e[n][1].type==="content"&&e.splice(n,1),!a&&e[n][1].type==="definition"&&(a=n);const s={type:"setextHeading",start:{...e[r][1].start},end:{...e[e.length-1][1].end}};return e[i][1].type="setextHeadingText",a?(e.splice(i,0,["enter",s,t]),e.splice(a+1,0,["exit",e[r][1],t]),e[r][1].end={...e[a][1].end}):e[r][1]=s,e.push(["exit",s,t]),e}function eH(e,t,n){const r=this;let i;return a;function a(c){let u=r.events.length,d;for(;u--;)if(r.events[u][1].type!=="lineEnding"&&r.events[u][1].type!=="linePrefix"&&r.events[u][1].type!=="content"){d=r.events[u][1].type==="paragraph";break}return!r.parser.lazy[r.now().line]&&(r.interrupt||d)?(e.enter("setextHeadingLine"),i=c,s(c)):n(c)}function s(c){return e.enter("setextHeadingLineSequence"),o(c)}function o(c){return c===i?(e.consume(c),o):(e.exit("setextHeadingLineSequence"),Ke(c)?Ze(e,l,"lineSuffix")(c):l(c))}function l(c){return c===null||$e(c)?(e.exit("setextHeadingLine"),t(c)):n(c)}}const tH={tokenize:nH};function nH(e){const t=this,n=e.attempt(hm,r,e.attempt(this.parser.constructs.flowInitial,i,Ze(e,e.attempt(this.parser.constructs.flow,i,e.attempt(oV,i)),"linePrefix")));return n;function r(a){if(a===null){e.consume(a);return}return e.enter("lineEndingBlank"),e.consume(a),e.exit("lineEndingBlank"),t.currentConstruct=void 0,n}function i(a){if(a===null){e.consume(a);return}return e.enter("lineEnding"),e.consume(a),e.exit("lineEnding"),t.currentConstruct=void 0,n}}const rH={resolveAll:hw()},iH=mw("string"),aH=mw("text");function mw(e){return{resolveAll:hw(e==="text"?sH:void 0),tokenize:t};function t(n){const r=this,i=this.parser.constructs[e],a=n.attempt(i,s,o);return s;function s(u){return c(u)?a(u):o(u)}function o(u){if(u===null){n.consume(u);return}return n.enter("data"),n.consume(u),l}function l(u){return c(u)?(n.exit("data"),a(u)):(n.consume(u),l)}function c(u){if(u===null)return!0;const d=i[u];let m=-1;if(d)for(;++m<d.length;){const p=d[m];if(!p.previous||p.previous.call(r,r.previous))return!0}return!1}}}function hw(e){return t;function t(n,r){let i=-1,a;for(;++i<=n.length;)a===void 0?n[i]&&n[i][1].type==="data"&&(a=i,i++):(!n[i]||n[i][1].type!=="data")&&(i!==a+2&&(n[a][1].end=n[i-1][1].end,n.splice(a+2,i-a-2),i=a+2),a=void 0);return e?e(n,r):n}}function sH(e,t){let n=0;for(;++n<=e.length;)if((n===e.length||e[n][1].type==="lineEnding")&&e[n-1][1].type==="data"){const r=e[n-1][1],i=t.sliceStream(r);let a=i.length,s=-1,o=0,l;for(;a--;){const c=i[a];if(typeof c=="string"){for(s=c.length;c.charCodeAt(s-1)===32;)o++,s--;if(s)break;s=-1}else if(c===-2)l=!0,o++;else if(c!==-1){a++;break}}if(t._contentTypeTextTrailing&&n===e.length&&(o=0),o){const c={type:n===e.length||l||o<2?"lineSuffix":"hardBreakTrailing",start:{_bufferIndex:a?s:r.start._bufferIndex+s,_index:r.start._index+a,line:r.end.line,column:r.end.column-o,offset:r.end.offset-o},end:{...r.end}};r.end={...c.start},r.start.offset===r.end.offset?Object.assign(r,c):(e.splice(n,0,["enter",c,t],["exit",c,t]),n+=2)}n++}return e}const oH={42:wn,43:wn,45:wn,48:wn,49:wn,50:wn,51:wn,52:wn,53:wn,54:wn,55:wn,56:wn,57:wn,62:aw},lH={91:mV},cH={[-2]:Ih,[-1]:Ih,32:Ih},uH={35:vV,42:$d,45:[n3,$d],60:_V,61:n3,95:$d,96:e3,126:e3},dH={38:ow,92:sw},mH={[-5]:Uh,[-4]:Uh,[-3]:Uh,33:BV,38:ow,42:Kf,60:[jj,CV],91:jV,92:[gV,sw],93:A2,95:Kf,96:tV},hH={null:[Kf,rH]},pH={null:[42,95]},fH={null:[]},gH=Object.freeze(Object.defineProperty({__proto__:null,attentionMarkers:pH,contentInitial:lH,disable:fH,document:oH,flow:uH,flowInitial:cH,insideSpan:hH,string:dH,text:mH},Symbol.toStringTag,{value:"Module"}));function bH(e,t,n){let r={_bufferIndex:-1,_index:0,line:n&&n.line||1,column:n&&n.column||1,offset:n&&n.offset||0};const i={},a=[];let s=[],o=[];const l={attempt:L(k),check:L(S),consume:$,enter:_,exit:C,interrupt:L(S,{interrupt:!0})},c={code:null,containerState:{},defineSkip:g,events:[],now:x,parser:e,previous:null,sliceSerialize:m,sliceStream:p,write:d};let u=t.tokenize.call(c,l);return t.resolveAll&&a.push(t),c;function d(G){return s=ir(s,G),w(),s[s.length-1]!==null?[]:(U(t,0),c.events=C2(a,c.events,c),c.events)}function m(G,H){return xH(p(G),H)}function p(G){return vH(s,G)}function x(){const{_bufferIndex:G,_index:H,line:ne,column:K,offset:te}=r;return{_bufferIndex:G,_index:H,line:ne,column:K,offset:te}}function g(G){i[G.line]=G.column,q()}function w(){let G;for(;r._index<s.length;){const H=s[r._index];if(typeof H=="string")for(G=r._index,r._bufferIndex<0&&(r._bufferIndex=0);r._index===G&&r._bufferIndex<H.length;)v(H.charCodeAt(r._bufferIndex));else v(H)}}function v(G){u=u(G)}function $(G){$e(G)?(r.line++,r.column=1,r.offset+=G===-3?2:1,q()):G!==-1&&(r.column++,r.offset++),r._bufferIndex<0?r._index++:(r._bufferIndex++,r._bufferIndex===s[r._index].length&&(r._bufferIndex=-1,r._index++)),c.previous=G}function _(G,H){const ne=H||{};return ne.type=G,ne.start=x(),c.events.push(["enter",ne,c]),o.push(ne),ne}function C(G){const H=o.pop();return H.end=x(),c.events.push(["exit",H,c]),H}function k(G,H){U(G,H.from)}function S(G,H){H.restore()}function L(G,H){return ne;function ne(K,te,J){let ae,B,X,P;return Array.isArray(K)?he(K):"tokenize"in K?he([K]):se(K);function se(xe){return be;function be(Ne){const He=Ne!==null&&xe[Ne],Le=Ne!==null&&xe.null,Be=[...Array.isArray(He)?He:He?[He]:[],...Array.isArray(Le)?Le:Le?[Le]:[]];return he(Be)(Ne)}}function he(xe){return ae=xe,B=0,xe.length===0?J:D(xe[B])}function D(xe){return be;function be(Ne){return P=F(),X=xe,xe.partial||(c.currentConstruct=xe),xe.name&&c.parser.constructs.disable.null.includes(xe.name)?je():xe.tokenize.call(H?Object.assign(Object.create(c),H):c,l,Ee,je)(Ne)}}function Ee(xe){return G(X,P),te}function je(xe){return P.restore(),++B<ae.length?D(ae[B]):J}}}function U(G,H){G.resolveAll&&!a.includes(G)&&a.push(G),G.resolve&&li(c.events,H,c.events.length-H,G.resolve(c.events.slice(H),c)),G.resolveTo&&(c.events=G.resolveTo(c.events,c))}function F(){const G=x(),H=c.previous,ne=c.currentConstruct,K=c.events.length,te=Array.from(o);return{from:K,restore:J};function J(){r=G,c.previous=H,c.currentConstruct=ne,c.events.length=K,o=te,q()}}function q(){r.line in i&&r.column<2&&(r.column=i[r.line],r.offset+=i[r.line]-1)}}function vH(e,t){const n=t.start._index,r=t.start._bufferIndex,i=t.end._index,a=t.end._bufferIndex;let s;if(n===i)s=[e[n].slice(r,a)];else{if(s=e.slice(n,i),r>-1){const o=s[0];typeof o=="string"?s[0]=o.slice(r):s.shift()}a>0&&s.push(e[i].slice(0,a))}return s}function xH(e,t){let n=-1;const r=[];let i;for(;++n<e.length;){const a=e[n];let s;if(typeof a=="string")s=a;else switch(a){case-5:{s="\r";break}case-4:{s=`
`;break}case-3:{s=`\r
`;break}case-2:{s=t?" ":"	";break}case-1:{if(!t&&i)continue;s=" ";break}default:s=String.fromCharCode(a)}i=a===-2,r.push(s)}return r.join("")}function $H(e){const r={constructs:Sj([gH,...(e||{}).extensions||[]]),content:i(Dj),defined:[],document:i(Rj),flow:i(tH),lazy:{},string:i(iH),text:i(aH)};return r;function i(a){return s;function s(o){return bH(r,a,o)}}}function yH(e){for(;!lw(e););return e}const r3=/[\0\t\n\r]/g;function _H(){let e=1,t="",n=!0,r;return i;function i(a,s,o){const l=[];let c,u,d,m,p;for(a=t+(typeof a=="string"?a.toString():new TextDecoder(s||void 0).decode(a)),d=0,t="",n&&(a.charCodeAt(0)===65279&&d++,n=void 0);d<a.length;){if(r3.lastIndex=d,c=r3.exec(a),m=c&&c.index!==void 0?c.index:a.length,p=a.charCodeAt(m),!c){t=a.slice(d);break}if(p===10&&d===m&&r)l.push(-3),r=void 0;else switch(r&&(l.push(-5),r=void 0),d<m&&(l.push(a.slice(d,m)),e+=m-d),p){case 0:{l.push(65533),e++;break}case 9:{for(u=Math.ceil(e/4)*4,l.push(-2);e++<u;)l.push(-1);break}case 10:{l.push(-4),e=1;break}default:r=!0,e=1}d=m+1}return o&&(r&&l.push(-5),t&&l.push(t),l.push(null)),l}}const wH=/\\([!-/:-@[-`{-~])|&(#(?:\d{1,7}|x[\da-f]{1,6})|[\da-z]{1,31});/gi;function TH(e){return e.replace(wH,EH)}function EH(e,t,n){if(t)return t;if(n.charCodeAt(0)===35){const i=n.charCodeAt(1),a=i===120||i===88;return iw(n.slice(a?2:1),a?16:10)}return N2(n)||e}const pw={}.hasOwnProperty;function kH(e,t,n){return typeof t!="string"&&(n=t,t=void 0),SH(n)(yH($H(n).document().write(_H()(e,t,!0))))}function SH(e){const t={transforms:[],canContainEols:["emphasis","fragment","heading","paragraph","strong"],enter:{autolink:a(st),autolinkProtocol:F,autolinkEmail:F,atxHeading:a(xn),blockQuote:a(Le),characterEscape:F,characterReference:F,codeFenced:a(Be),codeFencedFenceInfo:s,codeFencedFenceMeta:s,codeIndented:a(Be,s),codeText:a(Ve,s),codeTextData:F,data:F,codeFlowValue:F,definition:a(Dt),definitionDestinationString:s,definitionLabelString:s,definitionTitleString:s,emphasis:a(Mt),hardBreakEscape:a(Ft),hardBreakTrailing:a(Ft),htmlFlow:a($n,s),htmlFlowData:F,htmlText:a($n,s),htmlTextData:F,image:a(Ur),label:s,link:a(st),listItem:a(km),listItemValue:m,listOrdered:a(pi,d),listUnordered:a(pi),paragraph:a(Sm),reference:D,referenceString:s,resourceDestinationString:s,resourceTitleString:s,setextHeading:a(xn),strong:a(Ss),thematicBreak:a(au)},exit:{atxHeading:l(),atxHeadingSequence:k,autolink:l(),autolinkEmail:He,autolinkProtocol:Ne,blockQuote:l(),characterEscapeValue:q,characterReferenceMarkerHexadecimal:je,characterReferenceMarkerNumeric:je,characterReferenceValue:xe,characterReference:be,codeFenced:l(w),codeFencedFence:g,codeFencedFenceInfo:p,codeFencedFenceMeta:x,codeFlowValue:q,codeIndented:l(v),codeText:l(te),codeTextData:q,data:q,definition:l(),definitionDestinationString:C,definitionLabelString:$,definitionTitleString:_,emphasis:l(),hardBreakEscape:l(H),hardBreakTrailing:l(H),htmlFlow:l(ne),htmlFlowData:q,htmlText:l(K),htmlTextData:q,image:l(ae),label:X,labelText:B,lineEnding:G,link:l(J),listItem:l(),listOrdered:l(),listUnordered:l(),paragraph:l(),referenceString:Ee,resourceDestinationString:P,resourceTitleString:se,resource:he,setextHeading:l(U),setextHeadingLineSequence:L,setextHeadingText:S,strong:l(),thematicBreak:l()}};fw(t,(e||{}).mdastExtensions||[]);const n={};return r;function r(Y){let de={type:"root",children:[]};const Ce={stack:[de],tokenStack:[],config:t,enter:o,exit:c,buffer:s,resume:u,data:n},Pe=[];let Xe=-1;for(;++Xe<Y.length;)if(Y[Xe][1].type==="listOrdered"||Y[Xe][1].type==="listUnordered")if(Y[Xe][0]==="enter")Pe.push(Xe);else{const Dn=Pe.pop();Xe=i(Y,Dn,Xe)}for(Xe=-1;++Xe<Y.length;){const Dn=t[Y[Xe][0]];pw.call(Dn,Y[Xe][1].type)&&Dn[Y[Xe][1].type].call(Object.assign({sliceSerialize:Y[Xe][2].sliceSerialize},Ce),Y[Xe][1])}if(Ce.tokenStack.length>0){const Dn=Ce.tokenStack[Ce.tokenStack.length-1];(Dn[1]||i3).call(Ce,void 0,Dn[0])}for(de.position={start:Gi(Y.length>0?Y[0][1].start:{line:1,column:1,offset:0}),end:Gi(Y.length>0?Y[Y.length-2][1].end:{line:1,column:1,offset:0})},Xe=-1;++Xe<t.transforms.length;)de=t.transforms[Xe](de)||de;return de}function i(Y,de,Ce){let Pe=de-1,Xe=-1,Dn=!1,Dr,Zn,ja,fi;for(;++Pe<=Ce;){const _t=Y[Pe];switch(_t[1].type){case"listUnordered":case"listOrdered":case"blockQuote":{_t[0]==="enter"?Xe++:Xe--,fi=void 0;break}case"lineEndingBlank":{_t[0]==="enter"&&(Dr&&!fi&&!Xe&&!ja&&(ja=Pe),fi=void 0);break}case"linePrefix":case"listItemValue":case"listItemMarker":case"listItemPrefix":case"listItemPrefixWhitespace":break;default:fi=void 0}if(!Xe&&_t[0]==="enter"&&_t[1].type==="listItemPrefix"||Xe===-1&&_t[0]==="exit"&&(_t[1].type==="listUnordered"||_t[1].type==="listOrdered")){if(Dr){let ji=Pe;for(Zn=void 0;ji--;){const Te=Y[ji];if(Te[1].type==="lineEnding"||Te[1].type==="lineEndingBlank"){if(Te[0]==="exit")continue;Zn&&(Y[Zn][1].type="lineEndingBlank",Dn=!0),Te[1].type="lineEnding",Zn=ji}else if(!(Te[1].type==="linePrefix"||Te[1].type==="blockQuotePrefix"||Te[1].type==="blockQuotePrefixWhitespace"||Te[1].type==="blockQuoteMarker"||Te[1].type==="listItemIndent"))break}ja&&(!Zn||ja<Zn)&&(Dr._spread=!0),Dr.end=Object.assign({},Zn?Y[Zn][1].start:_t[1].end),Y.splice(Zn||Pe,0,["exit",Dr,_t[2]]),Pe++,Ce++}if(_t[1].type==="listItemPrefix"){const ji={type:"listItem",_spread:!1,start:Object.assign({},_t[1].start),end:void 0};Dr=ji,Y.splice(Pe,0,["enter",ji,_t[2]]),Pe++,Ce++,ja=void 0,fi=!0}}}return Y[de][1]._spread=Dn,Ce}function a(Y,de){return Ce;function Ce(Pe){o.call(this,Y(Pe),Pe),de&&de.call(this,Pe)}}function s(){this.stack.push({type:"fragment",children:[]})}function o(Y,de,Ce){this.stack[this.stack.length-1].children.push(Y),this.stack.push(Y),this.tokenStack.push([de,Ce||void 0]),Y.position={start:Gi(de.start),end:void 0}}function l(Y){return de;function de(Ce){Y&&Y.call(this,Ce),c.call(this,Ce)}}function c(Y,de){const Ce=this.stack.pop(),Pe=this.tokenStack.pop();if(Pe)Pe[0].type!==Y.type&&(de?de.call(this,Y,Pe[0]):(Pe[1]||i3).call(this,Y,Pe[0]));else throw new Error("Cannot close `"+Y.type+"` ("+jl({start:Y.start,end:Y.end})+"): it’s not open");Ce.position.end=Gi(Y.end)}function u(){return Ej(this.stack.pop())}function d(){this.data.expectingFirstListItemValue=!0}function m(Y){if(this.data.expectingFirstListItemValue){const de=this.stack[this.stack.length-2];de.start=Number.parseInt(this.sliceSerialize(Y),10),this.data.expectingFirstListItemValue=void 0}}function p(){const Y=this.resume(),de=this.stack[this.stack.length-1];de.lang=Y}function x(){const Y=this.resume(),de=this.stack[this.stack.length-1];de.meta=Y}function g(){this.data.flowCodeInside||(this.buffer(),this.data.flowCodeInside=!0)}function w(){const Y=this.resume(),de=this.stack[this.stack.length-1];de.value=Y.replace(/^(\r?\n|\r)|(\r?\n|\r)$/g,""),this.data.flowCodeInside=void 0}function v(){const Y=this.resume(),de=this.stack[this.stack.length-1];de.value=Y.replace(/(\r?\n|\r)$/g,"")}function $(Y){const de=this.resume(),Ce=this.stack[this.stack.length-1];Ce.label=de,Ce.identifier=co(this.sliceSerialize(Y)).toLowerCase()}function _(){const Y=this.resume(),de=this.stack[this.stack.length-1];de.title=Y}function C(){const Y=this.resume(),de=this.stack[this.stack.length-1];de.url=Y}function k(Y){const de=this.stack[this.stack.length-1];if(!de.depth){const Ce=this.sliceSerialize(Y).length;de.depth=Ce}}function S(){this.data.setextHeadingSlurpLineEnding=!0}function L(Y){const de=this.stack[this.stack.length-1];de.depth=this.sliceSerialize(Y).codePointAt(0)===61?1:2}function U(){this.data.setextHeadingSlurpLineEnding=void 0}function F(Y){const Ce=this.stack[this.stack.length-1].children;let Pe=Ce[Ce.length-1];(!Pe||Pe.type!=="text")&&(Pe=Nm(),Pe.position={start:Gi(Y.start),end:void 0},Ce.push(Pe)),this.stack.push(Pe)}function q(Y){const de=this.stack.pop();de.value+=this.sliceSerialize(Y),de.position.end=Gi(Y.end)}function G(Y){const de=this.stack[this.stack.length-1];if(this.data.atHardBreak){const Ce=de.children[de.children.length-1];Ce.position.end=Gi(Y.end),this.data.atHardBreak=void 0;return}!this.data.setextHeadingSlurpLineEnding&&t.canContainEols.includes(de.type)&&(F.call(this,Y),q.call(this,Y))}function H(){this.data.atHardBreak=!0}function ne(){const Y=this.resume(),de=this.stack[this.stack.length-1];de.value=Y}function K(){const Y=this.resume(),de=this.stack[this.stack.length-1];de.value=Y}function te(){const Y=this.resume(),de=this.stack[this.stack.length-1];de.value=Y}function J(){const Y=this.stack[this.stack.length-1];if(this.data.inReference){const de=this.data.referenceType||"shortcut";Y.type+="Reference",Y.referenceType=de,delete Y.url,delete Y.title}else delete Y.identifier,delete Y.label;this.data.referenceType=void 0}function ae(){const Y=this.stack[this.stack.length-1];if(this.data.inReference){const de=this.data.referenceType||"shortcut";Y.type+="Reference",Y.referenceType=de,delete Y.url,delete Y.title}else delete Y.identifier,delete Y.label;this.data.referenceType=void 0}function B(Y){const de=this.sliceSerialize(Y),Ce=this.stack[this.stack.length-2];Ce.label=TH(de),Ce.identifier=co(de).toLowerCase()}function X(){const Y=this.stack[this.stack.length-1],de=this.resume(),Ce=this.stack[this.stack.length-1];if(this.data.inReference=!0,Ce.type==="link"){const Pe=Y.children;Ce.children=Pe}else Ce.alt=de}function P(){const Y=this.resume(),de=this.stack[this.stack.length-1];de.url=Y}function se(){const Y=this.resume(),de=this.stack[this.stack.length-1];de.title=Y}function he(){this.data.inReference=void 0}function D(){this.data.referenceType="collapsed"}function Ee(Y){const de=this.resume(),Ce=this.stack[this.stack.length-1];Ce.label=de,Ce.identifier=co(this.sliceSerialize(Y)).toLowerCase(),this.data.referenceType="full"}function je(Y){this.data.characterReferenceType=Y.type}function xe(Y){const de=this.sliceSerialize(Y),Ce=this.data.characterReferenceType;let Pe;Ce?(Pe=iw(de,Ce==="characterReferenceMarkerNumeric"?10:16),this.data.characterReferenceType=void 0):Pe=N2(de);const Xe=this.stack[this.stack.length-1];Xe.value+=Pe}function be(Y){const de=this.stack.pop();de.position.end=Gi(Y.end)}function Ne(Y){q.call(this,Y);const de=this.stack[this.stack.length-1];de.url=this.sliceSerialize(Y)}function He(Y){q.call(this,Y);const de=this.stack[this.stack.length-1];de.url="mailto:"+this.sliceSerialize(Y)}function Le(){return{type:"blockquote",children:[]}}function Be(){return{type:"code",lang:null,meta:null,value:""}}function Ve(){return{type:"inlineCode",value:""}}function Dt(){return{type:"definition",identifier:"",label:null,title:null,url:""}}function Mt(){return{type:"emphasis",children:[]}}function xn(){return{type:"heading",depth:0,children:[]}}function Ft(){return{type:"break"}}function $n(){return{type:"html",value:""}}function Ur(){return{type:"image",title:null,url:"",alt:null}}function st(){return{type:"link",title:null,url:"",children:[]}}function pi(Y){return{type:"list",ordered:Y.type==="listOrdered",start:null,spread:Y._spread,children:[]}}function km(Y){return{type:"listItem",spread:Y._spread,checked:null,children:[]}}function Sm(){return{type:"paragraph",children:[]}}function Ss(){return{type:"strong",children:[]}}function Nm(){return{type:"text",value:""}}function au(){return{type:"thematicBreak"}}}function Gi(e){return{line:e.line,column:e.column,offset:e.offset}}function fw(e,t){let n=-1;for(;++n<t.length;){const r=t[n];Array.isArray(r)?fw(e,r):NH(e,r)}}function NH(e,t){let n;for(n in t)if(pw.call(t,n))switch(n){case"canContainEols":{const r=t[n];r&&e[n].push(...r);break}case"transforms":{const r=t[n];r&&e[n].push(...r);break}case"enter":case"exit":{const r=t[n];r&&Object.assign(e[n],r);break}}}function i3(e,t){throw e?new Error("Cannot close `"+e.type+"` ("+jl({start:e.start,end:e.end})+"): a different token (`"+t.type+"`, "+jl({start:t.start,end:t.end})+") is open"):new Error("Cannot close document, a token (`"+t.type+"`, "+jl({start:t.start,end:t.end})+") is still open")}function CH(e){const t=this;t.parser=n;function n(r){return kH(r,{...t.data("settings"),...e,extensions:t.data("micromarkExtensions")||[],mdastExtensions:t.data("fromMarkdownExtensions")||[]})}}function AH(e,t){const n={type:"element",tagName:"blockquote",properties:{},children:e.wrap(e.all(t),!0)};return e.patch(t,n),e.applyData(t,n)}function LH(e,t){const n={type:"element",tagName:"br",properties:{},children:[]};return e.patch(t,n),[e.applyData(t,n),{type:"text",value:`
`}]}function PH(e,t){const n=t.value?t.value+`
`:"",r={};t.lang&&(r.className=["language-"+t.lang]);let i={type:"element",tagName:"code",properties:r,children:[{type:"text",value:n}]};return t.meta&&(i.data={meta:t.meta}),e.patch(t,i),i=e.applyData(t,i),i={type:"element",tagName:"pre",properties:{},children:[i]},e.patch(t,i),i}function IH(e,t){const n={type:"element",tagName:"del",properties:{},children:e.all(t)};return e.patch(t,n),e.applyData(t,n)}function UH(e,t){const n={type:"element",tagName:"em",properties:{},children:e.all(t)};return e.patch(t,n),e.applyData(t,n)}function DH(e,t){const n=typeof e.options.clobberPrefix=="string"?e.options.clobberPrefix:"user-content-",r=String(t.identifier).toUpperCase(),i=Xo(r.toLowerCase()),a=e.footnoteOrder.indexOf(r);let s,o=e.footnoteCounts.get(r);o===void 0?(o=0,e.footnoteOrder.push(r),s=e.footnoteOrder.length):s=a+1,o+=1,e.footnoteCounts.set(r,o);const l={type:"element",tagName:"a",properties:{href:"#"+n+"fn-"+i,id:n+"fnref-"+i+(o>1?"-"+o:""),dataFootnoteRef:!0,ariaDescribedBy:["footnote-label"]},children:[{type:"text",value:String(s)}]};e.patch(t,l);const c={type:"element",tagName:"sup",properties:{},children:[l]};return e.patch(t,c),e.applyData(t,c)}function MH(e,t){const n={type:"element",tagName:"h"+t.depth,properties:{},children:e.all(t)};return e.patch(t,n),e.applyData(t,n)}function RH(e,t){if(e.options.allowDangerousHtml){const n={type:"raw",value:t.value};return e.patch(t,n),e.applyData(t,n)}}function gw(e,t){const n=t.referenceType;let r="]";if(n==="collapsed"?r+="[]":n==="full"&&(r+="["+(t.label||t.identifier)+"]"),t.type==="imageReference")return[{type:"text",value:"!["+t.alt+r}];const i=e.all(t),a=i[0];a&&a.type==="text"?a.value="["+a.value:i.unshift({type:"text",value:"["});const s=i[i.length-1];return s&&s.type==="text"?s.value+=r:i.push({type:"text",value:r}),i}function OH(e,t){const n=String(t.identifier).toUpperCase(),r=e.definitionById.get(n);if(!r)return gw(e,t);const i={src:Xo(r.url||""),alt:t.alt};r.title!==null&&r.title!==void 0&&(i.title=r.title);const a={type:"element",tagName:"img",properties:i,children:[]};return e.patch(t,a),e.applyData(t,a)}function zH(e,t){const n={src:Xo(t.url)};t.alt!==null&&t.alt!==void 0&&(n.alt=t.alt),t.title!==null&&t.title!==void 0&&(n.title=t.title);const r={type:"element",tagName:"img",properties:n,children:[]};return e.patch(t,r),e.applyData(t,r)}function BH(e,t){const n={type:"text",value:t.value.replace(/\r?\n|\r/g," ")};e.patch(t,n);const r={type:"element",tagName:"code",properties:{},children:[n]};return e.patch(t,r),e.applyData(t,r)}function FH(e,t){const n=String(t.identifier).toUpperCase(),r=e.definitionById.get(n);if(!r)return gw(e,t);const i={href:Xo(r.url||"")};r.title!==null&&r.title!==void 0&&(i.title=r.title);const a={type:"element",tagName:"a",properties:i,children:e.all(t)};return e.patch(t,a),e.applyData(t,a)}function jH(e,t){const n={href:Xo(t.url)};t.title!==null&&t.title!==void 0&&(n.title=t.title);const r={type:"element",tagName:"a",properties:n,children:e.all(t)};return e.patch(t,r),e.applyData(t,r)}function VH(e,t,n){const r=e.all(t),i=n?HH(n):bw(t),a={},s=[];if(typeof t.checked=="boolean"){const u=r[0];let d;u&&u.type==="element"&&u.tagName==="p"?d=u:(d={type:"element",tagName:"p",properties:{},children:[]},r.unshift(d)),d.children.length>0&&d.children.unshift({type:"text",value:" "}),d.children.unshift({type:"element",tagName:"input",properties:{type:"checkbox",checked:t.checked,disabled:!0},children:[]}),a.className=["task-list-item"]}let o=-1;for(;++o<r.length;){const u=r[o];(i||o!==0||u.type!=="element"||u.tagName!=="p")&&s.push({type:"text",value:`
`}),u.type==="element"&&u.tagName==="p"&&!i?s.push(...u.children):s.push(u)}const l=r[r.length-1];l&&(i||l.type!=="element"||l.tagName!=="p")&&s.push({type:"text",value:`
`});const c={type:"element",tagName:"li",properties:a,children:s};return e.patch(t,c),e.applyData(t,c)}function HH(e){let t=!1;if(e.type==="list"){t=e.spread||!1;const n=e.children;let r=-1;for(;!t&&++r<n.length;)t=bw(n[r])}return t}function bw(e){const t=e.spread;return t??e.children.length>1}function qH(e,t){const n={},r=e.all(t);let i=-1;for(typeof t.start=="number"&&t.start!==1&&(n.start=t.start);++i<r.length;){const s=r[i];if(s.type==="element"&&s.tagName==="li"&&s.properties&&Array.isArray(s.properties.className)&&s.properties.className.includes("task-list-item")){n.className=["contains-task-list"];break}}const a={type:"element",tagName:t.ordered?"ol":"ul",properties:n,children:e.wrap(r,!0)};return e.patch(t,a),e.applyData(t,a)}function GH(e,t){const n={type:"element",tagName:"p",properties:{},children:e.all(t)};return e.patch(t,n),e.applyData(t,n)}function WH(e,t){const n={type:"root",children:e.wrap(e.all(t))};return e.patch(t,n),e.applyData(t,n)}function KH(e,t){const n={type:"element",tagName:"strong",properties:{},children:e.all(t)};return e.patch(t,n),e.applyData(t,n)}function YH(e,t){const n=e.all(t),r=n.shift(),i=[];if(r){const s={type:"element",tagName:"thead",properties:{},children:e.wrap([r],!0)};e.patch(t.children[0],s),i.push(s)}if(n.length>0){const s={type:"element",tagName:"tbody",properties:{},children:e.wrap(n,!0)},o=ui(t.children[1]),l=mm(t.children[t.children.length-1]);o&&l&&(s.position={start:o,end:l}),i.push(s)}const a={type:"element",tagName:"table",properties:{},children:e.wrap(i,!0)};return e.patch(t,a),e.applyData(t,a)}function XH(e,t,n){const r=n?n.children:void 0,a=(r?r.indexOf(t):1)===0?"th":"td",s=n&&n.type==="table"?n.align:void 0,o=s?s.length:t.children.length;let l=-1;const c=[];for(;++l<o;){const d=t.children[l],m={},p=s?s[l]:void 0;p&&(m.align=p);let x={type:"element",tagName:a,properties:m,children:[]};d&&(x.children=e.all(d),e.patch(d,x),x=e.applyData(d,x)),c.push(x)}const u={type:"element",tagName:"tr",properties:{},children:e.wrap(c,!0)};return e.patch(t,u),e.applyData(t,u)}function QH(e,t){const n={type:"element",tagName:"td",properties:{},children:e.all(t)};return e.patch(t,n),e.applyData(t,n)}const a3=9,s3=32;function ZH(e){const t=String(e),n=/\r?\n|\r/g;let r=n.exec(t),i=0;const a=[];for(;r;)a.push(o3(t.slice(i,r.index),i>0,!0),r[0]),i=r.index+r[0].length,r=n.exec(t);return a.push(o3(t.slice(i),i>0,!1)),a.join("")}function o3(e,t,n){let r=0,i=e.length;if(t){let a=e.codePointAt(r);for(;a===a3||a===s3;)r++,a=e.codePointAt(r)}if(n){let a=e.codePointAt(i-1);for(;a===a3||a===s3;)i--,a=e.codePointAt(i-1)}return i>r?e.slice(r,i):""}function JH(e,t){const n={type:"text",value:ZH(String(t.value))};return e.patch(t,n),e.applyData(t,n)}function eq(e,t){const n={type:"element",tagName:"hr",properties:{},children:[]};return e.patch(t,n),e.applyData(t,n)}const tq={blockquote:AH,break:LH,code:PH,delete:IH,emphasis:UH,footnoteReference:DH,heading:MH,html:RH,imageReference:OH,image:zH,inlineCode:BH,linkReference:FH,link:jH,listItem:VH,list:qH,paragraph:GH,root:WH,strong:KH,table:YH,tableCell:QH,tableRow:XH,text:JH,thematicBreak:eq,toml:zu,yaml:zu,definition:zu,footnoteDefinition:zu};function zu(){}const vw=-1,pm=0,Hl=1,u0=2,L2=3,P2=4,I2=5,U2=6,xw=7,$w=8,l3=typeof self=="object"?self:globalThis,nq=(e,t)=>{const n=(i,a)=>(e.set(a,i),i),r=i=>{if(e.has(i))return e.get(i);const[a,s]=t[i];switch(a){case pm:case vw:return n(s,i);case Hl:{const o=n([],i);for(const l of s)o.push(r(l));return o}case u0:{const o=n({},i);for(const[l,c]of s)o[r(l)]=r(c);return o}case L2:return n(new Date(s),i);case P2:{const{source:o,flags:l}=s;return n(new RegExp(o,l),i)}case I2:{const o=n(new Map,i);for(const[l,c]of s)o.set(r(l),r(c));return o}case U2:{const o=n(new Set,i);for(const l of s)o.add(r(l));return o}case xw:{const{name:o,message:l}=s;return n(new l3[o](l),i)}case $w:return n(BigInt(s),i);case"BigInt":return n(Object(BigInt(s)),i);case"ArrayBuffer":return n(new Uint8Array(s).buffer,s);case"DataView":{const{buffer:o}=new Uint8Array(s);return n(new DataView(o),s)}}return n(new l3[a](s),i)};return r},c3=e=>nq(new Map,e)(0),Ps="",{toString:rq}={},{keys:iq}=Object,fl=e=>{const t=typeof e;if(t!=="object"||!e)return[pm,t];const n=rq.call(e).slice(8,-1);switch(n){case"Array":return[Hl,Ps];case"Object":return[u0,Ps];case"Date":return[L2,Ps];case"RegExp":return[P2,Ps];case"Map":return[I2,Ps];case"Set":return[U2,Ps];case"DataView":return[Hl,n]}return n.includes("Array")?[Hl,n]:n.includes("Error")?[xw,n]:[u0,n]},Bu=([e,t])=>e===pm&&(t==="function"||t==="symbol"),aq=(e,t,n,r)=>{const i=(s,o)=>{const l=r.push(s)-1;return n.set(o,l),l},a=s=>{if(n.has(s))return n.get(s);let[o,l]=fl(s);switch(o){case pm:{let u=s;switch(l){case"bigint":o=$w,u=s.toString();break;case"function":case"symbol":if(e)throw new TypeError("unable to serialize "+l);u=null;break;case"undefined":return i([vw],s)}return i([o,u],s)}case Hl:{if(l){let m=s;return l==="DataView"?m=new Uint8Array(s.buffer):l==="ArrayBuffer"&&(m=new Uint8Array(s)),i([l,[...m]],s)}const u=[],d=i([o,u],s);for(const m of s)u.push(a(m));return d}case u0:{if(l)switch(l){case"BigInt":return i([l,s.toString()],s);case"Boolean":case"Number":case"String":return i([l,s.valueOf()],s)}if(t&&"toJSON"in s)return a(s.toJSON());const u=[],d=i([o,u],s);for(const m of iq(s))(e||!Bu(fl(s[m])))&&u.push([a(m),a(s[m])]);return d}case L2:return i([o,s.toISOString()],s);case P2:{const{source:u,flags:d}=s;return i([o,{source:u,flags:d}],s)}case I2:{const u=[],d=i([o,u],s);for(const[m,p]of s)(e||!(Bu(fl(m))||Bu(fl(p))))&&u.push([a(m),a(p)]);return d}case U2:{const u=[],d=i([o,u],s);for(const m of s)(e||!Bu(fl(m)))&&u.push(a(m));return d}}const{message:c}=s;return i([o,{name:l,message:c}],s)};return a},u3=(e,{json:t,lossy:n}={})=>{const r=[];return aq(!(t||n),!!t,new Map,r)(e),r},Uo=typeof structuredClone=="function"?(e,t)=>t&&("json"in t||"lossy"in t)?c3(u3(e,t)):structuredClone(e):(e,t)=>c3(u3(e,t));function sq(e,t){const n=[{type:"text",value:"↩"}];return t>1&&n.push({type:"element",tagName:"sup",properties:{},children:[{type:"text",value:String(t)}]}),n}function oq(e,t){return"Back to reference "+(e+1)+(t>1?"-"+t:"")}function lq(e){const t=typeof e.options.clobberPrefix=="string"?e.options.clobberPrefix:"user-content-",n=e.options.footnoteBackContent||sq,r=e.options.footnoteBackLabel||oq,i=e.options.footnoteLabel||"Footnotes",a=e.options.footnoteLabelTagName||"h2",s=e.options.footnoteLabelProperties||{className:["sr-only"]},o=[];let l=-1;for(;++l<e.footnoteOrder.length;){const c=e.footnoteById.get(e.footnoteOrder[l]);if(!c)continue;const u=e.all(c),d=String(c.identifier).toUpperCase(),m=Xo(d.toLowerCase());let p=0;const x=[],g=e.footnoteCounts.get(d);for(;g!==void 0&&++p<=g;){x.length>0&&x.push({type:"text",value:" "});let $=typeof n=="string"?n:n(l,p);typeof $=="string"&&($={type:"text",value:$}),x.push({type:"element",tagName:"a",properties:{href:"#"+t+"fnref-"+m+(p>1?"-"+p:""),dataFootnoteBackref:"",ariaLabel:typeof r=="string"?r:r(l,p),className:["data-footnote-backref"]},children:Array.isArray($)?$:[$]})}const w=u[u.length-1];if(w&&w.type==="element"&&w.tagName==="p"){const $=w.children[w.children.length-1];$&&$.type==="text"?$.value+=" ":w.children.push({type:"text",value:" "}),w.children.push(...x)}else u.push(...x);const v={type:"element",tagName:"li",properties:{id:t+"fn-"+m},children:e.wrap(u,!0)};e.patch(c,v),o.push(v)}if(o.length!==0)return{type:"element",tagName:"section",properties:{dataFootnotes:!0,className:["footnotes"]},children:[{type:"element",tagName:a,properties:{...Uo(s),id:"footnote-label"},children:[{type:"text",value:i}]},{type:"text",value:`
`},{type:"element",tagName:"ol",properties:{},children:e.wrap(o,!0)},{type:"text",value:`
`}]}}const D2=function(e){if(e==null)return mq;if(typeof e=="function")return fm(e);if(typeof e=="object")return Array.isArray(e)?cq(e):uq(e);if(typeof e=="string")return dq(e);throw new Error("Expected function, string, or object as test")};function cq(e){const t=[];let n=-1;for(;++n<e.length;)t[n]=D2(e[n]);return fm(r);function r(...i){let a=-1;for(;++a<t.length;)if(t[a].apply(this,i))return!0;return!1}}function uq(e){const t=e;return fm(n);function n(r){const i=r;let a;for(a in e)if(i[a]!==t[a])return!1;return!0}}function dq(e){return fm(t);function t(n){return n&&n.type===e}}function fm(e){return t;function t(n,r,i){return!!(hq(n)&&e.call(this,n,typeof r=="number"?r:void 0,i||void 0))}}function mq(){return!0}function hq(e){return e!==null&&typeof e=="object"&&"type"in e}const yw=[],pq=!0,d3=!1,_w="skip";function ww(e,t,n,r){let i;typeof t=="function"&&typeof n!="function"?(r=n,n=t):i=t;const a=D2(i),s=r?-1:1;o(e,void 0,[])();function o(l,c,u){const d=l&&typeof l=="object"?l:{};if(typeof d.type=="string"){const p=typeof d.tagName=="string"?d.tagName:typeof d.name=="string"?d.name:void 0;Object.defineProperty(m,"name",{value:"node ("+(l.type+(p?"<"+p+">":""))+")"})}return m;function m(){let p=yw,x,g,w;if((!t||a(l,c,u[u.length-1]||void 0))&&(p=fq(n(l,u)),p[0]===d3))return p;if("children"in l&&l.children){const v=l;if(v.children&&p[0]!==_w)for(g=(r?v.children.length:-1)+s,w=u.concat(v);g>-1&&g<v.children.length;){const $=v.children[g];if(x=o($,g,w)(),x[0]===d3)return x;g=typeof x[1]=="number"?x[1]:g+s}}return p}}}function fq(e){return Array.isArray(e)?e:typeof e=="number"?[pq,e]:e==null?yw:[e]}function M2(e,t,n,r){let i,a,s;typeof t=="function"&&typeof n!="function"?(a=void 0,s=t,i=n):(a=t,s=n,i=r),ww(e,a,o,i);function o(l,c){const u=c[c.length-1],d=u?u.children.indexOf(l):void 0;return s(l,d,u)}}const Yf={}.hasOwnProperty,gq={};function bq(e,t){const n=t||gq,r=new Map,i=new Map,a=new Map,s={...tq,...n.handlers},o={all:c,applyData:xq,definitionById:r,footnoteById:i,footnoteCounts:a,footnoteOrder:[],handlers:s,one:l,options:n,patch:vq,wrap:yq};return M2(e,function(u){if(u.type==="definition"||u.type==="footnoteDefinition"){const d=u.type==="definition"?r:i,m=String(u.identifier).toUpperCase();d.has(m)||d.set(m,u)}}),o;function l(u,d){const m=u.type,p=o.handlers[m];if(Yf.call(o.handlers,m)&&p)return p(o,u,d);if(o.options.passThrough&&o.options.passThrough.includes(m)){if("children"in u){const{children:g,...w}=u,v=Uo(w);return v.children=o.all(u),v}return Uo(u)}return(o.options.unknownHandler||$q)(o,u,d)}function c(u){const d=[];if("children"in u){const m=u.children;let p=-1;for(;++p<m.length;){const x=o.one(m[p],u);if(x){if(p&&m[p-1].type==="break"&&(!Array.isArray(x)&&x.type==="text"&&(x.value=m3(x.value)),!Array.isArray(x)&&x.type==="element")){const g=x.children[0];g&&g.type==="text"&&(g.value=m3(g.value))}Array.isArray(x)?d.push(...x):d.push(x)}}}return d}}function vq(e,t){e.position&&(t.position=rj(e))}function xq(e,t){let n=t;if(e&&e.data){const r=e.data.hName,i=e.data.hChildren,a=e.data.hProperties;if(typeof r=="string")if(n.type==="element")n.tagName=r;else{const s="children"in n?n.children:[n];n={type:"element",tagName:r,properties:{},children:s}}n.type==="element"&&a&&Object.assign(n.properties,Uo(a)),"children"in n&&n.children&&i!==null&&i!==void 0&&(n.children=i)}return n}function $q(e,t){const n=t.data||{},r="value"in t&&!(Yf.call(n,"hProperties")||Yf.call(n,"hChildren"))?{type:"text",value:t.value}:{type:"element",tagName:"div",properties:{},children:e.all(t)};return e.patch(t,r),e.applyData(t,r)}function yq(e,t){const n=[];let r=-1;for(t&&n.push({type:"text",value:`
`});++r<e.length;)r&&n.push({type:"text",value:`
`}),n.push(e[r]);return t&&e.length>0&&n.push({type:"text",value:`
`}),n}function m3(e){let t=0,n=e.charCodeAt(t);for(;n===9||n===32;)t++,n=e.charCodeAt(t);return e.slice(t)}function h3(e,t){const n=bq(e,t),r=n.one(e,void 0),i=lq(n),a=Array.isArray(r)?{type:"root",children:r}:r||{type:"root",children:[]};return i&&a.children.push({type:"text",value:`
`},i),a}function _q(e,t){return e&&"run"in e?async function(n,r){const i=h3(n,{file:r,...t});await e.run(i,r)}:function(n,r){return h3(n,{file:r,...e||t})}}function p3(e){if(e)throw e}var yd=Object.prototype.hasOwnProperty,Tw=Object.prototype.toString,f3=Object.defineProperty,g3=Object.getOwnPropertyDescriptor,b3=function(t){return typeof Array.isArray=="function"?Array.isArray(t):Tw.call(t)==="[object Array]"},v3=function(t){if(!t||Tw.call(t)!=="[object Object]")return!1;var n=yd.call(t,"constructor"),r=t.constructor&&t.constructor.prototype&&yd.call(t.constructor.prototype,"isPrototypeOf");if(t.constructor&&!n&&!r)return!1;var i;for(i in t);return typeof i>"u"||yd.call(t,i)},x3=function(t,n){f3&&n.name==="__proto__"?f3(t,n.name,{enumerable:!0,configurable:!0,value:n.newValue,writable:!0}):t[n.name]=n.newValue},$3=function(t,n){if(n==="__proto__")if(yd.call(t,n)){if(g3)return g3(t,n).value}else return;return t[n]},wq=function e(){var t,n,r,i,a,s,o=arguments[0],l=1,c=arguments.length,u=!1;for(typeof o=="boolean"&&(u=o,o=arguments[1]||{},l=2),(o==null||typeof o!="object"&&typeof o!="function")&&(o={});l<c;++l)if(t=arguments[l],t!=null)for(n in t)r=$3(o,n),i=$3(t,n),o!==i&&(u&&i&&(v3(i)||(a=b3(i)))?(a?(a=!1,s=r&&b3(r)?r:[]):s=r&&v3(r)?r:{},x3(o,{name:n,newValue:e(u,s,i)})):typeof i<"u"&&x3(o,{name:n,newValue:i}));return o};const Dh=x0(wq);function Xf(e){if(typeof e!="object"||e===null)return!1;const t=Object.getPrototypeOf(e);return(t===null||t===Object.prototype||Object.getPrototypeOf(t)===null)&&!(Symbol.toStringTag in e)&&!(Symbol.iterator in e)}function Tq(){const e=[],t={run:n,use:r};return t;function n(...i){let a=-1;const s=i.pop();if(typeof s!="function")throw new TypeError("Expected function as last argument, not "+s);o(null,...i);function o(l,...c){const u=e[++a];let d=-1;if(l){s(l);return}for(;++d<i.length;)(c[d]===null||c[d]===void 0)&&(c[d]=i[d]);i=c,u?Eq(u,o)(...c):s(null,...c)}}function r(i){if(typeof i!="function")throw new TypeError("Expected `middelware` to be a function, not "+i);return e.push(i),t}}function Eq(e,t){let n;return r;function r(...s){const o=e.length>s.length;let l;o&&s.push(i);try{l=e.apply(this,s)}catch(c){const u=c;if(o&&n)throw u;return i(u)}o||(l&&l.then&&typeof l.then=="function"?l.then(a,i):l instanceof Error?i(l):a(l))}function i(s,...o){n||(n=!0,t(s,...o))}function a(s){i(null,s)}}const Hr={basename:kq,dirname:Sq,extname:Nq,join:Cq,sep:"/"};function kq(e,t){if(t!==void 0&&typeof t!="string")throw new TypeError('"ext" argument must be a string');Zc(e);let n=0,r=-1,i=e.length,a;if(t===void 0||t.length===0||t.length>e.length){for(;i--;)if(e.codePointAt(i)===47){if(a){n=i+1;break}}else r<0&&(a=!0,r=i+1);return r<0?"":e.slice(n,r)}if(t===e)return"";let s=-1,o=t.length-1;for(;i--;)if(e.codePointAt(i)===47){if(a){n=i+1;break}}else s<0&&(a=!0,s=i+1),o>-1&&(e.codePointAt(i)===t.codePointAt(o--)?o<0&&(r=i):(o=-1,r=s));return n===r?r=s:r<0&&(r=e.length),e.slice(n,r)}function Sq(e){if(Zc(e),e.length===0)return".";let t=-1,n=e.length,r;for(;--n;)if(e.codePointAt(n)===47){if(r){t=n;break}}else r||(r=!0);return t<0?e.codePointAt(0)===47?"/":".":t===1&&e.codePointAt(0)===47?"//":e.slice(0,t)}function Nq(e){Zc(e);let t=e.length,n=-1,r=0,i=-1,a=0,s;for(;t--;){const o=e.codePointAt(t);if(o===47){if(s){r=t+1;break}continue}n<0&&(s=!0,n=t+1),o===46?i<0?i=t:a!==1&&(a=1):i>-1&&(a=-1)}return i<0||n<0||a===0||a===1&&i===n-1&&i===r+1?"":e.slice(i,n)}function Cq(...e){let t=-1,n;for(;++t<e.length;)Zc(e[t]),e[t]&&(n=n===void 0?e[t]:n+"/"+e[t]);return n===void 0?".":Aq(n)}function Aq(e){Zc(e);const t=e.codePointAt(0)===47;let n=Lq(e,!t);return n.length===0&&!t&&(n="."),n.length>0&&e.codePointAt(e.length-1)===47&&(n+="/"),t?"/"+n:n}function Lq(e,t){let n="",r=0,i=-1,a=0,s=-1,o,l;for(;++s<=e.length;){if(s<e.length)o=e.codePointAt(s);else{if(o===47)break;o=47}if(o===47){if(!(i===s-1||a===1))if(i!==s-1&&a===2){if(n.length<2||r!==2||n.codePointAt(n.length-1)!==46||n.codePointAt(n.length-2)!==46){if(n.length>2){if(l=n.lastIndexOf("/"),l!==n.length-1){l<0?(n="",r=0):(n=n.slice(0,l),r=n.length-1-n.lastIndexOf("/")),i=s,a=0;continue}}else if(n.length>0){n="",r=0,i=s,a=0;continue}}t&&(n=n.length>0?n+"/..":"..",r=2)}else n.length>0?n+="/"+e.slice(i+1,s):n=e.slice(i+1,s),r=s-i-1;i=s,a=0}else o===46&&a>-1?a++:a=-1}return n}function Zc(e){if(typeof e!="string")throw new TypeError("Path must be a string. Received "+JSON.stringify(e))}const Pq={cwd:Iq};function Iq(){return"/"}function Qf(e){return!!(e!==null&&typeof e=="object"&&"href"in e&&e.href&&"protocol"in e&&e.protocol&&e.auth===void 0)}function Uq(e){if(typeof e=="string")e=new URL(e);else if(!Qf(e)){const t=new TypeError('The "path" argument must be of type string or an instance of URL. Received `'+e+"`");throw t.code="ERR_INVALID_ARG_TYPE",t}if(e.protocol!=="file:"){const t=new TypeError("The URL must be of scheme file");throw t.code="ERR_INVALID_URL_SCHEME",t}return Dq(e)}function Dq(e){if(e.hostname!==""){const r=new TypeError('File URL host must be "localhost" or empty on darwin');throw r.code="ERR_INVALID_FILE_URL_HOST",r}const t=e.pathname;let n=-1;for(;++n<t.length;)if(t.codePointAt(n)===37&&t.codePointAt(n+1)===50){const r=t.codePointAt(n+2);if(r===70||r===102){const i=new TypeError("File URL path must not include encoded / characters");throw i.code="ERR_INVALID_FILE_URL_PATH",i}}return decodeURIComponent(t)}const Mh=["history","path","basename","stem","extname","dirname"];class Ew{constructor(t){let n;t?Qf(t)?n={path:t}:typeof t=="string"||Mq(t)?n={value:t}:n=t:n={},this.cwd="cwd"in n?"":Pq.cwd(),this.data={},this.history=[],this.messages=[],this.value,this.map,this.result,this.stored;let r=-1;for(;++r<Mh.length;){const a=Mh[r];a in n&&n[a]!==void 0&&n[a]!==null&&(this[a]=a==="history"?[...n[a]]:n[a])}let i;for(i in n)Mh.includes(i)||(this[i]=n[i])}get basename(){return typeof this.path=="string"?Hr.basename(this.path):void 0}set basename(t){Oh(t,"basename"),Rh(t,"basename"),this.path=Hr.join(this.dirname||"",t)}get dirname(){return typeof this.path=="string"?Hr.dirname(this.path):void 0}set dirname(t){y3(this.basename,"dirname"),this.path=Hr.join(t||"",this.basename)}get extname(){return typeof this.path=="string"?Hr.extname(this.path):void 0}set extname(t){if(Rh(t,"extname"),y3(this.dirname,"extname"),t){if(t.codePointAt(0)!==46)throw new Error("`extname` must start with `.`");if(t.includes(".",1))throw new Error("`extname` cannot contain multiple dots")}this.path=Hr.join(this.dirname,this.stem+(t||""))}get path(){return this.history[this.history.length-1]}set path(t){Qf(t)&&(t=Uq(t)),Oh(t,"path"),this.path!==t&&this.history.push(t)}get stem(){return typeof this.path=="string"?Hr.basename(this.path,this.extname):void 0}set stem(t){Oh(t,"stem"),Rh(t,"stem"),this.path=Hr.join(this.dirname||"",t+(this.extname||""))}fail(t,n,r){const i=this.message(t,n,r);throw i.fatal=!0,i}info(t,n,r){const i=this.message(t,n,r);return i.fatal=void 0,i}message(t,n,r){const i=new on(t,n,r);return this.path&&(i.name=this.path+":"+i.name,i.file=this.path),i.fatal=!1,this.messages.push(i),i}toString(t){return this.value===void 0?"":typeof this.value=="string"?this.value:new TextDecoder(t||void 0).decode(this.value)}}function Rh(e,t){if(e&&e.includes(Hr.sep))throw new Error("`"+t+"` cannot be a path: did not expect `"+Hr.sep+"`")}function Oh(e,t){if(!e)throw new Error("`"+t+"` cannot be empty")}function y3(e,t){if(!e)throw new Error("Setting `"+t+"` requires `path` to be set too")}function Mq(e){return!!(e&&typeof e=="object"&&"byteLength"in e&&"byteOffset"in e)}const Rq=function(e){const r=this.constructor.prototype,i=r[e],a=function(){return i.apply(a,arguments)};return Object.setPrototypeOf(a,r),a},Oq={}.hasOwnProperty;class R2 extends Rq{constructor(){super("copy"),this.Compiler=void 0,this.Parser=void 0,this.attachers=[],this.compiler=void 0,this.freezeIndex=-1,this.frozen=void 0,this.namespace={},this.parser=void 0,this.transformers=Tq()}copy(){const t=new R2;let n=-1;for(;++n<this.attachers.length;){const r=this.attachers[n];t.use(...r)}return t.data(Dh(!0,{},this.namespace)),t}data(t,n){return typeof t=="string"?arguments.length===2?(Fh("data",this.frozen),this.namespace[t]=n,this):Oq.call(this.namespace,t)&&this.namespace[t]||void 0:t?(Fh("data",this.frozen),this.namespace=t,this):this.namespace}freeze(){if(this.frozen)return this;const t=this;for(;++this.freezeIndex<this.attachers.length;){const[n,...r]=this.attachers[this.freezeIndex];if(r[0]===!1)continue;r[0]===!0&&(r[0]=void 0);const i=n.call(t,...r);typeof i=="function"&&this.transformers.use(i)}return this.frozen=!0,this.freezeIndex=Number.POSITIVE_INFINITY,this}parse(t){this.freeze();const n=Fu(t),r=this.parser||this.Parser;return zh("parse",r),r(String(n),n)}process(t,n){const r=this;return this.freeze(),zh("process",this.parser||this.Parser),Bh("process",this.compiler||this.Compiler),n?i(void 0,n):new Promise(i);function i(a,s){const o=Fu(t),l=r.parse(o);r.run(l,o,function(u,d,m){if(u||!d||!m)return c(u);const p=d,x=r.stringify(p,m);Fq(x)?m.value=x:m.result=x,c(u,m)});function c(u,d){u||!d?s(u):a?a(d):n(void 0,d)}}}processSync(t){let n=!1,r;return this.freeze(),zh("processSync",this.parser||this.Parser),Bh("processSync",this.compiler||this.Compiler),this.process(t,i),w3("processSync","process",n),r;function i(a,s){n=!0,p3(a),r=s}}run(t,n,r){_3(t),this.freeze();const i=this.transformers;return!r&&typeof n=="function"&&(r=n,n=void 0),r?a(void 0,r):new Promise(a);function a(s,o){const l=Fu(n);i.run(t,l,c);function c(u,d,m){const p=d||t;u?o(u):s?s(p):r(void 0,p,m)}}}runSync(t,n){let r=!1,i;return this.run(t,n,a),w3("runSync","run",r),i;function a(s,o){p3(s),i=o,r=!0}}stringify(t,n){this.freeze();const r=Fu(n),i=this.compiler||this.Compiler;return Bh("stringify",i),_3(t),i(t,r)}use(t,...n){const r=this.attachers,i=this.namespace;if(Fh("use",this.frozen),t!=null)if(typeof t=="function")l(t,n);else if(typeof t=="object")Array.isArray(t)?o(t):s(t);else throw new TypeError("Expected usable value, not `"+t+"`");return this;function a(c){if(typeof c=="function")l(c,[]);else if(typeof c=="object")if(Array.isArray(c)){const[u,...d]=c;l(u,d)}else s(c);else throw new TypeError("Expected usable value, not `"+c+"`")}function s(c){if(!("plugins"in c)&&!("settings"in c))throw new Error("Expected usable value but received an empty preset, which is probably a mistake: presets typically come with `plugins` and sometimes with `settings`, but this has neither");o(c.plugins),c.settings&&(i.settings=Dh(!0,i.settings,c.settings))}function o(c){let u=-1;if(c!=null)if(Array.isArray(c))for(;++u<c.length;){const d=c[u];a(d)}else throw new TypeError("Expected a list of plugins, not `"+c+"`")}function l(c,u){let d=-1,m=-1;for(;++d<r.length;)if(r[d][0]===c){m=d;break}if(m===-1)r.push([c,...u]);else if(u.length>0){let[p,...x]=u;const g=r[m][1];Xf(g)&&Xf(p)&&(p=Dh(!0,g,p)),r[m]=[c,p,...x]}}}}const zq=new R2().freeze();function zh(e,t){if(typeof t!="function")throw new TypeError("Cannot `"+e+"` without `parser`")}function Bh(e,t){if(typeof t!="function")throw new TypeError("Cannot `"+e+"` without `compiler`")}function Fh(e,t){if(t)throw new Error("Cannot call `"+e+"` on a frozen processor.\nCreate a new processor first, by calling it: use `processor()` instead of `processor`.")}function _3(e){if(!Xf(e)||typeof e.type!="string")throw new TypeError("Expected node, got `"+e+"`")}function w3(e,t,n){if(!n)throw new Error("`"+e+"` finished async. Use `"+t+"` instead")}function Fu(e){return Bq(e)?e:new Ew(e)}function Bq(e){return!!(e&&typeof e=="object"&&"message"in e&&"messages"in e)}function Fq(e){return typeof e=="string"||jq(e)}function jq(e){return!!(e&&typeof e=="object"&&"byteLength"in e&&"byteOffset"in e)}const Vq="https://github.com/remarkjs/react-markdown/blob/main/changelog.md",T3=[],E3={allowDangerousHtml:!0},Hq=/^(https?|ircs?|mailto|xmpp)$/i,qq=[{from:"astPlugins",id:"remove-buggy-html-in-markdown-parser"},{from:"allowDangerousHtml",id:"remove-buggy-html-in-markdown-parser"},{from:"allowNode",id:"replace-allownode-allowedtypes-and-disallowedtypes",to:"allowElement"},{from:"allowedTypes",id:"replace-allownode-allowedtypes-and-disallowedtypes",to:"allowedElements"},{from:"className",id:"remove-classname"},{from:"disallowedTypes",id:"replace-allownode-allowedtypes-and-disallowedtypes",to:"disallowedElements"},{from:"escapeHtml",id:"remove-buggy-html-in-markdown-parser"},{from:"includeElementIndex",id:"#remove-includeelementindex"},{from:"includeNodeIndex",id:"change-includenodeindex-to-includeelementindex"},{from:"linkTarget",id:"remove-linktarget"},{from:"plugins",id:"change-plugins-to-remarkplugins",to:"remarkPlugins"},{from:"rawSourcePos",id:"#remove-rawsourcepos"},{from:"renderers",id:"change-renderers-to-components",to:"components"},{from:"source",id:"change-source-to-children",to:"children"},{from:"sourcePos",id:"#remove-sourcepos"},{from:"transformImageUri",id:"#add-urltransform",to:"urlTransform"},{from:"transformLinkUri",id:"#add-urltransform",to:"urlTransform"}];function Gq(e){const t=Wq(e),n=Kq(e);return Yq(t.runSync(t.parse(n),n),e)}function Wq(e){const t=e.rehypePlugins||T3,n=e.remarkPlugins||T3,r=e.remarkRehypeOptions?{...e.remarkRehypeOptions,...E3}:E3;return zq().use(CH).use(n).use(_q,r).use(t)}function Kq(e){const t=e.children||"",n=new Ew;return typeof t=="string"&&(n.value=t),n}function Yq(e,t){const n=t.allowedElements,r=t.allowElement,i=t.components,a=t.disallowedElements,s=t.skipHtml,o=t.unwrapDisallowed,l=t.urlTransform||Xq;for(const u of qq)Object.hasOwn(t,u.from)&&(""+u.from+(u.to?"use `"+u.to+"` instead":"remove it")+Vq+u.id,void 0);return M2(e,c),lj(e,{Fragment:h.Fragment,components:i,ignoreInvalidStyle:!0,jsx:h.jsx,jsxs:h.jsxs,passKeys:!0,passNode:!0});function c(u,d,m){if(u.type==="raw"&&m&&typeof d=="number")return s?m.children.splice(d,1):m.children[d]={type:"text",value:u.value},d;if(u.type==="element"){let p;for(p in Ph)if(Object.hasOwn(Ph,p)&&Object.hasOwn(u.properties,p)){const x=u.properties[p],g=Ph[p];(g===null||g.includes(u.tagName))&&(u.properties[p]=l(String(x||""),p,u))}}if(u.type==="element"){let p=n?!n.includes(u.tagName):a?a.includes(u.tagName):!1;if(!p&&r&&typeof d=="number"&&(p=!r(u,d,m)),p&&m&&typeof d=="number")return o&&u.children?m.children.splice(d,1,...u.children):m.children.splice(d,1),d}}}function Xq(e){const t=e.indexOf(":"),n=e.indexOf("?"),r=e.indexOf("#"),i=e.indexOf("/");return t===-1||i!==-1&&t>i||n!==-1&&t>n||r!==-1&&t>r||Hq.test(e.slice(0,t))?e:""}function Qq(e,t){const n=String(e);let r=n.indexOf(t),i=r,a=0,s=0;for(;r!==-1;)r===i?++a>s&&(s=a):a=1,i=r+t.length,r=n.indexOf(t,i);return s}function Zq(){return{enter:{mathFlow:e,mathFlowFenceMeta:t,mathText:a},exit:{mathFlow:i,mathFlowFence:r,mathFlowFenceMeta:n,mathFlowValue:o,mathText:s,mathTextData:o}};function e(l){const c={type:"element",tagName:"code",properties:{className:["language-math","math-display"]},children:[]};this.enter({type:"math",meta:null,value:"",data:{hName:"pre",hChildren:[c]}},l)}function t(){this.buffer()}function n(){const l=this.resume(),c=this.stack[this.stack.length-1];c.type,c.meta=l}function r(){this.data.mathFlowInside||(this.buffer(),this.data.mathFlowInside=!0)}function i(l){const c=this.resume().replace(/^(\r?\n|\r)|(\r?\n|\r)$/g,""),u=this.stack[this.stack.length-1];u.type,this.exit(l),u.value=c;const d=u.data.hChildren[0];d.type,d.tagName,d.children.push({type:"text",value:c}),this.data.mathFlowInside=void 0}function a(l){this.enter({type:"inlineMath",value:"",data:{hName:"code",hProperties:{className:["language-math","math-inline"]},hChildren:[]}},l),this.buffer()}function s(l){const c=this.resume(),u=this.stack[this.stack.length-1];u.type,this.exit(l),u.value=c,u.data.hChildren.push({type:"text",value:c})}function o(l){this.config.enter.data.call(this,l),this.config.exit.data.call(this,l)}}function Jq(e){let t=(e||{}).singleDollarTextMath;return t==null&&(t=!0),r.peek=i,{unsafe:[{character:"\r",inConstruct:"mathFlowMeta"},{character:`
`,inConstruct:"mathFlowMeta"},{character:"$",after:t?void 0:"\\$",inConstruct:"phrasing"},{character:"$",inConstruct:"mathFlowMeta"},{atBreak:!0,character:"$",after:"\\$"}],handlers:{math:n,inlineMath:r}};function n(a,s,o,l){const c=a.value||"",u=o.createTracker(l),d="$".repeat(Math.max(Qq(c,"$")+1,2)),m=o.enter("mathFlow");let p=u.move(d);if(a.meta){const x=o.enter("mathFlowMeta");p+=u.move(o.safe(a.meta,{after:`
`,before:p,encode:["$"],...u.current()})),x()}return p+=u.move(`
`),c&&(p+=u.move(c+`
`)),p+=u.move(d),m(),p}function r(a,s,o){let l=a.value||"",c=1;for(t||c++;new RegExp("(^|[^$])"+"\\$".repeat(c)+"([^$]|$)").test(l);)c++;const u="$".repeat(c);/[^ \r\n]/.test(l)&&(/^[ \r\n]/.test(l)&&/[ \r\n]$/.test(l)||/^\$|\$$/.test(l))&&(l=" "+l+" ");let d=-1;for(;++d<o.unsafe.length;){const m=o.unsafe[d];if(!m.atBreak)continue;const p=o.compilePattern(m);let x;for(;x=p.exec(l);){let g=x.index;l.codePointAt(g)===10&&l.codePointAt(g-1)===13&&g--,l=l.slice(0,g)+" "+l.slice(x.index+1)}}return u+l+u}function i(){return"$"}}const eG={tokenize:tG,concrete:!0,name:"mathFlow"},k3={tokenize:nG,partial:!0};function tG(e,t,n){const r=this,i=r.events[r.events.length-1],a=i&&i[1].type==="linePrefix"?i[2].sliceSerialize(i[1],!0).length:0;let s=0;return o;function o($){return e.enter("mathFlow"),e.enter("mathFlowFence"),e.enter("mathFlowFenceSequence"),l($)}function l($){return $===36?(e.consume($),s++,l):s<2?n($):(e.exit("mathFlowFenceSequence"),Ze(e,c,"whitespace")($))}function c($){return $===null||$e($)?d($):(e.enter("mathFlowFenceMeta"),e.enter("chunkString",{contentType:"string"}),u($))}function u($){return $===null||$e($)?(e.exit("chunkString"),e.exit("mathFlowFenceMeta"),d($)):$===36?n($):(e.consume($),u)}function d($){return e.exit("mathFlowFence"),r.interrupt?t($):e.attempt(k3,m,w)($)}function m($){return e.attempt({tokenize:v,partial:!0},w,p)($)}function p($){return(a?Ze(e,x,"linePrefix",a+1):x)($)}function x($){return $===null?w($):$e($)?e.attempt(k3,m,w)($):(e.enter("mathFlowValue"),g($))}function g($){return $===null||$e($)?(e.exit("mathFlowValue"),x($)):(e.consume($),g)}function w($){return e.exit("mathFlow"),t($)}function v($,_,C){let k=0;return Ze($,S,"linePrefix",r.parser.constructs.disable.null.includes("codeIndented")?void 0:4);function S(F){return $.enter("mathFlowFence"),$.enter("mathFlowFenceSequence"),L(F)}function L(F){return F===36?(k++,$.consume(F),L):k<s?C(F):($.exit("mathFlowFenceSequence"),Ze($,U,"whitespace")(F))}function U(F){return F===null||$e(F)?($.exit("mathFlowFence"),_(F)):C(F)}}}function nG(e,t,n){const r=this;return i;function i(s){return s===null?t(s):(e.enter("lineEnding"),e.consume(s),e.exit("lineEnding"),a)}function a(s){return r.parser.lazy[r.now().line]?n(s):t(s)}}function rG(e){let n=(e||{}).singleDollarTextMath;return n==null&&(n=!0),{tokenize:r,resolve:iG,previous:aG,name:"mathText"};function r(i,a,s){let o=0,l,c;return u;function u(g){return i.enter("mathText"),i.enter("mathTextSequence"),d(g)}function d(g){return g===36?(i.consume(g),o++,d):o<2&&!n?s(g):(i.exit("mathTextSequence"),m(g))}function m(g){return g===null?s(g):g===36?(c=i.enter("mathTextSequence"),l=0,x(g)):g===32?(i.enter("space"),i.consume(g),i.exit("space"),m):$e(g)?(i.enter("lineEnding"),i.consume(g),i.exit("lineEnding"),m):(i.enter("mathTextData"),p(g))}function p(g){return g===null||g===32||g===36||$e(g)?(i.exit("mathTextData"),m(g)):(i.consume(g),p)}function x(g){return g===36?(i.consume(g),l++,x):l===o?(i.exit("mathTextSequence"),i.exit("mathText"),a(g)):(c.type="mathTextData",p(g))}}}function iG(e){let t=e.length-4,n=3,r,i;if((e[n][1].type==="lineEnding"||e[n][1].type==="space")&&(e[t][1].type==="lineEnding"||e[t][1].type==="space")){for(r=n;++r<t;)if(e[r][1].type==="mathTextData"){e[t][1].type="mathTextPadding",e[n][1].type="mathTextPadding",n+=2,t-=2;break}}for(r=n-1,t++;++r<=t;)i===void 0?r!==t&&e[r][1].type!=="lineEnding"&&(i=r):(r===t||e[r][1].type==="lineEnding")&&(e[i][1].type="mathTextData",r!==i+2&&(e[i][1].end=e[r-1][1].end,e.splice(i+2,r-i-2),t-=r-i-2,r=i+2),i=void 0);return e}function aG(e){return e!==36||this.events[this.events.length-1][1].type==="characterEscape"}function sG(e){return{flow:{36:eG},text:{36:rG(e)}}}class Fn{constructor(t,n,r){this.lexer=void 0,this.start=void 0,this.end=void 0,this.lexer=t,this.start=n,this.end=r}static range(t,n){return n?!t||!t.loc||!n.loc||t.loc.lexer!==n.loc.lexer?null:new Fn(t.loc.lexer,t.loc.start,n.loc.end):t&&t.loc}}class dr{constructor(t,n){this.text=void 0,this.loc=void 0,this.noexpand=void 0,this.treatAsRelax=void 0,this.text=t,this.loc=n}range(t,n){return new dr(n,Fn.range(this,t))}}class oe{constructor(t,n){this.name=void 0,this.position=void 0,this.length=void 0,this.rawMessage=void 0;var r="KaTeX parse error: "+t,i,a,s=n&&n.loc;if(s&&s.start<=s.end){var o=s.lexer.input;i=s.start,a=s.end,i===o.length?r+=" at end of input: ":r+=" at position "+(i+1)+": ";var l=o.slice(i,a).replace(/[^]/g,"$&̲"),c;i>15?c="…"+o.slice(i-15,i):c=o.slice(0,i);var u;a+15<o.length?u=o.slice(a,a+15)+"…":u=o.slice(a),r+=c+l+u}var d=new Error(r);return d.name="ParseError",d.__proto__=oe.prototype,d.position=i,i!=null&&a!=null&&(d.length=a-i),d.rawMessage=t,d}}oe.prototype.__proto__=Error.prototype;var oG=function(t,n){return t.indexOf(n)!==-1},lG=function(t,n){return t===void 0?n:t},cG=/([A-Z])/g,uG=function(t){return t.replace(cG,"-$1").toLowerCase()},dG={"&":"&amp;",">":"&gt;","<":"&lt;",'"':"&quot;","'":"&#x27;"},mG=/[&><"']/g;function hG(e){return String(e).replace(mG,t=>dG[t])}var kw=function e(t){return t.type==="ordgroup"||t.type==="color"?t.body.length===1?e(t.body[0]):t:t.type==="font"?e(t.body):t},pG=function(t){var n=kw(t);return n.type==="mathord"||n.type==="textord"||n.type==="atom"},fG=function(t){if(!t)throw new Error("Expected non-null, but got "+String(t));return t},gG=function(t){var n=/^[\x00-\x20]*([^\\/#?]*?)(:|&#0*58|&#x0*3a|&colon)/i.exec(t);return n?n[2]!==":"||!/^[a-zA-Z][a-zA-Z0-9+\-.]*$/.test(n[1])?null:n[1].toLowerCase():"_relative"},ye={contains:oG,deflt:lG,escape:hG,hyphenate:uG,getBaseElem:kw,isCharacterBox:pG,protocolFromUrl:gG},_d={displayMode:{type:"boolean",description:"Render math in display mode, which puts the math in display style (so \\int and \\sum are large, for example), and centers the math on the page on its own line.",cli:"-d, --display-mode"},output:{type:{enum:["htmlAndMathml","html","mathml"]},description:"Determines the markup language of the output.",cli:"-F, --format <type>"},leqno:{type:"boolean",description:"Render display math in leqno style (left-justified tags)."},fleqn:{type:"boolean",description:"Render display math flush left."},throwOnError:{type:"boolean",default:!0,cli:"-t, --no-throw-on-error",cliDescription:"Render errors (in the color given by --error-color) instead of throwing a ParseError exception when encountering an error."},errorColor:{type:"string",default:"#cc0000",cli:"-c, --error-color <color>",cliDescription:"A color string given in the format 'rgb' or 'rrggbb' (no #). This option determines the color of errors rendered by the -t option.",cliProcessor:e=>"#"+e},macros:{type:"object",cli:"-m, --macro <def>",cliDescription:"Define custom macro of the form '\\foo:expansion' (use multiple -m arguments for multiple macros).",cliDefault:[],cliProcessor:(e,t)=>(t.push(e),t)},minRuleThickness:{type:"number",description:"Specifies a minimum thickness, in ems, for fraction lines, `\\sqrt` top lines, `{array}` vertical lines, `\\hline`, `\\hdashline`, `\\underline`, `\\overline`, and the borders of `\\fbox`, `\\boxed`, and `\\fcolorbox`.",processor:e=>Math.max(0,e),cli:"--min-rule-thickness <size>",cliProcessor:parseFloat},colorIsTextColor:{type:"boolean",description:"Makes \\color behave like LaTeX's 2-argument \\textcolor, instead of LaTeX's one-argument \\color mode change.",cli:"-b, --color-is-text-color"},strict:{type:[{enum:["warn","ignore","error"]},"boolean","function"],description:"Turn on strict / LaTeX faithfulness mode, which throws an error if the input uses features that are not supported by LaTeX.",cli:"-S, --strict",cliDefault:!1},trust:{type:["boolean","function"],description:"Trust the input, enabling all HTML features such as \\url.",cli:"-T, --trust"},maxSize:{type:"number",default:1/0,description:"If non-zero, all user-specified sizes, e.g. in \\rule{500em}{500em}, will be capped to maxSize ems. Otherwise, elements and spaces can be arbitrarily large",processor:e=>Math.max(0,e),cli:"-s, --max-size <n>",cliProcessor:parseInt},maxExpand:{type:"number",default:1e3,description:"Limit the number of macro expansions to the specified number, to prevent e.g. infinite macro loops. If set to Infinity, the macro expander will try to fully expand as in LaTeX.",processor:e=>Math.max(0,e),cli:"-e, --max-expand <n>",cliProcessor:e=>e==="Infinity"?1/0:parseInt(e)},globalGroup:{type:"boolean",cli:!1}};function bG(e){if(e.default)return e.default;var t=e.type,n=Array.isArray(t)?t[0]:t;if(typeof n!="string")return n.enum[0];switch(n){case"boolean":return!1;case"string":return"";case"number":return 0;case"object":return{}}}class O2{constructor(t){this.displayMode=void 0,this.output=void 0,this.leqno=void 0,this.fleqn=void 0,this.throwOnError=void 0,this.errorColor=void 0,this.macros=void 0,this.minRuleThickness=void 0,this.colorIsTextColor=void 0,this.strict=void 0,this.trust=void 0,this.maxSize=void 0,this.maxExpand=void 0,this.globalGroup=void 0,t=t||{};for(var n in _d)if(_d.hasOwnProperty(n)){var r=_d[n];this[n]=t[n]!==void 0?r.processor?r.processor(t[n]):t[n]:bG(r)}}reportNonstrict(t,n,r){var i=this.strict;if(typeof i=="function"&&(i=i(t,n,r)),!(!i||i==="ignore")){if(i===!0||i==="error")throw new oe("LaTeX-incompatible input and strict mode is set to 'error': "+(n+" ["+t+"]"),r);i==="warn"?typeof console<"u"&&console.warn("LaTeX-incompatible input and strict mode is set to 'warn': "+(n+" ["+t+"]")):typeof console<"u"&&console.warn("LaTeX-incompatible input and strict mode is set to "+("unrecognized '"+i+"': "+n+" ["+t+"]"))}}useStrictBehavior(t,n,r){var i=this.strict;if(typeof i=="function")try{i=i(t,n,r)}catch{i="error"}return!i||i==="ignore"?!1:i===!0||i==="error"?!0:i==="warn"?(typeof console<"u"&&console.warn("LaTeX-incompatible input and strict mode is set to 'warn': "+(n+" ["+t+"]")),!1):(typeof console<"u"&&console.warn("LaTeX-incompatible input and strict mode is set to "+("unrecognized '"+i+"': "+n+" ["+t+"]")),!1)}isTrusted(t){if(t.url&&!t.protocol){var n=ye.protocolFromUrl(t.url);if(n==null)return!1;t.protocol=n}var r=typeof this.trust=="function"?this.trust(t):this.trust;return!!r}}class Wi{constructor(t,n,r){this.id=void 0,this.size=void 0,this.cramped=void 0,this.id=t,this.size=n,this.cramped=r}sup(){return Kr[vG[this.id]]}sub(){return Kr[xG[this.id]]}fracNum(){return Kr[$G[this.id]]}fracDen(){return Kr[yG[this.id]]}cramp(){return Kr[_G[this.id]]}text(){return Kr[wG[this.id]]}isTight(){return this.size>=2}}var z2=0,d0=1,uo=2,Si=3,Pc=4,lr=5,Do=6,pn=7,Kr=[new Wi(z2,0,!1),new Wi(d0,0,!0),new Wi(uo,1,!1),new Wi(Si,1,!0),new Wi(Pc,2,!1),new Wi(lr,2,!0),new Wi(Do,3,!1),new Wi(pn,3,!0)],vG=[Pc,lr,Pc,lr,Do,pn,Do,pn],xG=[lr,lr,lr,lr,pn,pn,pn,pn],$G=[uo,Si,Pc,lr,Do,pn,Do,pn],yG=[Si,Si,lr,lr,pn,pn,pn,pn],_G=[d0,d0,Si,Si,lr,lr,pn,pn],wG=[z2,d0,uo,Si,uo,Si,uo,Si],we={DISPLAY:Kr[z2],TEXT:Kr[uo],SCRIPT:Kr[Pc],SCRIPTSCRIPT:Kr[Do]},Zf=[{name:"latin",blocks:[[256,591],[768,879]]},{name:"cyrillic",blocks:[[1024,1279]]},{name:"armenian",blocks:[[1328,1423]]},{name:"brahmic",blocks:[[2304,4255]]},{name:"georgian",blocks:[[4256,4351]]},{name:"cjk",blocks:[[12288,12543],[19968,40879],[65280,65376]]},{name:"hangul",blocks:[[44032,55215]]}];function TG(e){for(var t=0;t<Zf.length;t++)for(var n=Zf[t],r=0;r<n.blocks.length;r++){var i=n.blocks[r];if(e>=i[0]&&e<=i[1])return n.name}return null}var wd=[];Zf.forEach(e=>e.blocks.forEach(t=>wd.push(...t)));function Sw(e){for(var t=0;t<wd.length;t+=2)if(e>=wd[t]&&e<=wd[t+1])return!0;return!1}var Is=80,EG=function(t,n){return"M95,"+(622+t+n)+`
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l`+t/2.075+" -"+t+`
c5.3,-9.3,12,-14,20,-14
H400000v`+(40+t)+`H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M`+(834+t)+" "+n+"h400000v"+(40+t)+"h-400000z"},kG=function(t,n){return"M263,"+(601+t+n)+`c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l`+t/2.084+" -"+t+`
c4.7,-7.3,11,-11,19,-11
H40000v`+(40+t)+`H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M`+(1001+t)+" "+n+"h400000v"+(40+t)+"h-400000z"},SG=function(t,n){return"M983 "+(10+t+n)+`
l`+t/3.13+" -"+t+`
c4,-6.7,10,-10,18,-10 H400000v`+(40+t)+`
H1013.1s-83.4,268,-264.1,840c-180.7,572,-277,876.3,-289,913c-4.7,4.7,-12.7,7,-24,7
s-12,0,-12,0c-1.3,-3.3,-3.7,-11.7,-7,-25c-35.3,-125.3,-106.7,-373.3,-214,-744
c-10,12,-21,25,-33,39s-32,39,-32,39c-6,-5.3,-15,-14,-27,-26s25,-30,25,-30
c26.7,-32.7,52,-63,76,-91s52,-60,52,-60s208,722,208,722
c56,-175.3,126.3,-397.3,211,-666c84.7,-268.7,153.8,-488.2,207.5,-658.5
c53.7,-170.3,84.5,-266.8,92.5,-289.5z
M`+(1001+t)+" "+n+"h400000v"+(40+t)+"h-400000z"},NG=function(t,n){return"M424,"+(2398+t+n)+`
c-1.3,-0.7,-38.5,-172,-111.5,-514c-73,-342,-109.8,-513.3,-110.5,-514
c0,-2,-10.7,14.3,-32,49c-4.7,7.3,-9.8,15.7,-15.5,25c-5.7,9.3,-9.8,16,-12.5,20
s-5,7,-5,7c-4,-3.3,-8.3,-7.7,-13,-13s-13,-13,-13,-13s76,-122,76,-122s77,-121,77,-121
s209,968,209,968c0,-2,84.7,-361.7,254,-1079c169.3,-717.3,254.7,-1077.7,256,-1081
l`+t/4.223+" -"+t+`c4,-6.7,10,-10,18,-10 H400000
v`+(40+t)+`H1014.6
s-87.3,378.7,-272.6,1166c-185.3,787.3,-279.3,1182.3,-282,1185
c-2,6,-10,9,-24,9
c-8,0,-12,-0.7,-12,-2z M`+(1001+t)+" "+n+`
h400000v`+(40+t)+"h-400000z"},CG=function(t,n){return"M473,"+(2713+t+n)+`
c339.3,-1799.3,509.3,-2700,510,-2702 l`+t/5.298+" -"+t+`
c3.3,-7.3,9.3,-11,18,-11 H400000v`+(40+t)+`H1017.7
s-90.5,478,-276.2,1466c-185.7,988,-279.5,1483,-281.5,1485c-2,6,-10,9,-24,9
c-8,0,-12,-0.7,-12,-2c0,-1.3,-5.3,-32,-16,-92c-50.7,-293.3,-119.7,-693.3,-207,-1200
c0,-1.3,-5.3,8.7,-16,30c-10.7,21.3,-21.3,42.7,-32,64s-16,33,-16,33s-26,-26,-26,-26
s76,-153,76,-153s77,-151,77,-151c0.7,0.7,35.7,202,105,604c67.3,400.7,102,602.7,104,
606zM`+(1001+t)+" "+n+"h400000v"+(40+t)+"H1017.7z"},AG=function(t){var n=t/2;return"M400000 "+t+" H0 L"+n+" 0 l65 45 L145 "+(t-80)+" H400000z"},LG=function(t,n,r){var i=r-54-n-t;return"M702 "+(t+n)+"H400000"+(40+t)+`
H742v`+i+`l-4 4-4 4c-.667.7 -2 1.5-4 2.5s-4.167 1.833-6.5 2.5-5.5 1-9.5 1
h-12l-28-84c-16.667-52-96.667 -294.333-240-727l-212 -643 -85 170
c-4-3.333-8.333-7.667-13 -13l-13-13l77-155 77-156c66 199.333 139 419.667
219 661 l218 661zM702 `+n+"H400000v"+(40+t)+"H742z"},PG=function(t,n,r){n=1e3*n;var i="";switch(t){case"sqrtMain":i=EG(n,Is);break;case"sqrtSize1":i=kG(n,Is);break;case"sqrtSize2":i=SG(n,Is);break;case"sqrtSize3":i=NG(n,Is);break;case"sqrtSize4":i=CG(n,Is);break;case"sqrtTall":i=LG(n,Is,r)}return i},IG=function(t,n){switch(t){case"⎜":return"M291 0 H417 V"+n+" H291z M291 0 H417 V"+n+" H291z";case"∣":return"M145 0 H188 V"+n+" H145z M145 0 H188 V"+n+" H145z";case"∥":return"M145 0 H188 V"+n+" H145z M145 0 H188 V"+n+" H145z"+("M367 0 H410 V"+n+" H367z M367 0 H410 V"+n+" H367z");case"⎟":return"M457 0 H583 V"+n+" H457z M457 0 H583 V"+n+" H457z";case"⎢":return"M319 0 H403 V"+n+" H319z M319 0 H403 V"+n+" H319z";case"⎥":return"M263 0 H347 V"+n+" H263z M263 0 H347 V"+n+" H263z";case"⎪":return"M384 0 H504 V"+n+" H384z M384 0 H504 V"+n+" H384z";case"⏐":return"M312 0 H355 V"+n+" H312z M312 0 H355 V"+n+" H312z";case"‖":return"M257 0 H300 V"+n+" H257z M257 0 H300 V"+n+" H257z"+("M478 0 H521 V"+n+" H478z M478 0 H521 V"+n+" H478z");default:return""}},S3={doubleleftarrow:`M262 157
l10-10c34-36 62.7-77 86-123 3.3-8 5-13.3 5-16 0-5.3-6.7-8-20-8-7.3
 0-12.2.5-14.5 1.5-2.3 1-4.8 4.5-7.5 10.5-49.3 97.3-121.7 169.3-217 216-28
 14-57.3 25-88 33-6.7 2-11 3.8-13 5.5-2 1.7-3 4.2-3 7.5s1 5.8 3 7.5
c2 1.7 6.3 3.5 13 5.5 68 17.3 128.2 47.8 180.5 91.5 52.3 43.7 93.8 96.2 124.5
 157.5 9.3 8 15.3 12.3 18 13h6c12-.7 18-4 18-10 0-2-1.7-7-5-15-23.3-46-52-87
-86-123l-10-10h399738v-40H218c328 0 0 0 0 0l-10-8c-26.7-20-65.7-43-117-69 2.7
-2 6-3.7 10-5 36.7-16 72.3-37.3 107-64l10-8h399782v-40z
m8 0v40h399730v-40zm0 194v40h399730v-40z`,doublerightarrow:`M399738 392l
-10 10c-34 36-62.7 77-86 123-3.3 8-5 13.3-5 16 0 5.3 6.7 8 20 8 7.3 0 12.2-.5
 14.5-1.5 2.3-1 4.8-4.5 7.5-10.5 49.3-97.3 121.7-169.3 217-216 28-14 57.3-25 88
-33 6.7-2 11-3.8 13-5.5 2-1.7 3-4.2 3-7.5s-1-5.8-3-7.5c-2-1.7-6.3-3.5-13-5.5-68
-17.3-128.2-47.8-180.5-91.5-52.3-43.7-93.8-96.2-124.5-157.5-9.3-8-15.3-12.3-18
-13h-6c-12 .7-18 4-18 10 0 2 1.7 7 5 15 23.3 46 52 87 86 123l10 10H0v40h399782
c-328 0 0 0 0 0l10 8c26.7 20 65.7 43 117 69-2.7 2-6 3.7-10 5-36.7 16-72.3 37.3
-107 64l-10 8H0v40zM0 157v40h399730v-40zm0 194v40h399730v-40z`,leftarrow:`M400000 241H110l3-3c68.7-52.7 113.7-120
 135-202 4-14.7 6-23 6-25 0-7.3-7-11-21-11-8 0-13.2.8-15.5 2.5-2.3 1.7-4.2 5.8
-5.5 12.5-1.3 4.7-2.7 10.3-4 17-12 48.7-34.8 92-68.5 130S65.3 228.3 18 247
c-10 4-16 7.7-18 11 0 8.7 6 14.3 18 17 47.3 18.7 87.8 47 121.5 85S196 441.3 208
 490c.7 2 1.3 5 2 9s1.2 6.7 1.5 8c.3 1.3 1 3.3 2 6s2.2 4.5 3.5 5.5c1.3 1 3.3
 1.8 6 2.5s6 1 10 1c14 0 21-3.7 21-11 0-2-2-10.3-6-25-20-79.3-65-146.7-135-202
 l-3-3h399890zM100 241v40h399900v-40z`,leftbrace:`M6 548l-6-6v-35l6-11c56-104 135.3-181.3 238-232 57.3-28.7 117
-45 179-50h399577v120H403c-43.3 7-81 15-113 26-100.7 33-179.7 91-237 174-2.7
 5-6 9-10 13-.7 1-7.3 1-20 1H6z`,leftbraceunder:`M0 6l6-6h17c12.688 0 19.313.3 20 1 4 4 7.313 8.3 10 13
 35.313 51.3 80.813 93.8 136.5 127.5 55.688 33.7 117.188 55.8 184.5 66.5.688
 0 2 .3 4 1 18.688 2.7 76 4.3 172 5h399450v120H429l-6-1c-124.688-8-235-61.7
-331-161C60.687 138.7 32.312 99.3 7 54L0 41V6z`,leftgroup:`M400000 80
H435C64 80 168.3 229.4 21 260c-5.9 1.2-18 0-18 0-2 0-3-1-3-3v-38C76 61 257 0
 435 0h399565z`,leftgroupunder:`M400000 262
H435C64 262 168.3 112.6 21 82c-5.9-1.2-18 0-18 0-2 0-3 1-3 3v38c76 158 257 219
 435 219h399565z`,leftharpoon:`M0 267c.7 5.3 3 10 7 14h399993v-40H93c3.3
-3.3 10.2-9.5 20.5-18.5s17.8-15.8 22.5-20.5c50.7-52 88-110.3 112-175 4-11.3 5
-18.3 3-21-1.3-4-7.3-6-18-6-8 0-13 .7-15 2s-4.7 6.7-8 16c-42 98.7-107.3 174.7
-196 228-6.7 4.7-10.7 8-12 10-1.3 2-2 5.7-2 11zm100-26v40h399900v-40z`,leftharpoonplus:`M0 267c.7 5.3 3 10 7 14h399993v-40H93c3.3-3.3 10.2-9.5
 20.5-18.5s17.8-15.8 22.5-20.5c50.7-52 88-110.3 112-175 4-11.3 5-18.3 3-21-1.3
-4-7.3-6-18-6-8 0-13 .7-15 2s-4.7 6.7-8 16c-42 98.7-107.3 174.7-196 228-6.7 4.7
-10.7 8-12 10-1.3 2-2 5.7-2 11zm100-26v40h399900v-40zM0 435v40h400000v-40z
m0 0v40h400000v-40z`,leftharpoondown:`M7 241c-4 4-6.333 8.667-7 14 0 5.333.667 9 2 11s5.333
 5.333 12 10c90.667 54 156 130 196 228 3.333 10.667 6.333 16.333 9 17 2 .667 5
 1 9 1h5c10.667 0 16.667-2 18-6 2-2.667 1-9.667-3-21-32-87.333-82.667-157.667
-152-211l-3-3h399907v-40zM93 281 H400000 v-40L7 241z`,leftharpoondownplus:`M7 435c-4 4-6.3 8.7-7 14 0 5.3.7 9 2 11s5.3 5.3 12
 10c90.7 54 156 130 196 228 3.3 10.7 6.3 16.3 9 17 2 .7 5 1 9 1h5c10.7 0 16.7
-2 18-6 2-2.7 1-9.7-3-21-32-87.3-82.7-157.7-152-211l-3-3h399907v-40H7zm93 0
v40h399900v-40zM0 241v40h399900v-40zm0 0v40h399900v-40z`,lefthook:`M400000 281 H103s-33-11.2-61-33.5S0 197.3 0 164s14.2-61.2 42.5
-83.5C70.8 58.2 104 47 142 47 c16.7 0 25 6.7 25 20 0 12-8.7 18.7-26 20-40 3.3
-68.7 15.7-86 37-10 12-15 25.3-15 40 0 22.7 9.8 40.7 29.5 54 19.7 13.3 43.5 21
 71.5 23h399859zM103 281v-40h399897v40z`,leftlinesegment:`M40 281 V428 H0 V94 H40 V241 H400000 v40z
M40 281 V428 H0 V94 H40 V241 H400000 v40z`,leftmapsto:`M40 281 V448H0V74H40V241H400000v40z
M40 281 V448H0V74H40V241H400000v40z`,leftToFrom:`M0 147h400000v40H0zm0 214c68 40 115.7 95.7 143 167h22c15.3 0 23
-.3 23-1 0-1.3-5.3-13.7-16-37-18-35.3-41.3-69-70-101l-7-8h399905v-40H95l7-8
c28.7-32 52-65.7 70-101 10.7-23.3 16-35.7 16-37 0-.7-7.7-1-23-1h-22C115.7 265.3
 68 321 0 361zm0-174v-40h399900v40zm100 154v40h399900v-40z`,longequal:`M0 50 h400000 v40H0z m0 194h40000v40H0z
M0 50 h400000 v40H0z m0 194h40000v40H0z`,midbrace:`M200428 334
c-100.7-8.3-195.3-44-280-108-55.3-42-101.7-93-139-153l-9-14c-2.7 4-5.7 8.7-9 14
-53.3 86.7-123.7 153-211 199-66.7 36-137.3 56.3-212 62H0V214h199568c178.3-11.7
 311.7-78.3 403-201 6-8 9.7-12 11-12 .7-.7 6.7-1 18-1s17.3.3 18 1c1.3 0 5 4 11
 12 44.7 59.3 101.3 106.3 170 141s145.3 54.3 229 60h199572v120z`,midbraceunder:`M199572 214
c100.7 8.3 195.3 44 280 108 55.3 42 101.7 93 139 153l9 14c2.7-4 5.7-8.7 9-14
 53.3-86.7 123.7-153 211-199 66.7-36 137.3-56.3 212-62h199568v120H200432c-178.3
 11.7-311.7 78.3-403 201-6 8-9.7 12-11 12-.7.7-6.7 1-18 1s-17.3-.3-18-1c-1.3 0
-5-4-11-12-44.7-59.3-101.3-106.3-170-141s-145.3-54.3-229-60H0V214z`,oiintSize1:`M512.6 71.6c272.6 0 320.3 106.8 320.3 178.2 0 70.8-47.7 177.6
-320.3 177.6S193.1 320.6 193.1 249.8c0-71.4 46.9-178.2 319.5-178.2z
m368.1 178.2c0-86.4-60.9-215.4-368.1-215.4-306.4 0-367.3 129-367.3 215.4 0 85.8
60.9 214.8 367.3 214.8 307.2 0 368.1-129 368.1-214.8z`,oiintSize2:`M757.8 100.1c384.7 0 451.1 137.6 451.1 230 0 91.3-66.4 228.8
-451.1 228.8-386.3 0-452.7-137.5-452.7-228.8 0-92.4 66.4-230 452.7-230z
m502.4 230c0-111.2-82.4-277.2-502.4-277.2s-504 166-504 277.2
c0 110 84 276 504 276s502.4-166 502.4-276z`,oiiintSize1:`M681.4 71.6c408.9 0 480.5 106.8 480.5 178.2 0 70.8-71.6 177.6
-480.5 177.6S202.1 320.6 202.1 249.8c0-71.4 70.5-178.2 479.3-178.2z
m525.8 178.2c0-86.4-86.8-215.4-525.7-215.4-437.9 0-524.7 129-524.7 215.4 0
85.8 86.8 214.8 524.7 214.8 438.9 0 525.7-129 525.7-214.8z`,oiiintSize2:`M1021.2 53c603.6 0 707.8 165.8 707.8 277.2 0 110-104.2 275.8
-707.8 275.8-606 0-710.2-165.8-710.2-275.8C311 218.8 415.2 53 1021.2 53z
m770.4 277.1c0-131.2-126.4-327.6-770.5-327.6S248.4 198.9 248.4 330.1
c0 130 128.8 326.4 772.7 326.4s770.5-196.4 770.5-326.4z`,rightarrow:`M0 241v40h399891c-47.3 35.3-84 78-110 128
-16.7 32-27.7 63.7-33 95 0 1.3-.2 2.7-.5 4-.3 1.3-.5 2.3-.5 3 0 7.3 6.7 11 20
 11 8 0 13.2-.8 15.5-2.5 2.3-1.7 4.2-5.5 5.5-11.5 2-13.3 5.7-27 11-41 14.7-44.7
 39-84.5 73-119.5s73.7-60.2 119-75.5c6-2 9-5.7 9-11s-3-9-9-11c-45.3-15.3-85
-40.5-119-75.5s-58.3-74.8-73-119.5c-4.7-14-8.3-27.3-11-40-1.3-6.7-3.2-10.8-5.5
-12.5-2.3-1.7-7.5-2.5-15.5-2.5-14 0-21 3.7-21 11 0 2 2 10.3 6 25 20.7 83.3 67
 151.7 139 205zm0 0v40h399900v-40z`,rightbrace:`M400000 542l
-6 6h-17c-12.7 0-19.3-.3-20-1-4-4-7.3-8.3-10-13-35.3-51.3-80.8-93.8-136.5-127.5
s-117.2-55.8-184.5-66.5c-.7 0-2-.3-4-1-18.7-2.7-76-4.3-172-5H0V214h399571l6 1
c124.7 8 235 61.7 331 161 31.3 33.3 59.7 72.7 85 118l7 13v35z`,rightbraceunder:`M399994 0l6 6v35l-6 11c-56 104-135.3 181.3-238 232-57.3
 28.7-117 45-179 50H-300V214h399897c43.3-7 81-15 113-26 100.7-33 179.7-91 237
-174 2.7-5 6-9 10-13 .7-1 7.3-1 20-1h17z`,rightgroup:`M0 80h399565c371 0 266.7 149.4 414 180 5.9 1.2 18 0 18 0 2 0
 3-1 3-3v-38c-76-158-257-219-435-219H0z`,rightgroupunder:`M0 262h399565c371 0 266.7-149.4 414-180 5.9-1.2 18 0 18
 0 2 0 3 1 3 3v38c-76 158-257 219-435 219H0z`,rightharpoon:`M0 241v40h399993c4.7-4.7 7-9.3 7-14 0-9.3
-3.7-15.3-11-18-92.7-56.7-159-133.7-199-231-3.3-9.3-6-14.7-8-16-2-1.3-7-2-15-2
-10.7 0-16.7 2-18 6-2 2.7-1 9.7 3 21 15.3 42 36.7 81.8 64 119.5 27.3 37.7 58
 69.2 92 94.5zm0 0v40h399900v-40z`,rightharpoonplus:`M0 241v40h399993c4.7-4.7 7-9.3 7-14 0-9.3-3.7-15.3-11
-18-92.7-56.7-159-133.7-199-231-3.3-9.3-6-14.7-8-16-2-1.3-7-2-15-2-10.7 0-16.7
 2-18 6-2 2.7-1 9.7 3 21 15.3 42 36.7 81.8 64 119.5 27.3 37.7 58 69.2 92 94.5z
m0 0v40h399900v-40z m100 194v40h399900v-40zm0 0v40h399900v-40z`,rightharpoondown:`M399747 511c0 7.3 6.7 11 20 11 8 0 13-.8 15-2.5s4.7-6.8
 8-15.5c40-94 99.3-166.3 178-217 13.3-8 20.3-12.3 21-13 5.3-3.3 8.5-5.8 9.5
-7.5 1-1.7 1.5-5.2 1.5-10.5s-2.3-10.3-7-15H0v40h399908c-34 25.3-64.7 57-92 95
-27.3 38-48.7 77.7-64 119-3.3 8.7-5 14-5 16zM0 241v40h399900v-40z`,rightharpoondownplus:`M399747 705c0 7.3 6.7 11 20 11 8 0 13-.8
 15-2.5s4.7-6.8 8-15.5c40-94 99.3-166.3 178-217 13.3-8 20.3-12.3 21-13 5.3-3.3
 8.5-5.8 9.5-7.5 1-1.7 1.5-5.2 1.5-10.5s-2.3-10.3-7-15H0v40h399908c-34 25.3
-64.7 57-92 95-27.3 38-48.7 77.7-64 119-3.3 8.7-5 14-5 16zM0 435v40h399900v-40z
m0-194v40h400000v-40zm0 0v40h400000v-40z`,righthook:`M399859 241c-764 0 0 0 0 0 40-3.3 68.7-15.7 86-37 10-12 15-25.3
 15-40 0-22.7-9.8-40.7-29.5-54-19.7-13.3-43.5-21-71.5-23-17.3-1.3-26-8-26-20 0
-13.3 8.7-20 26-20 38 0 71 11.2 99 33.5 0 0 7 5.6 21 16.7 14 11.2 21 33.5 21
 66.8s-14 61.2-42 83.5c-28 22.3-61 33.5-99 33.5L0 241z M0 281v-40h399859v40z`,rightlinesegment:`M399960 241 V94 h40 V428 h-40 V281 H0 v-40z
M399960 241 V94 h40 V428 h-40 V281 H0 v-40z`,rightToFrom:`M400000 167c-70.7-42-118-97.7-142-167h-23c-15.3 0-23 .3-23
 1 0 1.3 5.3 13.7 16 37 18 35.3 41.3 69 70 101l7 8H0v40h399905l-7 8c-28.7 32
-52 65.7-70 101-10.7 23.3-16 35.7-16 37 0 .7 7.7 1 23 1h23c24-69.3 71.3-125 142
-167z M100 147v40h399900v-40zM0 341v40h399900v-40z`,twoheadleftarrow:`M0 167c68 40
 115.7 95.7 143 167h22c15.3 0 23-.3 23-1 0-1.3-5.3-13.7-16-37-18-35.3-41.3-69
-70-101l-7-8h125l9 7c50.7 39.3 85 86 103 140h46c0-4.7-6.3-18.7-19-42-18-35.3
-40-67.3-66-96l-9-9h399716v-40H284l9-9c26-28.7 48-60.7 66-96 12.7-23.333 19
-37.333 19-42h-46c-18 54-52.3 100.7-103 140l-9 7H95l7-8c28.7-32 52-65.7 70-101
 10.7-23.333 16-35.7 16-37 0-.7-7.7-1-23-1h-22C115.7 71.3 68 127 0 167z`,twoheadrightarrow:`M400000 167
c-68-40-115.7-95.7-143-167h-22c-15.3 0-23 .3-23 1 0 1.3 5.3 13.7 16 37 18 35.3
 41.3 69 70 101l7 8h-125l-9-7c-50.7-39.3-85-86-103-140h-46c0 4.7 6.3 18.7 19 42
 18 35.3 40 67.3 66 96l9 9H0v40h399716l-9 9c-26 28.7-48 60.7-66 96-12.7 23.333
-19 37.333-19 42h46c18-54 52.3-100.7 103-140l9-7h125l-7 8c-28.7 32-52 65.7-70
 101-10.7 23.333-16 35.7-16 37 0 .7 7.7 1 23 1h22c27.3-71.3 75-127 143-167z`,tilde1:`M200 55.538c-77 0-168 73.953-177 73.953-3 0-7
-2.175-9-5.437L2 97c-1-2-2-4-2-6 0-4 2-7 5-9l20-12C116 12 171 0 207 0c86 0
 114 68 191 68 78 0 168-68 177-68 4 0 7 2 9 5l12 19c1 2.175 2 4.35 2 6.525 0
 4.35-2 7.613-5 9.788l-19 13.05c-92 63.077-116.937 75.308-183 76.128
-68.267.847-113-73.952-191-73.952z`,tilde2:`M344 55.266c-142 0-300.638 81.316-311.5 86.418
-8.01 3.762-22.5 10.91-23.5 5.562L1 120c-1-2-1-3-1-4 0-5 3-9 8-10l18.4-9C160.9
 31.9 283 0 358 0c148 0 188 122 331 122s314-97 326-97c4 0 8 2 10 7l7 21.114
c1 2.14 1 3.21 1 4.28 0 5.347-3 9.626-7 10.696l-22.3 12.622C852.6 158.372 751
 181.476 676 181.476c-149 0-189-126.21-332-126.21z`,tilde3:`M786 59C457 59 32 175.242 13 175.242c-6 0-10-3.457
-11-10.37L.15 138c-1-7 3-12 10-13l19.2-6.4C378.4 40.7 634.3 0 804.3 0c337 0
 411.8 157 746.8 157 328 0 754-112 773-112 5 0 10 3 11 9l1 14.075c1 8.066-.697
 16.595-6.697 17.492l-21.052 7.31c-367.9 98.146-609.15 122.696-778.15 122.696
 -338 0-409-156.573-744-156.573z`,tilde4:`M786 58C457 58 32 177.487 13 177.487c-6 0-10-3.345
-11-10.035L.15 143c-1-7 3-12 10-13l22-6.7C381.2 35 637.15 0 807.15 0c337 0 409
 177 744 177 328 0 754-127 773-127 5 0 10 3 11 9l1 14.794c1 7.805-3 13.38-9
 14.495l-20.7 5.574c-366.85 99.79-607.3 139.372-776.3 139.372-338 0-409
 -175.236-744-175.236z`,vec:`M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z`,widehat1:`M529 0h5l519 115c5 1 9 5 9 10 0 1-1 2-1 3l-4 22
c-1 5-5 9-11 9h-2L532 67 19 159h-2c-5 0-9-4-11-9l-5-22c-1-6 2-12 8-13z`,widehat2:`M1181 0h2l1171 176c6 0 10 5 10 11l-2 23c-1 6-5 10
-11 10h-1L1182 67 15 220h-1c-6 0-10-4-11-10l-2-23c-1-6 4-11 10-11z`,widehat3:`M1181 0h2l1171 236c6 0 10 5 10 11l-2 23c-1 6-5 10
-11 10h-1L1182 67 15 280h-1c-6 0-10-4-11-10l-2-23c-1-6 4-11 10-11z`,widehat4:`M1181 0h2l1171 296c6 0 10 5 10 11l-2 23c-1 6-5 10
-11 10h-1L1182 67 15 340h-1c-6 0-10-4-11-10l-2-23c-1-6 4-11 10-11z`,widecheck1:`M529,159h5l519,-115c5,-1,9,-5,9,-10c0,-1,-1,-2,-1,-3l-4,-22c-1,
-5,-5,-9,-11,-9h-2l-512,92l-513,-92h-2c-5,0,-9,4,-11,9l-5,22c-1,6,2,12,8,13z`,widecheck2:`M1181,220h2l1171,-176c6,0,10,-5,10,-11l-2,-23c-1,-6,-5,-10,
-11,-10h-1l-1168,153l-1167,-153h-1c-6,0,-10,4,-11,10l-2,23c-1,6,4,11,10,11z`,widecheck3:`M1181,280h2l1171,-236c6,0,10,-5,10,-11l-2,-23c-1,-6,-5,-10,
-11,-10h-1l-1168,213l-1167,-213h-1c-6,0,-10,4,-11,10l-2,23c-1,6,4,11,10,11z`,widecheck4:`M1181,340h2l1171,-296c6,0,10,-5,10,-11l-2,-23c-1,-6,-5,-10,
-11,-10h-1l-1168,273l-1167,-273h-1c-6,0,-10,4,-11,10l-2,23c-1,6,4,11,10,11z`,baraboveleftarrow:`M400000 620h-399890l3 -3c68.7 -52.7 113.7 -120 135 -202
c4 -14.7 6 -23 6 -25c0 -7.3 -7 -11 -21 -11c-8 0 -13.2 0.8 -15.5 2.5
c-2.3 1.7 -4.2 5.8 -5.5 12.5c-1.3 4.7 -2.7 10.3 -4 17c-12 48.7 -34.8 92 -68.5 130
s-74.2 66.3 -121.5 85c-10 4 -16 7.7 -18 11c0 8.7 6 14.3 18 17c47.3 18.7 87.8 47
121.5 85s56.5 81.3 68.5 130c0.7 2 1.3 5 2 9s1.2 6.7 1.5 8c0.3 1.3 1 3.3 2 6
s2.2 4.5 3.5 5.5c1.3 1 3.3 1.8 6 2.5s6 1 10 1c14 0 21 -3.7 21 -11
c0 -2 -2 -10.3 -6 -25c-20 -79.3 -65 -146.7 -135 -202l-3 -3h399890z
M100 620v40h399900v-40z M0 241v40h399900v-40zM0 241v40h399900v-40z`,rightarrowabovebar:`M0 241v40h399891c-47.3 35.3-84 78-110 128-16.7 32
-27.7 63.7-33 95 0 1.3-.2 2.7-.5 4-.3 1.3-.5 2.3-.5 3 0 7.3 6.7 11 20 11 8 0
13.2-.8 15.5-2.5 2.3-1.7 4.2-5.5 5.5-11.5 2-13.3 5.7-27 11-41 14.7-44.7 39
-84.5 73-119.5s73.7-60.2 119-75.5c6-2 9-5.7 9-11s-3-9-9-11c-45.3-15.3-85-40.5
-119-75.5s-58.3-74.8-73-119.5c-4.7-14-8.3-27.3-11-40-1.3-6.7-3.2-10.8-5.5
-12.5-2.3-1.7-7.5-2.5-15.5-2.5-14 0-21 3.7-21 11 0 2 2 10.3 6 25 20.7 83.3 67
151.7 139 205zm96 379h399894v40H0zm0 0h399904v40H0z`,baraboveshortleftharpoon:`M507,435c-4,4,-6.3,8.7,-7,14c0,5.3,0.7,9,2,11
c1.3,2,5.3,5.3,12,10c90.7,54,156,130,196,228c3.3,10.7,6.3,16.3,9,17
c2,0.7,5,1,9,1c0,0,5,0,5,0c10.7,0,16.7,-2,18,-6c2,-2.7,1,-9.7,-3,-21
c-32,-87.3,-82.7,-157.7,-152,-211c0,0,-3,-3,-3,-3l399351,0l0,-40
c-398570,0,-399437,0,-399437,0z M593 435 v40 H399500 v-40z
M0 281 v-40 H399908 v40z M0 281 v-40 H399908 v40z`,rightharpoonaboveshortbar:`M0,241 l0,40c399126,0,399993,0,399993,0
c4.7,-4.7,7,-9.3,7,-14c0,-9.3,-3.7,-15.3,-11,-18c-92.7,-56.7,-159,-133.7,-199,
-231c-3.3,-9.3,-6,-14.7,-8,-16c-2,-1.3,-7,-2,-15,-2c-10.7,0,-16.7,2,-18,6
c-2,2.7,-1,9.7,3,21c15.3,42,36.7,81.8,64,119.5c27.3,37.7,58,69.2,92,94.5z
M0 241 v40 H399908 v-40z M0 475 v-40 H399500 v40z M0 475 v-40 H399500 v40z`,shortbaraboveleftharpoon:`M7,435c-4,4,-6.3,8.7,-7,14c0,5.3,0.7,9,2,11
c1.3,2,5.3,5.3,12,10c90.7,54,156,130,196,228c3.3,10.7,6.3,16.3,9,17c2,0.7,5,1,9,
1c0,0,5,0,5,0c10.7,0,16.7,-2,18,-6c2,-2.7,1,-9.7,-3,-21c-32,-87.3,-82.7,-157.7,
-152,-211c0,0,-3,-3,-3,-3l399907,0l0,-40c-399126,0,-399993,0,-399993,0z
M93 435 v40 H400000 v-40z M500 241 v40 H400000 v-40z M500 241 v40 H400000 v-40z`,shortrightharpoonabovebar:`M53,241l0,40c398570,0,399437,0,399437,0
c4.7,-4.7,7,-9.3,7,-14c0,-9.3,-3.7,-15.3,-11,-18c-92.7,-56.7,-159,-133.7,-199,
-231c-3.3,-9.3,-6,-14.7,-8,-16c-2,-1.3,-7,-2,-15,-2c-10.7,0,-16.7,2,-18,6
c-2,2.7,-1,9.7,3,21c15.3,42,36.7,81.8,64,119.5c27.3,37.7,58,69.2,92,94.5z
M500 241 v40 H399408 v-40z M500 435 v40 H400000 v-40z`},UG=function(t,n){switch(t){case"lbrack":return"M403 1759 V84 H666 V0 H319 V1759 v"+n+` v1759 h347 v-84
H403z M403 1759 V0 H319 V1759 v`+n+" v1759 h84z";case"rbrack":return"M347 1759 V0 H0 V84 H263 V1759 v"+n+` v1759 H0 v84 H347z
M347 1759 V0 H263 V1759 v`+n+" v1759 h84z";case"vert":return"M145 15 v585 v"+n+` v585 c2.667,10,9.667,15,21,15
c10,0,16.667,-5,20,-15 v-585 v`+-n+` v-585 c-2.667,-10,-9.667,-15,-21,-15
c-10,0,-16.667,5,-20,15z M188 15 H145 v585 v`+n+" v585 h43z";case"doublevert":return"M145 15 v585 v"+n+` v585 c2.667,10,9.667,15,21,15
c10,0,16.667,-5,20,-15 v-585 v`+-n+` v-585 c-2.667,-10,-9.667,-15,-21,-15
c-10,0,-16.667,5,-20,15z M188 15 H145 v585 v`+n+` v585 h43z
M367 15 v585 v`+n+` v585 c2.667,10,9.667,15,21,15
c10,0,16.667,-5,20,-15 v-585 v`+-n+` v-585 c-2.667,-10,-9.667,-15,-21,-15
c-10,0,-16.667,5,-20,15z M410 15 H367 v585 v`+n+" v585 h43z";case"lfloor":return"M319 602 V0 H403 V602 v"+n+` v1715 h263 v84 H319z
MM319 602 V0 H403 V602 v`+n+" v1715 H319z";case"rfloor":return"M319 602 V0 H403 V602 v"+n+` v1799 H0 v-84 H319z
MM319 602 V0 H403 V602 v`+n+" v1715 H319z";case"lceil":return"M403 1759 V84 H666 V0 H319 V1759 v"+n+` v602 h84z
M403 1759 V0 H319 V1759 v`+n+" v602 h84z";case"rceil":return"M347 1759 V0 H0 V84 H263 V1759 v"+n+` v602 h84z
M347 1759 V0 h-84 V1759 v`+n+" v602 h84z";case"lparen":return`M863,9c0,-2,-2,-5,-6,-9c0,0,-17,0,-17,0c-12.7,0,-19.3,0.3,-20,1
c-5.3,5.3,-10.3,11,-15,17c-242.7,294.7,-395.3,682,-458,1162c-21.3,163.3,-33.3,349,
-36,557 l0,`+(n+84)+`c0.2,6,0,26,0,60c2,159.3,10,310.7,24,454c53.3,528,210,
949.7,470,1265c4.7,6,9.7,11.7,15,17c0.7,0.7,7,1,19,1c0,0,18,0,18,0c4,-4,6,-7,6,-9
c0,-2.7,-3.3,-8.7,-10,-18c-135.3,-192.7,-235.5,-414.3,-300.5,-665c-65,-250.7,-102.5,
-544.7,-112.5,-882c-2,-104,-3,-167,-3,-189
l0,-`+(n+92)+`c0,-162.7,5.7,-314,17,-454c20.7,-272,63.7,-513,129,-723c65.3,
-210,155.3,-396.3,270,-559c6.7,-9.3,10,-15.3,10,-18z`;case"rparen":return`M76,0c-16.7,0,-25,3,-25,9c0,2,2,6.3,6,13c21.3,28.7,42.3,60.3,
63,95c96.7,156.7,172.8,332.5,228.5,527.5c55.7,195,92.8,416.5,111.5,664.5
c11.3,139.3,17,290.7,17,454c0,28,1.7,43,3.3,45l0,`+(n+9)+`
c-3,4,-3.3,16.7,-3.3,38c0,162,-5.7,313.7,-17,455c-18.7,248,-55.8,469.3,-111.5,664
c-55.7,194.7,-131.8,370.3,-228.5,527c-20.7,34.7,-41.7,66.3,-63,95c-2,3.3,-4,7,-6,11
c0,7.3,5.7,11,17,11c0,0,11,0,11,0c9.3,0,14.3,-0.3,15,-1c5.3,-5.3,10.3,-11,15,-17
c242.7,-294.7,395.3,-681.7,458,-1161c21.3,-164.7,33.3,-350.7,36,-558
l0,-`+(n+144)+`c-2,-159.3,-10,-310.7,-24,-454c-53.3,-528,-210,-949.7,
-470,-1265c-4.7,-6,-9.7,-11.7,-15,-17c-0.7,-0.7,-6.7,-1,-18,-1z`;default:throw new Error("Unknown stretchy delimiter.")}};let Jc=class{constructor(t){this.children=void 0,this.classes=void 0,this.height=void 0,this.depth=void 0,this.maxFontSize=void 0,this.style=void 0,this.children=t,this.classes=[],this.height=0,this.depth=0,this.maxFontSize=0,this.style={}}hasClass(t){return ye.contains(this.classes,t)}toNode(){for(var t=document.createDocumentFragment(),n=0;n<this.children.length;n++)t.appendChild(this.children[n].toNode());return t}toMarkup(){for(var t="",n=0;n<this.children.length;n++)t+=this.children[n].toMarkup();return t}toText(){var t=n=>n.toText();return this.children.map(t).join("")}};var Jr={"AMS-Regular":{32:[0,0,0,0,.25],65:[0,.68889,0,0,.72222],66:[0,.68889,0,0,.66667],67:[0,.68889,0,0,.72222],68:[0,.68889,0,0,.72222],69:[0,.68889,0,0,.66667],70:[0,.68889,0,0,.61111],71:[0,.68889,0,0,.77778],72:[0,.68889,0,0,.77778],73:[0,.68889,0,0,.38889],74:[.16667,.68889,0,0,.5],75:[0,.68889,0,0,.77778],76:[0,.68889,0,0,.66667],77:[0,.68889,0,0,.94445],78:[0,.68889,0,0,.72222],79:[.16667,.68889,0,0,.77778],80:[0,.68889,0,0,.61111],81:[.16667,.68889,0,0,.77778],82:[0,.68889,0,0,.72222],83:[0,.68889,0,0,.55556],84:[0,.68889,0,0,.66667],85:[0,.68889,0,0,.72222],86:[0,.68889,0,0,.72222],87:[0,.68889,0,0,1],88:[0,.68889,0,0,.72222],89:[0,.68889,0,0,.72222],90:[0,.68889,0,0,.66667],107:[0,.68889,0,0,.55556],160:[0,0,0,0,.25],165:[0,.675,.025,0,.75],174:[.15559,.69224,0,0,.94666],240:[0,.68889,0,0,.55556],295:[0,.68889,0,0,.54028],710:[0,.825,0,0,2.33334],732:[0,.9,0,0,2.33334],770:[0,.825,0,0,2.33334],771:[0,.9,0,0,2.33334],989:[.08167,.58167,0,0,.77778],1008:[0,.43056,.04028,0,.66667],8245:[0,.54986,0,0,.275],8463:[0,.68889,0,0,.54028],8487:[0,.68889,0,0,.72222],8498:[0,.68889,0,0,.55556],8502:[0,.68889,0,0,.66667],8503:[0,.68889,0,0,.44445],8504:[0,.68889,0,0,.66667],8513:[0,.68889,0,0,.63889],8592:[-.03598,.46402,0,0,.5],8594:[-.03598,.46402,0,0,.5],8602:[-.13313,.36687,0,0,1],8603:[-.13313,.36687,0,0,1],8606:[.01354,.52239,0,0,1],8608:[.01354,.52239,0,0,1],8610:[.01354,.52239,0,0,1.11111],8611:[.01354,.52239,0,0,1.11111],8619:[0,.54986,0,0,1],8620:[0,.54986,0,0,1],8621:[-.13313,.37788,0,0,1.38889],8622:[-.13313,.36687,0,0,1],8624:[0,.69224,0,0,.5],8625:[0,.69224,0,0,.5],8630:[0,.43056,0,0,1],8631:[0,.43056,0,0,1],8634:[.08198,.58198,0,0,.77778],8635:[.08198,.58198,0,0,.77778],8638:[.19444,.69224,0,0,.41667],8639:[.19444,.69224,0,0,.41667],8642:[.19444,.69224,0,0,.41667],8643:[.19444,.69224,0,0,.41667],8644:[.1808,.675,0,0,1],8646:[.1808,.675,0,0,1],8647:[.1808,.675,0,0,1],8648:[.19444,.69224,0,0,.83334],8649:[.1808,.675,0,0,1],8650:[.19444,.69224,0,0,.83334],8651:[.01354,.52239,0,0,1],8652:[.01354,.52239,0,0,1],8653:[-.13313,.36687,0,0,1],8654:[-.13313,.36687,0,0,1],8655:[-.13313,.36687,0,0,1],8666:[.13667,.63667,0,0,1],8667:[.13667,.63667,0,0,1],8669:[-.13313,.37788,0,0,1],8672:[-.064,.437,0,0,1.334],8674:[-.064,.437,0,0,1.334],8705:[0,.825,0,0,.5],8708:[0,.68889,0,0,.55556],8709:[.08167,.58167,0,0,.77778],8717:[0,.43056,0,0,.42917],8722:[-.03598,.46402,0,0,.5],8724:[.08198,.69224,0,0,.77778],8726:[.08167,.58167,0,0,.77778],8733:[0,.69224,0,0,.77778],8736:[0,.69224,0,0,.72222],8737:[0,.69224,0,0,.72222],8738:[.03517,.52239,0,0,.72222],8739:[.08167,.58167,0,0,.22222],8740:[.25142,.74111,0,0,.27778],8741:[.08167,.58167,0,0,.38889],8742:[.25142,.74111,0,0,.5],8756:[0,.69224,0,0,.66667],8757:[0,.69224,0,0,.66667],8764:[-.13313,.36687,0,0,.77778],8765:[-.13313,.37788,0,0,.77778],8769:[-.13313,.36687,0,0,.77778],8770:[-.03625,.46375,0,0,.77778],8774:[.30274,.79383,0,0,.77778],8776:[-.01688,.48312,0,0,.77778],8778:[.08167,.58167,0,0,.77778],8782:[.06062,.54986,0,0,.77778],8783:[.06062,.54986,0,0,.77778],8785:[.08198,.58198,0,0,.77778],8786:[.08198,.58198,0,0,.77778],8787:[.08198,.58198,0,0,.77778],8790:[0,.69224,0,0,.77778],8791:[.22958,.72958,0,0,.77778],8796:[.08198,.91667,0,0,.77778],8806:[.25583,.75583,0,0,.77778],8807:[.25583,.75583,0,0,.77778],8808:[.25142,.75726,0,0,.77778],8809:[.25142,.75726,0,0,.77778],8812:[.25583,.75583,0,0,.5],8814:[.20576,.70576,0,0,.77778],8815:[.20576,.70576,0,0,.77778],8816:[.30274,.79383,0,0,.77778],8817:[.30274,.79383,0,0,.77778],8818:[.22958,.72958,0,0,.77778],8819:[.22958,.72958,0,0,.77778],8822:[.1808,.675,0,0,.77778],8823:[.1808,.675,0,0,.77778],8828:[.13667,.63667,0,0,.77778],8829:[.13667,.63667,0,0,.77778],8830:[.22958,.72958,0,0,.77778],8831:[.22958,.72958,0,0,.77778],8832:[.20576,.70576,0,0,.77778],8833:[.20576,.70576,0,0,.77778],8840:[.30274,.79383,0,0,.77778],8841:[.30274,.79383,0,0,.77778],8842:[.13597,.63597,0,0,.77778],8843:[.13597,.63597,0,0,.77778],8847:[.03517,.54986,0,0,.77778],8848:[.03517,.54986,0,0,.77778],8858:[.08198,.58198,0,0,.77778],8859:[.08198,.58198,0,0,.77778],8861:[.08198,.58198,0,0,.77778],8862:[0,.675,0,0,.77778],8863:[0,.675,0,0,.77778],8864:[0,.675,0,0,.77778],8865:[0,.675,0,0,.77778],8872:[0,.69224,0,0,.61111],8873:[0,.69224,0,0,.72222],8874:[0,.69224,0,0,.88889],8876:[0,.68889,0,0,.61111],8877:[0,.68889,0,0,.61111],8878:[0,.68889,0,0,.72222],8879:[0,.68889,0,0,.72222],8882:[.03517,.54986,0,0,.77778],8883:[.03517,.54986,0,0,.77778],8884:[.13667,.63667,0,0,.77778],8885:[.13667,.63667,0,0,.77778],8888:[0,.54986,0,0,1.11111],8890:[.19444,.43056,0,0,.55556],8891:[.19444,.69224,0,0,.61111],8892:[.19444,.69224,0,0,.61111],8901:[0,.54986,0,0,.27778],8903:[.08167,.58167,0,0,.77778],8905:[.08167,.58167,0,0,.77778],8906:[.08167,.58167,0,0,.77778],8907:[0,.69224,0,0,.77778],8908:[0,.69224,0,0,.77778],8909:[-.03598,.46402,0,0,.77778],8910:[0,.54986,0,0,.76042],8911:[0,.54986,0,0,.76042],8912:[.03517,.54986,0,0,.77778],8913:[.03517,.54986,0,0,.77778],8914:[0,.54986,0,0,.66667],8915:[0,.54986,0,0,.66667],8916:[0,.69224,0,0,.66667],8918:[.0391,.5391,0,0,.77778],8919:[.0391,.5391,0,0,.77778],8920:[.03517,.54986,0,0,1.33334],8921:[.03517,.54986,0,0,1.33334],8922:[.38569,.88569,0,0,.77778],8923:[.38569,.88569,0,0,.77778],8926:[.13667,.63667,0,0,.77778],8927:[.13667,.63667,0,0,.77778],8928:[.30274,.79383,0,0,.77778],8929:[.30274,.79383,0,0,.77778],8934:[.23222,.74111,0,0,.77778],8935:[.23222,.74111,0,0,.77778],8936:[.23222,.74111,0,0,.77778],8937:[.23222,.74111,0,0,.77778],8938:[.20576,.70576,0,0,.77778],8939:[.20576,.70576,0,0,.77778],8940:[.30274,.79383,0,0,.77778],8941:[.30274,.79383,0,0,.77778],8994:[.19444,.69224,0,0,.77778],8995:[.19444,.69224,0,0,.77778],9416:[.15559,.69224,0,0,.90222],9484:[0,.69224,0,0,.5],9488:[0,.69224,0,0,.5],9492:[0,.37788,0,0,.5],9496:[0,.37788,0,0,.5],9585:[.19444,.68889,0,0,.88889],9586:[.19444,.74111,0,0,.88889],9632:[0,.675,0,0,.77778],9633:[0,.675,0,0,.77778],9650:[0,.54986,0,0,.72222],9651:[0,.54986,0,0,.72222],9654:[.03517,.54986,0,0,.77778],9660:[0,.54986,0,0,.72222],9661:[0,.54986,0,0,.72222],9664:[.03517,.54986,0,0,.77778],9674:[.11111,.69224,0,0,.66667],9733:[.19444,.69224,0,0,.94445],10003:[0,.69224,0,0,.83334],10016:[0,.69224,0,0,.83334],10731:[.11111,.69224,0,0,.66667],10846:[.19444,.75583,0,0,.61111],10877:[.13667,.63667,0,0,.77778],10878:[.13667,.63667,0,0,.77778],10885:[.25583,.75583,0,0,.77778],10886:[.25583,.75583,0,0,.77778],10887:[.13597,.63597,0,0,.77778],10888:[.13597,.63597,0,0,.77778],10889:[.26167,.75726,0,0,.77778],10890:[.26167,.75726,0,0,.77778],10891:[.48256,.98256,0,0,.77778],10892:[.48256,.98256,0,0,.77778],10901:[.13667,.63667,0,0,.77778],10902:[.13667,.63667,0,0,.77778],10933:[.25142,.75726,0,0,.77778],10934:[.25142,.75726,0,0,.77778],10935:[.26167,.75726,0,0,.77778],10936:[.26167,.75726,0,0,.77778],10937:[.26167,.75726,0,0,.77778],10938:[.26167,.75726,0,0,.77778],10949:[.25583,.75583,0,0,.77778],10950:[.25583,.75583,0,0,.77778],10955:[.28481,.79383,0,0,.77778],10956:[.28481,.79383,0,0,.77778],57350:[.08167,.58167,0,0,.22222],57351:[.08167,.58167,0,0,.38889],57352:[.08167,.58167,0,0,.77778],57353:[0,.43056,.04028,0,.66667],57356:[.25142,.75726,0,0,.77778],57357:[.25142,.75726,0,0,.77778],57358:[.41951,.91951,0,0,.77778],57359:[.30274,.79383,0,0,.77778],57360:[.30274,.79383,0,0,.77778],57361:[.41951,.91951,0,0,.77778],57366:[.25142,.75726,0,0,.77778],57367:[.25142,.75726,0,0,.77778],57368:[.25142,.75726,0,0,.77778],57369:[.25142,.75726,0,0,.77778],57370:[.13597,.63597,0,0,.77778],57371:[.13597,.63597,0,0,.77778]},"Caligraphic-Regular":{32:[0,0,0,0,.25],65:[0,.68333,0,.19445,.79847],66:[0,.68333,.03041,.13889,.65681],67:[0,.68333,.05834,.13889,.52653],68:[0,.68333,.02778,.08334,.77139],69:[0,.68333,.08944,.11111,.52778],70:[0,.68333,.09931,.11111,.71875],71:[.09722,.68333,.0593,.11111,.59487],72:[0,.68333,.00965,.11111,.84452],73:[0,.68333,.07382,0,.54452],74:[.09722,.68333,.18472,.16667,.67778],75:[0,.68333,.01445,.05556,.76195],76:[0,.68333,0,.13889,.68972],77:[0,.68333,0,.13889,1.2009],78:[0,.68333,.14736,.08334,.82049],79:[0,.68333,.02778,.11111,.79611],80:[0,.68333,.08222,.08334,.69556],81:[.09722,.68333,0,.11111,.81667],82:[0,.68333,0,.08334,.8475],83:[0,.68333,.075,.13889,.60556],84:[0,.68333,.25417,0,.54464],85:[0,.68333,.09931,.08334,.62583],86:[0,.68333,.08222,0,.61278],87:[0,.68333,.08222,.08334,.98778],88:[0,.68333,.14643,.13889,.7133],89:[.09722,.68333,.08222,.08334,.66834],90:[0,.68333,.07944,.13889,.72473],160:[0,0,0,0,.25]},"Fraktur-Regular":{32:[0,0,0,0,.25],33:[0,.69141,0,0,.29574],34:[0,.69141,0,0,.21471],38:[0,.69141,0,0,.73786],39:[0,.69141,0,0,.21201],40:[.24982,.74947,0,0,.38865],41:[.24982,.74947,0,0,.38865],42:[0,.62119,0,0,.27764],43:[.08319,.58283,0,0,.75623],44:[0,.10803,0,0,.27764],45:[.08319,.58283,0,0,.75623],46:[0,.10803,0,0,.27764],47:[.24982,.74947,0,0,.50181],48:[0,.47534,0,0,.50181],49:[0,.47534,0,0,.50181],50:[0,.47534,0,0,.50181],51:[.18906,.47534,0,0,.50181],52:[.18906,.47534,0,0,.50181],53:[.18906,.47534,0,0,.50181],54:[0,.69141,0,0,.50181],55:[.18906,.47534,0,0,.50181],56:[0,.69141,0,0,.50181],57:[.18906,.47534,0,0,.50181],58:[0,.47534,0,0,.21606],59:[.12604,.47534,0,0,.21606],61:[-.13099,.36866,0,0,.75623],63:[0,.69141,0,0,.36245],65:[0,.69141,0,0,.7176],66:[0,.69141,0,0,.88397],67:[0,.69141,0,0,.61254],68:[0,.69141,0,0,.83158],69:[0,.69141,0,0,.66278],70:[.12604,.69141,0,0,.61119],71:[0,.69141,0,0,.78539],72:[.06302,.69141,0,0,.7203],73:[0,.69141,0,0,.55448],74:[.12604,.69141,0,0,.55231],75:[0,.69141,0,0,.66845],76:[0,.69141,0,0,.66602],77:[0,.69141,0,0,1.04953],78:[0,.69141,0,0,.83212],79:[0,.69141,0,0,.82699],80:[.18906,.69141,0,0,.82753],81:[.03781,.69141,0,0,.82699],82:[0,.69141,0,0,.82807],83:[0,.69141,0,0,.82861],84:[0,.69141,0,0,.66899],85:[0,.69141,0,0,.64576],86:[0,.69141,0,0,.83131],87:[0,.69141,0,0,1.04602],88:[0,.69141,0,0,.71922],89:[.18906,.69141,0,0,.83293],90:[.12604,.69141,0,0,.60201],91:[.24982,.74947,0,0,.27764],93:[.24982,.74947,0,0,.27764],94:[0,.69141,0,0,.49965],97:[0,.47534,0,0,.50046],98:[0,.69141,0,0,.51315],99:[0,.47534,0,0,.38946],100:[0,.62119,0,0,.49857],101:[0,.47534,0,0,.40053],102:[.18906,.69141,0,0,.32626],103:[.18906,.47534,0,0,.5037],104:[.18906,.69141,0,0,.52126],105:[0,.69141,0,0,.27899],106:[0,.69141,0,0,.28088],107:[0,.69141,0,0,.38946],108:[0,.69141,0,0,.27953],109:[0,.47534,0,0,.76676],110:[0,.47534,0,0,.52666],111:[0,.47534,0,0,.48885],112:[.18906,.52396,0,0,.50046],113:[.18906,.47534,0,0,.48912],114:[0,.47534,0,0,.38919],115:[0,.47534,0,0,.44266],116:[0,.62119,0,0,.33301],117:[0,.47534,0,0,.5172],118:[0,.52396,0,0,.5118],119:[0,.52396,0,0,.77351],120:[.18906,.47534,0,0,.38865],121:[.18906,.47534,0,0,.49884],122:[.18906,.47534,0,0,.39054],160:[0,0,0,0,.25],8216:[0,.69141,0,0,.21471],8217:[0,.69141,0,0,.21471],58112:[0,.62119,0,0,.49749],58113:[0,.62119,0,0,.4983],58114:[.18906,.69141,0,0,.33328],58115:[.18906,.69141,0,0,.32923],58116:[.18906,.47534,0,0,.50343],58117:[0,.69141,0,0,.33301],58118:[0,.62119,0,0,.33409],58119:[0,.47534,0,0,.50073]},"Main-Bold":{32:[0,0,0,0,.25],33:[0,.69444,0,0,.35],34:[0,.69444,0,0,.60278],35:[.19444,.69444,0,0,.95833],36:[.05556,.75,0,0,.575],37:[.05556,.75,0,0,.95833],38:[0,.69444,0,0,.89444],39:[0,.69444,0,0,.31944],40:[.25,.75,0,0,.44722],41:[.25,.75,0,0,.44722],42:[0,.75,0,0,.575],43:[.13333,.63333,0,0,.89444],44:[.19444,.15556,0,0,.31944],45:[0,.44444,0,0,.38333],46:[0,.15556,0,0,.31944],47:[.25,.75,0,0,.575],48:[0,.64444,0,0,.575],49:[0,.64444,0,0,.575],50:[0,.64444,0,0,.575],51:[0,.64444,0,0,.575],52:[0,.64444,0,0,.575],53:[0,.64444,0,0,.575],54:[0,.64444,0,0,.575],55:[0,.64444,0,0,.575],56:[0,.64444,0,0,.575],57:[0,.64444,0,0,.575],58:[0,.44444,0,0,.31944],59:[.19444,.44444,0,0,.31944],60:[.08556,.58556,0,0,.89444],61:[-.10889,.39111,0,0,.89444],62:[.08556,.58556,0,0,.89444],63:[0,.69444,0,0,.54305],64:[0,.69444,0,0,.89444],65:[0,.68611,0,0,.86944],66:[0,.68611,0,0,.81805],67:[0,.68611,0,0,.83055],68:[0,.68611,0,0,.88194],69:[0,.68611,0,0,.75555],70:[0,.68611,0,0,.72361],71:[0,.68611,0,0,.90416],72:[0,.68611,0,0,.9],73:[0,.68611,0,0,.43611],74:[0,.68611,0,0,.59444],75:[0,.68611,0,0,.90138],76:[0,.68611,0,0,.69166],77:[0,.68611,0,0,1.09166],78:[0,.68611,0,0,.9],79:[0,.68611,0,0,.86388],80:[0,.68611,0,0,.78611],81:[.19444,.68611,0,0,.86388],82:[0,.68611,0,0,.8625],83:[0,.68611,0,0,.63889],84:[0,.68611,0,0,.8],85:[0,.68611,0,0,.88472],86:[0,.68611,.01597,0,.86944],87:[0,.68611,.01597,0,1.18888],88:[0,.68611,0,0,.86944],89:[0,.68611,.02875,0,.86944],90:[0,.68611,0,0,.70277],91:[.25,.75,0,0,.31944],92:[.25,.75,0,0,.575],93:[.25,.75,0,0,.31944],94:[0,.69444,0,0,.575],95:[.31,.13444,.03194,0,.575],97:[0,.44444,0,0,.55902],98:[0,.69444,0,0,.63889],99:[0,.44444,0,0,.51111],100:[0,.69444,0,0,.63889],101:[0,.44444,0,0,.52708],102:[0,.69444,.10903,0,.35139],103:[.19444,.44444,.01597,0,.575],104:[0,.69444,0,0,.63889],105:[0,.69444,0,0,.31944],106:[.19444,.69444,0,0,.35139],107:[0,.69444,0,0,.60694],108:[0,.69444,0,0,.31944],109:[0,.44444,0,0,.95833],110:[0,.44444,0,0,.63889],111:[0,.44444,0,0,.575],112:[.19444,.44444,0,0,.63889],113:[.19444,.44444,0,0,.60694],114:[0,.44444,0,0,.47361],115:[0,.44444,0,0,.45361],116:[0,.63492,0,0,.44722],117:[0,.44444,0,0,.63889],118:[0,.44444,.01597,0,.60694],119:[0,.44444,.01597,0,.83055],120:[0,.44444,0,0,.60694],121:[.19444,.44444,.01597,0,.60694],122:[0,.44444,0,0,.51111],123:[.25,.75,0,0,.575],124:[.25,.75,0,0,.31944],125:[.25,.75,0,0,.575],126:[.35,.34444,0,0,.575],160:[0,0,0,0,.25],163:[0,.69444,0,0,.86853],168:[0,.69444,0,0,.575],172:[0,.44444,0,0,.76666],176:[0,.69444,0,0,.86944],177:[.13333,.63333,0,0,.89444],184:[.17014,0,0,0,.51111],198:[0,.68611,0,0,1.04166],215:[.13333,.63333,0,0,.89444],216:[.04861,.73472,0,0,.89444],223:[0,.69444,0,0,.59722],230:[0,.44444,0,0,.83055],247:[.13333,.63333,0,0,.89444],248:[.09722,.54167,0,0,.575],305:[0,.44444,0,0,.31944],338:[0,.68611,0,0,1.16944],339:[0,.44444,0,0,.89444],567:[.19444,.44444,0,0,.35139],710:[0,.69444,0,0,.575],711:[0,.63194,0,0,.575],713:[0,.59611,0,0,.575],714:[0,.69444,0,0,.575],715:[0,.69444,0,0,.575],728:[0,.69444,0,0,.575],729:[0,.69444,0,0,.31944],730:[0,.69444,0,0,.86944],732:[0,.69444,0,0,.575],733:[0,.69444,0,0,.575],915:[0,.68611,0,0,.69166],916:[0,.68611,0,0,.95833],920:[0,.68611,0,0,.89444],923:[0,.68611,0,0,.80555],926:[0,.68611,0,0,.76666],928:[0,.68611,0,0,.9],931:[0,.68611,0,0,.83055],933:[0,.68611,0,0,.89444],934:[0,.68611,0,0,.83055],936:[0,.68611,0,0,.89444],937:[0,.68611,0,0,.83055],8211:[0,.44444,.03194,0,.575],8212:[0,.44444,.03194,0,1.14999],8216:[0,.69444,0,0,.31944],8217:[0,.69444,0,0,.31944],8220:[0,.69444,0,0,.60278],8221:[0,.69444,0,0,.60278],8224:[.19444,.69444,0,0,.51111],8225:[.19444,.69444,0,0,.51111],8242:[0,.55556,0,0,.34444],8407:[0,.72444,.15486,0,.575],8463:[0,.69444,0,0,.66759],8465:[0,.69444,0,0,.83055],8467:[0,.69444,0,0,.47361],8472:[.19444,.44444,0,0,.74027],8476:[0,.69444,0,0,.83055],8501:[0,.69444,0,0,.70277],8592:[-.10889,.39111,0,0,1.14999],8593:[.19444,.69444,0,0,.575],8594:[-.10889,.39111,0,0,1.14999],8595:[.19444,.69444,0,0,.575],8596:[-.10889,.39111,0,0,1.14999],8597:[.25,.75,0,0,.575],8598:[.19444,.69444,0,0,1.14999],8599:[.19444,.69444,0,0,1.14999],8600:[.19444,.69444,0,0,1.14999],8601:[.19444,.69444,0,0,1.14999],8636:[-.10889,.39111,0,0,1.14999],8637:[-.10889,.39111,0,0,1.14999],8640:[-.10889,.39111,0,0,1.14999],8641:[-.10889,.39111,0,0,1.14999],8656:[-.10889,.39111,0,0,1.14999],8657:[.19444,.69444,0,0,.70277],8658:[-.10889,.39111,0,0,1.14999],8659:[.19444,.69444,0,0,.70277],8660:[-.10889,.39111,0,0,1.14999],8661:[.25,.75,0,0,.70277],8704:[0,.69444,0,0,.63889],8706:[0,.69444,.06389,0,.62847],8707:[0,.69444,0,0,.63889],8709:[.05556,.75,0,0,.575],8711:[0,.68611,0,0,.95833],8712:[.08556,.58556,0,0,.76666],8715:[.08556,.58556,0,0,.76666],8722:[.13333,.63333,0,0,.89444],8723:[.13333,.63333,0,0,.89444],8725:[.25,.75,0,0,.575],8726:[.25,.75,0,0,.575],8727:[-.02778,.47222,0,0,.575],8728:[-.02639,.47361,0,0,.575],8729:[-.02639,.47361,0,0,.575],8730:[.18,.82,0,0,.95833],8733:[0,.44444,0,0,.89444],8734:[0,.44444,0,0,1.14999],8736:[0,.69224,0,0,.72222],8739:[.25,.75,0,0,.31944],8741:[.25,.75,0,0,.575],8743:[0,.55556,0,0,.76666],8744:[0,.55556,0,0,.76666],8745:[0,.55556,0,0,.76666],8746:[0,.55556,0,0,.76666],8747:[.19444,.69444,.12778,0,.56875],8764:[-.10889,.39111,0,0,.89444],8768:[.19444,.69444,0,0,.31944],8771:[.00222,.50222,0,0,.89444],8773:[.027,.638,0,0,.894],8776:[.02444,.52444,0,0,.89444],8781:[.00222,.50222,0,0,.89444],8801:[.00222,.50222,0,0,.89444],8804:[.19667,.69667,0,0,.89444],8805:[.19667,.69667,0,0,.89444],8810:[.08556,.58556,0,0,1.14999],8811:[.08556,.58556,0,0,1.14999],8826:[.08556,.58556,0,0,.89444],8827:[.08556,.58556,0,0,.89444],8834:[.08556,.58556,0,0,.89444],8835:[.08556,.58556,0,0,.89444],8838:[.19667,.69667,0,0,.89444],8839:[.19667,.69667,0,0,.89444],8846:[0,.55556,0,0,.76666],8849:[.19667,.69667,0,0,.89444],8850:[.19667,.69667,0,0,.89444],8851:[0,.55556,0,0,.76666],8852:[0,.55556,0,0,.76666],8853:[.13333,.63333,0,0,.89444],8854:[.13333,.63333,0,0,.89444],8855:[.13333,.63333,0,0,.89444],8856:[.13333,.63333,0,0,.89444],8857:[.13333,.63333,0,0,.89444],8866:[0,.69444,0,0,.70277],8867:[0,.69444,0,0,.70277],8868:[0,.69444,0,0,.89444],8869:[0,.69444,0,0,.89444],8900:[-.02639,.47361,0,0,.575],8901:[-.02639,.47361,0,0,.31944],8902:[-.02778,.47222,0,0,.575],8968:[.25,.75,0,0,.51111],8969:[.25,.75,0,0,.51111],8970:[.25,.75,0,0,.51111],8971:[.25,.75,0,0,.51111],8994:[-.13889,.36111,0,0,1.14999],8995:[-.13889,.36111,0,0,1.14999],9651:[.19444,.69444,0,0,1.02222],9657:[-.02778,.47222,0,0,.575],9661:[.19444,.69444,0,0,1.02222],9667:[-.02778,.47222,0,0,.575],9711:[.19444,.69444,0,0,1.14999],9824:[.12963,.69444,0,0,.89444],9825:[.12963,.69444,0,0,.89444],9826:[.12963,.69444,0,0,.89444],9827:[.12963,.69444,0,0,.89444],9837:[0,.75,0,0,.44722],9838:[.19444,.69444,0,0,.44722],9839:[.19444,.69444,0,0,.44722],10216:[.25,.75,0,0,.44722],10217:[.25,.75,0,0,.44722],10815:[0,.68611,0,0,.9],10927:[.19667,.69667,0,0,.89444],10928:[.19667,.69667,0,0,.89444],57376:[.19444,.69444,0,0,0]},"Main-BoldItalic":{32:[0,0,0,0,.25],33:[0,.69444,.11417,0,.38611],34:[0,.69444,.07939,0,.62055],35:[.19444,.69444,.06833,0,.94444],37:[.05556,.75,.12861,0,.94444],38:[0,.69444,.08528,0,.88555],39:[0,.69444,.12945,0,.35555],40:[.25,.75,.15806,0,.47333],41:[.25,.75,.03306,0,.47333],42:[0,.75,.14333,0,.59111],43:[.10333,.60333,.03306,0,.88555],44:[.19444,.14722,0,0,.35555],45:[0,.44444,.02611,0,.41444],46:[0,.14722,0,0,.35555],47:[.25,.75,.15806,0,.59111],48:[0,.64444,.13167,0,.59111],49:[0,.64444,.13167,0,.59111],50:[0,.64444,.13167,0,.59111],51:[0,.64444,.13167,0,.59111],52:[.19444,.64444,.13167,0,.59111],53:[0,.64444,.13167,0,.59111],54:[0,.64444,.13167,0,.59111],55:[.19444,.64444,.13167,0,.59111],56:[0,.64444,.13167,0,.59111],57:[0,.64444,.13167,0,.59111],58:[0,.44444,.06695,0,.35555],59:[.19444,.44444,.06695,0,.35555],61:[-.10889,.39111,.06833,0,.88555],63:[0,.69444,.11472,0,.59111],64:[0,.69444,.09208,0,.88555],65:[0,.68611,0,0,.86555],66:[0,.68611,.0992,0,.81666],67:[0,.68611,.14208,0,.82666],68:[0,.68611,.09062,0,.87555],69:[0,.68611,.11431,0,.75666],70:[0,.68611,.12903,0,.72722],71:[0,.68611,.07347,0,.89527],72:[0,.68611,.17208,0,.8961],73:[0,.68611,.15681,0,.47166],74:[0,.68611,.145,0,.61055],75:[0,.68611,.14208,0,.89499],76:[0,.68611,0,0,.69777],77:[0,.68611,.17208,0,1.07277],78:[0,.68611,.17208,0,.8961],79:[0,.68611,.09062,0,.85499],80:[0,.68611,.0992,0,.78721],81:[.19444,.68611,.09062,0,.85499],82:[0,.68611,.02559,0,.85944],83:[0,.68611,.11264,0,.64999],84:[0,.68611,.12903,0,.7961],85:[0,.68611,.17208,0,.88083],86:[0,.68611,.18625,0,.86555],87:[0,.68611,.18625,0,1.15999],88:[0,.68611,.15681,0,.86555],89:[0,.68611,.19803,0,.86555],90:[0,.68611,.14208,0,.70888],91:[.25,.75,.1875,0,.35611],93:[.25,.75,.09972,0,.35611],94:[0,.69444,.06709,0,.59111],95:[.31,.13444,.09811,0,.59111],97:[0,.44444,.09426,0,.59111],98:[0,.69444,.07861,0,.53222],99:[0,.44444,.05222,0,.53222],100:[0,.69444,.10861,0,.59111],101:[0,.44444,.085,0,.53222],102:[.19444,.69444,.21778,0,.4],103:[.19444,.44444,.105,0,.53222],104:[0,.69444,.09426,0,.59111],105:[0,.69326,.11387,0,.35555],106:[.19444,.69326,.1672,0,.35555],107:[0,.69444,.11111,0,.53222],108:[0,.69444,.10861,0,.29666],109:[0,.44444,.09426,0,.94444],110:[0,.44444,.09426,0,.64999],111:[0,.44444,.07861,0,.59111],112:[.19444,.44444,.07861,0,.59111],113:[.19444,.44444,.105,0,.53222],114:[0,.44444,.11111,0,.50167],115:[0,.44444,.08167,0,.48694],116:[0,.63492,.09639,0,.385],117:[0,.44444,.09426,0,.62055],118:[0,.44444,.11111,0,.53222],119:[0,.44444,.11111,0,.76777],120:[0,.44444,.12583,0,.56055],121:[.19444,.44444,.105,0,.56166],122:[0,.44444,.13889,0,.49055],126:[.35,.34444,.11472,0,.59111],160:[0,0,0,0,.25],168:[0,.69444,.11473,0,.59111],176:[0,.69444,0,0,.94888],184:[.17014,0,0,0,.53222],198:[0,.68611,.11431,0,1.02277],216:[.04861,.73472,.09062,0,.88555],223:[.19444,.69444,.09736,0,.665],230:[0,.44444,.085,0,.82666],248:[.09722,.54167,.09458,0,.59111],305:[0,.44444,.09426,0,.35555],338:[0,.68611,.11431,0,1.14054],339:[0,.44444,.085,0,.82666],567:[.19444,.44444,.04611,0,.385],710:[0,.69444,.06709,0,.59111],711:[0,.63194,.08271,0,.59111],713:[0,.59444,.10444,0,.59111],714:[0,.69444,.08528,0,.59111],715:[0,.69444,0,0,.59111],728:[0,.69444,.10333,0,.59111],729:[0,.69444,.12945,0,.35555],730:[0,.69444,0,0,.94888],732:[0,.69444,.11472,0,.59111],733:[0,.69444,.11472,0,.59111],915:[0,.68611,.12903,0,.69777],916:[0,.68611,0,0,.94444],920:[0,.68611,.09062,0,.88555],923:[0,.68611,0,0,.80666],926:[0,.68611,.15092,0,.76777],928:[0,.68611,.17208,0,.8961],931:[0,.68611,.11431,0,.82666],933:[0,.68611,.10778,0,.88555],934:[0,.68611,.05632,0,.82666],936:[0,.68611,.10778,0,.88555],937:[0,.68611,.0992,0,.82666],8211:[0,.44444,.09811,0,.59111],8212:[0,.44444,.09811,0,1.18221],8216:[0,.69444,.12945,0,.35555],8217:[0,.69444,.12945,0,.35555],8220:[0,.69444,.16772,0,.62055],8221:[0,.69444,.07939,0,.62055]},"Main-Italic":{32:[0,0,0,0,.25],33:[0,.69444,.12417,0,.30667],34:[0,.69444,.06961,0,.51444],35:[.19444,.69444,.06616,0,.81777],37:[.05556,.75,.13639,0,.81777],38:[0,.69444,.09694,0,.76666],39:[0,.69444,.12417,0,.30667],40:[.25,.75,.16194,0,.40889],41:[.25,.75,.03694,0,.40889],42:[0,.75,.14917,0,.51111],43:[.05667,.56167,.03694,0,.76666],44:[.19444,.10556,0,0,.30667],45:[0,.43056,.02826,0,.35778],46:[0,.10556,0,0,.30667],47:[.25,.75,.16194,0,.51111],48:[0,.64444,.13556,0,.51111],49:[0,.64444,.13556,0,.51111],50:[0,.64444,.13556,0,.51111],51:[0,.64444,.13556,0,.51111],52:[.19444,.64444,.13556,0,.51111],53:[0,.64444,.13556,0,.51111],54:[0,.64444,.13556,0,.51111],55:[.19444,.64444,.13556,0,.51111],56:[0,.64444,.13556,0,.51111],57:[0,.64444,.13556,0,.51111],58:[0,.43056,.0582,0,.30667],59:[.19444,.43056,.0582,0,.30667],61:[-.13313,.36687,.06616,0,.76666],63:[0,.69444,.1225,0,.51111],64:[0,.69444,.09597,0,.76666],65:[0,.68333,0,0,.74333],66:[0,.68333,.10257,0,.70389],67:[0,.68333,.14528,0,.71555],68:[0,.68333,.09403,0,.755],69:[0,.68333,.12028,0,.67833],70:[0,.68333,.13305,0,.65277],71:[0,.68333,.08722,0,.77361],72:[0,.68333,.16389,0,.74333],73:[0,.68333,.15806,0,.38555],74:[0,.68333,.14028,0,.525],75:[0,.68333,.14528,0,.76888],76:[0,.68333,0,0,.62722],77:[0,.68333,.16389,0,.89666],78:[0,.68333,.16389,0,.74333],79:[0,.68333,.09403,0,.76666],80:[0,.68333,.10257,0,.67833],81:[.19444,.68333,.09403,0,.76666],82:[0,.68333,.03868,0,.72944],83:[0,.68333,.11972,0,.56222],84:[0,.68333,.13305,0,.71555],85:[0,.68333,.16389,0,.74333],86:[0,.68333,.18361,0,.74333],87:[0,.68333,.18361,0,.99888],88:[0,.68333,.15806,0,.74333],89:[0,.68333,.19383,0,.74333],90:[0,.68333,.14528,0,.61333],91:[.25,.75,.1875,0,.30667],93:[.25,.75,.10528,0,.30667],94:[0,.69444,.06646,0,.51111],95:[.31,.12056,.09208,0,.51111],97:[0,.43056,.07671,0,.51111],98:[0,.69444,.06312,0,.46],99:[0,.43056,.05653,0,.46],100:[0,.69444,.10333,0,.51111],101:[0,.43056,.07514,0,.46],102:[.19444,.69444,.21194,0,.30667],103:[.19444,.43056,.08847,0,.46],104:[0,.69444,.07671,0,.51111],105:[0,.65536,.1019,0,.30667],106:[.19444,.65536,.14467,0,.30667],107:[0,.69444,.10764,0,.46],108:[0,.69444,.10333,0,.25555],109:[0,.43056,.07671,0,.81777],110:[0,.43056,.07671,0,.56222],111:[0,.43056,.06312,0,.51111],112:[.19444,.43056,.06312,0,.51111],113:[.19444,.43056,.08847,0,.46],114:[0,.43056,.10764,0,.42166],115:[0,.43056,.08208,0,.40889],116:[0,.61508,.09486,0,.33222],117:[0,.43056,.07671,0,.53666],118:[0,.43056,.10764,0,.46],119:[0,.43056,.10764,0,.66444],120:[0,.43056,.12042,0,.46389],121:[.19444,.43056,.08847,0,.48555],122:[0,.43056,.12292,0,.40889],126:[.35,.31786,.11585,0,.51111],160:[0,0,0,0,.25],168:[0,.66786,.10474,0,.51111],176:[0,.69444,0,0,.83129],184:[.17014,0,0,0,.46],198:[0,.68333,.12028,0,.88277],216:[.04861,.73194,.09403,0,.76666],223:[.19444,.69444,.10514,0,.53666],230:[0,.43056,.07514,0,.71555],248:[.09722,.52778,.09194,0,.51111],338:[0,.68333,.12028,0,.98499],339:[0,.43056,.07514,0,.71555],710:[0,.69444,.06646,0,.51111],711:[0,.62847,.08295,0,.51111],713:[0,.56167,.10333,0,.51111],714:[0,.69444,.09694,0,.51111],715:[0,.69444,0,0,.51111],728:[0,.69444,.10806,0,.51111],729:[0,.66786,.11752,0,.30667],730:[0,.69444,0,0,.83129],732:[0,.66786,.11585,0,.51111],733:[0,.69444,.1225,0,.51111],915:[0,.68333,.13305,0,.62722],916:[0,.68333,0,0,.81777],920:[0,.68333,.09403,0,.76666],923:[0,.68333,0,0,.69222],926:[0,.68333,.15294,0,.66444],928:[0,.68333,.16389,0,.74333],931:[0,.68333,.12028,0,.71555],933:[0,.68333,.11111,0,.76666],934:[0,.68333,.05986,0,.71555],936:[0,.68333,.11111,0,.76666],937:[0,.68333,.10257,0,.71555],8211:[0,.43056,.09208,0,.51111],8212:[0,.43056,.09208,0,1.02222],8216:[0,.69444,.12417,0,.30667],8217:[0,.69444,.12417,0,.30667],8220:[0,.69444,.1685,0,.51444],8221:[0,.69444,.06961,0,.51444],8463:[0,.68889,0,0,.54028]},"Main-Regular":{32:[0,0,0,0,.25],33:[0,.69444,0,0,.27778],34:[0,.69444,0,0,.5],35:[.19444,.69444,0,0,.83334],36:[.05556,.75,0,0,.5],37:[.05556,.75,0,0,.83334],38:[0,.69444,0,0,.77778],39:[0,.69444,0,0,.27778],40:[.25,.75,0,0,.38889],41:[.25,.75,0,0,.38889],42:[0,.75,0,0,.5],43:[.08333,.58333,0,0,.77778],44:[.19444,.10556,0,0,.27778],45:[0,.43056,0,0,.33333],46:[0,.10556,0,0,.27778],47:[.25,.75,0,0,.5],48:[0,.64444,0,0,.5],49:[0,.64444,0,0,.5],50:[0,.64444,0,0,.5],51:[0,.64444,0,0,.5],52:[0,.64444,0,0,.5],53:[0,.64444,0,0,.5],54:[0,.64444,0,0,.5],55:[0,.64444,0,0,.5],56:[0,.64444,0,0,.5],57:[0,.64444,0,0,.5],58:[0,.43056,0,0,.27778],59:[.19444,.43056,0,0,.27778],60:[.0391,.5391,0,0,.77778],61:[-.13313,.36687,0,0,.77778],62:[.0391,.5391,0,0,.77778],63:[0,.69444,0,0,.47222],64:[0,.69444,0,0,.77778],65:[0,.68333,0,0,.75],66:[0,.68333,0,0,.70834],67:[0,.68333,0,0,.72222],68:[0,.68333,0,0,.76389],69:[0,.68333,0,0,.68056],70:[0,.68333,0,0,.65278],71:[0,.68333,0,0,.78472],72:[0,.68333,0,0,.75],73:[0,.68333,0,0,.36111],74:[0,.68333,0,0,.51389],75:[0,.68333,0,0,.77778],76:[0,.68333,0,0,.625],77:[0,.68333,0,0,.91667],78:[0,.68333,0,0,.75],79:[0,.68333,0,0,.77778],80:[0,.68333,0,0,.68056],81:[.19444,.68333,0,0,.77778],82:[0,.68333,0,0,.73611],83:[0,.68333,0,0,.55556],84:[0,.68333,0,0,.72222],85:[0,.68333,0,0,.75],86:[0,.68333,.01389,0,.75],87:[0,.68333,.01389,0,1.02778],88:[0,.68333,0,0,.75],89:[0,.68333,.025,0,.75],90:[0,.68333,0,0,.61111],91:[.25,.75,0,0,.27778],92:[.25,.75,0,0,.5],93:[.25,.75,0,0,.27778],94:[0,.69444,0,0,.5],95:[.31,.12056,.02778,0,.5],97:[0,.43056,0,0,.5],98:[0,.69444,0,0,.55556],99:[0,.43056,0,0,.44445],100:[0,.69444,0,0,.55556],101:[0,.43056,0,0,.44445],102:[0,.69444,.07778,0,.30556],103:[.19444,.43056,.01389,0,.5],104:[0,.69444,0,0,.55556],105:[0,.66786,0,0,.27778],106:[.19444,.66786,0,0,.30556],107:[0,.69444,0,0,.52778],108:[0,.69444,0,0,.27778],109:[0,.43056,0,0,.83334],110:[0,.43056,0,0,.55556],111:[0,.43056,0,0,.5],112:[.19444,.43056,0,0,.55556],113:[.19444,.43056,0,0,.52778],114:[0,.43056,0,0,.39167],115:[0,.43056,0,0,.39445],116:[0,.61508,0,0,.38889],117:[0,.43056,0,0,.55556],118:[0,.43056,.01389,0,.52778],119:[0,.43056,.01389,0,.72222],120:[0,.43056,0,0,.52778],121:[.19444,.43056,.01389,0,.52778],122:[0,.43056,0,0,.44445],123:[.25,.75,0,0,.5],124:[.25,.75,0,0,.27778],125:[.25,.75,0,0,.5],126:[.35,.31786,0,0,.5],160:[0,0,0,0,.25],163:[0,.69444,0,0,.76909],167:[.19444,.69444,0,0,.44445],168:[0,.66786,0,0,.5],172:[0,.43056,0,0,.66667],176:[0,.69444,0,0,.75],177:[.08333,.58333,0,0,.77778],182:[.19444,.69444,0,0,.61111],184:[.17014,0,0,0,.44445],198:[0,.68333,0,0,.90278],215:[.08333,.58333,0,0,.77778],216:[.04861,.73194,0,0,.77778],223:[0,.69444,0,0,.5],230:[0,.43056,0,0,.72222],247:[.08333,.58333,0,0,.77778],248:[.09722,.52778,0,0,.5],305:[0,.43056,0,0,.27778],338:[0,.68333,0,0,1.01389],339:[0,.43056,0,0,.77778],567:[.19444,.43056,0,0,.30556],710:[0,.69444,0,0,.5],711:[0,.62847,0,0,.5],713:[0,.56778,0,0,.5],714:[0,.69444,0,0,.5],715:[0,.69444,0,0,.5],728:[0,.69444,0,0,.5],729:[0,.66786,0,0,.27778],730:[0,.69444,0,0,.75],732:[0,.66786,0,0,.5],733:[0,.69444,0,0,.5],915:[0,.68333,0,0,.625],916:[0,.68333,0,0,.83334],920:[0,.68333,0,0,.77778],923:[0,.68333,0,0,.69445],926:[0,.68333,0,0,.66667],928:[0,.68333,0,0,.75],931:[0,.68333,0,0,.72222],933:[0,.68333,0,0,.77778],934:[0,.68333,0,0,.72222],936:[0,.68333,0,0,.77778],937:[0,.68333,0,0,.72222],8211:[0,.43056,.02778,0,.5],8212:[0,.43056,.02778,0,1],8216:[0,.69444,0,0,.27778],8217:[0,.69444,0,0,.27778],8220:[0,.69444,0,0,.5],8221:[0,.69444,0,0,.5],8224:[.19444,.69444,0,0,.44445],8225:[.19444,.69444,0,0,.44445],8230:[0,.123,0,0,1.172],8242:[0,.55556,0,0,.275],8407:[0,.71444,.15382,0,.5],8463:[0,.68889,0,0,.54028],8465:[0,.69444,0,0,.72222],8467:[0,.69444,0,.11111,.41667],8472:[.19444,.43056,0,.11111,.63646],8476:[0,.69444,0,0,.72222],8501:[0,.69444,0,0,.61111],8592:[-.13313,.36687,0,0,1],8593:[.19444,.69444,0,0,.5],8594:[-.13313,.36687,0,0,1],8595:[.19444,.69444,0,0,.5],8596:[-.13313,.36687,0,0,1],8597:[.25,.75,0,0,.5],8598:[.19444,.69444,0,0,1],8599:[.19444,.69444,0,0,1],8600:[.19444,.69444,0,0,1],8601:[.19444,.69444,0,0,1],8614:[.011,.511,0,0,1],8617:[.011,.511,0,0,1.126],8618:[.011,.511,0,0,1.126],8636:[-.13313,.36687,0,0,1],8637:[-.13313,.36687,0,0,1],8640:[-.13313,.36687,0,0,1],8641:[-.13313,.36687,0,0,1],8652:[.011,.671,0,0,1],8656:[-.13313,.36687,0,0,1],8657:[.19444,.69444,0,0,.61111],8658:[-.13313,.36687,0,0,1],8659:[.19444,.69444,0,0,.61111],8660:[-.13313,.36687,0,0,1],8661:[.25,.75,0,0,.61111],8704:[0,.69444,0,0,.55556],8706:[0,.69444,.05556,.08334,.5309],8707:[0,.69444,0,0,.55556],8709:[.05556,.75,0,0,.5],8711:[0,.68333,0,0,.83334],8712:[.0391,.5391,0,0,.66667],8715:[.0391,.5391,0,0,.66667],8722:[.08333,.58333,0,0,.77778],8723:[.08333,.58333,0,0,.77778],8725:[.25,.75,0,0,.5],8726:[.25,.75,0,0,.5],8727:[-.03472,.46528,0,0,.5],8728:[-.05555,.44445,0,0,.5],8729:[-.05555,.44445,0,0,.5],8730:[.2,.8,0,0,.83334],8733:[0,.43056,0,0,.77778],8734:[0,.43056,0,0,1],8736:[0,.69224,0,0,.72222],8739:[.25,.75,0,0,.27778],8741:[.25,.75,0,0,.5],8743:[0,.55556,0,0,.66667],8744:[0,.55556,0,0,.66667],8745:[0,.55556,0,0,.66667],8746:[0,.55556,0,0,.66667],8747:[.19444,.69444,.11111,0,.41667],8764:[-.13313,.36687,0,0,.77778],8768:[.19444,.69444,0,0,.27778],8771:[-.03625,.46375,0,0,.77778],8773:[-.022,.589,0,0,.778],8776:[-.01688,.48312,0,0,.77778],8781:[-.03625,.46375,0,0,.77778],8784:[-.133,.673,0,0,.778],8801:[-.03625,.46375,0,0,.77778],8804:[.13597,.63597,0,0,.77778],8805:[.13597,.63597,0,0,.77778],8810:[.0391,.5391,0,0,1],8811:[.0391,.5391,0,0,1],8826:[.0391,.5391,0,0,.77778],8827:[.0391,.5391,0,0,.77778],8834:[.0391,.5391,0,0,.77778],8835:[.0391,.5391,0,0,.77778],8838:[.13597,.63597,0,0,.77778],8839:[.13597,.63597,0,0,.77778],8846:[0,.55556,0,0,.66667],8849:[.13597,.63597,0,0,.77778],8850:[.13597,.63597,0,0,.77778],8851:[0,.55556,0,0,.66667],8852:[0,.55556,0,0,.66667],8853:[.08333,.58333,0,0,.77778],8854:[.08333,.58333,0,0,.77778],8855:[.08333,.58333,0,0,.77778],8856:[.08333,.58333,0,0,.77778],8857:[.08333,.58333,0,0,.77778],8866:[0,.69444,0,0,.61111],8867:[0,.69444,0,0,.61111],8868:[0,.69444,0,0,.77778],8869:[0,.69444,0,0,.77778],8872:[.249,.75,0,0,.867],8900:[-.05555,.44445,0,0,.5],8901:[-.05555,.44445,0,0,.27778],8902:[-.03472,.46528,0,0,.5],8904:[.005,.505,0,0,.9],8942:[.03,.903,0,0,.278],8943:[-.19,.313,0,0,1.172],8945:[-.1,.823,0,0,1.282],8968:[.25,.75,0,0,.44445],8969:[.25,.75,0,0,.44445],8970:[.25,.75,0,0,.44445],8971:[.25,.75,0,0,.44445],8994:[-.14236,.35764,0,0,1],8995:[-.14236,.35764,0,0,1],9136:[.244,.744,0,0,.412],9137:[.244,.745,0,0,.412],9651:[.19444,.69444,0,0,.88889],9657:[-.03472,.46528,0,0,.5],9661:[.19444,.69444,0,0,.88889],9667:[-.03472,.46528,0,0,.5],9711:[.19444,.69444,0,0,1],9824:[.12963,.69444,0,0,.77778],9825:[.12963,.69444,0,0,.77778],9826:[.12963,.69444,0,0,.77778],9827:[.12963,.69444,0,0,.77778],9837:[0,.75,0,0,.38889],9838:[.19444,.69444,0,0,.38889],9839:[.19444,.69444,0,0,.38889],10216:[.25,.75,0,0,.38889],10217:[.25,.75,0,0,.38889],10222:[.244,.744,0,0,.412],10223:[.244,.745,0,0,.412],10229:[.011,.511,0,0,1.609],10230:[.011,.511,0,0,1.638],10231:[.011,.511,0,0,1.859],10232:[.024,.525,0,0,1.609],10233:[.024,.525,0,0,1.638],10234:[.024,.525,0,0,1.858],10236:[.011,.511,0,0,1.638],10815:[0,.68333,0,0,.75],10927:[.13597,.63597,0,0,.77778],10928:[.13597,.63597,0,0,.77778],57376:[.19444,.69444,0,0,0]},"Math-BoldItalic":{32:[0,0,0,0,.25],48:[0,.44444,0,0,.575],49:[0,.44444,0,0,.575],50:[0,.44444,0,0,.575],51:[.19444,.44444,0,0,.575],52:[.19444,.44444,0,0,.575],53:[.19444,.44444,0,0,.575],54:[0,.64444,0,0,.575],55:[.19444,.44444,0,0,.575],56:[0,.64444,0,0,.575],57:[.19444,.44444,0,0,.575],65:[0,.68611,0,0,.86944],66:[0,.68611,.04835,0,.8664],67:[0,.68611,.06979,0,.81694],68:[0,.68611,.03194,0,.93812],69:[0,.68611,.05451,0,.81007],70:[0,.68611,.15972,0,.68889],71:[0,.68611,0,0,.88673],72:[0,.68611,.08229,0,.98229],73:[0,.68611,.07778,0,.51111],74:[0,.68611,.10069,0,.63125],75:[0,.68611,.06979,0,.97118],76:[0,.68611,0,0,.75555],77:[0,.68611,.11424,0,1.14201],78:[0,.68611,.11424,0,.95034],79:[0,.68611,.03194,0,.83666],80:[0,.68611,.15972,0,.72309],81:[.19444,.68611,0,0,.86861],82:[0,.68611,.00421,0,.87235],83:[0,.68611,.05382,0,.69271],84:[0,.68611,.15972,0,.63663],85:[0,.68611,.11424,0,.80027],86:[0,.68611,.25555,0,.67778],87:[0,.68611,.15972,0,1.09305],88:[0,.68611,.07778,0,.94722],89:[0,.68611,.25555,0,.67458],90:[0,.68611,.06979,0,.77257],97:[0,.44444,0,0,.63287],98:[0,.69444,0,0,.52083],99:[0,.44444,0,0,.51342],100:[0,.69444,0,0,.60972],101:[0,.44444,0,0,.55361],102:[.19444,.69444,.11042,0,.56806],103:[.19444,.44444,.03704,0,.5449],104:[0,.69444,0,0,.66759],105:[0,.69326,0,0,.4048],106:[.19444,.69326,.0622,0,.47083],107:[0,.69444,.01852,0,.6037],108:[0,.69444,.0088,0,.34815],109:[0,.44444,0,0,1.0324],110:[0,.44444,0,0,.71296],111:[0,.44444,0,0,.58472],112:[.19444,.44444,0,0,.60092],113:[.19444,.44444,.03704,0,.54213],114:[0,.44444,.03194,0,.5287],115:[0,.44444,0,0,.53125],116:[0,.63492,0,0,.41528],117:[0,.44444,0,0,.68102],118:[0,.44444,.03704,0,.56666],119:[0,.44444,.02778,0,.83148],120:[0,.44444,0,0,.65903],121:[.19444,.44444,.03704,0,.59028],122:[0,.44444,.04213,0,.55509],160:[0,0,0,0,.25],915:[0,.68611,.15972,0,.65694],916:[0,.68611,0,0,.95833],920:[0,.68611,.03194,0,.86722],923:[0,.68611,0,0,.80555],926:[0,.68611,.07458,0,.84125],928:[0,.68611,.08229,0,.98229],931:[0,.68611,.05451,0,.88507],933:[0,.68611,.15972,0,.67083],934:[0,.68611,0,0,.76666],936:[0,.68611,.11653,0,.71402],937:[0,.68611,.04835,0,.8789],945:[0,.44444,0,0,.76064],946:[.19444,.69444,.03403,0,.65972],947:[.19444,.44444,.06389,0,.59003],948:[0,.69444,.03819,0,.52222],949:[0,.44444,0,0,.52882],950:[.19444,.69444,.06215,0,.50833],951:[.19444,.44444,.03704,0,.6],952:[0,.69444,.03194,0,.5618],953:[0,.44444,0,0,.41204],954:[0,.44444,0,0,.66759],955:[0,.69444,0,0,.67083],956:[.19444,.44444,0,0,.70787],957:[0,.44444,.06898,0,.57685],958:[.19444,.69444,.03021,0,.50833],959:[0,.44444,0,0,.58472],960:[0,.44444,.03704,0,.68241],961:[.19444,.44444,0,0,.6118],962:[.09722,.44444,.07917,0,.42361],963:[0,.44444,.03704,0,.68588],964:[0,.44444,.13472,0,.52083],965:[0,.44444,.03704,0,.63055],966:[.19444,.44444,0,0,.74722],967:[.19444,.44444,0,0,.71805],968:[.19444,.69444,.03704,0,.75833],969:[0,.44444,.03704,0,.71782],977:[0,.69444,0,0,.69155],981:[.19444,.69444,0,0,.7125],982:[0,.44444,.03194,0,.975],1009:[.19444,.44444,0,0,.6118],1013:[0,.44444,0,0,.48333],57649:[0,.44444,0,0,.39352],57911:[.19444,.44444,0,0,.43889]},"Math-Italic":{32:[0,0,0,0,.25],48:[0,.43056,0,0,.5],49:[0,.43056,0,0,.5],50:[0,.43056,0,0,.5],51:[.19444,.43056,0,0,.5],52:[.19444,.43056,0,0,.5],53:[.19444,.43056,0,0,.5],54:[0,.64444,0,0,.5],55:[.19444,.43056,0,0,.5],56:[0,.64444,0,0,.5],57:[.19444,.43056,0,0,.5],65:[0,.68333,0,.13889,.75],66:[0,.68333,.05017,.08334,.75851],67:[0,.68333,.07153,.08334,.71472],68:[0,.68333,.02778,.05556,.82792],69:[0,.68333,.05764,.08334,.7382],70:[0,.68333,.13889,.08334,.64306],71:[0,.68333,0,.08334,.78625],72:[0,.68333,.08125,.05556,.83125],73:[0,.68333,.07847,.11111,.43958],74:[0,.68333,.09618,.16667,.55451],75:[0,.68333,.07153,.05556,.84931],76:[0,.68333,0,.02778,.68056],77:[0,.68333,.10903,.08334,.97014],78:[0,.68333,.10903,.08334,.80347],79:[0,.68333,.02778,.08334,.76278],80:[0,.68333,.13889,.08334,.64201],81:[.19444,.68333,0,.08334,.79056],82:[0,.68333,.00773,.08334,.75929],83:[0,.68333,.05764,.08334,.6132],84:[0,.68333,.13889,.08334,.58438],85:[0,.68333,.10903,.02778,.68278],86:[0,.68333,.22222,0,.58333],87:[0,.68333,.13889,0,.94445],88:[0,.68333,.07847,.08334,.82847],89:[0,.68333,.22222,0,.58056],90:[0,.68333,.07153,.08334,.68264],97:[0,.43056,0,0,.52859],98:[0,.69444,0,0,.42917],99:[0,.43056,0,.05556,.43276],100:[0,.69444,0,.16667,.52049],101:[0,.43056,0,.05556,.46563],102:[.19444,.69444,.10764,.16667,.48959],103:[.19444,.43056,.03588,.02778,.47697],104:[0,.69444,0,0,.57616],105:[0,.65952,0,0,.34451],106:[.19444,.65952,.05724,0,.41181],107:[0,.69444,.03148,0,.5206],108:[0,.69444,.01968,.08334,.29838],109:[0,.43056,0,0,.87801],110:[0,.43056,0,0,.60023],111:[0,.43056,0,.05556,.48472],112:[.19444,.43056,0,.08334,.50313],113:[.19444,.43056,.03588,.08334,.44641],114:[0,.43056,.02778,.05556,.45116],115:[0,.43056,0,.05556,.46875],116:[0,.61508,0,.08334,.36111],117:[0,.43056,0,.02778,.57246],118:[0,.43056,.03588,.02778,.48472],119:[0,.43056,.02691,.08334,.71592],120:[0,.43056,0,.02778,.57153],121:[.19444,.43056,.03588,.05556,.49028],122:[0,.43056,.04398,.05556,.46505],160:[0,0,0,0,.25],915:[0,.68333,.13889,.08334,.61528],916:[0,.68333,0,.16667,.83334],920:[0,.68333,.02778,.08334,.76278],923:[0,.68333,0,.16667,.69445],926:[0,.68333,.07569,.08334,.74236],928:[0,.68333,.08125,.05556,.83125],931:[0,.68333,.05764,.08334,.77986],933:[0,.68333,.13889,.05556,.58333],934:[0,.68333,0,.08334,.66667],936:[0,.68333,.11,.05556,.61222],937:[0,.68333,.05017,.08334,.7724],945:[0,.43056,.0037,.02778,.6397],946:[.19444,.69444,.05278,.08334,.56563],947:[.19444,.43056,.05556,0,.51773],948:[0,.69444,.03785,.05556,.44444],949:[0,.43056,0,.08334,.46632],950:[.19444,.69444,.07378,.08334,.4375],951:[.19444,.43056,.03588,.05556,.49653],952:[0,.69444,.02778,.08334,.46944],953:[0,.43056,0,.05556,.35394],954:[0,.43056,0,0,.57616],955:[0,.69444,0,0,.58334],956:[.19444,.43056,0,.02778,.60255],957:[0,.43056,.06366,.02778,.49398],958:[.19444,.69444,.04601,.11111,.4375],959:[0,.43056,0,.05556,.48472],960:[0,.43056,.03588,0,.57003],961:[.19444,.43056,0,.08334,.51702],962:[.09722,.43056,.07986,.08334,.36285],963:[0,.43056,.03588,0,.57141],964:[0,.43056,.1132,.02778,.43715],965:[0,.43056,.03588,.02778,.54028],966:[.19444,.43056,0,.08334,.65417],967:[.19444,.43056,0,.05556,.62569],968:[.19444,.69444,.03588,.11111,.65139],969:[0,.43056,.03588,0,.62245],977:[0,.69444,0,.08334,.59144],981:[.19444,.69444,0,.08334,.59583],982:[0,.43056,.02778,0,.82813],1009:[.19444,.43056,0,.08334,.51702],1013:[0,.43056,0,.05556,.4059],57649:[0,.43056,0,.02778,.32246],57911:[.19444,.43056,0,.08334,.38403]},"SansSerif-Bold":{32:[0,0,0,0,.25],33:[0,.69444,0,0,.36667],34:[0,.69444,0,0,.55834],35:[.19444,.69444,0,0,.91667],36:[.05556,.75,0,0,.55],37:[.05556,.75,0,0,1.02912],38:[0,.69444,0,0,.83056],39:[0,.69444,0,0,.30556],40:[.25,.75,0,0,.42778],41:[.25,.75,0,0,.42778],42:[0,.75,0,0,.55],43:[.11667,.61667,0,0,.85556],44:[.10556,.13056,0,0,.30556],45:[0,.45833,0,0,.36667],46:[0,.13056,0,0,.30556],47:[.25,.75,0,0,.55],48:[0,.69444,0,0,.55],49:[0,.69444,0,0,.55],50:[0,.69444,0,0,.55],51:[0,.69444,0,0,.55],52:[0,.69444,0,0,.55],53:[0,.69444,0,0,.55],54:[0,.69444,0,0,.55],55:[0,.69444,0,0,.55],56:[0,.69444,0,0,.55],57:[0,.69444,0,0,.55],58:[0,.45833,0,0,.30556],59:[.10556,.45833,0,0,.30556],61:[-.09375,.40625,0,0,.85556],63:[0,.69444,0,0,.51945],64:[0,.69444,0,0,.73334],65:[0,.69444,0,0,.73334],66:[0,.69444,0,0,.73334],67:[0,.69444,0,0,.70278],68:[0,.69444,0,0,.79445],69:[0,.69444,0,0,.64167],70:[0,.69444,0,0,.61111],71:[0,.69444,0,0,.73334],72:[0,.69444,0,0,.79445],73:[0,.69444,0,0,.33056],74:[0,.69444,0,0,.51945],75:[0,.69444,0,0,.76389],76:[0,.69444,0,0,.58056],77:[0,.69444,0,0,.97778],78:[0,.69444,0,0,.79445],79:[0,.69444,0,0,.79445],80:[0,.69444,0,0,.70278],81:[.10556,.69444,0,0,.79445],82:[0,.69444,0,0,.70278],83:[0,.69444,0,0,.61111],84:[0,.69444,0,0,.73334],85:[0,.69444,0,0,.76389],86:[0,.69444,.01528,0,.73334],87:[0,.69444,.01528,0,1.03889],88:[0,.69444,0,0,.73334],89:[0,.69444,.0275,0,.73334],90:[0,.69444,0,0,.67223],91:[.25,.75,0,0,.34306],93:[.25,.75,0,0,.34306],94:[0,.69444,0,0,.55],95:[.35,.10833,.03056,0,.55],97:[0,.45833,0,0,.525],98:[0,.69444,0,0,.56111],99:[0,.45833,0,0,.48889],100:[0,.69444,0,0,.56111],101:[0,.45833,0,0,.51111],102:[0,.69444,.07639,0,.33611],103:[.19444,.45833,.01528,0,.55],104:[0,.69444,0,0,.56111],105:[0,.69444,0,0,.25556],106:[.19444,.69444,0,0,.28611],107:[0,.69444,0,0,.53056],108:[0,.69444,0,0,.25556],109:[0,.45833,0,0,.86667],110:[0,.45833,0,0,.56111],111:[0,.45833,0,0,.55],112:[.19444,.45833,0,0,.56111],113:[.19444,.45833,0,0,.56111],114:[0,.45833,.01528,0,.37222],115:[0,.45833,0,0,.42167],116:[0,.58929,0,0,.40417],117:[0,.45833,0,0,.56111],118:[0,.45833,.01528,0,.5],119:[0,.45833,.01528,0,.74445],120:[0,.45833,0,0,.5],121:[.19444,.45833,.01528,0,.5],122:[0,.45833,0,0,.47639],126:[.35,.34444,0,0,.55],160:[0,0,0,0,.25],168:[0,.69444,0,0,.55],176:[0,.69444,0,0,.73334],180:[0,.69444,0,0,.55],184:[.17014,0,0,0,.48889],305:[0,.45833,0,0,.25556],567:[.19444,.45833,0,0,.28611],710:[0,.69444,0,0,.55],711:[0,.63542,0,0,.55],713:[0,.63778,0,0,.55],728:[0,.69444,0,0,.55],729:[0,.69444,0,0,.30556],730:[0,.69444,0,0,.73334],732:[0,.69444,0,0,.55],733:[0,.69444,0,0,.55],915:[0,.69444,0,0,.58056],916:[0,.69444,0,0,.91667],920:[0,.69444,0,0,.85556],923:[0,.69444,0,0,.67223],926:[0,.69444,0,0,.73334],928:[0,.69444,0,0,.79445],931:[0,.69444,0,0,.79445],933:[0,.69444,0,0,.85556],934:[0,.69444,0,0,.79445],936:[0,.69444,0,0,.85556],937:[0,.69444,0,0,.79445],8211:[0,.45833,.03056,0,.55],8212:[0,.45833,.03056,0,1.10001],8216:[0,.69444,0,0,.30556],8217:[0,.69444,0,0,.30556],8220:[0,.69444,0,0,.55834],8221:[0,.69444,0,0,.55834]},"SansSerif-Italic":{32:[0,0,0,0,.25],33:[0,.69444,.05733,0,.31945],34:[0,.69444,.00316,0,.5],35:[.19444,.69444,.05087,0,.83334],36:[.05556,.75,.11156,0,.5],37:[.05556,.75,.03126,0,.83334],38:[0,.69444,.03058,0,.75834],39:[0,.69444,.07816,0,.27778],40:[.25,.75,.13164,0,.38889],41:[.25,.75,.02536,0,.38889],42:[0,.75,.11775,0,.5],43:[.08333,.58333,.02536,0,.77778],44:[.125,.08333,0,0,.27778],45:[0,.44444,.01946,0,.33333],46:[0,.08333,0,0,.27778],47:[.25,.75,.13164,0,.5],48:[0,.65556,.11156,0,.5],49:[0,.65556,.11156,0,.5],50:[0,.65556,.11156,0,.5],51:[0,.65556,.11156,0,.5],52:[0,.65556,.11156,0,.5],53:[0,.65556,.11156,0,.5],54:[0,.65556,.11156,0,.5],55:[0,.65556,.11156,0,.5],56:[0,.65556,.11156,0,.5],57:[0,.65556,.11156,0,.5],58:[0,.44444,.02502,0,.27778],59:[.125,.44444,.02502,0,.27778],61:[-.13,.37,.05087,0,.77778],63:[0,.69444,.11809,0,.47222],64:[0,.69444,.07555,0,.66667],65:[0,.69444,0,0,.66667],66:[0,.69444,.08293,0,.66667],67:[0,.69444,.11983,0,.63889],68:[0,.69444,.07555,0,.72223],69:[0,.69444,.11983,0,.59722],70:[0,.69444,.13372,0,.56945],71:[0,.69444,.11983,0,.66667],72:[0,.69444,.08094,0,.70834],73:[0,.69444,.13372,0,.27778],74:[0,.69444,.08094,0,.47222],75:[0,.69444,.11983,0,.69445],76:[0,.69444,0,0,.54167],77:[0,.69444,.08094,0,.875],78:[0,.69444,.08094,0,.70834],79:[0,.69444,.07555,0,.73611],80:[0,.69444,.08293,0,.63889],81:[.125,.69444,.07555,0,.73611],82:[0,.69444,.08293,0,.64584],83:[0,.69444,.09205,0,.55556],84:[0,.69444,.13372,0,.68056],85:[0,.69444,.08094,0,.6875],86:[0,.69444,.1615,0,.66667],87:[0,.69444,.1615,0,.94445],88:[0,.69444,.13372,0,.66667],89:[0,.69444,.17261,0,.66667],90:[0,.69444,.11983,0,.61111],91:[.25,.75,.15942,0,.28889],93:[.25,.75,.08719,0,.28889],94:[0,.69444,.0799,0,.5],95:[.35,.09444,.08616,0,.5],97:[0,.44444,.00981,0,.48056],98:[0,.69444,.03057,0,.51667],99:[0,.44444,.08336,0,.44445],100:[0,.69444,.09483,0,.51667],101:[0,.44444,.06778,0,.44445],102:[0,.69444,.21705,0,.30556],103:[.19444,.44444,.10836,0,.5],104:[0,.69444,.01778,0,.51667],105:[0,.67937,.09718,0,.23889],106:[.19444,.67937,.09162,0,.26667],107:[0,.69444,.08336,0,.48889],108:[0,.69444,.09483,0,.23889],109:[0,.44444,.01778,0,.79445],110:[0,.44444,.01778,0,.51667],111:[0,.44444,.06613,0,.5],112:[.19444,.44444,.0389,0,.51667],113:[.19444,.44444,.04169,0,.51667],114:[0,.44444,.10836,0,.34167],115:[0,.44444,.0778,0,.38333],116:[0,.57143,.07225,0,.36111],117:[0,.44444,.04169,0,.51667],118:[0,.44444,.10836,0,.46111],119:[0,.44444,.10836,0,.68334],120:[0,.44444,.09169,0,.46111],121:[.19444,.44444,.10836,0,.46111],122:[0,.44444,.08752,0,.43472],126:[.35,.32659,.08826,0,.5],160:[0,0,0,0,.25],168:[0,.67937,.06385,0,.5],176:[0,.69444,0,0,.73752],184:[.17014,0,0,0,.44445],305:[0,.44444,.04169,0,.23889],567:[.19444,.44444,.04169,0,.26667],710:[0,.69444,.0799,0,.5],711:[0,.63194,.08432,0,.5],713:[0,.60889,.08776,0,.5],714:[0,.69444,.09205,0,.5],715:[0,.69444,0,0,.5],728:[0,.69444,.09483,0,.5],729:[0,.67937,.07774,0,.27778],730:[0,.69444,0,0,.73752],732:[0,.67659,.08826,0,.5],733:[0,.69444,.09205,0,.5],915:[0,.69444,.13372,0,.54167],916:[0,.69444,0,0,.83334],920:[0,.69444,.07555,0,.77778],923:[0,.69444,0,0,.61111],926:[0,.69444,.12816,0,.66667],928:[0,.69444,.08094,0,.70834],931:[0,.69444,.11983,0,.72222],933:[0,.69444,.09031,0,.77778],934:[0,.69444,.04603,0,.72222],936:[0,.69444,.09031,0,.77778],937:[0,.69444,.08293,0,.72222],8211:[0,.44444,.08616,0,.5],8212:[0,.44444,.08616,0,1],8216:[0,.69444,.07816,0,.27778],8217:[0,.69444,.07816,0,.27778],8220:[0,.69444,.14205,0,.5],8221:[0,.69444,.00316,0,.5]},"SansSerif-Regular":{32:[0,0,0,0,.25],33:[0,.69444,0,0,.31945],34:[0,.69444,0,0,.5],35:[.19444,.69444,0,0,.83334],36:[.05556,.75,0,0,.5],37:[.05556,.75,0,0,.83334],38:[0,.69444,0,0,.75834],39:[0,.69444,0,0,.27778],40:[.25,.75,0,0,.38889],41:[.25,.75,0,0,.38889],42:[0,.75,0,0,.5],43:[.08333,.58333,0,0,.77778],44:[.125,.08333,0,0,.27778],45:[0,.44444,0,0,.33333],46:[0,.08333,0,0,.27778],47:[.25,.75,0,0,.5],48:[0,.65556,0,0,.5],49:[0,.65556,0,0,.5],50:[0,.65556,0,0,.5],51:[0,.65556,0,0,.5],52:[0,.65556,0,0,.5],53:[0,.65556,0,0,.5],54:[0,.65556,0,0,.5],55:[0,.65556,0,0,.5],56:[0,.65556,0,0,.5],57:[0,.65556,0,0,.5],58:[0,.44444,0,0,.27778],59:[.125,.44444,0,0,.27778],61:[-.13,.37,0,0,.77778],63:[0,.69444,0,0,.47222],64:[0,.69444,0,0,.66667],65:[0,.69444,0,0,.66667],66:[0,.69444,0,0,.66667],67:[0,.69444,0,0,.63889],68:[0,.69444,0,0,.72223],69:[0,.69444,0,0,.59722],70:[0,.69444,0,0,.56945],71:[0,.69444,0,0,.66667],72:[0,.69444,0,0,.70834],73:[0,.69444,0,0,.27778],74:[0,.69444,0,0,.47222],75:[0,.69444,0,0,.69445],76:[0,.69444,0,0,.54167],77:[0,.69444,0,0,.875],78:[0,.69444,0,0,.70834],79:[0,.69444,0,0,.73611],80:[0,.69444,0,0,.63889],81:[.125,.69444,0,0,.73611],82:[0,.69444,0,0,.64584],83:[0,.69444,0,0,.55556],84:[0,.69444,0,0,.68056],85:[0,.69444,0,0,.6875],86:[0,.69444,.01389,0,.66667],87:[0,.69444,.01389,0,.94445],88:[0,.69444,0,0,.66667],89:[0,.69444,.025,0,.66667],90:[0,.69444,0,0,.61111],91:[.25,.75,0,0,.28889],93:[.25,.75,0,0,.28889],94:[0,.69444,0,0,.5],95:[.35,.09444,.02778,0,.5],97:[0,.44444,0,0,.48056],98:[0,.69444,0,0,.51667],99:[0,.44444,0,0,.44445],100:[0,.69444,0,0,.51667],101:[0,.44444,0,0,.44445],102:[0,.69444,.06944,0,.30556],103:[.19444,.44444,.01389,0,.5],104:[0,.69444,0,0,.51667],105:[0,.67937,0,0,.23889],106:[.19444,.67937,0,0,.26667],107:[0,.69444,0,0,.48889],108:[0,.69444,0,0,.23889],109:[0,.44444,0,0,.79445],110:[0,.44444,0,0,.51667],111:[0,.44444,0,0,.5],112:[.19444,.44444,0,0,.51667],113:[.19444,.44444,0,0,.51667],114:[0,.44444,.01389,0,.34167],115:[0,.44444,0,0,.38333],116:[0,.57143,0,0,.36111],117:[0,.44444,0,0,.51667],118:[0,.44444,.01389,0,.46111],119:[0,.44444,.01389,0,.68334],120:[0,.44444,0,0,.46111],121:[.19444,.44444,.01389,0,.46111],122:[0,.44444,0,0,.43472],126:[.35,.32659,0,0,.5],160:[0,0,0,0,.25],168:[0,.67937,0,0,.5],176:[0,.69444,0,0,.66667],184:[.17014,0,0,0,.44445],305:[0,.44444,0,0,.23889],567:[.19444,.44444,0,0,.26667],710:[0,.69444,0,0,.5],711:[0,.63194,0,0,.5],713:[0,.60889,0,0,.5],714:[0,.69444,0,0,.5],715:[0,.69444,0,0,.5],728:[0,.69444,0,0,.5],729:[0,.67937,0,0,.27778],730:[0,.69444,0,0,.66667],732:[0,.67659,0,0,.5],733:[0,.69444,0,0,.5],915:[0,.69444,0,0,.54167],916:[0,.69444,0,0,.83334],920:[0,.69444,0,0,.77778],923:[0,.69444,0,0,.61111],926:[0,.69444,0,0,.66667],928:[0,.69444,0,0,.70834],931:[0,.69444,0,0,.72222],933:[0,.69444,0,0,.77778],934:[0,.69444,0,0,.72222],936:[0,.69444,0,0,.77778],937:[0,.69444,0,0,.72222],8211:[0,.44444,.02778,0,.5],8212:[0,.44444,.02778,0,1],8216:[0,.69444,0,0,.27778],8217:[0,.69444,0,0,.27778],8220:[0,.69444,0,0,.5],8221:[0,.69444,0,0,.5]},"Script-Regular":{32:[0,0,0,0,.25],65:[0,.7,.22925,0,.80253],66:[0,.7,.04087,0,.90757],67:[0,.7,.1689,0,.66619],68:[0,.7,.09371,0,.77443],69:[0,.7,.18583,0,.56162],70:[0,.7,.13634,0,.89544],71:[0,.7,.17322,0,.60961],72:[0,.7,.29694,0,.96919],73:[0,.7,.19189,0,.80907],74:[.27778,.7,.19189,0,1.05159],75:[0,.7,.31259,0,.91364],76:[0,.7,.19189,0,.87373],77:[0,.7,.15981,0,1.08031],78:[0,.7,.3525,0,.9015],79:[0,.7,.08078,0,.73787],80:[0,.7,.08078,0,1.01262],81:[0,.7,.03305,0,.88282],82:[0,.7,.06259,0,.85],83:[0,.7,.19189,0,.86767],84:[0,.7,.29087,0,.74697],85:[0,.7,.25815,0,.79996],86:[0,.7,.27523,0,.62204],87:[0,.7,.27523,0,.80532],88:[0,.7,.26006,0,.94445],89:[0,.7,.2939,0,.70961],90:[0,.7,.24037,0,.8212],160:[0,0,0,0,.25]},"Size1-Regular":{32:[0,0,0,0,.25],40:[.35001,.85,0,0,.45834],41:[.35001,.85,0,0,.45834],47:[.35001,.85,0,0,.57778],91:[.35001,.85,0,0,.41667],92:[.35001,.85,0,0,.57778],93:[.35001,.85,0,0,.41667],123:[.35001,.85,0,0,.58334],125:[.35001,.85,0,0,.58334],160:[0,0,0,0,.25],710:[0,.72222,0,0,.55556],732:[0,.72222,0,0,.55556],770:[0,.72222,0,0,.55556],771:[0,.72222,0,0,.55556],8214:[-99e-5,.601,0,0,.77778],8593:[1e-5,.6,0,0,.66667],8595:[1e-5,.6,0,0,.66667],8657:[1e-5,.6,0,0,.77778],8659:[1e-5,.6,0,0,.77778],8719:[.25001,.75,0,0,.94445],8720:[.25001,.75,0,0,.94445],8721:[.25001,.75,0,0,1.05556],8730:[.35001,.85,0,0,1],8739:[-.00599,.606,0,0,.33333],8741:[-.00599,.606,0,0,.55556],8747:[.30612,.805,.19445,0,.47222],8748:[.306,.805,.19445,0,.47222],8749:[.306,.805,.19445,0,.47222],8750:[.30612,.805,.19445,0,.47222],8896:[.25001,.75,0,0,.83334],8897:[.25001,.75,0,0,.83334],8898:[.25001,.75,0,0,.83334],8899:[.25001,.75,0,0,.83334],8968:[.35001,.85,0,0,.47222],8969:[.35001,.85,0,0,.47222],8970:[.35001,.85,0,0,.47222],8971:[.35001,.85,0,0,.47222],9168:[-99e-5,.601,0,0,.66667],10216:[.35001,.85,0,0,.47222],10217:[.35001,.85,0,0,.47222],10752:[.25001,.75,0,0,1.11111],10753:[.25001,.75,0,0,1.11111],10754:[.25001,.75,0,0,1.11111],10756:[.25001,.75,0,0,.83334],10758:[.25001,.75,0,0,.83334]},"Size2-Regular":{32:[0,0,0,0,.25],40:[.65002,1.15,0,0,.59722],41:[.65002,1.15,0,0,.59722],47:[.65002,1.15,0,0,.81111],91:[.65002,1.15,0,0,.47222],92:[.65002,1.15,0,0,.81111],93:[.65002,1.15,0,0,.47222],123:[.65002,1.15,0,0,.66667],125:[.65002,1.15,0,0,.66667],160:[0,0,0,0,.25],710:[0,.75,0,0,1],732:[0,.75,0,0,1],770:[0,.75,0,0,1],771:[0,.75,0,0,1],8719:[.55001,1.05,0,0,1.27778],8720:[.55001,1.05,0,0,1.27778],8721:[.55001,1.05,0,0,1.44445],8730:[.65002,1.15,0,0,1],8747:[.86225,1.36,.44445,0,.55556],8748:[.862,1.36,.44445,0,.55556],8749:[.862,1.36,.44445,0,.55556],8750:[.86225,1.36,.44445,0,.55556],8896:[.55001,1.05,0,0,1.11111],8897:[.55001,1.05,0,0,1.11111],8898:[.55001,1.05,0,0,1.11111],8899:[.55001,1.05,0,0,1.11111],8968:[.65002,1.15,0,0,.52778],8969:[.65002,1.15,0,0,.52778],8970:[.65002,1.15,0,0,.52778],8971:[.65002,1.15,0,0,.52778],10216:[.65002,1.15,0,0,.61111],10217:[.65002,1.15,0,0,.61111],10752:[.55001,1.05,0,0,1.51112],10753:[.55001,1.05,0,0,1.51112],10754:[.55001,1.05,0,0,1.51112],10756:[.55001,1.05,0,0,1.11111],10758:[.55001,1.05,0,0,1.11111]},"Size3-Regular":{32:[0,0,0,0,.25],40:[.95003,1.45,0,0,.73611],41:[.95003,1.45,0,0,.73611],47:[.95003,1.45,0,0,1.04445],91:[.95003,1.45,0,0,.52778],92:[.95003,1.45,0,0,1.04445],93:[.95003,1.45,0,0,.52778],123:[.95003,1.45,0,0,.75],125:[.95003,1.45,0,0,.75],160:[0,0,0,0,.25],710:[0,.75,0,0,1.44445],732:[0,.75,0,0,1.44445],770:[0,.75,0,0,1.44445],771:[0,.75,0,0,1.44445],8730:[.95003,1.45,0,0,1],8968:[.95003,1.45,0,0,.58334],8969:[.95003,1.45,0,0,.58334],8970:[.95003,1.45,0,0,.58334],8971:[.95003,1.45,0,0,.58334],10216:[.95003,1.45,0,0,.75],10217:[.95003,1.45,0,0,.75]},"Size4-Regular":{32:[0,0,0,0,.25],40:[1.25003,1.75,0,0,.79167],41:[1.25003,1.75,0,0,.79167],47:[1.25003,1.75,0,0,1.27778],91:[1.25003,1.75,0,0,.58334],92:[1.25003,1.75,0,0,1.27778],93:[1.25003,1.75,0,0,.58334],123:[1.25003,1.75,0,0,.80556],125:[1.25003,1.75,0,0,.80556],160:[0,0,0,0,.25],710:[0,.825,0,0,1.8889],732:[0,.825,0,0,1.8889],770:[0,.825,0,0,1.8889],771:[0,.825,0,0,1.8889],8730:[1.25003,1.75,0,0,1],8968:[1.25003,1.75,0,0,.63889],8969:[1.25003,1.75,0,0,.63889],8970:[1.25003,1.75,0,0,.63889],8971:[1.25003,1.75,0,0,.63889],9115:[.64502,1.155,0,0,.875],9116:[1e-5,.6,0,0,.875],9117:[.64502,1.155,0,0,.875],9118:[.64502,1.155,0,0,.875],9119:[1e-5,.6,0,0,.875],9120:[.64502,1.155,0,0,.875],9121:[.64502,1.155,0,0,.66667],9122:[-99e-5,.601,0,0,.66667],9123:[.64502,1.155,0,0,.66667],9124:[.64502,1.155,0,0,.66667],9125:[-99e-5,.601,0,0,.66667],9126:[.64502,1.155,0,0,.66667],9127:[1e-5,.9,0,0,.88889],9128:[.65002,1.15,0,0,.88889],9129:[.90001,0,0,0,.88889],9130:[0,.3,0,0,.88889],9131:[1e-5,.9,0,0,.88889],9132:[.65002,1.15,0,0,.88889],9133:[.90001,0,0,0,.88889],9143:[.88502,.915,0,0,1.05556],10216:[1.25003,1.75,0,0,.80556],10217:[1.25003,1.75,0,0,.80556],57344:[-.00499,.605,0,0,1.05556],57345:[-.00499,.605,0,0,1.05556],57680:[0,.12,0,0,.45],57681:[0,.12,0,0,.45],57682:[0,.12,0,0,.45],57683:[0,.12,0,0,.45]},"Typewriter-Regular":{32:[0,0,0,0,.525],33:[0,.61111,0,0,.525],34:[0,.61111,0,0,.525],35:[0,.61111,0,0,.525],36:[.08333,.69444,0,0,.525],37:[.08333,.69444,0,0,.525],38:[0,.61111,0,0,.525],39:[0,.61111,0,0,.525],40:[.08333,.69444,0,0,.525],41:[.08333,.69444,0,0,.525],42:[0,.52083,0,0,.525],43:[-.08056,.53055,0,0,.525],44:[.13889,.125,0,0,.525],45:[-.08056,.53055,0,0,.525],46:[0,.125,0,0,.525],47:[.08333,.69444,0,0,.525],48:[0,.61111,0,0,.525],49:[0,.61111,0,0,.525],50:[0,.61111,0,0,.525],51:[0,.61111,0,0,.525],52:[0,.61111,0,0,.525],53:[0,.61111,0,0,.525],54:[0,.61111,0,0,.525],55:[0,.61111,0,0,.525],56:[0,.61111,0,0,.525],57:[0,.61111,0,0,.525],58:[0,.43056,0,0,.525],59:[.13889,.43056,0,0,.525],60:[-.05556,.55556,0,0,.525],61:[-.19549,.41562,0,0,.525],62:[-.05556,.55556,0,0,.525],63:[0,.61111,0,0,.525],64:[0,.61111,0,0,.525],65:[0,.61111,0,0,.525],66:[0,.61111,0,0,.525],67:[0,.61111,0,0,.525],68:[0,.61111,0,0,.525],69:[0,.61111,0,0,.525],70:[0,.61111,0,0,.525],71:[0,.61111,0,0,.525],72:[0,.61111,0,0,.525],73:[0,.61111,0,0,.525],74:[0,.61111,0,0,.525],75:[0,.61111,0,0,.525],76:[0,.61111,0,0,.525],77:[0,.61111,0,0,.525],78:[0,.61111,0,0,.525],79:[0,.61111,0,0,.525],80:[0,.61111,0,0,.525],81:[.13889,.61111,0,0,.525],82:[0,.61111,0,0,.525],83:[0,.61111,0,0,.525],84:[0,.61111,0,0,.525],85:[0,.61111,0,0,.525],86:[0,.61111,0,0,.525],87:[0,.61111,0,0,.525],88:[0,.61111,0,0,.525],89:[0,.61111,0,0,.525],90:[0,.61111,0,0,.525],91:[.08333,.69444,0,0,.525],92:[.08333,.69444,0,0,.525],93:[.08333,.69444,0,0,.525],94:[0,.61111,0,0,.525],95:[.09514,0,0,0,.525],96:[0,.61111,0,0,.525],97:[0,.43056,0,0,.525],98:[0,.61111,0,0,.525],99:[0,.43056,0,0,.525],100:[0,.61111,0,0,.525],101:[0,.43056,0,0,.525],102:[0,.61111,0,0,.525],103:[.22222,.43056,0,0,.525],104:[0,.61111,0,0,.525],105:[0,.61111,0,0,.525],106:[.22222,.61111,0,0,.525],107:[0,.61111,0,0,.525],108:[0,.61111,0,0,.525],109:[0,.43056,0,0,.525],110:[0,.43056,0,0,.525],111:[0,.43056,0,0,.525],112:[.22222,.43056,0,0,.525],113:[.22222,.43056,0,0,.525],114:[0,.43056,0,0,.525],115:[0,.43056,0,0,.525],116:[0,.55358,0,0,.525],117:[0,.43056,0,0,.525],118:[0,.43056,0,0,.525],119:[0,.43056,0,0,.525],120:[0,.43056,0,0,.525],121:[.22222,.43056,0,0,.525],122:[0,.43056,0,0,.525],123:[.08333,.69444,0,0,.525],124:[.08333,.69444,0,0,.525],125:[.08333,.69444,0,0,.525],126:[0,.61111,0,0,.525],127:[0,.61111,0,0,.525],160:[0,0,0,0,.525],176:[0,.61111,0,0,.525],184:[.19445,0,0,0,.525],305:[0,.43056,0,0,.525],567:[.22222,.43056,0,0,.525],711:[0,.56597,0,0,.525],713:[0,.56555,0,0,.525],714:[0,.61111,0,0,.525],715:[0,.61111,0,0,.525],728:[0,.61111,0,0,.525],730:[0,.61111,0,0,.525],770:[0,.61111,0,0,.525],771:[0,.61111,0,0,.525],776:[0,.61111,0,0,.525],915:[0,.61111,0,0,.525],916:[0,.61111,0,0,.525],920:[0,.61111,0,0,.525],923:[0,.61111,0,0,.525],926:[0,.61111,0,0,.525],928:[0,.61111,0,0,.525],931:[0,.61111,0,0,.525],933:[0,.61111,0,0,.525],934:[0,.61111,0,0,.525],936:[0,.61111,0,0,.525],937:[0,.61111,0,0,.525],8216:[0,.61111,0,0,.525],8217:[0,.61111,0,0,.525],8242:[0,.61111,0,0,.525],9251:[.11111,.21944,0,0,.525]}},ju={slant:[.25,.25,.25],space:[0,0,0],stretch:[0,0,0],shrink:[0,0,0],xHeight:[.431,.431,.431],quad:[1,1.171,1.472],extraSpace:[0,0,0],num1:[.677,.732,.925],num2:[.394,.384,.387],num3:[.444,.471,.504],denom1:[.686,.752,1.025],denom2:[.345,.344,.532],sup1:[.413,.503,.504],sup2:[.363,.431,.404],sup3:[.289,.286,.294],sub1:[.15,.143,.2],sub2:[.247,.286,.4],supDrop:[.386,.353,.494],subDrop:[.05,.071,.1],delim1:[2.39,1.7,1.98],delim2:[1.01,1.157,1.42],axisHeight:[.25,.25,.25],defaultRuleThickness:[.04,.049,.049],bigOpSpacing1:[.111,.111,.111],bigOpSpacing2:[.166,.166,.166],bigOpSpacing3:[.2,.2,.2],bigOpSpacing4:[.6,.611,.611],bigOpSpacing5:[.1,.143,.143],sqrtRuleThickness:[.04,.04,.04],ptPerEm:[10,10,10],doubleRuleSep:[.2,.2,.2],arrayRuleWidth:[.04,.04,.04],fboxsep:[.3,.3,.3],fboxrule:[.04,.04,.04]},N3={Å:"A",Ð:"D",Þ:"o",å:"a",ð:"d",þ:"o",А:"A",Б:"B",В:"B",Г:"F",Д:"A",Е:"E",Ж:"K",З:"3",И:"N",Й:"N",К:"K",Л:"N",М:"M",Н:"H",О:"O",П:"N",Р:"P",С:"C",Т:"T",У:"y",Ф:"O",Х:"X",Ц:"U",Ч:"h",Ш:"W",Щ:"W",Ъ:"B",Ы:"X",Ь:"B",Э:"3",Ю:"X",Я:"R",а:"a",б:"b",в:"a",г:"r",д:"y",е:"e",ж:"m",з:"e",и:"n",й:"n",к:"n",л:"n",м:"m",н:"n",о:"o",п:"n",р:"p",с:"c",т:"o",у:"y",ф:"b",х:"x",ц:"n",ч:"n",ш:"w",щ:"w",ъ:"a",ы:"m",ь:"a",э:"e",ю:"m",я:"r"};function DG(e,t){Jr[e]=t}function B2(e,t,n){if(!Jr[t])throw new Error("Font metrics not found for font: "+t+".");var r=e.charCodeAt(0),i=Jr[t][r];if(!i&&e[0]in N3&&(r=N3[e[0]].charCodeAt(0),i=Jr[t][r]),!i&&n==="text"&&Sw(r)&&(i=Jr[t][77]),i)return{depth:i[0],height:i[1],italic:i[2],skew:i[3],width:i[4]}}var jh={};function MG(e){var t;if(e>=5?t=0:e>=3?t=1:t=2,!jh[t]){var n=jh[t]={cssEmPerMu:ju.quad[t]/18};for(var r in ju)ju.hasOwnProperty(r)&&(n[r]=ju[r][t])}return jh[t]}var RG=[[1,1,1],[2,1,1],[3,1,1],[4,2,1],[5,2,1],[6,3,1],[7,4,2],[8,6,3],[9,7,6],[10,8,7],[11,10,9]],C3=[.5,.6,.7,.8,.9,1,1.2,1.44,1.728,2.074,2.488],A3=function(t,n){return n.size<2?t:RG[t-1][n.size-1]};class yi{constructor(t){this.style=void 0,this.color=void 0,this.size=void 0,this.textSize=void 0,this.phantom=void 0,this.font=void 0,this.fontFamily=void 0,this.fontWeight=void 0,this.fontShape=void 0,this.sizeMultiplier=void 0,this.maxSize=void 0,this.minRuleThickness=void 0,this._fontMetrics=void 0,this.style=t.style,this.color=t.color,this.size=t.size||yi.BASESIZE,this.textSize=t.textSize||this.size,this.phantom=!!t.phantom,this.font=t.font||"",this.fontFamily=t.fontFamily||"",this.fontWeight=t.fontWeight||"",this.fontShape=t.fontShape||"",this.sizeMultiplier=C3[this.size-1],this.maxSize=t.maxSize,this.minRuleThickness=t.minRuleThickness,this._fontMetrics=void 0}extend(t){var n={style:this.style,size:this.size,textSize:this.textSize,color:this.color,phantom:this.phantom,font:this.font,fontFamily:this.fontFamily,fontWeight:this.fontWeight,fontShape:this.fontShape,maxSize:this.maxSize,minRuleThickness:this.minRuleThickness};for(var r in t)t.hasOwnProperty(r)&&(n[r]=t[r]);return new yi(n)}havingStyle(t){return this.style===t?this:this.extend({style:t,size:A3(this.textSize,t)})}havingCrampedStyle(){return this.havingStyle(this.style.cramp())}havingSize(t){return this.size===t&&this.textSize===t?this:this.extend({style:this.style.text(),size:t,textSize:t,sizeMultiplier:C3[t-1]})}havingBaseStyle(t){t=t||this.style.text();var n=A3(yi.BASESIZE,t);return this.size===n&&this.textSize===yi.BASESIZE&&this.style===t?this:this.extend({style:t,size:n})}havingBaseSizing(){var t;switch(this.style.id){case 4:case 5:t=3;break;case 6:case 7:t=1;break;default:t=6}return this.extend({style:this.style.text(),size:t})}withColor(t){return this.extend({color:t})}withPhantom(){return this.extend({phantom:!0})}withFont(t){return this.extend({font:t})}withTextFontFamily(t){return this.extend({fontFamily:t,font:""})}withTextFontWeight(t){return this.extend({fontWeight:t,font:""})}withTextFontShape(t){return this.extend({fontShape:t,font:""})}sizingClasses(t){return t.size!==this.size?["sizing","reset-size"+t.size,"size"+this.size]:[]}baseSizingClasses(){return this.size!==yi.BASESIZE?["sizing","reset-size"+this.size,"size"+yi.BASESIZE]:[]}fontMetrics(){return this._fontMetrics||(this._fontMetrics=MG(this.size)),this._fontMetrics}getColor(){return this.phantom?"transparent":this.color}}yi.BASESIZE=6;var Jf={pt:1,mm:7227/2540,cm:7227/254,in:72.27,bp:803/800,pc:12,dd:1238/1157,cc:14856/1157,nd:685/642,nc:1370/107,sp:1/65536,px:803/800},OG={ex:!0,em:!0,mu:!0},Nw=function(t){return typeof t!="string"&&(t=t.unit),t in Jf||t in OG||t==="ex"},gt=function(t,n){var r;if(t.unit in Jf)r=Jf[t.unit]/n.fontMetrics().ptPerEm/n.sizeMultiplier;else if(t.unit==="mu")r=n.fontMetrics().cssEmPerMu;else{var i;if(n.style.isTight()?i=n.havingStyle(n.style.text()):i=n,t.unit==="ex")r=i.fontMetrics().xHeight;else if(t.unit==="em")r=i.fontMetrics().quad;else throw new oe("Invalid unit: '"+t.unit+"'");i!==n&&(r*=i.sizeMultiplier/n.sizeMultiplier)}return Math.min(t.number*r,n.maxSize)},me=function(t){return+t.toFixed(4)+"em"},La=function(t){return t.filter(n=>n).join(" ")},Cw=function(t,n,r){if(this.classes=t||[],this.attributes={},this.height=0,this.depth=0,this.maxFontSize=0,this.style=r||{},n){n.style.isTight()&&this.classes.push("mtight");var i=n.getColor();i&&(this.style.color=i)}},Aw=function(t){var n=document.createElement(t);n.className=La(this.classes);for(var r in this.style)this.style.hasOwnProperty(r)&&(n.style[r]=this.style[r]);for(var i in this.attributes)this.attributes.hasOwnProperty(i)&&n.setAttribute(i,this.attributes[i]);for(var a=0;a<this.children.length;a++)n.appendChild(this.children[a].toNode());return n},zG=/[\s"'>/=\x00-\x1f]/,Lw=function(t){var n="<"+t;this.classes.length&&(n+=' class="'+ye.escape(La(this.classes))+'"');var r="";for(var i in this.style)this.style.hasOwnProperty(i)&&(r+=ye.hyphenate(i)+":"+this.style[i]+";");r&&(n+=' style="'+ye.escape(r)+'"');for(var a in this.attributes)if(this.attributes.hasOwnProperty(a)){if(zG.test(a))throw new oe("Invalid attribute name '"+a+"'");n+=" "+a+'="'+ye.escape(this.attributes[a])+'"'}n+=">";for(var s=0;s<this.children.length;s++)n+=this.children[s].toMarkup();return n+="</"+t+">",n};class eu{constructor(t,n,r,i){this.children=void 0,this.attributes=void 0,this.classes=void 0,this.height=void 0,this.depth=void 0,this.width=void 0,this.maxFontSize=void 0,this.style=void 0,Cw.call(this,t,r,i),this.children=n||[]}setAttribute(t,n){this.attributes[t]=n}hasClass(t){return ye.contains(this.classes,t)}toNode(){return Aw.call(this,"span")}toMarkup(){return Lw.call(this,"span")}}class F2{constructor(t,n,r,i){this.children=void 0,this.attributes=void 0,this.classes=void 0,this.height=void 0,this.depth=void 0,this.maxFontSize=void 0,this.style=void 0,Cw.call(this,n,i),this.children=r||[],this.setAttribute("href",t)}setAttribute(t,n){this.attributes[t]=n}hasClass(t){return ye.contains(this.classes,t)}toNode(){return Aw.call(this,"a")}toMarkup(){return Lw.call(this,"a")}}class BG{constructor(t,n,r){this.src=void 0,this.alt=void 0,this.classes=void 0,this.height=void 0,this.depth=void 0,this.maxFontSize=void 0,this.style=void 0,this.alt=n,this.src=t,this.classes=["mord"],this.style=r}hasClass(t){return ye.contains(this.classes,t)}toNode(){var t=document.createElement("img");t.src=this.src,t.alt=this.alt,t.className="mord";for(var n in this.style)this.style.hasOwnProperty(n)&&(t.style[n]=this.style[n]);return t}toMarkup(){var t='<img src="'+ye.escape(this.src)+'"'+(' alt="'+ye.escape(this.alt)+'"'),n="";for(var r in this.style)this.style.hasOwnProperty(r)&&(n+=ye.hyphenate(r)+":"+this.style[r]+";");return n&&(t+=' style="'+ye.escape(n)+'"'),t+="'/>",t}}var FG={î:"ı̂",ï:"ı̈",í:"ı́",ì:"ı̀"};class pr{constructor(t,n,r,i,a,s,o,l){this.text=void 0,this.height=void 0,this.depth=void 0,this.italic=void 0,this.skew=void 0,this.width=void 0,this.maxFontSize=void 0,this.classes=void 0,this.style=void 0,this.text=t,this.height=n||0,this.depth=r||0,this.italic=i||0,this.skew=a||0,this.width=s||0,this.classes=o||[],this.style=l||{},this.maxFontSize=0;var c=TG(this.text.charCodeAt(0));c&&this.classes.push(c+"_fallback"),/[îïíì]/.test(this.text)&&(this.text=FG[this.text])}hasClass(t){return ye.contains(this.classes,t)}toNode(){var t=document.createTextNode(this.text),n=null;this.italic>0&&(n=document.createElement("span"),n.style.marginRight=me(this.italic)),this.classes.length>0&&(n=n||document.createElement("span"),n.className=La(this.classes));for(var r in this.style)this.style.hasOwnProperty(r)&&(n=n||document.createElement("span"),n.style[r]=this.style[r]);return n?(n.appendChild(t),n):t}toMarkup(){var t=!1,n="<span";this.classes.length&&(t=!0,n+=' class="',n+=ye.escape(La(this.classes)),n+='"');var r="";this.italic>0&&(r+="margin-right:"+this.italic+"em;");for(var i in this.style)this.style.hasOwnProperty(i)&&(r+=ye.hyphenate(i)+":"+this.style[i]+";");r&&(t=!0,n+=' style="'+ye.escape(r)+'"');var a=ye.escape(this.text);return t?(n+=">",n+=a,n+="</span>",n):a}}class Ri{constructor(t,n){this.children=void 0,this.attributes=void 0,this.children=t||[],this.attributes=n||{}}toNode(){var t="http://www.w3.org/2000/svg",n=document.createElementNS(t,"svg");for(var r in this.attributes)Object.prototype.hasOwnProperty.call(this.attributes,r)&&n.setAttribute(r,this.attributes[r]);for(var i=0;i<this.children.length;i++)n.appendChild(this.children[i].toNode());return n}toMarkup(){var t='<svg xmlns="http://www.w3.org/2000/svg"';for(var n in this.attributes)Object.prototype.hasOwnProperty.call(this.attributes,n)&&(t+=" "+n+'="'+ye.escape(this.attributes[n])+'"');t+=">";for(var r=0;r<this.children.length;r++)t+=this.children[r].toMarkup();return t+="</svg>",t}}class Pa{constructor(t,n){this.pathName=void 0,this.alternate=void 0,this.pathName=t,this.alternate=n}toNode(){var t="http://www.w3.org/2000/svg",n=document.createElementNS(t,"path");return this.alternate?n.setAttribute("d",this.alternate):n.setAttribute("d",S3[this.pathName]),n}toMarkup(){return this.alternate?'<path d="'+ye.escape(this.alternate)+'"/>':'<path d="'+ye.escape(S3[this.pathName])+'"/>'}}class eg{constructor(t){this.attributes=void 0,this.attributes=t||{}}toNode(){var t="http://www.w3.org/2000/svg",n=document.createElementNS(t,"line");for(var r in this.attributes)Object.prototype.hasOwnProperty.call(this.attributes,r)&&n.setAttribute(r,this.attributes[r]);return n}toMarkup(){var t="<line";for(var n in this.attributes)Object.prototype.hasOwnProperty.call(this.attributes,n)&&(t+=" "+n+'="'+ye.escape(this.attributes[n])+'"');return t+="/>",t}}function L3(e){if(e instanceof pr)return e;throw new Error("Expected symbolNode but got "+String(e)+".")}function jG(e){if(e instanceof eu)return e;throw new Error("Expected span<HtmlDomNode> but got "+String(e)+".")}var VG={bin:1,close:1,inner:1,open:1,punct:1,rel:1},HG={"accent-token":1,mathord:1,"op-token":1,spacing:1,textord:1},at={math:{},text:{}};function b(e,t,n,r,i,a){at[e][i]={font:t,group:n,replace:r},a&&r&&(at[e][r]=at[e][i])}var y="math",Z="text",E="main",M="ams",ht="accent-token",ge="bin",vn="close",Qo="inner",_e="mathord",Ut="op-token",Qn="open",gm="punct",R="rel",Fi="spacing",z="textord";b(y,E,R,"≡","\\equiv",!0);b(y,E,R,"≺","\\prec",!0);b(y,E,R,"≻","\\succ",!0);b(y,E,R,"∼","\\sim",!0);b(y,E,R,"⊥","\\perp");b(y,E,R,"⪯","\\preceq",!0);b(y,E,R,"⪰","\\succeq",!0);b(y,E,R,"≃","\\simeq",!0);b(y,E,R,"∣","\\mid",!0);b(y,E,R,"≪","\\ll",!0);b(y,E,R,"≫","\\gg",!0);b(y,E,R,"≍","\\asymp",!0);b(y,E,R,"∥","\\parallel");b(y,E,R,"⋈","\\bowtie",!0);b(y,E,R,"⌣","\\smile",!0);b(y,E,R,"⊑","\\sqsubseteq",!0);b(y,E,R,"⊒","\\sqsupseteq",!0);b(y,E,R,"≐","\\doteq",!0);b(y,E,R,"⌢","\\frown",!0);b(y,E,R,"∋","\\ni",!0);b(y,E,R,"∝","\\propto",!0);b(y,E,R,"⊢","\\vdash",!0);b(y,E,R,"⊣","\\dashv",!0);b(y,E,R,"∋","\\owns");b(y,E,gm,".","\\ldotp");b(y,E,gm,"⋅","\\cdotp");b(y,E,z,"#","\\#");b(Z,E,z,"#","\\#");b(y,E,z,"&","\\&");b(Z,E,z,"&","\\&");b(y,E,z,"ℵ","\\aleph",!0);b(y,E,z,"∀","\\forall",!0);b(y,E,z,"ℏ","\\hbar",!0);b(y,E,z,"∃","\\exists",!0);b(y,E,z,"∇","\\nabla",!0);b(y,E,z,"♭","\\flat",!0);b(y,E,z,"ℓ","\\ell",!0);b(y,E,z,"♮","\\natural",!0);b(y,E,z,"♣","\\clubsuit",!0);b(y,E,z,"℘","\\wp",!0);b(y,E,z,"♯","\\sharp",!0);b(y,E,z,"♢","\\diamondsuit",!0);b(y,E,z,"ℜ","\\Re",!0);b(y,E,z,"♡","\\heartsuit",!0);b(y,E,z,"ℑ","\\Im",!0);b(y,E,z,"♠","\\spadesuit",!0);b(y,E,z,"§","\\S",!0);b(Z,E,z,"§","\\S");b(y,E,z,"¶","\\P",!0);b(Z,E,z,"¶","\\P");b(y,E,z,"†","\\dag");b(Z,E,z,"†","\\dag");b(Z,E,z,"†","\\textdagger");b(y,E,z,"‡","\\ddag");b(Z,E,z,"‡","\\ddag");b(Z,E,z,"‡","\\textdaggerdbl");b(y,E,vn,"⎱","\\rmoustache",!0);b(y,E,Qn,"⎰","\\lmoustache",!0);b(y,E,vn,"⟯","\\rgroup",!0);b(y,E,Qn,"⟮","\\lgroup",!0);b(y,E,ge,"∓","\\mp",!0);b(y,E,ge,"⊖","\\ominus",!0);b(y,E,ge,"⊎","\\uplus",!0);b(y,E,ge,"⊓","\\sqcap",!0);b(y,E,ge,"∗","\\ast");b(y,E,ge,"⊔","\\sqcup",!0);b(y,E,ge,"◯","\\bigcirc",!0);b(y,E,ge,"∙","\\bullet",!0);b(y,E,ge,"‡","\\ddagger");b(y,E,ge,"≀","\\wr",!0);b(y,E,ge,"⨿","\\amalg");b(y,E,ge,"&","\\And");b(y,E,R,"⟵","\\longleftarrow",!0);b(y,E,R,"⇐","\\Leftarrow",!0);b(y,E,R,"⟸","\\Longleftarrow",!0);b(y,E,R,"⟶","\\longrightarrow",!0);b(y,E,R,"⇒","\\Rightarrow",!0);b(y,E,R,"⟹","\\Longrightarrow",!0);b(y,E,R,"↔","\\leftrightarrow",!0);b(y,E,R,"⟷","\\longleftrightarrow",!0);b(y,E,R,"⇔","\\Leftrightarrow",!0);b(y,E,R,"⟺","\\Longleftrightarrow",!0);b(y,E,R,"↦","\\mapsto",!0);b(y,E,R,"⟼","\\longmapsto",!0);b(y,E,R,"↗","\\nearrow",!0);b(y,E,R,"↩","\\hookleftarrow",!0);b(y,E,R,"↪","\\hookrightarrow",!0);b(y,E,R,"↘","\\searrow",!0);b(y,E,R,"↼","\\leftharpoonup",!0);b(y,E,R,"⇀","\\rightharpoonup",!0);b(y,E,R,"↙","\\swarrow",!0);b(y,E,R,"↽","\\leftharpoondown",!0);b(y,E,R,"⇁","\\rightharpoondown",!0);b(y,E,R,"↖","\\nwarrow",!0);b(y,E,R,"⇌","\\rightleftharpoons",!0);b(y,M,R,"≮","\\nless",!0);b(y,M,R,"","\\@nleqslant");b(y,M,R,"","\\@nleqq");b(y,M,R,"⪇","\\lneq",!0);b(y,M,R,"≨","\\lneqq",!0);b(y,M,R,"","\\@lvertneqq");b(y,M,R,"⋦","\\lnsim",!0);b(y,M,R,"⪉","\\lnapprox",!0);b(y,M,R,"⊀","\\nprec",!0);b(y,M,R,"⋠","\\npreceq",!0);b(y,M,R,"⋨","\\precnsim",!0);b(y,M,R,"⪹","\\precnapprox",!0);b(y,M,R,"≁","\\nsim",!0);b(y,M,R,"","\\@nshortmid");b(y,M,R,"∤","\\nmid",!0);b(y,M,R,"⊬","\\nvdash",!0);b(y,M,R,"⊭","\\nvDash",!0);b(y,M,R,"⋪","\\ntriangleleft");b(y,M,R,"⋬","\\ntrianglelefteq",!0);b(y,M,R,"⊊","\\subsetneq",!0);b(y,M,R,"","\\@varsubsetneq");b(y,M,R,"⫋","\\subsetneqq",!0);b(y,M,R,"","\\@varsubsetneqq");b(y,M,R,"≯","\\ngtr",!0);b(y,M,R,"","\\@ngeqslant");b(y,M,R,"","\\@ngeqq");b(y,M,R,"⪈","\\gneq",!0);b(y,M,R,"≩","\\gneqq",!0);b(y,M,R,"","\\@gvertneqq");b(y,M,R,"⋧","\\gnsim",!0);b(y,M,R,"⪊","\\gnapprox",!0);b(y,M,R,"⊁","\\nsucc",!0);b(y,M,R,"⋡","\\nsucceq",!0);b(y,M,R,"⋩","\\succnsim",!0);b(y,M,R,"⪺","\\succnapprox",!0);b(y,M,R,"≆","\\ncong",!0);b(y,M,R,"","\\@nshortparallel");b(y,M,R,"∦","\\nparallel",!0);b(y,M,R,"⊯","\\nVDash",!0);b(y,M,R,"⋫","\\ntriangleright");b(y,M,R,"⋭","\\ntrianglerighteq",!0);b(y,M,R,"","\\@nsupseteqq");b(y,M,R,"⊋","\\supsetneq",!0);b(y,M,R,"","\\@varsupsetneq");b(y,M,R,"⫌","\\supsetneqq",!0);b(y,M,R,"","\\@varsupsetneqq");b(y,M,R,"⊮","\\nVdash",!0);b(y,M,R,"⪵","\\precneqq",!0);b(y,M,R,"⪶","\\succneqq",!0);b(y,M,R,"","\\@nsubseteqq");b(y,M,ge,"⊴","\\unlhd");b(y,M,ge,"⊵","\\unrhd");b(y,M,R,"↚","\\nleftarrow",!0);b(y,M,R,"↛","\\nrightarrow",!0);b(y,M,R,"⇍","\\nLeftarrow",!0);b(y,M,R,"⇏","\\nRightarrow",!0);b(y,M,R,"↮","\\nleftrightarrow",!0);b(y,M,R,"⇎","\\nLeftrightarrow",!0);b(y,M,R,"△","\\vartriangle");b(y,M,z,"ℏ","\\hslash");b(y,M,z,"▽","\\triangledown");b(y,M,z,"◊","\\lozenge");b(y,M,z,"Ⓢ","\\circledS");b(y,M,z,"®","\\circledR");b(Z,M,z,"®","\\circledR");b(y,M,z,"∡","\\measuredangle",!0);b(y,M,z,"∄","\\nexists");b(y,M,z,"℧","\\mho");b(y,M,z,"Ⅎ","\\Finv",!0);b(y,M,z,"⅁","\\Game",!0);b(y,M,z,"‵","\\backprime");b(y,M,z,"▲","\\blacktriangle");b(y,M,z,"▼","\\blacktriangledown");b(y,M,z,"■","\\blacksquare");b(y,M,z,"⧫","\\blacklozenge");b(y,M,z,"★","\\bigstar");b(y,M,z,"∢","\\sphericalangle",!0);b(y,M,z,"∁","\\complement",!0);b(y,M,z,"ð","\\eth",!0);b(Z,E,z,"ð","ð");b(y,M,z,"╱","\\diagup");b(y,M,z,"╲","\\diagdown");b(y,M,z,"□","\\square");b(y,M,z,"□","\\Box");b(y,M,z,"◊","\\Diamond");b(y,M,z,"¥","\\yen",!0);b(Z,M,z,"¥","\\yen",!0);b(y,M,z,"✓","\\checkmark",!0);b(Z,M,z,"✓","\\checkmark");b(y,M,z,"ℶ","\\beth",!0);b(y,M,z,"ℸ","\\daleth",!0);b(y,M,z,"ℷ","\\gimel",!0);b(y,M,z,"ϝ","\\digamma",!0);b(y,M,z,"ϰ","\\varkappa");b(y,M,Qn,"┌","\\@ulcorner",!0);b(y,M,vn,"┐","\\@urcorner",!0);b(y,M,Qn,"└","\\@llcorner",!0);b(y,M,vn,"┘","\\@lrcorner",!0);b(y,M,R,"≦","\\leqq",!0);b(y,M,R,"⩽","\\leqslant",!0);b(y,M,R,"⪕","\\eqslantless",!0);b(y,M,R,"≲","\\lesssim",!0);b(y,M,R,"⪅","\\lessapprox",!0);b(y,M,R,"≊","\\approxeq",!0);b(y,M,ge,"⋖","\\lessdot");b(y,M,R,"⋘","\\lll",!0);b(y,M,R,"≶","\\lessgtr",!0);b(y,M,R,"⋚","\\lesseqgtr",!0);b(y,M,R,"⪋","\\lesseqqgtr",!0);b(y,M,R,"≑","\\doteqdot");b(y,M,R,"≓","\\risingdotseq",!0);b(y,M,R,"≒","\\fallingdotseq",!0);b(y,M,R,"∽","\\backsim",!0);b(y,M,R,"⋍","\\backsimeq",!0);b(y,M,R,"⫅","\\subseteqq",!0);b(y,M,R,"⋐","\\Subset",!0);b(y,M,R,"⊏","\\sqsubset",!0);b(y,M,R,"≼","\\preccurlyeq",!0);b(y,M,R,"⋞","\\curlyeqprec",!0);b(y,M,R,"≾","\\precsim",!0);b(y,M,R,"⪷","\\precapprox",!0);b(y,M,R,"⊲","\\vartriangleleft");b(y,M,R,"⊴","\\trianglelefteq");b(y,M,R,"⊨","\\vDash",!0);b(y,M,R,"⊪","\\Vvdash",!0);b(y,M,R,"⌣","\\smallsmile");b(y,M,R,"⌢","\\smallfrown");b(y,M,R,"≏","\\bumpeq",!0);b(y,M,R,"≎","\\Bumpeq",!0);b(y,M,R,"≧","\\geqq",!0);b(y,M,R,"⩾","\\geqslant",!0);b(y,M,R,"⪖","\\eqslantgtr",!0);b(y,M,R,"≳","\\gtrsim",!0);b(y,M,R,"⪆","\\gtrapprox",!0);b(y,M,ge,"⋗","\\gtrdot");b(y,M,R,"⋙","\\ggg",!0);b(y,M,R,"≷","\\gtrless",!0);b(y,M,R,"⋛","\\gtreqless",!0);b(y,M,R,"⪌","\\gtreqqless",!0);b(y,M,R,"≖","\\eqcirc",!0);b(y,M,R,"≗","\\circeq",!0);b(y,M,R,"≜","\\triangleq",!0);b(y,M,R,"∼","\\thicksim");b(y,M,R,"≈","\\thickapprox");b(y,M,R,"⫆","\\supseteqq",!0);b(y,M,R,"⋑","\\Supset",!0);b(y,M,R,"⊐","\\sqsupset",!0);b(y,M,R,"≽","\\succcurlyeq",!0);b(y,M,R,"⋟","\\curlyeqsucc",!0);b(y,M,R,"≿","\\succsim",!0);b(y,M,R,"⪸","\\succapprox",!0);b(y,M,R,"⊳","\\vartriangleright");b(y,M,R,"⊵","\\trianglerighteq");b(y,M,R,"⊩","\\Vdash",!0);b(y,M,R,"∣","\\shortmid");b(y,M,R,"∥","\\shortparallel");b(y,M,R,"≬","\\between",!0);b(y,M,R,"⋔","\\pitchfork",!0);b(y,M,R,"∝","\\varpropto");b(y,M,R,"◀","\\blacktriangleleft");b(y,M,R,"∴","\\therefore",!0);b(y,M,R,"∍","\\backepsilon");b(y,M,R,"▶","\\blacktriangleright");b(y,M,R,"∵","\\because",!0);b(y,M,R,"⋘","\\llless");b(y,M,R,"⋙","\\gggtr");b(y,M,ge,"⊲","\\lhd");b(y,M,ge,"⊳","\\rhd");b(y,M,R,"≂","\\eqsim",!0);b(y,E,R,"⋈","\\Join");b(y,M,R,"≑","\\Doteq",!0);b(y,M,ge,"∔","\\dotplus",!0);b(y,M,ge,"∖","\\smallsetminus");b(y,M,ge,"⋒","\\Cap",!0);b(y,M,ge,"⋓","\\Cup",!0);b(y,M,ge,"⩞","\\doublebarwedge",!0);b(y,M,ge,"⊟","\\boxminus",!0);b(y,M,ge,"⊞","\\boxplus",!0);b(y,M,ge,"⋇","\\divideontimes",!0);b(y,M,ge,"⋉","\\ltimes",!0);b(y,M,ge,"⋊","\\rtimes",!0);b(y,M,ge,"⋋","\\leftthreetimes",!0);b(y,M,ge,"⋌","\\rightthreetimes",!0);b(y,M,ge,"⋏","\\curlywedge",!0);b(y,M,ge,"⋎","\\curlyvee",!0);b(y,M,ge,"⊝","\\circleddash",!0);b(y,M,ge,"⊛","\\circledast",!0);b(y,M,ge,"⋅","\\centerdot");b(y,M,ge,"⊺","\\intercal",!0);b(y,M,ge,"⋒","\\doublecap");b(y,M,ge,"⋓","\\doublecup");b(y,M,ge,"⊠","\\boxtimes",!0);b(y,M,R,"⇢","\\dashrightarrow",!0);b(y,M,R,"⇠","\\dashleftarrow",!0);b(y,M,R,"⇇","\\leftleftarrows",!0);b(y,M,R,"⇆","\\leftrightarrows",!0);b(y,M,R,"⇚","\\Lleftarrow",!0);b(y,M,R,"↞","\\twoheadleftarrow",!0);b(y,M,R,"↢","\\leftarrowtail",!0);b(y,M,R,"↫","\\looparrowleft",!0);b(y,M,R,"⇋","\\leftrightharpoons",!0);b(y,M,R,"↶","\\curvearrowleft",!0);b(y,M,R,"↺","\\circlearrowleft",!0);b(y,M,R,"↰","\\Lsh",!0);b(y,M,R,"⇈","\\upuparrows",!0);b(y,M,R,"↿","\\upharpoonleft",!0);b(y,M,R,"⇃","\\downharpoonleft",!0);b(y,E,R,"⊶","\\origof",!0);b(y,E,R,"⊷","\\imageof",!0);b(y,M,R,"⊸","\\multimap",!0);b(y,M,R,"↭","\\leftrightsquigarrow",!0);b(y,M,R,"⇉","\\rightrightarrows",!0);b(y,M,R,"⇄","\\rightleftarrows",!0);b(y,M,R,"↠","\\twoheadrightarrow",!0);b(y,M,R,"↣","\\rightarrowtail",!0);b(y,M,R,"↬","\\looparrowright",!0);b(y,M,R,"↷","\\curvearrowright",!0);b(y,M,R,"↻","\\circlearrowright",!0);b(y,M,R,"↱","\\Rsh",!0);b(y,M,R,"⇊","\\downdownarrows",!0);b(y,M,R,"↾","\\upharpoonright",!0);b(y,M,R,"⇂","\\downharpoonright",!0);b(y,M,R,"⇝","\\rightsquigarrow",!0);b(y,M,R,"⇝","\\leadsto");b(y,M,R,"⇛","\\Rrightarrow",!0);b(y,M,R,"↾","\\restriction");b(y,E,z,"‘","`");b(y,E,z,"$","\\$");b(Z,E,z,"$","\\$");b(Z,E,z,"$","\\textdollar");b(y,E,z,"%","\\%");b(Z,E,z,"%","\\%");b(y,E,z,"_","\\_");b(Z,E,z,"_","\\_");b(Z,E,z,"_","\\textunderscore");b(y,E,z,"∠","\\angle",!0);b(y,E,z,"∞","\\infty",!0);b(y,E,z,"′","\\prime");b(y,E,z,"△","\\triangle");b(y,E,z,"Γ","\\Gamma",!0);b(y,E,z,"Δ","\\Delta",!0);b(y,E,z,"Θ","\\Theta",!0);b(y,E,z,"Λ","\\Lambda",!0);b(y,E,z,"Ξ","\\Xi",!0);b(y,E,z,"Π","\\Pi",!0);b(y,E,z,"Σ","\\Sigma",!0);b(y,E,z,"Υ","\\Upsilon",!0);b(y,E,z,"Φ","\\Phi",!0);b(y,E,z,"Ψ","\\Psi",!0);b(y,E,z,"Ω","\\Omega",!0);b(y,E,z,"A","Α");b(y,E,z,"B","Β");b(y,E,z,"E","Ε");b(y,E,z,"Z","Ζ");b(y,E,z,"H","Η");b(y,E,z,"I","Ι");b(y,E,z,"K","Κ");b(y,E,z,"M","Μ");b(y,E,z,"N","Ν");b(y,E,z,"O","Ο");b(y,E,z,"P","Ρ");b(y,E,z,"T","Τ");b(y,E,z,"X","Χ");b(y,E,z,"¬","\\neg",!0);b(y,E,z,"¬","\\lnot");b(y,E,z,"⊤","\\top");b(y,E,z,"⊥","\\bot");b(y,E,z,"∅","\\emptyset");b(y,M,z,"∅","\\varnothing");b(y,E,_e,"α","\\alpha",!0);b(y,E,_e,"β","\\beta",!0);b(y,E,_e,"γ","\\gamma",!0);b(y,E,_e,"δ","\\delta",!0);b(y,E,_e,"ϵ","\\epsilon",!0);b(y,E,_e,"ζ","\\zeta",!0);b(y,E,_e,"η","\\eta",!0);b(y,E,_e,"θ","\\theta",!0);b(y,E,_e,"ι","\\iota",!0);b(y,E,_e,"κ","\\kappa",!0);b(y,E,_e,"λ","\\lambda",!0);b(y,E,_e,"μ","\\mu",!0);b(y,E,_e,"ν","\\nu",!0);b(y,E,_e,"ξ","\\xi",!0);b(y,E,_e,"ο","\\omicron",!0);b(y,E,_e,"π","\\pi",!0);b(y,E,_e,"ρ","\\rho",!0);b(y,E,_e,"σ","\\sigma",!0);b(y,E,_e,"τ","\\tau",!0);b(y,E,_e,"υ","\\upsilon",!0);b(y,E,_e,"ϕ","\\phi",!0);b(y,E,_e,"χ","\\chi",!0);b(y,E,_e,"ψ","\\psi",!0);b(y,E,_e,"ω","\\omega",!0);b(y,E,_e,"ε","\\varepsilon",!0);b(y,E,_e,"ϑ","\\vartheta",!0);b(y,E,_e,"ϖ","\\varpi",!0);b(y,E,_e,"ϱ","\\varrho",!0);b(y,E,_e,"ς","\\varsigma",!0);b(y,E,_e,"φ","\\varphi",!0);b(y,E,ge,"∗","*",!0);b(y,E,ge,"+","+");b(y,E,ge,"−","-",!0);b(y,E,ge,"⋅","\\cdot",!0);b(y,E,ge,"∘","\\circ",!0);b(y,E,ge,"÷","\\div",!0);b(y,E,ge,"±","\\pm",!0);b(y,E,ge,"×","\\times",!0);b(y,E,ge,"∩","\\cap",!0);b(y,E,ge,"∪","\\cup",!0);b(y,E,ge,"∖","\\setminus",!0);b(y,E,ge,"∧","\\land");b(y,E,ge,"∨","\\lor");b(y,E,ge,"∧","\\wedge",!0);b(y,E,ge,"∨","\\vee",!0);b(y,E,z,"√","\\surd");b(y,E,Qn,"⟨","\\langle",!0);b(y,E,Qn,"∣","\\lvert");b(y,E,Qn,"∥","\\lVert");b(y,E,vn,"?","?");b(y,E,vn,"!","!");b(y,E,vn,"⟩","\\rangle",!0);b(y,E,vn,"∣","\\rvert");b(y,E,vn,"∥","\\rVert");b(y,E,R,"=","=");b(y,E,R,":",":");b(y,E,R,"≈","\\approx",!0);b(y,E,R,"≅","\\cong",!0);b(y,E,R,"≥","\\ge");b(y,E,R,"≥","\\geq",!0);b(y,E,R,"←","\\gets");b(y,E,R,">","\\gt",!0);b(y,E,R,"∈","\\in",!0);b(y,E,R,"","\\@not");b(y,E,R,"⊂","\\subset",!0);b(y,E,R,"⊃","\\supset",!0);b(y,E,R,"⊆","\\subseteq",!0);b(y,E,R,"⊇","\\supseteq",!0);b(y,M,R,"⊈","\\nsubseteq",!0);b(y,M,R,"⊉","\\nsupseteq",!0);b(y,E,R,"⊨","\\models");b(y,E,R,"←","\\leftarrow",!0);b(y,E,R,"≤","\\le");b(y,E,R,"≤","\\leq",!0);b(y,E,R,"<","\\lt",!0);b(y,E,R,"→","\\rightarrow",!0);b(y,E,R,"→","\\to");b(y,M,R,"≱","\\ngeq",!0);b(y,M,R,"≰","\\nleq",!0);b(y,E,Fi," ","\\ ");b(y,E,Fi," ","\\space");b(y,E,Fi," ","\\nobreakspace");b(Z,E,Fi," ","\\ ");b(Z,E,Fi," "," ");b(Z,E,Fi," ","\\space");b(Z,E,Fi," ","\\nobreakspace");b(y,E,Fi,null,"\\nobreak");b(y,E,Fi,null,"\\allowbreak");b(y,E,gm,",",",");b(y,E,gm,";",";");b(y,M,ge,"⊼","\\barwedge",!0);b(y,M,ge,"⊻","\\veebar",!0);b(y,E,ge,"⊙","\\odot",!0);b(y,E,ge,"⊕","\\oplus",!0);b(y,E,ge,"⊗","\\otimes",!0);b(y,E,z,"∂","\\partial",!0);b(y,E,ge,"⊘","\\oslash",!0);b(y,M,ge,"⊚","\\circledcirc",!0);b(y,M,ge,"⊡","\\boxdot",!0);b(y,E,ge,"△","\\bigtriangleup");b(y,E,ge,"▽","\\bigtriangledown");b(y,E,ge,"†","\\dagger");b(y,E,ge,"⋄","\\diamond");b(y,E,ge,"⋆","\\star");b(y,E,ge,"◃","\\triangleleft");b(y,E,ge,"▹","\\triangleright");b(y,E,Qn,"{","\\{");b(Z,E,z,"{","\\{");b(Z,E,z,"{","\\textbraceleft");b(y,E,vn,"}","\\}");b(Z,E,z,"}","\\}");b(Z,E,z,"}","\\textbraceright");b(y,E,Qn,"{","\\lbrace");b(y,E,vn,"}","\\rbrace");b(y,E,Qn,"[","\\lbrack",!0);b(Z,E,z,"[","\\lbrack",!0);b(y,E,vn,"]","\\rbrack",!0);b(Z,E,z,"]","\\rbrack",!0);b(y,E,Qn,"(","\\lparen",!0);b(y,E,vn,")","\\rparen",!0);b(Z,E,z,"<","\\textless",!0);b(Z,E,z,">","\\textgreater",!0);b(y,E,Qn,"⌊","\\lfloor",!0);b(y,E,vn,"⌋","\\rfloor",!0);b(y,E,Qn,"⌈","\\lceil",!0);b(y,E,vn,"⌉","\\rceil",!0);b(y,E,z,"\\","\\backslash");b(y,E,z,"∣","|");b(y,E,z,"∣","\\vert");b(Z,E,z,"|","\\textbar",!0);b(y,E,z,"∥","\\|");b(y,E,z,"∥","\\Vert");b(Z,E,z,"∥","\\textbardbl");b(Z,E,z,"~","\\textasciitilde");b(Z,E,z,"\\","\\textbackslash");b(Z,E,z,"^","\\textasciicircum");b(y,E,R,"↑","\\uparrow",!0);b(y,E,R,"⇑","\\Uparrow",!0);b(y,E,R,"↓","\\downarrow",!0);b(y,E,R,"⇓","\\Downarrow",!0);b(y,E,R,"↕","\\updownarrow",!0);b(y,E,R,"⇕","\\Updownarrow",!0);b(y,E,Ut,"∐","\\coprod");b(y,E,Ut,"⋁","\\bigvee");b(y,E,Ut,"⋀","\\bigwedge");b(y,E,Ut,"⨄","\\biguplus");b(y,E,Ut,"⋂","\\bigcap");b(y,E,Ut,"⋃","\\bigcup");b(y,E,Ut,"∫","\\int");b(y,E,Ut,"∫","\\intop");b(y,E,Ut,"∬","\\iint");b(y,E,Ut,"∭","\\iiint");b(y,E,Ut,"∏","\\prod");b(y,E,Ut,"∑","\\sum");b(y,E,Ut,"⨂","\\bigotimes");b(y,E,Ut,"⨁","\\bigoplus");b(y,E,Ut,"⨀","\\bigodot");b(y,E,Ut,"∮","\\oint");b(y,E,Ut,"∯","\\oiint");b(y,E,Ut,"∰","\\oiiint");b(y,E,Ut,"⨆","\\bigsqcup");b(y,E,Ut,"∫","\\smallint");b(Z,E,Qo,"…","\\textellipsis");b(y,E,Qo,"…","\\mathellipsis");b(Z,E,Qo,"…","\\ldots",!0);b(y,E,Qo,"…","\\ldots",!0);b(y,E,Qo,"⋯","\\@cdots",!0);b(y,E,Qo,"⋱","\\ddots",!0);b(y,E,z,"⋮","\\varvdots");b(Z,E,z,"⋮","\\varvdots");b(y,E,ht,"ˊ","\\acute");b(y,E,ht,"ˋ","\\grave");b(y,E,ht,"¨","\\ddot");b(y,E,ht,"~","\\tilde");b(y,E,ht,"ˉ","\\bar");b(y,E,ht,"˘","\\breve");b(y,E,ht,"ˇ","\\check");b(y,E,ht,"^","\\hat");b(y,E,ht,"⃗","\\vec");b(y,E,ht,"˙","\\dot");b(y,E,ht,"˚","\\mathring");b(y,E,_e,"","\\@imath");b(y,E,_e,"","\\@jmath");b(y,E,z,"ı","ı");b(y,E,z,"ȷ","ȷ");b(Z,E,z,"ı","\\i",!0);b(Z,E,z,"ȷ","\\j",!0);b(Z,E,z,"ß","\\ss",!0);b(Z,E,z,"æ","\\ae",!0);b(Z,E,z,"œ","\\oe",!0);b(Z,E,z,"ø","\\o",!0);b(Z,E,z,"Æ","\\AE",!0);b(Z,E,z,"Œ","\\OE",!0);b(Z,E,z,"Ø","\\O",!0);b(Z,E,ht,"ˊ","\\'");b(Z,E,ht,"ˋ","\\`");b(Z,E,ht,"ˆ","\\^");b(Z,E,ht,"˜","\\~");b(Z,E,ht,"ˉ","\\=");b(Z,E,ht,"˘","\\u");b(Z,E,ht,"˙","\\.");b(Z,E,ht,"¸","\\c");b(Z,E,ht,"˚","\\r");b(Z,E,ht,"ˇ","\\v");b(Z,E,ht,"¨",'\\"');b(Z,E,ht,"˝","\\H");b(Z,E,ht,"◯","\\textcircled");var Pw={"--":!0,"---":!0,"``":!0,"''":!0};b(Z,E,z,"–","--",!0);b(Z,E,z,"–","\\textendash");b(Z,E,z,"—","---",!0);b(Z,E,z,"—","\\textemdash");b(Z,E,z,"‘","`",!0);b(Z,E,z,"‘","\\textquoteleft");b(Z,E,z,"’","'",!0);b(Z,E,z,"’","\\textquoteright");b(Z,E,z,"“","``",!0);b(Z,E,z,"“","\\textquotedblleft");b(Z,E,z,"”","''",!0);b(Z,E,z,"”","\\textquotedblright");b(y,E,z,"°","\\degree",!0);b(Z,E,z,"°","\\degree");b(Z,E,z,"°","\\textdegree",!0);b(y,E,z,"£","\\pounds");b(y,E,z,"£","\\mathsterling",!0);b(Z,E,z,"£","\\pounds");b(Z,E,z,"£","\\textsterling",!0);b(y,M,z,"✠","\\maltese");b(Z,M,z,"✠","\\maltese");var P3='0123456789/@."';for(var Vh=0;Vh<P3.length;Vh++){var I3=P3.charAt(Vh);b(y,E,z,I3,I3)}var U3='0123456789!@*()-=+";:?/.,';for(var Hh=0;Hh<U3.length;Hh++){var D3=U3.charAt(Hh);b(Z,E,z,D3,D3)}var m0="ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz";for(var qh=0;qh<m0.length;qh++){var Vu=m0.charAt(qh);b(y,E,_e,Vu,Vu),b(Z,E,z,Vu,Vu)}b(y,M,z,"C","ℂ");b(Z,M,z,"C","ℂ");b(y,M,z,"H","ℍ");b(Z,M,z,"H","ℍ");b(y,M,z,"N","ℕ");b(Z,M,z,"N","ℕ");b(y,M,z,"P","ℙ");b(Z,M,z,"P","ℙ");b(y,M,z,"Q","ℚ");b(Z,M,z,"Q","ℚ");b(y,M,z,"R","ℝ");b(Z,M,z,"R","ℝ");b(y,M,z,"Z","ℤ");b(Z,M,z,"Z","ℤ");b(y,E,_e,"h","ℎ");b(Z,E,_e,"h","ℎ");var ke="";for(var cn=0;cn<m0.length;cn++){var wt=m0.charAt(cn);ke=String.fromCharCode(55349,56320+cn),b(y,E,_e,wt,ke),b(Z,E,z,wt,ke),ke=String.fromCharCode(55349,56372+cn),b(y,E,_e,wt,ke),b(Z,E,z,wt,ke),ke=String.fromCharCode(55349,56424+cn),b(y,E,_e,wt,ke),b(Z,E,z,wt,ke),ke=String.fromCharCode(55349,56580+cn),b(y,E,_e,wt,ke),b(Z,E,z,wt,ke),ke=String.fromCharCode(55349,56684+cn),b(y,E,_e,wt,ke),b(Z,E,z,wt,ke),ke=String.fromCharCode(55349,56736+cn),b(y,E,_e,wt,ke),b(Z,E,z,wt,ke),ke=String.fromCharCode(55349,56788+cn),b(y,E,_e,wt,ke),b(Z,E,z,wt,ke),ke=String.fromCharCode(55349,56840+cn),b(y,E,_e,wt,ke),b(Z,E,z,wt,ke),ke=String.fromCharCode(55349,56944+cn),b(y,E,_e,wt,ke),b(Z,E,z,wt,ke),cn<26&&(ke=String.fromCharCode(55349,56632+cn),b(y,E,_e,wt,ke),b(Z,E,z,wt,ke),ke=String.fromCharCode(55349,56476+cn),b(y,E,_e,wt,ke),b(Z,E,z,wt,ke))}ke="𝕜";b(y,E,_e,"k",ke);b(Z,E,z,"k",ke);for(var Ha=0;Ha<10;Ha++){var Ki=Ha.toString();ke=String.fromCharCode(55349,57294+Ha),b(y,E,_e,Ki,ke),b(Z,E,z,Ki,ke),ke=String.fromCharCode(55349,57314+Ha),b(y,E,_e,Ki,ke),b(Z,E,z,Ki,ke),ke=String.fromCharCode(55349,57324+Ha),b(y,E,_e,Ki,ke),b(Z,E,z,Ki,ke),ke=String.fromCharCode(55349,57334+Ha),b(y,E,_e,Ki,ke),b(Z,E,z,Ki,ke)}var tg="ÐÞþ";for(var Gh=0;Gh<tg.length;Gh++){var Hu=tg.charAt(Gh);b(y,E,_e,Hu,Hu),b(Z,E,z,Hu,Hu)}var qu=[["mathbf","textbf","Main-Bold"],["mathbf","textbf","Main-Bold"],["mathnormal","textit","Math-Italic"],["mathnormal","textit","Math-Italic"],["boldsymbol","boldsymbol","Main-BoldItalic"],["boldsymbol","boldsymbol","Main-BoldItalic"],["mathscr","textscr","Script-Regular"],["","",""],["","",""],["","",""],["mathfrak","textfrak","Fraktur-Regular"],["mathfrak","textfrak","Fraktur-Regular"],["mathbb","textbb","AMS-Regular"],["mathbb","textbb","AMS-Regular"],["mathboldfrak","textboldfrak","Fraktur-Regular"],["mathboldfrak","textboldfrak","Fraktur-Regular"],["mathsf","textsf","SansSerif-Regular"],["mathsf","textsf","SansSerif-Regular"],["mathboldsf","textboldsf","SansSerif-Bold"],["mathboldsf","textboldsf","SansSerif-Bold"],["mathitsf","textitsf","SansSerif-Italic"],["mathitsf","textitsf","SansSerif-Italic"],["","",""],["","",""],["mathtt","texttt","Typewriter-Regular"],["mathtt","texttt","Typewriter-Regular"]],M3=[["mathbf","textbf","Main-Bold"],["","",""],["mathsf","textsf","SansSerif-Regular"],["mathboldsf","textboldsf","SansSerif-Bold"],["mathtt","texttt","Typewriter-Regular"]],qG=function(t,n){var r=t.charCodeAt(0),i=t.charCodeAt(1),a=(r-55296)*1024+(i-56320)+65536,s=n==="math"?0:1;if(119808<=a&&a<120484){var o=Math.floor((a-119808)/26);return[qu[o][2],qu[o][s]]}else if(120782<=a&&a<=120831){var l=Math.floor((a-120782)/10);return[M3[l][2],M3[l][s]]}else{if(a===120485||a===120486)return[qu[0][2],qu[0][s]];if(120486<a&&a<120782)return["",""];throw new oe("Unsupported character: "+t)}},bm=function(t,n,r){return at[r][t]&&at[r][t].replace&&(t=at[r][t].replace),{value:t,metrics:B2(t,n,r)}},kr=function(t,n,r,i,a){var s=bm(t,n,r),o=s.metrics;t=s.value;var l;if(o){var c=o.italic;(r==="text"||i&&i.font==="mathit")&&(c=0),l=new pr(t,o.height,o.depth,c,o.skew,o.width,a)}else typeof console<"u"&&console.warn("No character metrics "+("for '"+t+"' in style '"+n+"' and mode '"+r+"'")),l=new pr(t,0,0,0,0,0,a);if(i){l.maxFontSize=i.sizeMultiplier,i.style.isTight()&&l.classes.push("mtight");var u=i.getColor();u&&(l.style.color=u)}return l},GG=function(t,n,r,i){return i===void 0&&(i=[]),r.font==="boldsymbol"&&bm(t,"Main-Bold",n).metrics?kr(t,"Main-Bold",n,r,i.concat(["mathbf"])):t==="\\"||at[n][t].font==="main"?kr(t,"Main-Regular",n,r,i):kr(t,"AMS-Regular",n,r,i.concat(["amsrm"]))},WG=function(t,n,r,i,a){return a!=="textord"&&bm(t,"Math-BoldItalic",n).metrics?{fontName:"Math-BoldItalic",fontClass:"boldsymbol"}:{fontName:"Main-Bold",fontClass:"mathbf"}},KG=function(t,n,r){var i=t.mode,a=t.text,s=["mord"],o=i==="math"||i==="text"&&n.font,l=o?n.font:n.fontFamily,c="",u="";if(a.charCodeAt(0)===55349&&([c,u]=qG(a,i)),c.length>0)return kr(a,c,i,n,s.concat(u));if(l){var d,m;if(l==="boldsymbol"){var p=WG(a,i,n,s,r);d=p.fontName,m=[p.fontClass]}else o?(d=Dw[l].fontName,m=[l]):(d=Gu(l,n.fontWeight,n.fontShape),m=[l,n.fontWeight,n.fontShape]);if(bm(a,d,i).metrics)return kr(a,d,i,n,s.concat(m));if(Pw.hasOwnProperty(a)&&d.slice(0,10)==="Typewriter"){for(var x=[],g=0;g<a.length;g++)x.push(kr(a[g],d,i,n,s.concat(m)));return Uw(x)}}if(r==="mathord")return kr(a,"Math-Italic",i,n,s.concat(["mathnormal"]));if(r==="textord"){var w=at[i][a]&&at[i][a].font;if(w==="ams"){var v=Gu("amsrm",n.fontWeight,n.fontShape);return kr(a,v,i,n,s.concat("amsrm",n.fontWeight,n.fontShape))}else if(w==="main"||!w){var $=Gu("textrm",n.fontWeight,n.fontShape);return kr(a,$,i,n,s.concat(n.fontWeight,n.fontShape))}else{var _=Gu(w,n.fontWeight,n.fontShape);return kr(a,_,i,n,s.concat(_,n.fontWeight,n.fontShape))}}else throw new Error("unexpected type: "+r+" in makeOrd")},YG=(e,t)=>{if(La(e.classes)!==La(t.classes)||e.skew!==t.skew||e.maxFontSize!==t.maxFontSize)return!1;if(e.classes.length===1){var n=e.classes[0];if(n==="mbin"||n==="mord")return!1}for(var r in e.style)if(e.style.hasOwnProperty(r)&&e.style[r]!==t.style[r])return!1;for(var i in t.style)if(t.style.hasOwnProperty(i)&&e.style[i]!==t.style[i])return!1;return!0},XG=e=>{for(var t=0;t<e.length-1;t++){var n=e[t],r=e[t+1];n instanceof pr&&r instanceof pr&&YG(n,r)&&(n.text+=r.text,n.height=Math.max(n.height,r.height),n.depth=Math.max(n.depth,r.depth),n.italic=r.italic,e.splice(t+1,1),t--)}return e},j2=function(t){for(var n=0,r=0,i=0,a=0;a<t.children.length;a++){var s=t.children[a];s.height>n&&(n=s.height),s.depth>r&&(r=s.depth),s.maxFontSize>i&&(i=s.maxFontSize)}t.height=n,t.depth=r,t.maxFontSize=i},Tn=function(t,n,r,i){var a=new eu(t,n,r,i);return j2(a),a},Iw=(e,t,n,r)=>new eu(e,t,n,r),QG=function(t,n,r){var i=Tn([t],[],n);return i.height=Math.max(r||n.fontMetrics().defaultRuleThickness,n.minRuleThickness),i.style.borderBottomWidth=me(i.height),i.maxFontSize=1,i},ZG=function(t,n,r,i){var a=new F2(t,n,r,i);return j2(a),a},Uw=function(t){var n=new Jc(t);return j2(n),n},JG=function(t,n){return t instanceof Jc?Tn([],[t],n):t},eW=function(t){if(t.positionType==="individualShift"){for(var n=t.children,r=[n[0]],i=-n[0].shift-n[0].elem.depth,a=i,s=1;s<n.length;s++){var o=-n[s].shift-a-n[s].elem.depth,l=o-(n[s-1].elem.height+n[s-1].elem.depth);a=a+o,r.push({type:"kern",size:l}),r.push(n[s])}return{children:r,depth:i}}var c;if(t.positionType==="top"){for(var u=t.positionData,d=0;d<t.children.length;d++){var m=t.children[d];u-=m.type==="kern"?m.size:m.elem.height+m.elem.depth}c=u}else if(t.positionType==="bottom")c=-t.positionData;else{var p=t.children[0];if(p.type!=="elem")throw new Error('First child must have type "elem".');if(t.positionType==="shift")c=-p.elem.depth-t.positionData;else if(t.positionType==="firstBaseline")c=-p.elem.depth;else throw new Error("Invalid positionType "+t.positionType+".")}return{children:t.children,depth:c}},tW=function(t,n){for(var{children:r,depth:i}=eW(t),a=0,s=0;s<r.length;s++){var o=r[s];if(o.type==="elem"){var l=o.elem;a=Math.max(a,l.maxFontSize,l.height)}}a+=2;var c=Tn(["pstrut"],[]);c.style.height=me(a);for(var u=[],d=i,m=i,p=i,x=0;x<r.length;x++){var g=r[x];if(g.type==="kern")p+=g.size;else{var w=g.elem,v=g.wrapperClasses||[],$=g.wrapperStyle||{},_=Tn(v,[c,w],void 0,$);_.style.top=me(-a-p-w.depth),g.marginLeft&&(_.style.marginLeft=g.marginLeft),g.marginRight&&(_.style.marginRight=g.marginRight),u.push(_),p+=w.height+w.depth}d=Math.min(d,p),m=Math.max(m,p)}var C=Tn(["vlist"],u);C.style.height=me(m);var k;if(d<0){var S=Tn([],[]),L=Tn(["vlist"],[S]);L.style.height=me(-d);var U=Tn(["vlist-s"],[new pr("​")]);k=[Tn(["vlist-r"],[C,U]),Tn(["vlist-r"],[L])]}else k=[Tn(["vlist-r"],[C])];var F=Tn(["vlist-t"],k);return k.length===2&&F.classes.push("vlist-t2"),F.height=m,F.depth=-d,F},nW=(e,t)=>{var n=Tn(["mspace"],[],t),r=gt(e,t);return n.style.marginRight=me(r),n},Gu=function(t,n,r){var i="";switch(t){case"amsrm":i="AMS";break;case"textrm":i="Main";break;case"textsf":i="SansSerif";break;case"texttt":i="Typewriter";break;default:i=t}var a;return n==="textbf"&&r==="textit"?a="BoldItalic":n==="textbf"?a="Bold":n==="textit"?a="Italic":a="Regular",i+"-"+a},Dw={mathbf:{variant:"bold",fontName:"Main-Bold"},mathrm:{variant:"normal",fontName:"Main-Regular"},textit:{variant:"italic",fontName:"Main-Italic"},mathit:{variant:"italic",fontName:"Main-Italic"},mathnormal:{variant:"italic",fontName:"Math-Italic"},mathsfit:{variant:"sans-serif-italic",fontName:"SansSerif-Italic"},mathbb:{variant:"double-struck",fontName:"AMS-Regular"},mathcal:{variant:"script",fontName:"Caligraphic-Regular"},mathfrak:{variant:"fraktur",fontName:"Fraktur-Regular"},mathscr:{variant:"script",fontName:"Script-Regular"},mathsf:{variant:"sans-serif",fontName:"SansSerif-Regular"},mathtt:{variant:"monospace",fontName:"Typewriter-Regular"}},Mw={vec:["vec",.471,.714],oiintSize1:["oiintSize1",.957,.499],oiintSize2:["oiintSize2",1.472,.659],oiiintSize1:["oiiintSize1",1.304,.499],oiiintSize2:["oiiintSize2",1.98,.659]},rW=function(t,n){var[r,i,a]=Mw[t],s=new Pa(r),o=new Ri([s],{width:me(i),height:me(a),style:"width:"+me(i),viewBox:"0 0 "+1e3*i+" "+1e3*a,preserveAspectRatio:"xMinYMin"}),l=Iw(["overlay"],[o],n);return l.height=a,l.style.height=me(a),l.style.width=me(i),l},V={fontMap:Dw,makeSymbol:kr,mathsym:GG,makeSpan:Tn,makeSvgSpan:Iw,makeLineSpan:QG,makeAnchor:ZG,makeFragment:Uw,wrapFragment:JG,makeVList:tW,makeOrd:KG,makeGlue:nW,staticSvg:rW,svgData:Mw,tryCombineChars:XG},pt={number:3,unit:"mu"},qa={number:4,unit:"mu"},bi={number:5,unit:"mu"},iW={mord:{mop:pt,mbin:qa,mrel:bi,minner:pt},mop:{mord:pt,mop:pt,mrel:bi,minner:pt},mbin:{mord:qa,mop:qa,mopen:qa,minner:qa},mrel:{mord:bi,mop:bi,mopen:bi,minner:bi},mopen:{},mclose:{mop:pt,mbin:qa,mrel:bi,minner:pt},mpunct:{mord:pt,mop:pt,mrel:bi,mopen:pt,mclose:pt,mpunct:pt,minner:pt},minner:{mord:pt,mop:pt,mbin:qa,mrel:bi,mopen:pt,mpunct:pt,minner:pt}},aW={mord:{mop:pt},mop:{mord:pt,mop:pt},mbin:{},mrel:{},mopen:{},mclose:{mop:pt},mpunct:{},minner:{mop:pt}},Rw={},h0={},p0={};function fe(e){for(var{type:t,names:n,props:r,handler:i,htmlBuilder:a,mathmlBuilder:s}=e,o={type:t,numArgs:r.numArgs,argTypes:r.argTypes,allowedInArgument:!!r.allowedInArgument,allowedInText:!!r.allowedInText,allowedInMath:r.allowedInMath===void 0?!0:r.allowedInMath,numOptionalArgs:r.numOptionalArgs||0,infix:!!r.infix,primitive:!!r.primitive,handler:i},l=0;l<n.length;++l)Rw[n[l]]=o;t&&(a&&(h0[t]=a),s&&(p0[t]=s))}function ws(e){var{type:t,htmlBuilder:n,mathmlBuilder:r}=e;fe({type:t,names:[],props:{numArgs:0},handler(){throw new Error("Should never be called.")},htmlBuilder:n,mathmlBuilder:r})}var f0=function(t){return t.type==="ordgroup"&&t.body.length===1?t.body[0]:t},At=function(t){return t.type==="ordgroup"?t.body:[t]},Oi=V.makeSpan,sW=["leftmost","mbin","mopen","mrel","mop","mpunct"],oW=["rightmost","mrel","mclose","mpunct"],lW={display:we.DISPLAY,text:we.TEXT,script:we.SCRIPT,scriptscript:we.SCRIPTSCRIPT},cW={mord:"mord",mop:"mop",mbin:"mbin",mrel:"mrel",mopen:"mopen",mclose:"mclose",mpunct:"mpunct",minner:"minner"},Bt=function(t,n,r,i){i===void 0&&(i=[null,null]);for(var a=[],s=0;s<t.length;s++){var o=Ge(t[s],n);if(o instanceof Jc){var l=o.children;a.push(...l)}else a.push(o)}if(V.tryCombineChars(a),!r)return a;var c=n;if(t.length===1){var u=t[0];u.type==="sizing"?c=n.havingSize(u.size):u.type==="styling"&&(c=n.havingStyle(lW[u.style]))}var d=Oi([i[0]||"leftmost"],[],n),m=Oi([i[1]||"rightmost"],[],n),p=r==="root";return R3(a,(x,g)=>{var w=g.classes[0],v=x.classes[0];w==="mbin"&&ye.contains(oW,v)?g.classes[0]="mord":v==="mbin"&&ye.contains(sW,w)&&(x.classes[0]="mord")},{node:d},m,p),R3(a,(x,g)=>{var w=ng(g),v=ng(x),$=w&&v?x.hasClass("mtight")?aW[w][v]:iW[w][v]:null;if($)return V.makeGlue($,c)},{node:d},m,p),a},R3=function e(t,n,r,i,a){i&&t.push(i);for(var s=0;s<t.length;s++){var o=t[s],l=Ow(o);if(l){e(l.children,n,r,null,a);continue}var c=!o.hasClass("mspace");if(c){var u=n(o,r.node);u&&(r.insertAfter?r.insertAfter(u):(t.unshift(u),s++))}c?r.node=o:a&&o.hasClass("newline")&&(r.node=Oi(["leftmost"])),r.insertAfter=(d=>m=>{t.splice(d+1,0,m),s++})(s)}i&&t.pop()},Ow=function(t){return t instanceof Jc||t instanceof F2||t instanceof eu&&t.hasClass("enclosing")?t:null},uW=function e(t,n){var r=Ow(t);if(r){var i=r.children;if(i.length){if(n==="right")return e(i[i.length-1],"right");if(n==="left")return e(i[0],"left")}}return t},ng=function(t,n){return t?(n&&(t=uW(t,n)),cW[t.classes[0]]||null):null},Ic=function(t,n){var r=["nulldelimiter"].concat(t.baseSizingClasses());return Oi(n.concat(r))},Ge=function(t,n,r){if(!t)return Oi();if(h0[t.type]){var i=h0[t.type](t,n);if(r&&n.size!==r.size){i=Oi(n.sizingClasses(r),[i],n);var a=n.sizeMultiplier/r.sizeMultiplier;i.height*=a,i.depth*=a}return i}else throw new oe("Got group of unknown type: '"+t.type+"'")};function Wu(e,t){var n=Oi(["base"],e,t),r=Oi(["strut"]);return r.style.height=me(n.height+n.depth),n.depth&&(r.style.verticalAlign=me(-n.depth)),n.children.unshift(r),n}function rg(e,t){var n=null;e.length===1&&e[0].type==="tag"&&(n=e[0].tag,e=e[0].body);var r=Bt(e,t,"root"),i;r.length===2&&r[1].hasClass("tag")&&(i=r.pop());for(var a=[],s=[],o=0;o<r.length;o++)if(s.push(r[o]),r[o].hasClass("mbin")||r[o].hasClass("mrel")||r[o].hasClass("allowbreak")){for(var l=!1;o<r.length-1&&r[o+1].hasClass("mspace")&&!r[o+1].hasClass("newline");)o++,s.push(r[o]),r[o].hasClass("nobreak")&&(l=!0);l||(a.push(Wu(s,t)),s=[])}else r[o].hasClass("newline")&&(s.pop(),s.length>0&&(a.push(Wu(s,t)),s=[]),a.push(r[o]));s.length>0&&a.push(Wu(s,t));var c;n?(c=Wu(Bt(n,t,!0)),c.classes=["tag"],a.push(c)):i&&a.push(i);var u=Oi(["katex-html"],a);if(u.setAttribute("aria-hidden","true"),c){var d=c.children[0];d.style.height=me(u.height+u.depth),u.depth&&(d.style.verticalAlign=me(-u.depth))}return u}function zw(e){return new Jc(e)}class Vn{constructor(t,n,r){this.type=void 0,this.attributes=void 0,this.children=void 0,this.classes=void 0,this.type=t,this.attributes={},this.children=n||[],this.classes=r||[]}setAttribute(t,n){this.attributes[t]=n}getAttribute(t){return this.attributes[t]}toNode(){var t=document.createElementNS("http://www.w3.org/1998/Math/MathML",this.type);for(var n in this.attributes)Object.prototype.hasOwnProperty.call(this.attributes,n)&&t.setAttribute(n,this.attributes[n]);this.classes.length>0&&(t.className=La(this.classes));for(var r=0;r<this.children.length;r++)if(this.children[r]instanceof ei&&this.children[r+1]instanceof ei){for(var i=this.children[r].toText()+this.children[++r].toText();this.children[r+1]instanceof ei;)i+=this.children[++r].toText();t.appendChild(new ei(i).toNode())}else t.appendChild(this.children[r].toNode());return t}toMarkup(){var t="<"+this.type;for(var n in this.attributes)Object.prototype.hasOwnProperty.call(this.attributes,n)&&(t+=" "+n+'="',t+=ye.escape(this.attributes[n]),t+='"');this.classes.length>0&&(t+=' class ="'+ye.escape(La(this.classes))+'"'),t+=">";for(var r=0;r<this.children.length;r++)t+=this.children[r].toMarkup();return t+="</"+this.type+">",t}toText(){return this.children.map(t=>t.toText()).join("")}}class ei{constructor(t){this.text=void 0,this.text=t}toNode(){return document.createTextNode(this.text)}toMarkup(){return ye.escape(this.toText())}toText(){return this.text}}class dW{constructor(t){this.width=void 0,this.character=void 0,this.width=t,t>=.05555&&t<=.05556?this.character=" ":t>=.1666&&t<=.1667?this.character=" ":t>=.2222&&t<=.2223?this.character=" ":t>=.2777&&t<=.2778?this.character="  ":t>=-.05556&&t<=-.05555?this.character=" ⁣":t>=-.1667&&t<=-.1666?this.character=" ⁣":t>=-.2223&&t<=-.2222?this.character=" ⁣":t>=-.2778&&t<=-.2777?this.character=" ⁣":this.character=null}toNode(){if(this.character)return document.createTextNode(this.character);var t=document.createElementNS("http://www.w3.org/1998/Math/MathML","mspace");return t.setAttribute("width",me(this.width)),t}toMarkup(){return this.character?"<mtext>"+this.character+"</mtext>":'<mspace width="'+me(this.width)+'"/>'}toText(){return this.character?this.character:" "}}var ie={MathNode:Vn,TextNode:ei,SpaceNode:dW,newDocumentFragment:zw},fr=function(t,n,r){return at[n][t]&&at[n][t].replace&&t.charCodeAt(0)!==55349&&!(Pw.hasOwnProperty(t)&&r&&(r.fontFamily&&r.fontFamily.slice(4,6)==="tt"||r.font&&r.font.slice(4,6)==="tt"))&&(t=at[n][t].replace),new ie.TextNode(t)},V2=function(t){return t.length===1?t[0]:new ie.MathNode("mrow",t)},H2=function(t,n){if(n.fontFamily==="texttt")return"monospace";if(n.fontFamily==="textsf")return n.fontShape==="textit"&&n.fontWeight==="textbf"?"sans-serif-bold-italic":n.fontShape==="textit"?"sans-serif-italic":n.fontWeight==="textbf"?"bold-sans-serif":"sans-serif";if(n.fontShape==="textit"&&n.fontWeight==="textbf")return"bold-italic";if(n.fontShape==="textit")return"italic";if(n.fontWeight==="textbf")return"bold";var r=n.font;if(!r||r==="mathnormal")return null;var i=t.mode;if(r==="mathit")return"italic";if(r==="boldsymbol")return t.type==="textord"?"bold":"bold-italic";if(r==="mathbf")return"bold";if(r==="mathbb")return"double-struck";if(r==="mathsfit")return"sans-serif-italic";if(r==="mathfrak")return"fraktur";if(r==="mathscr"||r==="mathcal")return"script";if(r==="mathsf")return"sans-serif";if(r==="mathtt")return"monospace";var a=t.text;if(ye.contains(["\\imath","\\jmath"],a))return null;at[i][a]&&at[i][a].replace&&(a=at[i][a].replace);var s=V.fontMap[r].fontName;return B2(a,s,i)?V.fontMap[r].variant:null};function Wh(e){if(!e)return!1;if(e.type==="mi"&&e.children.length===1){var t=e.children[0];return t instanceof ei&&t.text==="."}else if(e.type==="mo"&&e.children.length===1&&e.getAttribute("separator")==="true"&&e.getAttribute("lspace")==="0em"&&e.getAttribute("rspace")==="0em"){var n=e.children[0];return n instanceof ei&&n.text===","}else return!1}var Un=function(t,n,r){if(t.length===1){var i=et(t[0],n);return r&&i instanceof Vn&&i.type==="mo"&&(i.setAttribute("lspace","0em"),i.setAttribute("rspace","0em")),[i]}for(var a=[],s,o=0;o<t.length;o++){var l=et(t[o],n);if(l instanceof Vn&&s instanceof Vn){if(l.type==="mtext"&&s.type==="mtext"&&l.getAttribute("mathvariant")===s.getAttribute("mathvariant")){s.children.push(...l.children);continue}else if(l.type==="mn"&&s.type==="mn"){s.children.push(...l.children);continue}else if(Wh(l)&&s.type==="mn"){s.children.push(...l.children);continue}else if(l.type==="mn"&&Wh(s))l.children=[...s.children,...l.children],a.pop();else if((l.type==="msup"||l.type==="msub")&&l.children.length>=1&&(s.type==="mn"||Wh(s))){var c=l.children[0];c instanceof Vn&&c.type==="mn"&&(c.children=[...s.children,...c.children],a.pop())}else if(s.type==="mi"&&s.children.length===1){var u=s.children[0];if(u instanceof ei&&u.text==="̸"&&(l.type==="mo"||l.type==="mi"||l.type==="mn")){var d=l.children[0];d instanceof ei&&d.text.length>0&&(d.text=d.text.slice(0,1)+"̸"+d.text.slice(1),a.pop())}}}a.push(l),s=l}return a},Ia=function(t,n,r){return V2(Un(t,n,r))},et=function(t,n){if(!t)return new ie.MathNode("mrow");if(p0[t.type]){var r=p0[t.type](t,n);return r}else throw new oe("Got group of unknown type: '"+t.type+"'")};function O3(e,t,n,r,i){var a=Un(e,n),s;a.length===1&&a[0]instanceof Vn&&ye.contains(["mrow","mtable"],a[0].type)?s=a[0]:s=new ie.MathNode("mrow",a);var o=new ie.MathNode("annotation",[new ie.TextNode(t)]);o.setAttribute("encoding","application/x-tex");var l=new ie.MathNode("semantics",[s,o]),c=new ie.MathNode("math",[l]);c.setAttribute("xmlns","http://www.w3.org/1998/Math/MathML"),r&&c.setAttribute("display","block");var u=i?"katex":"katex-mathml";return V.makeSpan([u],[c])}var Bw=function(t){return new yi({style:t.displayMode?we.DISPLAY:we.TEXT,maxSize:t.maxSize,minRuleThickness:t.minRuleThickness})},Fw=function(t,n){if(n.displayMode){var r=["katex-display"];n.leqno&&r.push("leqno"),n.fleqn&&r.push("fleqn"),t=V.makeSpan(r,[t])}return t},mW=function(t,n,r){var i=Bw(r),a;if(r.output==="mathml")return O3(t,n,i,r.displayMode,!0);if(r.output==="html"){var s=rg(t,i);a=V.makeSpan(["katex"],[s])}else{var o=O3(t,n,i,r.displayMode,!1),l=rg(t,i);a=V.makeSpan(["katex"],[o,l])}return Fw(a,r)},hW=function(t,n,r){var i=Bw(r),a=rg(t,i),s=V.makeSpan(["katex"],[a]);return Fw(s,r)},pW={widehat:"^",widecheck:"ˇ",widetilde:"~",utilde:"~",overleftarrow:"←",underleftarrow:"←",xleftarrow:"←",overrightarrow:"→",underrightarrow:"→",xrightarrow:"→",underbrace:"⏟",overbrace:"⏞",overgroup:"⏠",undergroup:"⏡",overleftrightarrow:"↔",underleftrightarrow:"↔",xleftrightarrow:"↔",Overrightarrow:"⇒",xRightarrow:"⇒",overleftharpoon:"↼",xleftharpoonup:"↼",overrightharpoon:"⇀",xrightharpoonup:"⇀",xLeftarrow:"⇐",xLeftrightarrow:"⇔",xhookleftarrow:"↩",xhookrightarrow:"↪",xmapsto:"↦",xrightharpoondown:"⇁",xleftharpoondown:"↽",xrightleftharpoons:"⇌",xleftrightharpoons:"⇋",xtwoheadleftarrow:"↞",xtwoheadrightarrow:"↠",xlongequal:"=",xtofrom:"⇄",xrightleftarrows:"⇄",xrightequilibrium:"⇌",xleftequilibrium:"⇋","\\cdrightarrow":"→","\\cdleftarrow":"←","\\cdlongequal":"="},fW=function(t){var n=new ie.MathNode("mo",[new ie.TextNode(pW[t.replace(/^\\/,"")])]);return n.setAttribute("stretchy","true"),n},gW={overrightarrow:[["rightarrow"],.888,522,"xMaxYMin"],overleftarrow:[["leftarrow"],.888,522,"xMinYMin"],underrightarrow:[["rightarrow"],.888,522,"xMaxYMin"],underleftarrow:[["leftarrow"],.888,522,"xMinYMin"],xrightarrow:[["rightarrow"],1.469,522,"xMaxYMin"],"\\cdrightarrow":[["rightarrow"],3,522,"xMaxYMin"],xleftarrow:[["leftarrow"],1.469,522,"xMinYMin"],"\\cdleftarrow":[["leftarrow"],3,522,"xMinYMin"],Overrightarrow:[["doublerightarrow"],.888,560,"xMaxYMin"],xRightarrow:[["doublerightarrow"],1.526,560,"xMaxYMin"],xLeftarrow:[["doubleleftarrow"],1.526,560,"xMinYMin"],overleftharpoon:[["leftharpoon"],.888,522,"xMinYMin"],xleftharpoonup:[["leftharpoon"],.888,522,"xMinYMin"],xleftharpoondown:[["leftharpoondown"],.888,522,"xMinYMin"],overrightharpoon:[["rightharpoon"],.888,522,"xMaxYMin"],xrightharpoonup:[["rightharpoon"],.888,522,"xMaxYMin"],xrightharpoondown:[["rightharpoondown"],.888,522,"xMaxYMin"],xlongequal:[["longequal"],.888,334,"xMinYMin"],"\\cdlongequal":[["longequal"],3,334,"xMinYMin"],xtwoheadleftarrow:[["twoheadleftarrow"],.888,334,"xMinYMin"],xtwoheadrightarrow:[["twoheadrightarrow"],.888,334,"xMaxYMin"],overleftrightarrow:[["leftarrow","rightarrow"],.888,522],overbrace:[["leftbrace","midbrace","rightbrace"],1.6,548],underbrace:[["leftbraceunder","midbraceunder","rightbraceunder"],1.6,548],underleftrightarrow:[["leftarrow","rightarrow"],.888,522],xleftrightarrow:[["leftarrow","rightarrow"],1.75,522],xLeftrightarrow:[["doubleleftarrow","doublerightarrow"],1.75,560],xrightleftharpoons:[["leftharpoondownplus","rightharpoonplus"],1.75,716],xleftrightharpoons:[["leftharpoonplus","rightharpoondownplus"],1.75,716],xhookleftarrow:[["leftarrow","righthook"],1.08,522],xhookrightarrow:[["lefthook","rightarrow"],1.08,522],overlinesegment:[["leftlinesegment","rightlinesegment"],.888,522],underlinesegment:[["leftlinesegment","rightlinesegment"],.888,522],overgroup:[["leftgroup","rightgroup"],.888,342],undergroup:[["leftgroupunder","rightgroupunder"],.888,342],xmapsto:[["leftmapsto","rightarrow"],1.5,522],xtofrom:[["leftToFrom","rightToFrom"],1.75,528],xrightleftarrows:[["baraboveleftarrow","rightarrowabovebar"],1.75,901],xrightequilibrium:[["baraboveshortleftharpoon","rightharpoonaboveshortbar"],1.75,716],xleftequilibrium:[["shortbaraboveleftharpoon","shortrightharpoonabovebar"],1.75,716]},bW=function(t){return t.type==="ordgroup"?t.body.length:1},vW=function(t,n){function r(){var o=4e5,l=t.label.slice(1);if(ye.contains(["widehat","widecheck","widetilde","utilde"],l)){var c=t,u=bW(c.base),d,m,p;if(u>5)l==="widehat"||l==="widecheck"?(d=420,o=2364,p=.42,m=l+"4"):(d=312,o=2340,p=.34,m="tilde4");else{var x=[1,1,2,2,3,3][u];l==="widehat"||l==="widecheck"?(o=[0,1062,2364,2364,2364][x],d=[0,239,300,360,420][x],p=[0,.24,.3,.3,.36,.42][x],m=l+x):(o=[0,600,1033,2339,2340][x],d=[0,260,286,306,312][x],p=[0,.26,.286,.3,.306,.34][x],m="tilde"+x)}var g=new Pa(m),w=new Ri([g],{width:"100%",height:me(p),viewBox:"0 0 "+o+" "+d,preserveAspectRatio:"none"});return{span:V.makeSvgSpan([],[w],n),minWidth:0,height:p}}else{var v=[],$=gW[l],[_,C,k]=$,S=k/1e3,L=_.length,U,F;if(L===1){var q=$[3];U=["hide-tail"],F=[q]}else if(L===2)U=["halfarrow-left","halfarrow-right"],F=["xMinYMin","xMaxYMin"];else if(L===3)U=["brace-left","brace-center","brace-right"],F=["xMinYMin","xMidYMin","xMaxYMin"];else throw new Error(`Correct katexImagesData or update code here to support
                    `+L+" children.");for(var G=0;G<L;G++){var H=new Pa(_[G]),ne=new Ri([H],{width:"400em",height:me(S),viewBox:"0 0 "+o+" "+k,preserveAspectRatio:F[G]+" slice"}),K=V.makeSvgSpan([U[G]],[ne],n);if(L===1)return{span:K,minWidth:C,height:S};K.style.height=me(S),v.push(K)}return{span:V.makeSpan(["stretchy"],v,n),minWidth:C,height:S}}}var{span:i,minWidth:a,height:s}=r();return i.height=s,i.style.height=me(s),a>0&&(i.style.minWidth=me(a)),i},xW=function(t,n,r,i,a){var s,o=t.height+t.depth+r+i;if(/fbox|color|angl/.test(n)){if(s=V.makeSpan(["stretchy",n],[],a),n==="fbox"){var l=a.color&&a.getColor();l&&(s.style.borderColor=l)}}else{var c=[];/^[bx]cancel$/.test(n)&&c.push(new eg({x1:"0",y1:"0",x2:"100%",y2:"100%","stroke-width":"0.046em"})),/^x?cancel$/.test(n)&&c.push(new eg({x1:"0",y1:"100%",x2:"100%",y2:"0","stroke-width":"0.046em"}));var u=new Ri(c,{width:"100%",height:me(o)});s=V.makeSvgSpan([],[u],a)}return s.height=o,s.style.height=me(o),s},zi={encloseSpan:xW,mathMLnode:fW,svgSpan:vW};function Re(e,t){if(!e||e.type!==t)throw new Error("Expected node of type "+t+", but got "+(e?"node of type "+e.type:String(e)));return e}function q2(e){var t=vm(e);if(!t)throw new Error("Expected node of symbol group type, but got "+(e?"node of type "+e.type:String(e)));return t}function vm(e){return e&&(e.type==="atom"||HG.hasOwnProperty(e.type))?e:null}var G2=(e,t)=>{var n,r,i;e&&e.type==="supsub"?(r=Re(e.base,"accent"),n=r.base,e.base=n,i=jG(Ge(e,t)),e.base=r):(r=Re(e,"accent"),n=r.base);var a=Ge(n,t.havingCrampedStyle()),s=r.isShifty&&ye.isCharacterBox(n),o=0;if(s){var l=ye.getBaseElem(n),c=Ge(l,t.havingCrampedStyle());o=L3(c).skew}var u=r.label==="\\c",d=u?a.height+a.depth:Math.min(a.height,t.fontMetrics().xHeight),m;if(r.isStretchy)m=zi.svgSpan(r,t),m=V.makeVList({positionType:"firstBaseline",children:[{type:"elem",elem:a},{type:"elem",elem:m,wrapperClasses:["svg-align"],wrapperStyle:o>0?{width:"calc(100% - "+me(2*o)+")",marginLeft:me(2*o)}:void 0}]},t);else{var p,x;r.label==="\\vec"?(p=V.staticSvg("vec",t),x=V.svgData.vec[1]):(p=V.makeOrd({mode:r.mode,text:r.label},t,"textord"),p=L3(p),p.italic=0,x=p.width,u&&(d+=p.depth)),m=V.makeSpan(["accent-body"],[p]);var g=r.label==="\\textcircled";g&&(m.classes.push("accent-full"),d=a.height);var w=o;g||(w-=x/2),m.style.left=me(w),r.label==="\\textcircled"&&(m.style.top=".2em"),m=V.makeVList({positionType:"firstBaseline",children:[{type:"elem",elem:a},{type:"kern",size:-d},{type:"elem",elem:m}]},t)}var v=V.makeSpan(["mord","accent"],[m],t);return i?(i.children[0]=v,i.height=Math.max(v.height,i.height),i.classes[0]="mord",i):v},jw=(e,t)=>{var n=e.isStretchy?zi.mathMLnode(e.label):new ie.MathNode("mo",[fr(e.label,e.mode)]),r=new ie.MathNode("mover",[et(e.base,t),n]);return r.setAttribute("accent","true"),r},$W=new RegExp(["\\acute","\\grave","\\ddot","\\tilde","\\bar","\\breve","\\check","\\hat","\\vec","\\dot","\\mathring"].map(e=>"\\"+e).join("|"));fe({type:"accent",names:["\\acute","\\grave","\\ddot","\\tilde","\\bar","\\breve","\\check","\\hat","\\vec","\\dot","\\mathring","\\widecheck","\\widehat","\\widetilde","\\overrightarrow","\\overleftarrow","\\Overrightarrow","\\overleftrightarrow","\\overgroup","\\overlinesegment","\\overleftharpoon","\\overrightharpoon"],props:{numArgs:1},handler:(e,t)=>{var n=f0(t[0]),r=!$W.test(e.funcName),i=!r||e.funcName==="\\widehat"||e.funcName==="\\widetilde"||e.funcName==="\\widecheck";return{type:"accent",mode:e.parser.mode,label:e.funcName,isStretchy:r,isShifty:i,base:n}},htmlBuilder:G2,mathmlBuilder:jw});fe({type:"accent",names:["\\'","\\`","\\^","\\~","\\=","\\u","\\.",'\\"',"\\c","\\r","\\H","\\v","\\textcircled"],props:{numArgs:1,allowedInText:!0,allowedInMath:!0,argTypes:["primitive"]},handler:(e,t)=>{var n=t[0],r=e.parser.mode;return r==="math"&&(e.parser.settings.reportNonstrict("mathVsTextAccents","LaTeX's accent "+e.funcName+" works only in text mode"),r="text"),{type:"accent",mode:r,label:e.funcName,isStretchy:!1,isShifty:!0,base:n}},htmlBuilder:G2,mathmlBuilder:jw});fe({type:"accentUnder",names:["\\underleftarrow","\\underrightarrow","\\underleftrightarrow","\\undergroup","\\underlinesegment","\\utilde"],props:{numArgs:1},handler:(e,t)=>{var{parser:n,funcName:r}=e,i=t[0];return{type:"accentUnder",mode:n.mode,label:r,base:i}},htmlBuilder:(e,t)=>{var n=Ge(e.base,t),r=zi.svgSpan(e,t),i=e.label==="\\utilde"?.12:0,a=V.makeVList({positionType:"top",positionData:n.height,children:[{type:"elem",elem:r,wrapperClasses:["svg-align"]},{type:"kern",size:i},{type:"elem",elem:n}]},t);return V.makeSpan(["mord","accentunder"],[a],t)},mathmlBuilder:(e,t)=>{var n=zi.mathMLnode(e.label),r=new ie.MathNode("munder",[et(e.base,t),n]);return r.setAttribute("accentunder","true"),r}});var Ku=e=>{var t=new ie.MathNode("mpadded",e?[e]:[]);return t.setAttribute("width","+0.6em"),t.setAttribute("lspace","0.3em"),t};fe({type:"xArrow",names:["\\xleftarrow","\\xrightarrow","\\xLeftarrow","\\xRightarrow","\\xleftrightarrow","\\xLeftrightarrow","\\xhookleftarrow","\\xhookrightarrow","\\xmapsto","\\xrightharpoondown","\\xrightharpoonup","\\xleftharpoondown","\\xleftharpoonup","\\xrightleftharpoons","\\xleftrightharpoons","\\xlongequal","\\xtwoheadrightarrow","\\xtwoheadleftarrow","\\xtofrom","\\xrightleftarrows","\\xrightequilibrium","\\xleftequilibrium","\\\\cdrightarrow","\\\\cdleftarrow","\\\\cdlongequal"],props:{numArgs:1,numOptionalArgs:1},handler(e,t,n){var{parser:r,funcName:i}=e;return{type:"xArrow",mode:r.mode,label:i,body:t[0],below:n[0]}},htmlBuilder(e,t){var n=t.style,r=t.havingStyle(n.sup()),i=V.wrapFragment(Ge(e.body,r,t),t),a=e.label.slice(0,2)==="\\x"?"x":"cd";i.classes.push(a+"-arrow-pad");var s;e.below&&(r=t.havingStyle(n.sub()),s=V.wrapFragment(Ge(e.below,r,t),t),s.classes.push(a+"-arrow-pad"));var o=zi.svgSpan(e,t),l=-t.fontMetrics().axisHeight+.5*o.height,c=-t.fontMetrics().axisHeight-.5*o.height-.111;(i.depth>.25||e.label==="\\xleftequilibrium")&&(c-=i.depth);var u;if(s){var d=-t.fontMetrics().axisHeight+s.height+.5*o.height+.111;u=V.makeVList({positionType:"individualShift",children:[{type:"elem",elem:i,shift:c},{type:"elem",elem:o,shift:l},{type:"elem",elem:s,shift:d}]},t)}else u=V.makeVList({positionType:"individualShift",children:[{type:"elem",elem:i,shift:c},{type:"elem",elem:o,shift:l}]},t);return u.children[0].children[0].children[1].classes.push("svg-align"),V.makeSpan(["mrel","x-arrow"],[u],t)},mathmlBuilder(e,t){var n=zi.mathMLnode(e.label);n.setAttribute("minsize",e.label.charAt(0)==="x"?"1.75em":"3.0em");var r;if(e.body){var i=Ku(et(e.body,t));if(e.below){var a=Ku(et(e.below,t));r=new ie.MathNode("munderover",[n,a,i])}else r=new ie.MathNode("mover",[n,i])}else if(e.below){var s=Ku(et(e.below,t));r=new ie.MathNode("munder",[n,s])}else r=Ku(),r=new ie.MathNode("mover",[n,r]);return r}});var yW=V.makeSpan;function Vw(e,t){var n=Bt(e.body,t,!0);return yW([e.mclass],n,t)}function Hw(e,t){var n,r=Un(e.body,t);return e.mclass==="minner"?n=new ie.MathNode("mpadded",r):e.mclass==="mord"?e.isCharacterBox?(n=r[0],n.type="mi"):n=new ie.MathNode("mi",r):(e.isCharacterBox?(n=r[0],n.type="mo"):n=new ie.MathNode("mo",r),e.mclass==="mbin"?(n.attributes.lspace="0.22em",n.attributes.rspace="0.22em"):e.mclass==="mpunct"?(n.attributes.lspace="0em",n.attributes.rspace="0.17em"):e.mclass==="mopen"||e.mclass==="mclose"?(n.attributes.lspace="0em",n.attributes.rspace="0em"):e.mclass==="minner"&&(n.attributes.lspace="0.0556em",n.attributes.width="+0.1111em")),n}fe({type:"mclass",names:["\\mathord","\\mathbin","\\mathrel","\\mathopen","\\mathclose","\\mathpunct","\\mathinner"],props:{numArgs:1,primitive:!0},handler(e,t){var{parser:n,funcName:r}=e,i=t[0];return{type:"mclass",mode:n.mode,mclass:"m"+r.slice(5),body:At(i),isCharacterBox:ye.isCharacterBox(i)}},htmlBuilder:Vw,mathmlBuilder:Hw});var xm=e=>{var t=e.type==="ordgroup"&&e.body.length?e.body[0]:e;return t.type==="atom"&&(t.family==="bin"||t.family==="rel")?"m"+t.family:"mord"};fe({type:"mclass",names:["\\@binrel"],props:{numArgs:2},handler(e,t){var{parser:n}=e;return{type:"mclass",mode:n.mode,mclass:xm(t[0]),body:At(t[1]),isCharacterBox:ye.isCharacterBox(t[1])}}});fe({type:"mclass",names:["\\stackrel","\\overset","\\underset"],props:{numArgs:2},handler(e,t){var{parser:n,funcName:r}=e,i=t[1],a=t[0],s;r!=="\\stackrel"?s=xm(i):s="mrel";var o={type:"op",mode:i.mode,limits:!0,alwaysHandleSupSub:!0,parentIsSupSub:!1,symbol:!1,suppressBaseShift:r!=="\\stackrel",body:At(i)},l={type:"supsub",mode:a.mode,base:o,sup:r==="\\underset"?null:a,sub:r==="\\underset"?a:null};return{type:"mclass",mode:n.mode,mclass:s,body:[l],isCharacterBox:ye.isCharacterBox(l)}},htmlBuilder:Vw,mathmlBuilder:Hw});fe({type:"pmb",names:["\\pmb"],props:{numArgs:1,allowedInText:!0},handler(e,t){var{parser:n}=e;return{type:"pmb",mode:n.mode,mclass:xm(t[0]),body:At(t[0])}},htmlBuilder(e,t){var n=Bt(e.body,t,!0),r=V.makeSpan([e.mclass],n,t);return r.style.textShadow="0.02em 0.01em 0.04px",r},mathmlBuilder(e,t){var n=Un(e.body,t),r=new ie.MathNode("mstyle",n);return r.setAttribute("style","text-shadow: 0.02em 0.01em 0.04px"),r}});var _W={">":"\\\\cdrightarrow","<":"\\\\cdleftarrow","=":"\\\\cdlongequal",A:"\\uparrow",V:"\\downarrow","|":"\\Vert",".":"no arrow"},z3=()=>({type:"styling",body:[],mode:"math",style:"display"}),B3=e=>e.type==="textord"&&e.text==="@",wW=(e,t)=>(e.type==="mathord"||e.type==="atom")&&e.text===t;function TW(e,t,n){var r=_W[e];switch(r){case"\\\\cdrightarrow":case"\\\\cdleftarrow":return n.callFunction(r,[t[0]],[t[1]]);case"\\uparrow":case"\\downarrow":{var i=n.callFunction("\\\\cdleft",[t[0]],[]),a={type:"atom",text:r,mode:"math",family:"rel"},s=n.callFunction("\\Big",[a],[]),o=n.callFunction("\\\\cdright",[t[1]],[]),l={type:"ordgroup",mode:"math",body:[i,s,o]};return n.callFunction("\\\\cdparent",[l],[])}case"\\\\cdlongequal":return n.callFunction("\\\\cdlongequal",[],[]);case"\\Vert":{var c={type:"textord",text:"\\Vert",mode:"math"};return n.callFunction("\\Big",[c],[])}default:return{type:"textord",text:" ",mode:"math"}}}function EW(e){var t=[];for(e.gullet.beginGroup(),e.gullet.macros.set("\\cr","\\\\\\relax"),e.gullet.beginGroup();;){t.push(e.parseExpression(!1,"\\\\")),e.gullet.endGroup(),e.gullet.beginGroup();var n=e.fetch().text;if(n==="&"||n==="\\\\")e.consume();else if(n==="\\end"){t[t.length-1].length===0&&t.pop();break}else throw new oe("Expected \\\\ or \\cr or \\end",e.nextToken)}for(var r=[],i=[r],a=0;a<t.length;a++){for(var s=t[a],o=z3(),l=0;l<s.length;l++)if(!B3(s[l]))o.body.push(s[l]);else{r.push(o),l+=1;var c=q2(s[l]).text,u=new Array(2);if(u[0]={type:"ordgroup",mode:"math",body:[]},u[1]={type:"ordgroup",mode:"math",body:[]},!("=|.".indexOf(c)>-1))if("<>AV".indexOf(c)>-1)for(var d=0;d<2;d++){for(var m=!0,p=l+1;p<s.length;p++){if(wW(s[p],c)){m=!1,l=p;break}if(B3(s[p]))throw new oe("Missing a "+c+" character to complete a CD arrow.",s[p]);u[d].body.push(s[p])}if(m)throw new oe("Missing a "+c+" character to complete a CD arrow.",s[l])}else throw new oe('Expected one of "<>AV=|." after @',s[l]);var x=TW(c,u,e),g={type:"styling",body:[x],mode:"math",style:"display"};r.push(g),o=z3()}a%2===0?r.push(o):r.shift(),r=[],i.push(r)}e.gullet.endGroup(),e.gullet.endGroup();var w=new Array(i[0].length).fill({type:"align",align:"c",pregap:.25,postgap:.25});return{type:"array",mode:"math",body:i,arraystretch:1,addJot:!0,rowGaps:[null],cols:w,colSeparationType:"CD",hLinesBeforeRow:new Array(i.length+1).fill([])}}fe({type:"cdlabel",names:["\\\\cdleft","\\\\cdright"],props:{numArgs:1},handler(e,t){var{parser:n,funcName:r}=e;return{type:"cdlabel",mode:n.mode,side:r.slice(4),label:t[0]}},htmlBuilder(e,t){var n=t.havingStyle(t.style.sup()),r=V.wrapFragment(Ge(e.label,n,t),t);return r.classes.push("cd-label-"+e.side),r.style.bottom=me(.8-r.depth),r.height=0,r.depth=0,r},mathmlBuilder(e,t){var n=new ie.MathNode("mrow",[et(e.label,t)]);return n=new ie.MathNode("mpadded",[n]),n.setAttribute("width","0"),e.side==="left"&&n.setAttribute("lspace","-1width"),n.setAttribute("voffset","0.7em"),n=new ie.MathNode("mstyle",[n]),n.setAttribute("displaystyle","false"),n.setAttribute("scriptlevel","1"),n}});fe({type:"cdlabelparent",names:["\\\\cdparent"],props:{numArgs:1},handler(e,t){var{parser:n}=e;return{type:"cdlabelparent",mode:n.mode,fragment:t[0]}},htmlBuilder(e,t){var n=V.wrapFragment(Ge(e.fragment,t),t);return n.classes.push("cd-vert-arrow"),n},mathmlBuilder(e,t){return new ie.MathNode("mrow",[et(e.fragment,t)])}});fe({type:"textord",names:["\\@char"],props:{numArgs:1,allowedInText:!0},handler(e,t){for(var{parser:n}=e,r=Re(t[0],"ordgroup"),i=r.body,a="",s=0;s<i.length;s++){var o=Re(i[s],"textord");a+=o.text}var l=parseInt(a),c;if(isNaN(l))throw new oe("\\@char has non-numeric argument "+a);if(l<0||l>=1114111)throw new oe("\\@char with invalid code point "+a);return l<=65535?c=String.fromCharCode(l):(l-=65536,c=String.fromCharCode((l>>10)+55296,(l&1023)+56320)),{type:"textord",mode:n.mode,text:c}}});var qw=(e,t)=>{var n=Bt(e.body,t.withColor(e.color),!1);return V.makeFragment(n)},Gw=(e,t)=>{var n=Un(e.body,t.withColor(e.color)),r=new ie.MathNode("mstyle",n);return r.setAttribute("mathcolor",e.color),r};fe({type:"color",names:["\\textcolor"],props:{numArgs:2,allowedInText:!0,argTypes:["color","original"]},handler(e,t){var{parser:n}=e,r=Re(t[0],"color-token").color,i=t[1];return{type:"color",mode:n.mode,color:r,body:At(i)}},htmlBuilder:qw,mathmlBuilder:Gw});fe({type:"color",names:["\\color"],props:{numArgs:1,allowedInText:!0,argTypes:["color"]},handler(e,t){var{parser:n,breakOnTokenText:r}=e,i=Re(t[0],"color-token").color;n.gullet.macros.set("\\current@color",i);var a=n.parseExpression(!0,r);return{type:"color",mode:n.mode,color:i,body:a}},htmlBuilder:qw,mathmlBuilder:Gw});fe({type:"cr",names:["\\\\"],props:{numArgs:0,numOptionalArgs:0,allowedInText:!0},handler(e,t,n){var{parser:r}=e,i=r.gullet.future().text==="["?r.parseSizeGroup(!0):null,a=!r.settings.displayMode||!r.settings.useStrictBehavior("newLineInDisplayMode","In LaTeX, \\\\ or \\newline does nothing in display mode");return{type:"cr",mode:r.mode,newLine:a,size:i&&Re(i,"size").value}},htmlBuilder(e,t){var n=V.makeSpan(["mspace"],[],t);return e.newLine&&(n.classes.push("newline"),e.size&&(n.style.marginTop=me(gt(e.size,t)))),n},mathmlBuilder(e,t){var n=new ie.MathNode("mspace");return e.newLine&&(n.setAttribute("linebreak","newline"),e.size&&n.setAttribute("height",me(gt(e.size,t)))),n}});var ig={"\\global":"\\global","\\long":"\\\\globallong","\\\\globallong":"\\\\globallong","\\def":"\\gdef","\\gdef":"\\gdef","\\edef":"\\xdef","\\xdef":"\\xdef","\\let":"\\\\globallet","\\futurelet":"\\\\globalfuture"},Ww=e=>{var t=e.text;if(/^(?:[\\{}$&#^_]|EOF)$/.test(t))throw new oe("Expected a control sequence",e);return t},kW=e=>{var t=e.gullet.popToken();return t.text==="="&&(t=e.gullet.popToken(),t.text===" "&&(t=e.gullet.popToken())),t},Kw=(e,t,n,r)=>{var i=e.gullet.macros.get(n.text);i==null&&(n.noexpand=!0,i={tokens:[n],numArgs:0,unexpandable:!e.gullet.isExpandable(n.text)}),e.gullet.macros.set(t,i,r)};fe({type:"internal",names:["\\global","\\long","\\\\globallong"],props:{numArgs:0,allowedInText:!0},handler(e){var{parser:t,funcName:n}=e;t.consumeSpaces();var r=t.fetch();if(ig[r.text])return(n==="\\global"||n==="\\\\globallong")&&(r.text=ig[r.text]),Re(t.parseFunction(),"internal");throw new oe("Invalid token after macro prefix",r)}});fe({type:"internal",names:["\\def","\\gdef","\\edef","\\xdef"],props:{numArgs:0,allowedInText:!0,primitive:!0},handler(e){var{parser:t,funcName:n}=e,r=t.gullet.popToken(),i=r.text;if(/^(?:[\\{}$&#^_]|EOF)$/.test(i))throw new oe("Expected a control sequence",r);for(var a=0,s,o=[[]];t.gullet.future().text!=="{";)if(r=t.gullet.popToken(),r.text==="#"){if(t.gullet.future().text==="{"){s=t.gullet.future(),o[a].push("{");break}if(r=t.gullet.popToken(),!/^[1-9]$/.test(r.text))throw new oe('Invalid argument number "'+r.text+'"');if(parseInt(r.text)!==a+1)throw new oe('Argument number "'+r.text+'" out of order');a++,o.push([])}else{if(r.text==="EOF")throw new oe("Expected a macro definition");o[a].push(r.text)}var{tokens:l}=t.gullet.consumeArg();return s&&l.unshift(s),(n==="\\edef"||n==="\\xdef")&&(l=t.gullet.expandTokens(l),l.reverse()),t.gullet.macros.set(i,{tokens:l,numArgs:a,delimiters:o},n===ig[n]),{type:"internal",mode:t.mode}}});fe({type:"internal",names:["\\let","\\\\globallet"],props:{numArgs:0,allowedInText:!0,primitive:!0},handler(e){var{parser:t,funcName:n}=e,r=Ww(t.gullet.popToken());t.gullet.consumeSpaces();var i=kW(t);return Kw(t,r,i,n==="\\\\globallet"),{type:"internal",mode:t.mode}}});fe({type:"internal",names:["\\futurelet","\\\\globalfuture"],props:{numArgs:0,allowedInText:!0,primitive:!0},handler(e){var{parser:t,funcName:n}=e,r=Ww(t.gullet.popToken()),i=t.gullet.popToken(),a=t.gullet.popToken();return Kw(t,r,a,n==="\\\\globalfuture"),t.gullet.pushToken(a),t.gullet.pushToken(i),{type:"internal",mode:t.mode}}});var Tl=function(t,n,r){var i=at.math[t]&&at.math[t].replace,a=B2(i||t,n,r);if(!a)throw new Error("Unsupported symbol "+t+" and font size "+n+".");return a},W2=function(t,n,r,i){var a=r.havingBaseStyle(n),s=V.makeSpan(i.concat(a.sizingClasses(r)),[t],r),o=a.sizeMultiplier/r.sizeMultiplier;return s.height*=o,s.depth*=o,s.maxFontSize=a.sizeMultiplier,s},Yw=function(t,n,r){var i=n.havingBaseStyle(r),a=(1-n.sizeMultiplier/i.sizeMultiplier)*n.fontMetrics().axisHeight;t.classes.push("delimcenter"),t.style.top=me(a),t.height-=a,t.depth+=a},SW=function(t,n,r,i,a,s){var o=V.makeSymbol(t,"Main-Regular",a,i),l=W2(o,n,i,s);return r&&Yw(l,i,n),l},NW=function(t,n,r,i){return V.makeSymbol(t,"Size"+n+"-Regular",r,i)},Xw=function(t,n,r,i,a,s){var o=NW(t,n,a,i),l=W2(V.makeSpan(["delimsizing","size"+n],[o],i),we.TEXT,i,s);return r&&Yw(l,i,we.TEXT),l},Kh=function(t,n,r){var i;n==="Size1-Regular"?i="delim-size1":i="delim-size4";var a=V.makeSpan(["delimsizinginner",i],[V.makeSpan([],[V.makeSymbol(t,n,r)])]);return{type:"elem",elem:a}},Yh=function(t,n,r){var i=Jr["Size4-Regular"][t.charCodeAt(0)]?Jr["Size4-Regular"][t.charCodeAt(0)][4]:Jr["Size1-Regular"][t.charCodeAt(0)][4],a=new Pa("inner",IG(t,Math.round(1e3*n))),s=new Ri([a],{width:me(i),height:me(n),style:"width:"+me(i),viewBox:"0 0 "+1e3*i+" "+Math.round(1e3*n),preserveAspectRatio:"xMinYMin"}),o=V.makeSvgSpan([],[s],r);return o.height=n,o.style.height=me(n),o.style.width=me(i),{type:"elem",elem:o}},ag=.008,Yu={type:"kern",size:-1*ag},CW=["|","\\lvert","\\rvert","\\vert"],AW=["\\|","\\lVert","\\rVert","\\Vert"],Qw=function(t,n,r,i,a,s){var o,l,c,u,d="",m=0;o=c=u=t,l=null;var p="Size1-Regular";t==="\\uparrow"?c=u="⏐":t==="\\Uparrow"?c=u="‖":t==="\\downarrow"?o=c="⏐":t==="\\Downarrow"?o=c="‖":t==="\\updownarrow"?(o="\\uparrow",c="⏐",u="\\downarrow"):t==="\\Updownarrow"?(o="\\Uparrow",c="‖",u="\\Downarrow"):ye.contains(CW,t)?(c="∣",d="vert",m=333):ye.contains(AW,t)?(c="∥",d="doublevert",m=556):t==="["||t==="\\lbrack"?(o="⎡",c="⎢",u="⎣",p="Size4-Regular",d="lbrack",m=667):t==="]"||t==="\\rbrack"?(o="⎤",c="⎥",u="⎦",p="Size4-Regular",d="rbrack",m=667):t==="\\lfloor"||t==="⌊"?(c=o="⎢",u="⎣",p="Size4-Regular",d="lfloor",m=667):t==="\\lceil"||t==="⌈"?(o="⎡",c=u="⎢",p="Size4-Regular",d="lceil",m=667):t==="\\rfloor"||t==="⌋"?(c=o="⎥",u="⎦",p="Size4-Regular",d="rfloor",m=667):t==="\\rceil"||t==="⌉"?(o="⎤",c=u="⎥",p="Size4-Regular",d="rceil",m=667):t==="("||t==="\\lparen"?(o="⎛",c="⎜",u="⎝",p="Size4-Regular",d="lparen",m=875):t===")"||t==="\\rparen"?(o="⎞",c="⎟",u="⎠",p="Size4-Regular",d="rparen",m=875):t==="\\{"||t==="\\lbrace"?(o="⎧",l="⎨",u="⎩",c="⎪",p="Size4-Regular"):t==="\\}"||t==="\\rbrace"?(o="⎫",l="⎬",u="⎭",c="⎪",p="Size4-Regular"):t==="\\lgroup"||t==="⟮"?(o="⎧",u="⎩",c="⎪",p="Size4-Regular"):t==="\\rgroup"||t==="⟯"?(o="⎫",u="⎭",c="⎪",p="Size4-Regular"):t==="\\lmoustache"||t==="⎰"?(o="⎧",u="⎭",c="⎪",p="Size4-Regular"):(t==="\\rmoustache"||t==="⎱")&&(o="⎫",u="⎩",c="⎪",p="Size4-Regular");var x=Tl(o,p,a),g=x.height+x.depth,w=Tl(c,p,a),v=w.height+w.depth,$=Tl(u,p,a),_=$.height+$.depth,C=0,k=1;if(l!==null){var S=Tl(l,p,a);C=S.height+S.depth,k=2}var L=g+_+C,U=Math.max(0,Math.ceil((n-L)/(k*v))),F=L+U*k*v,q=i.fontMetrics().axisHeight;r&&(q*=i.sizeMultiplier);var G=F/2-q,H=[];if(d.length>0){var ne=F-g-_,K=Math.round(F*1e3),te=UG(d,Math.round(ne*1e3)),J=new Pa(d,te),ae=(m/1e3).toFixed(3)+"em",B=(K/1e3).toFixed(3)+"em",X=new Ri([J],{width:ae,height:B,viewBox:"0 0 "+m+" "+K}),P=V.makeSvgSpan([],[X],i);P.height=K/1e3,P.style.width=ae,P.style.height=B,H.push({type:"elem",elem:P})}else{if(H.push(Kh(u,p,a)),H.push(Yu),l===null){var se=F-g-_+2*ag;H.push(Yh(c,se,i))}else{var he=(F-g-_-C)/2+2*ag;H.push(Yh(c,he,i)),H.push(Yu),H.push(Kh(l,p,a)),H.push(Yu),H.push(Yh(c,he,i))}H.push(Yu),H.push(Kh(o,p,a))}var D=i.havingBaseStyle(we.TEXT),Ee=V.makeVList({positionType:"bottom",positionData:G,children:H},D);return W2(V.makeSpan(["delimsizing","mult"],[Ee],D),we.TEXT,i,s)},Xh=80,Qh=.08,Zh=function(t,n,r,i,a){var s=PG(t,i,r),o=new Pa(t,s),l=new Ri([o],{width:"400em",height:me(n),viewBox:"0 0 400000 "+r,preserveAspectRatio:"xMinYMin slice"});return V.makeSvgSpan(["hide-tail"],[l],a)},LW=function(t,n){var r=n.havingBaseSizing(),i=tT("\\surd",t*r.sizeMultiplier,eT,r),a=r.sizeMultiplier,s=Math.max(0,n.minRuleThickness-n.fontMetrics().sqrtRuleThickness),o,l=0,c=0,u=0,d;return i.type==="small"?(u=1e3+1e3*s+Xh,t<1?a=1:t<1.4&&(a=.7),l=(1+s+Qh)/a,c=(1+s)/a,o=Zh("sqrtMain",l,u,s,n),o.style.minWidth="0.853em",d=.833/a):i.type==="large"?(u=(1e3+Xh)*ql[i.size],c=(ql[i.size]+s)/a,l=(ql[i.size]+s+Qh)/a,o=Zh("sqrtSize"+i.size,l,u,s,n),o.style.minWidth="1.02em",d=1/a):(l=t+s+Qh,c=t+s,u=Math.floor(1e3*t+s)+Xh,o=Zh("sqrtTall",l,u,s,n),o.style.minWidth="0.742em",d=1.056),o.height=c,o.style.height=me(l),{span:o,advanceWidth:d,ruleWidth:(n.fontMetrics().sqrtRuleThickness+s)*a}},Zw=["(","\\lparen",")","\\rparen","[","\\lbrack","]","\\rbrack","\\{","\\lbrace","\\}","\\rbrace","\\lfloor","\\rfloor","⌊","⌋","\\lceil","\\rceil","⌈","⌉","\\surd"],PW=["\\uparrow","\\downarrow","\\updownarrow","\\Uparrow","\\Downarrow","\\Updownarrow","|","\\|","\\vert","\\Vert","\\lvert","\\rvert","\\lVert","\\rVert","\\lgroup","\\rgroup","⟮","⟯","\\lmoustache","\\rmoustache","⎰","⎱"],Jw=["<",">","\\langle","\\rangle","/","\\backslash","\\lt","\\gt"],ql=[0,1.2,1.8,2.4,3],IW=function(t,n,r,i,a){if(t==="<"||t==="\\lt"||t==="⟨"?t="\\langle":(t===">"||t==="\\gt"||t==="⟩")&&(t="\\rangle"),ye.contains(Zw,t)||ye.contains(Jw,t))return Xw(t,n,!1,r,i,a);if(ye.contains(PW,t))return Qw(t,ql[n],!1,r,i,a);throw new oe("Illegal delimiter: '"+t+"'")},UW=[{type:"small",style:we.SCRIPTSCRIPT},{type:"small",style:we.SCRIPT},{type:"small",style:we.TEXT},{type:"large",size:1},{type:"large",size:2},{type:"large",size:3},{type:"large",size:4}],DW=[{type:"small",style:we.SCRIPTSCRIPT},{type:"small",style:we.SCRIPT},{type:"small",style:we.TEXT},{type:"stack"}],eT=[{type:"small",style:we.SCRIPTSCRIPT},{type:"small",style:we.SCRIPT},{type:"small",style:we.TEXT},{type:"large",size:1},{type:"large",size:2},{type:"large",size:3},{type:"large",size:4},{type:"stack"}],MW=function(t){if(t.type==="small")return"Main-Regular";if(t.type==="large")return"Size"+t.size+"-Regular";if(t.type==="stack")return"Size4-Regular";throw new Error("Add support for delim type '"+t.type+"' here.")},tT=function(t,n,r,i){for(var a=Math.min(2,3-i.style.size),s=a;s<r.length&&r[s].type!=="stack";s++){var o=Tl(t,MW(r[s]),"math"),l=o.height+o.depth;if(r[s].type==="small"){var c=i.havingBaseStyle(r[s].style);l*=c.sizeMultiplier}if(l>n)return r[s]}return r[r.length-1]},nT=function(t,n,r,i,a,s){t==="<"||t==="\\lt"||t==="⟨"?t="\\langle":(t===">"||t==="\\gt"||t==="⟩")&&(t="\\rangle");var o;ye.contains(Jw,t)?o=UW:ye.contains(Zw,t)?o=eT:o=DW;var l=tT(t,n,o,i);return l.type==="small"?SW(t,l.style,r,i,a,s):l.type==="large"?Xw(t,l.size,r,i,a,s):Qw(t,n,r,i,a,s)},RW=function(t,n,r,i,a,s){var o=i.fontMetrics().axisHeight*i.sizeMultiplier,l=901,c=5/i.fontMetrics().ptPerEm,u=Math.max(n-o,r+o),d=Math.max(u/500*l,2*u-c);return nT(t,d,!0,i,a,s)},Ni={sqrtImage:LW,sizedDelim:IW,sizeToMaxHeight:ql,customSizedDelim:nT,leftRightDelim:RW},F3={"\\bigl":{mclass:"mopen",size:1},"\\Bigl":{mclass:"mopen",size:2},"\\biggl":{mclass:"mopen",size:3},"\\Biggl":{mclass:"mopen",size:4},"\\bigr":{mclass:"mclose",size:1},"\\Bigr":{mclass:"mclose",size:2},"\\biggr":{mclass:"mclose",size:3},"\\Biggr":{mclass:"mclose",size:4},"\\bigm":{mclass:"mrel",size:1},"\\Bigm":{mclass:"mrel",size:2},"\\biggm":{mclass:"mrel",size:3},"\\Biggm":{mclass:"mrel",size:4},"\\big":{mclass:"mord",size:1},"\\Big":{mclass:"mord",size:2},"\\bigg":{mclass:"mord",size:3},"\\Bigg":{mclass:"mord",size:4}},OW=["(","\\lparen",")","\\rparen","[","\\lbrack","]","\\rbrack","\\{","\\lbrace","\\}","\\rbrace","\\lfloor","\\rfloor","⌊","⌋","\\lceil","\\rceil","⌈","⌉","<",">","\\langle","⟨","\\rangle","⟩","\\lt","\\gt","\\lvert","\\rvert","\\lVert","\\rVert","\\lgroup","\\rgroup","⟮","⟯","\\lmoustache","\\rmoustache","⎰","⎱","/","\\backslash","|","\\vert","\\|","\\Vert","\\uparrow","\\Uparrow","\\downarrow","\\Downarrow","\\updownarrow","\\Updownarrow","."];function $m(e,t){var n=vm(e);if(n&&ye.contains(OW,n.text))return n;throw n?new oe("Invalid delimiter '"+n.text+"' after '"+t.funcName+"'",e):new oe("Invalid delimiter type '"+e.type+"'",e)}fe({type:"delimsizing",names:["\\bigl","\\Bigl","\\biggl","\\Biggl","\\bigr","\\Bigr","\\biggr","\\Biggr","\\bigm","\\Bigm","\\biggm","\\Biggm","\\big","\\Big","\\bigg","\\Bigg"],props:{numArgs:1,argTypes:["primitive"]},handler:(e,t)=>{var n=$m(t[0],e);return{type:"delimsizing",mode:e.parser.mode,size:F3[e.funcName].size,mclass:F3[e.funcName].mclass,delim:n.text}},htmlBuilder:(e,t)=>e.delim==="."?V.makeSpan([e.mclass]):Ni.sizedDelim(e.delim,e.size,t,e.mode,[e.mclass]),mathmlBuilder:e=>{var t=[];e.delim!=="."&&t.push(fr(e.delim,e.mode));var n=new ie.MathNode("mo",t);e.mclass==="mopen"||e.mclass==="mclose"?n.setAttribute("fence","true"):n.setAttribute("fence","false"),n.setAttribute("stretchy","true");var r=me(Ni.sizeToMaxHeight[e.size]);return n.setAttribute("minsize",r),n.setAttribute("maxsize",r),n}});function j3(e){if(!e.body)throw new Error("Bug: The leftright ParseNode wasn't fully parsed.")}fe({type:"leftright-right",names:["\\right"],props:{numArgs:1,primitive:!0},handler:(e,t)=>{var n=e.parser.gullet.macros.get("\\current@color");if(n&&typeof n!="string")throw new oe("\\current@color set to non-string in \\right");return{type:"leftright-right",mode:e.parser.mode,delim:$m(t[0],e).text,color:n}}});fe({type:"leftright",names:["\\left"],props:{numArgs:1,primitive:!0},handler:(e,t)=>{var n=$m(t[0],e),r=e.parser;++r.leftrightDepth;var i=r.parseExpression(!1);--r.leftrightDepth,r.expect("\\right",!1);var a=Re(r.parseFunction(),"leftright-right");return{type:"leftright",mode:r.mode,body:i,left:n.text,right:a.delim,rightColor:a.color}},htmlBuilder:(e,t)=>{j3(e);for(var n=Bt(e.body,t,!0,["mopen","mclose"]),r=0,i=0,a=!1,s=0;s<n.length;s++)n[s].isMiddle?a=!0:(r=Math.max(n[s].height,r),i=Math.max(n[s].depth,i));r*=t.sizeMultiplier,i*=t.sizeMultiplier;var o;if(e.left==="."?o=Ic(t,["mopen"]):o=Ni.leftRightDelim(e.left,r,i,t,e.mode,["mopen"]),n.unshift(o),a)for(var l=1;l<n.length;l++){var c=n[l],u=c.isMiddle;u&&(n[l]=Ni.leftRightDelim(u.delim,r,i,u.options,e.mode,[]))}var d;if(e.right===".")d=Ic(t,["mclose"]);else{var m=e.rightColor?t.withColor(e.rightColor):t;d=Ni.leftRightDelim(e.right,r,i,m,e.mode,["mclose"])}return n.push(d),V.makeSpan(["minner"],n,t)},mathmlBuilder:(e,t)=>{j3(e);var n=Un(e.body,t);if(e.left!=="."){var r=new ie.MathNode("mo",[fr(e.left,e.mode)]);r.setAttribute("fence","true"),n.unshift(r)}if(e.right!=="."){var i=new ie.MathNode("mo",[fr(e.right,e.mode)]);i.setAttribute("fence","true"),e.rightColor&&i.setAttribute("mathcolor",e.rightColor),n.push(i)}return V2(n)}});fe({type:"middle",names:["\\middle"],props:{numArgs:1,primitive:!0},handler:(e,t)=>{var n=$m(t[0],e);if(!e.parser.leftrightDepth)throw new oe("\\middle without preceding \\left",n);return{type:"middle",mode:e.parser.mode,delim:n.text}},htmlBuilder:(e,t)=>{var n;if(e.delim===".")n=Ic(t,[]);else{n=Ni.sizedDelim(e.delim,1,t,e.mode,[]);var r={delim:e.delim,options:t};n.isMiddle=r}return n},mathmlBuilder:(e,t)=>{var n=e.delim==="\\vert"||e.delim==="|"?fr("|","text"):fr(e.delim,e.mode),r=new ie.MathNode("mo",[n]);return r.setAttribute("fence","true"),r.setAttribute("lspace","0.05em"),r.setAttribute("rspace","0.05em"),r}});var K2=(e,t)=>{var n=V.wrapFragment(Ge(e.body,t),t),r=e.label.slice(1),i=t.sizeMultiplier,a,s=0,o=ye.isCharacterBox(e.body);if(r==="sout")a=V.makeSpan(["stretchy","sout"]),a.height=t.fontMetrics().defaultRuleThickness/i,s=-.5*t.fontMetrics().xHeight;else if(r==="phase"){var l=gt({number:.6,unit:"pt"},t),c=gt({number:.35,unit:"ex"},t),u=t.havingBaseSizing();i=i/u.sizeMultiplier;var d=n.height+n.depth+l+c;n.style.paddingLeft=me(d/2+l);var m=Math.floor(1e3*d*i),p=AG(m),x=new Ri([new Pa("phase",p)],{width:"400em",height:me(m/1e3),viewBox:"0 0 400000 "+m,preserveAspectRatio:"xMinYMin slice"});a=V.makeSvgSpan(["hide-tail"],[x],t),a.style.height=me(d),s=n.depth+l+c}else{/cancel/.test(r)?o||n.classes.push("cancel-pad"):r==="angl"?n.classes.push("anglpad"):n.classes.push("boxpad");var g=0,w=0,v=0;/box/.test(r)?(v=Math.max(t.fontMetrics().fboxrule,t.minRuleThickness),g=t.fontMetrics().fboxsep+(r==="colorbox"?0:v),w=g):r==="angl"?(v=Math.max(t.fontMetrics().defaultRuleThickness,t.minRuleThickness),g=4*v,w=Math.max(0,.25-n.depth)):(g=o?.2:0,w=g),a=zi.encloseSpan(n,r,g,w,t),/fbox|boxed|fcolorbox/.test(r)?(a.style.borderStyle="solid",a.style.borderWidth=me(v)):r==="angl"&&v!==.049&&(a.style.borderTopWidth=me(v),a.style.borderRightWidth=me(v)),s=n.depth+w,e.backgroundColor&&(a.style.backgroundColor=e.backgroundColor,e.borderColor&&(a.style.borderColor=e.borderColor))}var $;if(e.backgroundColor)$=V.makeVList({positionType:"individualShift",children:[{type:"elem",elem:a,shift:s},{type:"elem",elem:n,shift:0}]},t);else{var _=/cancel|phase/.test(r)?["svg-align"]:[];$=V.makeVList({positionType:"individualShift",children:[{type:"elem",elem:n,shift:0},{type:"elem",elem:a,shift:s,wrapperClasses:_}]},t)}return/cancel/.test(r)&&($.height=n.height,$.depth=n.depth),/cancel/.test(r)&&!o?V.makeSpan(["mord","cancel-lap"],[$],t):V.makeSpan(["mord"],[$],t)},Y2=(e,t)=>{var n=0,r=new ie.MathNode(e.label.indexOf("colorbox")>-1?"mpadded":"menclose",[et(e.body,t)]);switch(e.label){case"\\cancel":r.setAttribute("notation","updiagonalstrike");break;case"\\bcancel":r.setAttribute("notation","downdiagonalstrike");break;case"\\phase":r.setAttribute("notation","phasorangle");break;case"\\sout":r.setAttribute("notation","horizontalstrike");break;case"\\fbox":r.setAttribute("notation","box");break;case"\\angl":r.setAttribute("notation","actuarial");break;case"\\fcolorbox":case"\\colorbox":if(n=t.fontMetrics().fboxsep*t.fontMetrics().ptPerEm,r.setAttribute("width","+"+2*n+"pt"),r.setAttribute("height","+"+2*n+"pt"),r.setAttribute("lspace",n+"pt"),r.setAttribute("voffset",n+"pt"),e.label==="\\fcolorbox"){var i=Math.max(t.fontMetrics().fboxrule,t.minRuleThickness);r.setAttribute("style","border: "+i+"em solid "+String(e.borderColor))}break;case"\\xcancel":r.setAttribute("notation","updiagonalstrike downdiagonalstrike");break}return e.backgroundColor&&r.setAttribute("mathbackground",e.backgroundColor),r};fe({type:"enclose",names:["\\colorbox"],props:{numArgs:2,allowedInText:!0,argTypes:["color","text"]},handler(e,t,n){var{parser:r,funcName:i}=e,a=Re(t[0],"color-token").color,s=t[1];return{type:"enclose",mode:r.mode,label:i,backgroundColor:a,body:s}},htmlBuilder:K2,mathmlBuilder:Y2});fe({type:"enclose",names:["\\fcolorbox"],props:{numArgs:3,allowedInText:!0,argTypes:["color","color","text"]},handler(e,t,n){var{parser:r,funcName:i}=e,a=Re(t[0],"color-token").color,s=Re(t[1],"color-token").color,o=t[2];return{type:"enclose",mode:r.mode,label:i,backgroundColor:s,borderColor:a,body:o}},htmlBuilder:K2,mathmlBuilder:Y2});fe({type:"enclose",names:["\\fbox"],props:{numArgs:1,argTypes:["hbox"],allowedInText:!0},handler(e,t){var{parser:n}=e;return{type:"enclose",mode:n.mode,label:"\\fbox",body:t[0]}}});fe({type:"enclose",names:["\\cancel","\\bcancel","\\xcancel","\\sout","\\phase"],props:{numArgs:1},handler(e,t){var{parser:n,funcName:r}=e,i=t[0];return{type:"enclose",mode:n.mode,label:r,body:i}},htmlBuilder:K2,mathmlBuilder:Y2});fe({type:"enclose",names:["\\angl"],props:{numArgs:1,argTypes:["hbox"],allowedInText:!1},handler(e,t){var{parser:n}=e;return{type:"enclose",mode:n.mode,label:"\\angl",body:t[0]}}});var rT={};function di(e){for(var{type:t,names:n,props:r,handler:i,htmlBuilder:a,mathmlBuilder:s}=e,o={type:t,numArgs:r.numArgs||0,allowedInText:!1,numOptionalArgs:0,handler:i},l=0;l<n.length;++l)rT[n[l]]=o;a&&(h0[t]=a),s&&(p0[t]=s)}var iT={};function N(e,t){iT[e]=t}function V3(e){var t=[];e.consumeSpaces();var n=e.fetch().text;for(n==="\\relax"&&(e.consume(),e.consumeSpaces(),n=e.fetch().text);n==="\\hline"||n==="\\hdashline";)e.consume(),t.push(n==="\\hdashline"),e.consumeSpaces(),n=e.fetch().text;return t}var ym=e=>{var t=e.parser.settings;if(!t.displayMode)throw new oe("{"+e.envName+"} can be used only in display mode.")};function X2(e){if(e.indexOf("ed")===-1)return e.indexOf("*")===-1}function Fa(e,t,n){var{hskipBeforeAndAfter:r,addJot:i,cols:a,arraystretch:s,colSeparationType:o,autoTag:l,singleRow:c,emptySingleRow:u,maxNumCols:d,leqno:m}=t;if(e.gullet.beginGroup(),c||e.gullet.macros.set("\\cr","\\\\\\relax"),!s){var p=e.gullet.expandMacroAsText("\\arraystretch");if(p==null)s=1;else if(s=parseFloat(p),!s||s<0)throw new oe("Invalid \\arraystretch: "+p)}e.gullet.beginGroup();var x=[],g=[x],w=[],v=[],$=l!=null?[]:void 0;function _(){l&&e.gullet.macros.set("\\@eqnsw","1",!0)}function C(){$&&(e.gullet.macros.get("\\df@tag")?($.push(e.subparse([new dr("\\df@tag")])),e.gullet.macros.set("\\df@tag",void 0,!0)):$.push(!!l&&e.gullet.macros.get("\\@eqnsw")==="1"))}for(_(),v.push(V3(e));;){var k=e.parseExpression(!1,c?"\\end":"\\\\");e.gullet.endGroup(),e.gullet.beginGroup(),k={type:"ordgroup",mode:e.mode,body:k},n&&(k={type:"styling",mode:e.mode,style:n,body:[k]}),x.push(k);var S=e.fetch().text;if(S==="&"){if(d&&x.length===d){if(c||o)throw new oe("Too many tab characters: &",e.nextToken);e.settings.reportNonstrict("textEnv","Too few columns specified in the {array} column argument.")}e.consume()}else if(S==="\\end"){C(),x.length===1&&k.type==="styling"&&k.body[0].body.length===0&&(g.length>1||!u)&&g.pop(),v.length<g.length+1&&v.push([]);break}else if(S==="\\\\"){e.consume();var L=void 0;e.gullet.future().text!==" "&&(L=e.parseSizeGroup(!0)),w.push(L?L.value:null),C(),v.push(V3(e)),x=[],g.push(x),_()}else throw new oe("Expected & or \\\\ or \\cr or \\end",e.nextToken)}return e.gullet.endGroup(),e.gullet.endGroup(),{type:"array",mode:e.mode,addJot:i,arraystretch:s,body:g,cols:a,rowGaps:w,hskipBeforeAndAfter:r,hLinesBeforeRow:v,colSeparationType:o,tags:$,leqno:m}}function Q2(e){return e.slice(0,1)==="d"?"display":"text"}var mi=function(t,n){var r,i,a=t.body.length,s=t.hLinesBeforeRow,o=0,l=new Array(a),c=[],u=Math.max(n.fontMetrics().arrayRuleWidth,n.minRuleThickness),d=1/n.fontMetrics().ptPerEm,m=5*d;if(t.colSeparationType&&t.colSeparationType==="small"){var p=n.havingStyle(we.SCRIPT).sizeMultiplier;m=.2778*(p/n.sizeMultiplier)}var x=t.colSeparationType==="CD"?gt({number:3,unit:"ex"},n):12*d,g=3*d,w=t.arraystretch*x,v=.7*w,$=.3*w,_=0;function C(Ur){for(var st=0;st<Ur.length;++st)st>0&&(_+=.25),c.push({pos:_,isDashed:Ur[st]})}for(C(s[0]),r=0;r<t.body.length;++r){var k=t.body[r],S=v,L=$;o<k.length&&(o=k.length);var U=new Array(k.length);for(i=0;i<k.length;++i){var F=Ge(k[i],n);L<F.depth&&(L=F.depth),S<F.height&&(S=F.height),U[i]=F}var q=t.rowGaps[r],G=0;q&&(G=gt(q,n),G>0&&(G+=$,L<G&&(L=G),G=0)),t.addJot&&(L+=g),U.height=S,U.depth=L,_+=S,U.pos=_,_+=L+G,l[r]=U,C(s[r+1])}var H=_/2+n.fontMetrics().axisHeight,ne=t.cols||[],K=[],te,J,ae=[];if(t.tags&&t.tags.some(Ur=>Ur))for(r=0;r<a;++r){var B=l[r],X=B.pos-H,P=t.tags[r],se=void 0;P===!0?se=V.makeSpan(["eqn-num"],[],n):P===!1?se=V.makeSpan([],[],n):se=V.makeSpan([],Bt(P,n,!0),n),se.depth=B.depth,se.height=B.height,ae.push({type:"elem",elem:se,shift:X})}for(i=0,J=0;i<o||J<ne.length;++i,++J){for(var he=ne[J]||{},D=!0;he.type==="separator";){if(D||(te=V.makeSpan(["arraycolsep"],[]),te.style.width=me(n.fontMetrics().doubleRuleSep),K.push(te)),he.separator==="|"||he.separator===":"){var Ee=he.separator==="|"?"solid":"dashed",je=V.makeSpan(["vertical-separator"],[],n);je.style.height=me(_),je.style.borderRightWidth=me(u),je.style.borderRightStyle=Ee,je.style.margin="0 "+me(-u/2);var xe=_-H;xe&&(je.style.verticalAlign=me(-xe)),K.push(je)}else throw new oe("Invalid separator type: "+he.separator);J++,he=ne[J]||{},D=!1}if(!(i>=o)){var be=void 0;(i>0||t.hskipBeforeAndAfter)&&(be=ye.deflt(he.pregap,m),be!==0&&(te=V.makeSpan(["arraycolsep"],[]),te.style.width=me(be),K.push(te)));var Ne=[];for(r=0;r<a;++r){var He=l[r],Le=He[i];if(Le){var Be=He.pos-H;Le.depth=He.depth,Le.height=He.height,Ne.push({type:"elem",elem:Le,shift:Be})}}Ne=V.makeVList({positionType:"individualShift",children:Ne},n),Ne=V.makeSpan(["col-align-"+(he.align||"c")],[Ne]),K.push(Ne),(i<o-1||t.hskipBeforeAndAfter)&&(be=ye.deflt(he.postgap,m),be!==0&&(te=V.makeSpan(["arraycolsep"],[]),te.style.width=me(be),K.push(te)))}}if(l=V.makeSpan(["mtable"],K),c.length>0){for(var Ve=V.makeLineSpan("hline",n,u),Dt=V.makeLineSpan("hdashline",n,u),Mt=[{type:"elem",elem:l,shift:0}];c.length>0;){var xn=c.pop(),Ft=xn.pos-H;xn.isDashed?Mt.push({type:"elem",elem:Dt,shift:Ft}):Mt.push({type:"elem",elem:Ve,shift:Ft})}l=V.makeVList({positionType:"individualShift",children:Mt},n)}if(ae.length===0)return V.makeSpan(["mord"],[l],n);var $n=V.makeVList({positionType:"individualShift",children:ae},n);return $n=V.makeSpan(["tag"],[$n],n),V.makeFragment([l,$n])},zW={c:"center ",l:"left ",r:"right "},hi=function(t,n){for(var r=[],i=new ie.MathNode("mtd",[],["mtr-glue"]),a=new ie.MathNode("mtd",[],["mml-eqn-num"]),s=0;s<t.body.length;s++){for(var o=t.body[s],l=[],c=0;c<o.length;c++)l.push(new ie.MathNode("mtd",[et(o[c],n)]));t.tags&&t.tags[s]&&(l.unshift(i),l.push(i),t.leqno?l.unshift(a):l.push(a)),r.push(new ie.MathNode("mtr",l))}var u=new ie.MathNode("mtable",r),d=t.arraystretch===.5?.1:.16+t.arraystretch-1+(t.addJot?.09:0);u.setAttribute("rowspacing",me(d));var m="",p="";if(t.cols&&t.cols.length>0){var x=t.cols,g="",w=!1,v=0,$=x.length;x[0].type==="separator"&&(m+="top ",v=1),x[x.length-1].type==="separator"&&(m+="bottom ",$-=1);for(var _=v;_<$;_++)x[_].type==="align"?(p+=zW[x[_].align],w&&(g+="none "),w=!0):x[_].type==="separator"&&w&&(g+=x[_].separator==="|"?"solid ":"dashed ",w=!1);u.setAttribute("columnalign",p.trim()),/[sd]/.test(g)&&u.setAttribute("columnlines",g.trim())}if(t.colSeparationType==="align"){for(var C=t.cols||[],k="",S=1;S<C.length;S++)k+=S%2?"0em ":"1em ";u.setAttribute("columnspacing",k.trim())}else t.colSeparationType==="alignat"||t.colSeparationType==="gather"?u.setAttribute("columnspacing","0em"):t.colSeparationType==="small"?u.setAttribute("columnspacing","0.2778em"):t.colSeparationType==="CD"?u.setAttribute("columnspacing","0.5em"):u.setAttribute("columnspacing","1em");var L="",U=t.hLinesBeforeRow;m+=U[0].length>0?"left ":"",m+=U[U.length-1].length>0?"right ":"";for(var F=1;F<U.length-1;F++)L+=U[F].length===0?"none ":U[F][0]?"dashed ":"solid ";return/[sd]/.test(L)&&u.setAttribute("rowlines",L.trim()),m!==""&&(u=new ie.MathNode("menclose",[u]),u.setAttribute("notation",m.trim())),t.arraystretch&&t.arraystretch<1&&(u=new ie.MathNode("mstyle",[u]),u.setAttribute("scriptlevel","1")),u},aT=function(t,n){t.envName.indexOf("ed")===-1&&ym(t);var r=[],i=t.envName.indexOf("at")>-1?"alignat":"align",a=t.envName==="split",s=Fa(t.parser,{cols:r,addJot:!0,autoTag:a?void 0:X2(t.envName),emptySingleRow:!0,colSeparationType:i,maxNumCols:a?2:void 0,leqno:t.parser.settings.leqno},"display"),o,l=0,c={type:"ordgroup",mode:t.mode,body:[]};if(n[0]&&n[0].type==="ordgroup"){for(var u="",d=0;d<n[0].body.length;d++){var m=Re(n[0].body[d],"textord");u+=m.text}o=Number(u),l=o*2}var p=!l;s.body.forEach(function(v){for(var $=1;$<v.length;$+=2){var _=Re(v[$],"styling"),C=Re(_.body[0],"ordgroup");C.body.unshift(c)}if(p)l<v.length&&(l=v.length);else{var k=v.length/2;if(o<k)throw new oe("Too many math in a row: "+("expected "+o+", but got "+k),v[0])}});for(var x=0;x<l;++x){var g="r",w=0;x%2===1?g="l":x>0&&p&&(w=1),r[x]={type:"align",align:g,pregap:w,postgap:0}}return s.colSeparationType=p?"align":"alignat",s};di({type:"array",names:["array","darray"],props:{numArgs:1},handler(e,t){var n=vm(t[0]),r=n?[t[0]]:Re(t[0],"ordgroup").body,i=r.map(function(s){var o=q2(s),l=o.text;if("lcr".indexOf(l)!==-1)return{type:"align",align:l};if(l==="|")return{type:"separator",separator:"|"};if(l===":")return{type:"separator",separator:":"};throw new oe("Unknown column alignment: "+l,s)}),a={cols:i,hskipBeforeAndAfter:!0,maxNumCols:i.length};return Fa(e.parser,a,Q2(e.envName))},htmlBuilder:mi,mathmlBuilder:hi});di({type:"array",names:["matrix","pmatrix","bmatrix","Bmatrix","vmatrix","Vmatrix","matrix*","pmatrix*","bmatrix*","Bmatrix*","vmatrix*","Vmatrix*"],props:{numArgs:0},handler(e){var t={matrix:null,pmatrix:["(",")"],bmatrix:["[","]"],Bmatrix:["\\{","\\}"],vmatrix:["|","|"],Vmatrix:["\\Vert","\\Vert"]}[e.envName.replace("*","")],n="c",r={hskipBeforeAndAfter:!1,cols:[{type:"align",align:n}]};if(e.envName.charAt(e.envName.length-1)==="*"){var i=e.parser;if(i.consumeSpaces(),i.fetch().text==="["){if(i.consume(),i.consumeSpaces(),n=i.fetch().text,"lcr".indexOf(n)===-1)throw new oe("Expected l or c or r",i.nextToken);i.consume(),i.consumeSpaces(),i.expect("]"),i.consume(),r.cols=[{type:"align",align:n}]}}var a=Fa(e.parser,r,Q2(e.envName)),s=Math.max(0,...a.body.map(o=>o.length));return a.cols=new Array(s).fill({type:"align",align:n}),t?{type:"leftright",mode:e.mode,body:[a],left:t[0],right:t[1],rightColor:void 0}:a},htmlBuilder:mi,mathmlBuilder:hi});di({type:"array",names:["smallmatrix"],props:{numArgs:0},handler(e){var t={arraystretch:.5},n=Fa(e.parser,t,"script");return n.colSeparationType="small",n},htmlBuilder:mi,mathmlBuilder:hi});di({type:"array",names:["subarray"],props:{numArgs:1},handler(e,t){var n=vm(t[0]),r=n?[t[0]]:Re(t[0],"ordgroup").body,i=r.map(function(s){var o=q2(s),l=o.text;if("lc".indexOf(l)!==-1)return{type:"align",align:l};throw new oe("Unknown column alignment: "+l,s)});if(i.length>1)throw new oe("{subarray} can contain only one column");var a={cols:i,hskipBeforeAndAfter:!1,arraystretch:.5};if(a=Fa(e.parser,a,"script"),a.body.length>0&&a.body[0].length>1)throw new oe("{subarray} can contain only one column");return a},htmlBuilder:mi,mathmlBuilder:hi});di({type:"array",names:["cases","dcases","rcases","drcases"],props:{numArgs:0},handler(e){var t={arraystretch:1.2,cols:[{type:"align",align:"l",pregap:0,postgap:1},{type:"align",align:"l",pregap:0,postgap:0}]},n=Fa(e.parser,t,Q2(e.envName));return{type:"leftright",mode:e.mode,body:[n],left:e.envName.indexOf("r")>-1?".":"\\{",right:e.envName.indexOf("r")>-1?"\\}":".",rightColor:void 0}},htmlBuilder:mi,mathmlBuilder:hi});di({type:"array",names:["align","align*","aligned","split"],props:{numArgs:0},handler:aT,htmlBuilder:mi,mathmlBuilder:hi});di({type:"array",names:["gathered","gather","gather*"],props:{numArgs:0},handler(e){ye.contains(["gather","gather*"],e.envName)&&ym(e);var t={cols:[{type:"align",align:"c"}],addJot:!0,colSeparationType:"gather",autoTag:X2(e.envName),emptySingleRow:!0,leqno:e.parser.settings.leqno};return Fa(e.parser,t,"display")},htmlBuilder:mi,mathmlBuilder:hi});di({type:"array",names:["alignat","alignat*","alignedat"],props:{numArgs:1},handler:aT,htmlBuilder:mi,mathmlBuilder:hi});di({type:"array",names:["equation","equation*"],props:{numArgs:0},handler(e){ym(e);var t={autoTag:X2(e.envName),emptySingleRow:!0,singleRow:!0,maxNumCols:1,leqno:e.parser.settings.leqno};return Fa(e.parser,t,"display")},htmlBuilder:mi,mathmlBuilder:hi});di({type:"array",names:["CD"],props:{numArgs:0},handler(e){return ym(e),EW(e.parser)},htmlBuilder:mi,mathmlBuilder:hi});N("\\nonumber","\\gdef\\@eqnsw{0}");N("\\notag","\\nonumber");fe({type:"text",names:["\\hline","\\hdashline"],props:{numArgs:0,allowedInText:!0,allowedInMath:!0},handler(e,t){throw new oe(e.funcName+" valid only within array environment")}});var H3=rT;fe({type:"environment",names:["\\begin","\\end"],props:{numArgs:1,argTypes:["text"]},handler(e,t){var{parser:n,funcName:r}=e,i=t[0];if(i.type!=="ordgroup")throw new oe("Invalid environment name",i);for(var a="",s=0;s<i.body.length;++s)a+=Re(i.body[s],"textord").text;if(r==="\\begin"){if(!H3.hasOwnProperty(a))throw new oe("No such environment: "+a,i);var o=H3[a],{args:l,optArgs:c}=n.parseArguments("\\begin{"+a+"}",o),u={mode:n.mode,envName:a,parser:n},d=o.handler(u,l,c);n.expect("\\end",!1);var m=n.nextToken,p=Re(n.parseFunction(),"environment");if(p.name!==a)throw new oe("Mismatch: \\begin{"+a+"} matched by \\end{"+p.name+"}",m);return d}return{type:"environment",mode:n.mode,name:a,nameGroup:i}}});var sT=(e,t)=>{var n=e.font,r=t.withFont(n);return Ge(e.body,r)},oT=(e,t)=>{var n=e.font,r=t.withFont(n);return et(e.body,r)},q3={"\\Bbb":"\\mathbb","\\bold":"\\mathbf","\\frak":"\\mathfrak","\\bm":"\\boldsymbol"};fe({type:"font",names:["\\mathrm","\\mathit","\\mathbf","\\mathnormal","\\mathsfit","\\mathbb","\\mathcal","\\mathfrak","\\mathscr","\\mathsf","\\mathtt","\\Bbb","\\bold","\\frak"],props:{numArgs:1,allowedInArgument:!0},handler:(e,t)=>{var{parser:n,funcName:r}=e,i=f0(t[0]),a=r;return a in q3&&(a=q3[a]),{type:"font",mode:n.mode,font:a.slice(1),body:i}},htmlBuilder:sT,mathmlBuilder:oT});fe({type:"mclass",names:["\\boldsymbol","\\bm"],props:{numArgs:1},handler:(e,t)=>{var{parser:n}=e,r=t[0],i=ye.isCharacterBox(r);return{type:"mclass",mode:n.mode,mclass:xm(r),body:[{type:"font",mode:n.mode,font:"boldsymbol",body:r}],isCharacterBox:i}}});fe({type:"font",names:["\\rm","\\sf","\\tt","\\bf","\\it","\\cal"],props:{numArgs:0,allowedInText:!0},handler:(e,t)=>{var{parser:n,funcName:r,breakOnTokenText:i}=e,{mode:a}=n,s=n.parseExpression(!0,i),o="math"+r.slice(1);return{type:"font",mode:a,font:o,body:{type:"ordgroup",mode:n.mode,body:s}}},htmlBuilder:sT,mathmlBuilder:oT});var lT=(e,t)=>{var n=t;return e==="display"?n=n.id>=we.SCRIPT.id?n.text():we.DISPLAY:e==="text"&&n.size===we.DISPLAY.size?n=we.TEXT:e==="script"?n=we.SCRIPT:e==="scriptscript"&&(n=we.SCRIPTSCRIPT),n},Z2=(e,t)=>{var n=lT(e.size,t.style),r=n.fracNum(),i=n.fracDen(),a;a=t.havingStyle(r);var s=Ge(e.numer,a,t);if(e.continued){var o=8.5/t.fontMetrics().ptPerEm,l=3.5/t.fontMetrics().ptPerEm;s.height=s.height<o?o:s.height,s.depth=s.depth<l?l:s.depth}a=t.havingStyle(i);var c=Ge(e.denom,a,t),u,d,m;e.hasBarLine?(e.barSize?(d=gt(e.barSize,t),u=V.makeLineSpan("frac-line",t,d)):u=V.makeLineSpan("frac-line",t),d=u.height,m=u.height):(u=null,d=0,m=t.fontMetrics().defaultRuleThickness);var p,x,g;n.size===we.DISPLAY.size||e.size==="display"?(p=t.fontMetrics().num1,d>0?x=3*m:x=7*m,g=t.fontMetrics().denom1):(d>0?(p=t.fontMetrics().num2,x=m):(p=t.fontMetrics().num3,x=3*m),g=t.fontMetrics().denom2);var w;if(u){var $=t.fontMetrics().axisHeight;p-s.depth-($+.5*d)<x&&(p+=x-(p-s.depth-($+.5*d))),$-.5*d-(c.height-g)<x&&(g+=x-($-.5*d-(c.height-g)));var _=-($-.5*d);w=V.makeVList({positionType:"individualShift",children:[{type:"elem",elem:c,shift:g},{type:"elem",elem:u,shift:_},{type:"elem",elem:s,shift:-p}]},t)}else{var v=p-s.depth-(c.height-g);v<x&&(p+=.5*(x-v),g+=.5*(x-v)),w=V.makeVList({positionType:"individualShift",children:[{type:"elem",elem:c,shift:g},{type:"elem",elem:s,shift:-p}]},t)}a=t.havingStyle(n),w.height*=a.sizeMultiplier/t.sizeMultiplier,w.depth*=a.sizeMultiplier/t.sizeMultiplier;var C;n.size===we.DISPLAY.size?C=t.fontMetrics().delim1:n.size===we.SCRIPTSCRIPT.size?C=t.havingStyle(we.SCRIPT).fontMetrics().delim2:C=t.fontMetrics().delim2;var k,S;return e.leftDelim==null?k=Ic(t,["mopen"]):k=Ni.customSizedDelim(e.leftDelim,C,!0,t.havingStyle(n),e.mode,["mopen"]),e.continued?S=V.makeSpan([]):e.rightDelim==null?S=Ic(t,["mclose"]):S=Ni.customSizedDelim(e.rightDelim,C,!0,t.havingStyle(n),e.mode,["mclose"]),V.makeSpan(["mord"].concat(a.sizingClasses(t)),[k,V.makeSpan(["mfrac"],[w]),S],t)},J2=(e,t)=>{var n=new ie.MathNode("mfrac",[et(e.numer,t),et(e.denom,t)]);if(!e.hasBarLine)n.setAttribute("linethickness","0px");else if(e.barSize){var r=gt(e.barSize,t);n.setAttribute("linethickness",me(r))}var i=lT(e.size,t.style);if(i.size!==t.style.size){n=new ie.MathNode("mstyle",[n]);var a=i.size===we.DISPLAY.size?"true":"false";n.setAttribute("displaystyle",a),n.setAttribute("scriptlevel","0")}if(e.leftDelim!=null||e.rightDelim!=null){var s=[];if(e.leftDelim!=null){var o=new ie.MathNode("mo",[new ie.TextNode(e.leftDelim.replace("\\",""))]);o.setAttribute("fence","true"),s.push(o)}if(s.push(n),e.rightDelim!=null){var l=new ie.MathNode("mo",[new ie.TextNode(e.rightDelim.replace("\\",""))]);l.setAttribute("fence","true"),s.push(l)}return V2(s)}return n};fe({type:"genfrac",names:["\\dfrac","\\frac","\\tfrac","\\dbinom","\\binom","\\tbinom","\\\\atopfrac","\\\\bracefrac","\\\\brackfrac"],props:{numArgs:2,allowedInArgument:!0},handler:(e,t)=>{var{parser:n,funcName:r}=e,i=t[0],a=t[1],s,o=null,l=null,c="auto";switch(r){case"\\dfrac":case"\\frac":case"\\tfrac":s=!0;break;case"\\\\atopfrac":s=!1;break;case"\\dbinom":case"\\binom":case"\\tbinom":s=!1,o="(",l=")";break;case"\\\\bracefrac":s=!1,o="\\{",l="\\}";break;case"\\\\brackfrac":s=!1,o="[",l="]";break;default:throw new Error("Unrecognized genfrac command")}switch(r){case"\\dfrac":case"\\dbinom":c="display";break;case"\\tfrac":case"\\tbinom":c="text";break}return{type:"genfrac",mode:n.mode,continued:!1,numer:i,denom:a,hasBarLine:s,leftDelim:o,rightDelim:l,size:c,barSize:null}},htmlBuilder:Z2,mathmlBuilder:J2});fe({type:"genfrac",names:["\\cfrac"],props:{numArgs:2},handler:(e,t)=>{var{parser:n,funcName:r}=e,i=t[0],a=t[1];return{type:"genfrac",mode:n.mode,continued:!0,numer:i,denom:a,hasBarLine:!0,leftDelim:null,rightDelim:null,size:"display",barSize:null}}});fe({type:"infix",names:["\\over","\\choose","\\atop","\\brace","\\brack"],props:{numArgs:0,infix:!0},handler(e){var{parser:t,funcName:n,token:r}=e,i;switch(n){case"\\over":i="\\frac";break;case"\\choose":i="\\binom";break;case"\\atop":i="\\\\atopfrac";break;case"\\brace":i="\\\\bracefrac";break;case"\\brack":i="\\\\brackfrac";break;default:throw new Error("Unrecognized infix genfrac command")}return{type:"infix",mode:t.mode,replaceWith:i,token:r}}});var G3=["display","text","script","scriptscript"],W3=function(t){var n=null;return t.length>0&&(n=t,n=n==="."?null:n),n};fe({type:"genfrac",names:["\\genfrac"],props:{numArgs:6,allowedInArgument:!0,argTypes:["math","math","size","text","math","math"]},handler(e,t){var{parser:n}=e,r=t[4],i=t[5],a=f0(t[0]),s=a.type==="atom"&&a.family==="open"?W3(a.text):null,o=f0(t[1]),l=o.type==="atom"&&o.family==="close"?W3(o.text):null,c=Re(t[2],"size"),u,d=null;c.isBlank?u=!0:(d=c.value,u=d.number>0);var m="auto",p=t[3];if(p.type==="ordgroup"){if(p.body.length>0){var x=Re(p.body[0],"textord");m=G3[Number(x.text)]}}else p=Re(p,"textord"),m=G3[Number(p.text)];return{type:"genfrac",mode:n.mode,numer:r,denom:i,continued:!1,hasBarLine:u,barSize:d,leftDelim:s,rightDelim:l,size:m}},htmlBuilder:Z2,mathmlBuilder:J2});fe({type:"infix",names:["\\above"],props:{numArgs:1,argTypes:["size"],infix:!0},handler(e,t){var{parser:n,funcName:r,token:i}=e;return{type:"infix",mode:n.mode,replaceWith:"\\\\abovefrac",size:Re(t[0],"size").value,token:i}}});fe({type:"genfrac",names:["\\\\abovefrac"],props:{numArgs:3,argTypes:["math","size","math"]},handler:(e,t)=>{var{parser:n,funcName:r}=e,i=t[0],a=fG(Re(t[1],"infix").size),s=t[2],o=a.number>0;return{type:"genfrac",mode:n.mode,numer:i,denom:s,continued:!1,hasBarLine:o,barSize:a,leftDelim:null,rightDelim:null,size:"auto"}},htmlBuilder:Z2,mathmlBuilder:J2});var cT=(e,t)=>{var n=t.style,r,i;e.type==="supsub"?(r=e.sup?Ge(e.sup,t.havingStyle(n.sup()),t):Ge(e.sub,t.havingStyle(n.sub()),t),i=Re(e.base,"horizBrace")):i=Re(e,"horizBrace");var a=Ge(i.base,t.havingBaseStyle(we.DISPLAY)),s=zi.svgSpan(i,t),o;if(i.isOver?(o=V.makeVList({positionType:"firstBaseline",children:[{type:"elem",elem:a},{type:"kern",size:.1},{type:"elem",elem:s}]},t),o.children[0].children[0].children[1].classes.push("svg-align")):(o=V.makeVList({positionType:"bottom",positionData:a.depth+.1+s.height,children:[{type:"elem",elem:s},{type:"kern",size:.1},{type:"elem",elem:a}]},t),o.children[0].children[0].children[0].classes.push("svg-align")),r){var l=V.makeSpan(["mord",i.isOver?"mover":"munder"],[o],t);i.isOver?o=V.makeVList({positionType:"firstBaseline",children:[{type:"elem",elem:l},{type:"kern",size:.2},{type:"elem",elem:r}]},t):o=V.makeVList({positionType:"bottom",positionData:l.depth+.2+r.height+r.depth,children:[{type:"elem",elem:r},{type:"kern",size:.2},{type:"elem",elem:l}]},t)}return V.makeSpan(["mord",i.isOver?"mover":"munder"],[o],t)},BW=(e,t)=>{var n=zi.mathMLnode(e.label);return new ie.MathNode(e.isOver?"mover":"munder",[et(e.base,t),n])};fe({type:"horizBrace",names:["\\overbrace","\\underbrace"],props:{numArgs:1},handler(e,t){var{parser:n,funcName:r}=e;return{type:"horizBrace",mode:n.mode,label:r,isOver:/^\\over/.test(r),base:t[0]}},htmlBuilder:cT,mathmlBuilder:BW});fe({type:"href",names:["\\href"],props:{numArgs:2,argTypes:["url","original"],allowedInText:!0},handler:(e,t)=>{var{parser:n}=e,r=t[1],i=Re(t[0],"url").url;return n.settings.isTrusted({command:"\\href",url:i})?{type:"href",mode:n.mode,href:i,body:At(r)}:n.formatUnsupportedCmd("\\href")},htmlBuilder:(e,t)=>{var n=Bt(e.body,t,!1);return V.makeAnchor(e.href,[],n,t)},mathmlBuilder:(e,t)=>{var n=Ia(e.body,t);return n instanceof Vn||(n=new Vn("mrow",[n])),n.setAttribute("href",e.href),n}});fe({type:"href",names:["\\url"],props:{numArgs:1,argTypes:["url"],allowedInText:!0},handler:(e,t)=>{var{parser:n}=e,r=Re(t[0],"url").url;if(!n.settings.isTrusted({command:"\\url",url:r}))return n.formatUnsupportedCmd("\\url");for(var i=[],a=0;a<r.length;a++){var s=r[a];s==="~"&&(s="\\textasciitilde"),i.push({type:"textord",mode:"text",text:s})}var o={type:"text",mode:n.mode,font:"\\texttt",body:i};return{type:"href",mode:n.mode,href:r,body:At(o)}}});fe({type:"hbox",names:["\\hbox"],props:{numArgs:1,argTypes:["text"],allowedInText:!0,primitive:!0},handler(e,t){var{parser:n}=e;return{type:"hbox",mode:n.mode,body:At(t[0])}},htmlBuilder(e,t){var n=Bt(e.body,t,!1);return V.makeFragment(n)},mathmlBuilder(e,t){return new ie.MathNode("mrow",Un(e.body,t))}});fe({type:"html",names:["\\htmlClass","\\htmlId","\\htmlStyle","\\htmlData"],props:{numArgs:2,argTypes:["raw","original"],allowedInText:!0},handler:(e,t)=>{var{parser:n,funcName:r,token:i}=e,a=Re(t[0],"raw").string,s=t[1];n.settings.strict&&n.settings.reportNonstrict("htmlExtension","HTML extension is disabled on strict mode");var o,l={};switch(r){case"\\htmlClass":l.class=a,o={command:"\\htmlClass",class:a};break;case"\\htmlId":l.id=a,o={command:"\\htmlId",id:a};break;case"\\htmlStyle":l.style=a,o={command:"\\htmlStyle",style:a};break;case"\\htmlData":{for(var c=a.split(","),u=0;u<c.length;u++){var d=c[u].split("=");if(d.length!==2)throw new oe("Error parsing key-value for \\htmlData");l["data-"+d[0].trim()]=d[1].trim()}o={command:"\\htmlData",attributes:l};break}default:throw new Error("Unrecognized html command")}return n.settings.isTrusted(o)?{type:"html",mode:n.mode,attributes:l,body:At(s)}:n.formatUnsupportedCmd(r)},htmlBuilder:(e,t)=>{var n=Bt(e.body,t,!1),r=["enclosing"];e.attributes.class&&r.push(...e.attributes.class.trim().split(/\s+/));var i=V.makeSpan(r,n,t);for(var a in e.attributes)a!=="class"&&e.attributes.hasOwnProperty(a)&&i.setAttribute(a,e.attributes[a]);return i},mathmlBuilder:(e,t)=>Ia(e.body,t)});fe({type:"htmlmathml",names:["\\html@mathml"],props:{numArgs:2,allowedInText:!0},handler:(e,t)=>{var{parser:n}=e;return{type:"htmlmathml",mode:n.mode,html:At(t[0]),mathml:At(t[1])}},htmlBuilder:(e,t)=>{var n=Bt(e.html,t,!1);return V.makeFragment(n)},mathmlBuilder:(e,t)=>Ia(e.mathml,t)});var Jh=function(t){if(/^[-+]? *(\d+(\.\d*)?|\.\d+)$/.test(t))return{number:+t,unit:"bp"};var n=/([-+]?) *(\d+(?:\.\d*)?|\.\d+) *([a-z]{2})/.exec(t);if(!n)throw new oe("Invalid size: '"+t+"' in \\includegraphics");var r={number:+(n[1]+n[2]),unit:n[3]};if(!Nw(r))throw new oe("Invalid unit: '"+r.unit+"' in \\includegraphics.");return r};fe({type:"includegraphics",names:["\\includegraphics"],props:{numArgs:1,numOptionalArgs:1,argTypes:["raw","url"],allowedInText:!1},handler:(e,t,n)=>{var{parser:r}=e,i={number:0,unit:"em"},a={number:.9,unit:"em"},s={number:0,unit:"em"},o="";if(n[0])for(var l=Re(n[0],"raw").string,c=l.split(","),u=0;u<c.length;u++){var d=c[u].split("=");if(d.length===2){var m=d[1].trim();switch(d[0].trim()){case"alt":o=m;break;case"width":i=Jh(m);break;case"height":a=Jh(m);break;case"totalheight":s=Jh(m);break;default:throw new oe("Invalid key: '"+d[0]+"' in \\includegraphics.")}}}var p=Re(t[0],"url").url;return o===""&&(o=p,o=o.replace(/^.*[\\/]/,""),o=o.substring(0,o.lastIndexOf("."))),r.settings.isTrusted({command:"\\includegraphics",url:p})?{type:"includegraphics",mode:r.mode,alt:o,width:i,height:a,totalheight:s,src:p}:r.formatUnsupportedCmd("\\includegraphics")},htmlBuilder:(e,t)=>{var n=gt(e.height,t),r=0;e.totalheight.number>0&&(r=gt(e.totalheight,t)-n);var i=0;e.width.number>0&&(i=gt(e.width,t));var a={height:me(n+r)};i>0&&(a.width=me(i)),r>0&&(a.verticalAlign=me(-r));var s=new BG(e.src,e.alt,a);return s.height=n,s.depth=r,s},mathmlBuilder:(e,t)=>{var n=new ie.MathNode("mglyph",[]);n.setAttribute("alt",e.alt);var r=gt(e.height,t),i=0;if(e.totalheight.number>0&&(i=gt(e.totalheight,t)-r,n.setAttribute("valign",me(-i))),n.setAttribute("height",me(r+i)),e.width.number>0){var a=gt(e.width,t);n.setAttribute("width",me(a))}return n.setAttribute("src",e.src),n}});fe({type:"kern",names:["\\kern","\\mkern","\\hskip","\\mskip"],props:{numArgs:1,argTypes:["size"],primitive:!0,allowedInText:!0},handler(e,t){var{parser:n,funcName:r}=e,i=Re(t[0],"size");if(n.settings.strict){var a=r[1]==="m",s=i.value.unit==="mu";a?(s||n.settings.reportNonstrict("mathVsTextUnits","LaTeX's "+r+" supports only mu units, "+("not "+i.value.unit+" units")),n.mode!=="math"&&n.settings.reportNonstrict("mathVsTextUnits","LaTeX's "+r+" works only in math mode")):s&&n.settings.reportNonstrict("mathVsTextUnits","LaTeX's "+r+" doesn't support mu units")}return{type:"kern",mode:n.mode,dimension:i.value}},htmlBuilder(e,t){return V.makeGlue(e.dimension,t)},mathmlBuilder(e,t){var n=gt(e.dimension,t);return new ie.SpaceNode(n)}});fe({type:"lap",names:["\\mathllap","\\mathrlap","\\mathclap"],props:{numArgs:1,allowedInText:!0},handler:(e,t)=>{var{parser:n,funcName:r}=e,i=t[0];return{type:"lap",mode:n.mode,alignment:r.slice(5),body:i}},htmlBuilder:(e,t)=>{var n;e.alignment==="clap"?(n=V.makeSpan([],[Ge(e.body,t)]),n=V.makeSpan(["inner"],[n],t)):n=V.makeSpan(["inner"],[Ge(e.body,t)]);var r=V.makeSpan(["fix"],[]),i=V.makeSpan([e.alignment],[n,r],t),a=V.makeSpan(["strut"]);return a.style.height=me(i.height+i.depth),i.depth&&(a.style.verticalAlign=me(-i.depth)),i.children.unshift(a),i=V.makeSpan(["thinbox"],[i],t),V.makeSpan(["mord","vbox"],[i],t)},mathmlBuilder:(e,t)=>{var n=new ie.MathNode("mpadded",[et(e.body,t)]);if(e.alignment!=="rlap"){var r=e.alignment==="llap"?"-1":"-0.5";n.setAttribute("lspace",r+"width")}return n.setAttribute("width","0px"),n}});fe({type:"styling",names:["\\(","$"],props:{numArgs:0,allowedInText:!0,allowedInMath:!1},handler(e,t){var{funcName:n,parser:r}=e,i=r.mode;r.switchMode("math");var a=n==="\\("?"\\)":"$",s=r.parseExpression(!1,a);return r.expect(a),r.switchMode(i),{type:"styling",mode:r.mode,style:"text",body:s}}});fe({type:"text",names:["\\)","\\]"],props:{numArgs:0,allowedInText:!0,allowedInMath:!1},handler(e,t){throw new oe("Mismatched "+e.funcName)}});var K3=(e,t)=>{switch(t.style.size){case we.DISPLAY.size:return e.display;case we.TEXT.size:return e.text;case we.SCRIPT.size:return e.script;case we.SCRIPTSCRIPT.size:return e.scriptscript;default:return e.text}};fe({type:"mathchoice",names:["\\mathchoice"],props:{numArgs:4,primitive:!0},handler:(e,t)=>{var{parser:n}=e;return{type:"mathchoice",mode:n.mode,display:At(t[0]),text:At(t[1]),script:At(t[2]),scriptscript:At(t[3])}},htmlBuilder:(e,t)=>{var n=K3(e,t),r=Bt(n,t,!1);return V.makeFragment(r)},mathmlBuilder:(e,t)=>{var n=K3(e,t);return Ia(n,t)}});var uT=(e,t,n,r,i,a,s)=>{e=V.makeSpan([],[e]);var o=n&&ye.isCharacterBox(n),l,c;if(t){var u=Ge(t,r.havingStyle(i.sup()),r);c={elem:u,kern:Math.max(r.fontMetrics().bigOpSpacing1,r.fontMetrics().bigOpSpacing3-u.depth)}}if(n){var d=Ge(n,r.havingStyle(i.sub()),r);l={elem:d,kern:Math.max(r.fontMetrics().bigOpSpacing2,r.fontMetrics().bigOpSpacing4-d.height)}}var m;if(c&&l){var p=r.fontMetrics().bigOpSpacing5+l.elem.height+l.elem.depth+l.kern+e.depth+s;m=V.makeVList({positionType:"bottom",positionData:p,children:[{type:"kern",size:r.fontMetrics().bigOpSpacing5},{type:"elem",elem:l.elem,marginLeft:me(-a)},{type:"kern",size:l.kern},{type:"elem",elem:e},{type:"kern",size:c.kern},{type:"elem",elem:c.elem,marginLeft:me(a)},{type:"kern",size:r.fontMetrics().bigOpSpacing5}]},r)}else if(l){var x=e.height-s;m=V.makeVList({positionType:"top",positionData:x,children:[{type:"kern",size:r.fontMetrics().bigOpSpacing5},{type:"elem",elem:l.elem,marginLeft:me(-a)},{type:"kern",size:l.kern},{type:"elem",elem:e}]},r)}else if(c){var g=e.depth+s;m=V.makeVList({positionType:"bottom",positionData:g,children:[{type:"elem",elem:e},{type:"kern",size:c.kern},{type:"elem",elem:c.elem,marginLeft:me(a)},{type:"kern",size:r.fontMetrics().bigOpSpacing5}]},r)}else return e;var w=[m];if(l&&a!==0&&!o){var v=V.makeSpan(["mspace"],[],r);v.style.marginRight=me(a),w.unshift(v)}return V.makeSpan(["mop","op-limits"],w,r)},dT=["\\smallint"],Zo=(e,t)=>{var n,r,i=!1,a;e.type==="supsub"?(n=e.sup,r=e.sub,a=Re(e.base,"op"),i=!0):a=Re(e,"op");var s=t.style,o=!1;s.size===we.DISPLAY.size&&a.symbol&&!ye.contains(dT,a.name)&&(o=!0);var l;if(a.symbol){var c=o?"Size2-Regular":"Size1-Regular",u="";if((a.name==="\\oiint"||a.name==="\\oiiint")&&(u=a.name.slice(1),a.name=u==="oiint"?"\\iint":"\\iiint"),l=V.makeSymbol(a.name,c,"math",t,["mop","op-symbol",o?"large-op":"small-op"]),u.length>0){var d=l.italic,m=V.staticSvg(u+"Size"+(o?"2":"1"),t);l=V.makeVList({positionType:"individualShift",children:[{type:"elem",elem:l,shift:0},{type:"elem",elem:m,shift:o?.08:0}]},t),a.name="\\"+u,l.classes.unshift("mop"),l.italic=d}}else if(a.body){var p=Bt(a.body,t,!0);p.length===1&&p[0]instanceof pr?(l=p[0],l.classes[0]="mop"):l=V.makeSpan(["mop"],p,t)}else{for(var x=[],g=1;g<a.name.length;g++)x.push(V.mathsym(a.name[g],a.mode,t));l=V.makeSpan(["mop"],x,t)}var w=0,v=0;return(l instanceof pr||a.name==="\\oiint"||a.name==="\\oiiint")&&!a.suppressBaseShift&&(w=(l.height-l.depth)/2-t.fontMetrics().axisHeight,v=l.italic),i?uT(l,n,r,t,s,v,w):(w&&(l.style.position="relative",l.style.top=me(w)),l)},tu=(e,t)=>{var n;if(e.symbol)n=new Vn("mo",[fr(e.name,e.mode)]),ye.contains(dT,e.name)&&n.setAttribute("largeop","false");else if(e.body)n=new Vn("mo",Un(e.body,t));else{n=new Vn("mi",[new ei(e.name.slice(1))]);var r=new Vn("mo",[fr("⁡","text")]);e.parentIsSupSub?n=new Vn("mrow",[n,r]):n=zw([n,r])}return n},FW={"∏":"\\prod","∐":"\\coprod","∑":"\\sum","⋀":"\\bigwedge","⋁":"\\bigvee","⋂":"\\bigcap","⋃":"\\bigcup","⨀":"\\bigodot","⨁":"\\bigoplus","⨂":"\\bigotimes","⨄":"\\biguplus","⨆":"\\bigsqcup"};fe({type:"op",names:["\\coprod","\\bigvee","\\bigwedge","\\biguplus","\\bigcap","\\bigcup","\\intop","\\prod","\\sum","\\bigotimes","\\bigoplus","\\bigodot","\\bigsqcup","\\smallint","∏","∐","∑","⋀","⋁","⋂","⋃","⨀","⨁","⨂","⨄","⨆"],props:{numArgs:0},handler:(e,t)=>{var{parser:n,funcName:r}=e,i=r;return i.length===1&&(i=FW[i]),{type:"op",mode:n.mode,limits:!0,parentIsSupSub:!1,symbol:!0,name:i}},htmlBuilder:Zo,mathmlBuilder:tu});fe({type:"op",names:["\\mathop"],props:{numArgs:1,primitive:!0},handler:(e,t)=>{var{parser:n}=e,r=t[0];return{type:"op",mode:n.mode,limits:!1,parentIsSupSub:!1,symbol:!1,body:At(r)}},htmlBuilder:Zo,mathmlBuilder:tu});var jW={"∫":"\\int","∬":"\\iint","∭":"\\iiint","∮":"\\oint","∯":"\\oiint","∰":"\\oiiint"};fe({type:"op",names:["\\arcsin","\\arccos","\\arctan","\\arctg","\\arcctg","\\arg","\\ch","\\cos","\\cosec","\\cosh","\\cot","\\cotg","\\coth","\\csc","\\ctg","\\cth","\\deg","\\dim","\\exp","\\hom","\\ker","\\lg","\\ln","\\log","\\sec","\\sin","\\sinh","\\sh","\\tan","\\tanh","\\tg","\\th"],props:{numArgs:0},handler(e){var{parser:t,funcName:n}=e;return{type:"op",mode:t.mode,limits:!1,parentIsSupSub:!1,symbol:!1,name:n}},htmlBuilder:Zo,mathmlBuilder:tu});fe({type:"op",names:["\\det","\\gcd","\\inf","\\lim","\\max","\\min","\\Pr","\\sup"],props:{numArgs:0},handler(e){var{parser:t,funcName:n}=e;return{type:"op",mode:t.mode,limits:!0,parentIsSupSub:!1,symbol:!1,name:n}},htmlBuilder:Zo,mathmlBuilder:tu});fe({type:"op",names:["\\int","\\iint","\\iiint","\\oint","\\oiint","\\oiiint","∫","∬","∭","∮","∯","∰"],props:{numArgs:0},handler(e){var{parser:t,funcName:n}=e,r=n;return r.length===1&&(r=jW[r]),{type:"op",mode:t.mode,limits:!1,parentIsSupSub:!1,symbol:!0,name:r}},htmlBuilder:Zo,mathmlBuilder:tu});var mT=(e,t)=>{var n,r,i=!1,a;e.type==="supsub"?(n=e.sup,r=e.sub,a=Re(e.base,"operatorname"),i=!0):a=Re(e,"operatorname");var s;if(a.body.length>0){for(var o=a.body.map(d=>{var m=d.text;return typeof m=="string"?{type:"textord",mode:d.mode,text:m}:d}),l=Bt(o,t.withFont("mathrm"),!0),c=0;c<l.length;c++){var u=l[c];u instanceof pr&&(u.text=u.text.replace(/\u2212/,"-").replace(/\u2217/,"*"))}s=V.makeSpan(["mop"],l,t)}else s=V.makeSpan(["mop"],[],t);return i?uT(s,n,r,t,t.style,0,0):s},VW=(e,t)=>{for(var n=Un(e.body,t.withFont("mathrm")),r=!0,i=0;i<n.length;i++){var a=n[i];if(!(a instanceof ie.SpaceNode))if(a instanceof ie.MathNode)switch(a.type){case"mi":case"mn":case"ms":case"mspace":case"mtext":break;case"mo":{var s=a.children[0];a.children.length===1&&s instanceof ie.TextNode?s.text=s.text.replace(/\u2212/,"-").replace(/\u2217/,"*"):r=!1;break}default:r=!1}else r=!1}if(r){var o=n.map(u=>u.toText()).join("");n=[new ie.TextNode(o)]}var l=new ie.MathNode("mi",n);l.setAttribute("mathvariant","normal");var c=new ie.MathNode("mo",[fr("⁡","text")]);return e.parentIsSupSub?new ie.MathNode("mrow",[l,c]):ie.newDocumentFragment([l,c])};fe({type:"operatorname",names:["\\operatorname@","\\operatornamewithlimits"],props:{numArgs:1},handler:(e,t)=>{var{parser:n,funcName:r}=e,i=t[0];return{type:"operatorname",mode:n.mode,body:At(i),alwaysHandleSupSub:r==="\\operatornamewithlimits",limits:!1,parentIsSupSub:!1}},htmlBuilder:mT,mathmlBuilder:VW});N("\\operatorname","\\@ifstar\\operatornamewithlimits\\operatorname@");ws({type:"ordgroup",htmlBuilder(e,t){return e.semisimple?V.makeFragment(Bt(e.body,t,!1)):V.makeSpan(["mord"],Bt(e.body,t,!0),t)},mathmlBuilder(e,t){return Ia(e.body,t,!0)}});fe({type:"overline",names:["\\overline"],props:{numArgs:1},handler(e,t){var{parser:n}=e,r=t[0];return{type:"overline",mode:n.mode,body:r}},htmlBuilder(e,t){var n=Ge(e.body,t.havingCrampedStyle()),r=V.makeLineSpan("overline-line",t),i=t.fontMetrics().defaultRuleThickness,a=V.makeVList({positionType:"firstBaseline",children:[{type:"elem",elem:n},{type:"kern",size:3*i},{type:"elem",elem:r},{type:"kern",size:i}]},t);return V.makeSpan(["mord","overline"],[a],t)},mathmlBuilder(e,t){var n=new ie.MathNode("mo",[new ie.TextNode("‾")]);n.setAttribute("stretchy","true");var r=new ie.MathNode("mover",[et(e.body,t),n]);return r.setAttribute("accent","true"),r}});fe({type:"phantom",names:["\\phantom"],props:{numArgs:1,allowedInText:!0},handler:(e,t)=>{var{parser:n}=e,r=t[0];return{type:"phantom",mode:n.mode,body:At(r)}},htmlBuilder:(e,t)=>{var n=Bt(e.body,t.withPhantom(),!1);return V.makeFragment(n)},mathmlBuilder:(e,t)=>{var n=Un(e.body,t);return new ie.MathNode("mphantom",n)}});fe({type:"hphantom",names:["\\hphantom"],props:{numArgs:1,allowedInText:!0},handler:(e,t)=>{var{parser:n}=e,r=t[0];return{type:"hphantom",mode:n.mode,body:r}},htmlBuilder:(e,t)=>{var n=V.makeSpan([],[Ge(e.body,t.withPhantom())]);if(n.height=0,n.depth=0,n.children)for(var r=0;r<n.children.length;r++)n.children[r].height=0,n.children[r].depth=0;return n=V.makeVList({positionType:"firstBaseline",children:[{type:"elem",elem:n}]},t),V.makeSpan(["mord"],[n],t)},mathmlBuilder:(e,t)=>{var n=Un(At(e.body),t),r=new ie.MathNode("mphantom",n),i=new ie.MathNode("mpadded",[r]);return i.setAttribute("height","0px"),i.setAttribute("depth","0px"),i}});fe({type:"vphantom",names:["\\vphantom"],props:{numArgs:1,allowedInText:!0},handler:(e,t)=>{var{parser:n}=e,r=t[0];return{type:"vphantom",mode:n.mode,body:r}},htmlBuilder:(e,t)=>{var n=V.makeSpan(["inner"],[Ge(e.body,t.withPhantom())]),r=V.makeSpan(["fix"],[]);return V.makeSpan(["mord","rlap"],[n,r],t)},mathmlBuilder:(e,t)=>{var n=Un(At(e.body),t),r=new ie.MathNode("mphantom",n),i=new ie.MathNode("mpadded",[r]);return i.setAttribute("width","0px"),i}});fe({type:"raisebox",names:["\\raisebox"],props:{numArgs:2,argTypes:["size","hbox"],allowedInText:!0},handler(e,t){var{parser:n}=e,r=Re(t[0],"size").value,i=t[1];return{type:"raisebox",mode:n.mode,dy:r,body:i}},htmlBuilder(e,t){var n=Ge(e.body,t),r=gt(e.dy,t);return V.makeVList({positionType:"shift",positionData:-r,children:[{type:"elem",elem:n}]},t)},mathmlBuilder(e,t){var n=new ie.MathNode("mpadded",[et(e.body,t)]),r=e.dy.number+e.dy.unit;return n.setAttribute("voffset",r),n}});fe({type:"internal",names:["\\relax"],props:{numArgs:0,allowedInText:!0,allowedInArgument:!0},handler(e){var{parser:t}=e;return{type:"internal",mode:t.mode}}});fe({type:"rule",names:["\\rule"],props:{numArgs:2,numOptionalArgs:1,allowedInText:!0,allowedInMath:!0,argTypes:["size","size","size"]},handler(e,t,n){var{parser:r}=e,i=n[0],a=Re(t[0],"size"),s=Re(t[1],"size");return{type:"rule",mode:r.mode,shift:i&&Re(i,"size").value,width:a.value,height:s.value}},htmlBuilder(e,t){var n=V.makeSpan(["mord","rule"],[],t),r=gt(e.width,t),i=gt(e.height,t),a=e.shift?gt(e.shift,t):0;return n.style.borderRightWidth=me(r),n.style.borderTopWidth=me(i),n.style.bottom=me(a),n.width=r,n.height=i+a,n.depth=-a,n.maxFontSize=i*1.125*t.sizeMultiplier,n},mathmlBuilder(e,t){var n=gt(e.width,t),r=gt(e.height,t),i=e.shift?gt(e.shift,t):0,a=t.color&&t.getColor()||"black",s=new ie.MathNode("mspace");s.setAttribute("mathbackground",a),s.setAttribute("width",me(n)),s.setAttribute("height",me(r));var o=new ie.MathNode("mpadded",[s]);return i>=0?o.setAttribute("height",me(i)):(o.setAttribute("height",me(i)),o.setAttribute("depth",me(-i))),o.setAttribute("voffset",me(i)),o}});function hT(e,t,n){for(var r=Bt(e,t,!1),i=t.sizeMultiplier/n.sizeMultiplier,a=0;a<r.length;a++){var s=r[a].classes.indexOf("sizing");s<0?Array.prototype.push.apply(r[a].classes,t.sizingClasses(n)):r[a].classes[s+1]==="reset-size"+t.size&&(r[a].classes[s+1]="reset-size"+n.size),r[a].height*=i,r[a].depth*=i}return V.makeFragment(r)}var Y3=["\\tiny","\\sixptsize","\\scriptsize","\\footnotesize","\\small","\\normalsize","\\large","\\Large","\\LARGE","\\huge","\\Huge"],HW=(e,t)=>{var n=t.havingSize(e.size);return hT(e.body,n,t)};fe({type:"sizing",names:Y3,props:{numArgs:0,allowedInText:!0},handler:(e,t)=>{var{breakOnTokenText:n,funcName:r,parser:i}=e,a=i.parseExpression(!1,n);return{type:"sizing",mode:i.mode,size:Y3.indexOf(r)+1,body:a}},htmlBuilder:HW,mathmlBuilder:(e,t)=>{var n=t.havingSize(e.size),r=Un(e.body,n),i=new ie.MathNode("mstyle",r);return i.setAttribute("mathsize",me(n.sizeMultiplier)),i}});fe({type:"smash",names:["\\smash"],props:{numArgs:1,numOptionalArgs:1,allowedInText:!0},handler:(e,t,n)=>{var{parser:r}=e,i=!1,a=!1,s=n[0]&&Re(n[0],"ordgroup");if(s)for(var o="",l=0;l<s.body.length;++l){var c=s.body[l];if(o=c.text,o==="t")i=!0;else if(o==="b")a=!0;else{i=!1,a=!1;break}}else i=!0,a=!0;var u=t[0];return{type:"smash",mode:r.mode,body:u,smashHeight:i,smashDepth:a}},htmlBuilder:(e,t)=>{var n=V.makeSpan([],[Ge(e.body,t)]);if(!e.smashHeight&&!e.smashDepth)return n;if(e.smashHeight&&(n.height=0,n.children))for(var r=0;r<n.children.length;r++)n.children[r].height=0;if(e.smashDepth&&(n.depth=0,n.children))for(var i=0;i<n.children.length;i++)n.children[i].depth=0;var a=V.makeVList({positionType:"firstBaseline",children:[{type:"elem",elem:n}]},t);return V.makeSpan(["mord"],[a],t)},mathmlBuilder:(e,t)=>{var n=new ie.MathNode("mpadded",[et(e.body,t)]);return e.smashHeight&&n.setAttribute("height","0px"),e.smashDepth&&n.setAttribute("depth","0px"),n}});fe({type:"sqrt",names:["\\sqrt"],props:{numArgs:1,numOptionalArgs:1},handler(e,t,n){var{parser:r}=e,i=n[0],a=t[0];return{type:"sqrt",mode:r.mode,body:a,index:i}},htmlBuilder(e,t){var n=Ge(e.body,t.havingCrampedStyle());n.height===0&&(n.height=t.fontMetrics().xHeight),n=V.wrapFragment(n,t);var r=t.fontMetrics(),i=r.defaultRuleThickness,a=i;t.style.id<we.TEXT.id&&(a=t.fontMetrics().xHeight);var s=i+a/4,o=n.height+n.depth+s+i,{span:l,ruleWidth:c,advanceWidth:u}=Ni.sqrtImage(o,t),d=l.height-c;d>n.height+n.depth+s&&(s=(s+d-n.height-n.depth)/2);var m=l.height-n.height-s-c;n.style.paddingLeft=me(u);var p=V.makeVList({positionType:"firstBaseline",children:[{type:"elem",elem:n,wrapperClasses:["svg-align"]},{type:"kern",size:-(n.height+m)},{type:"elem",elem:l},{type:"kern",size:c}]},t);if(e.index){var x=t.havingStyle(we.SCRIPTSCRIPT),g=Ge(e.index,x,t),w=.6*(p.height-p.depth),v=V.makeVList({positionType:"shift",positionData:-w,children:[{type:"elem",elem:g}]},t),$=V.makeSpan(["root"],[v]);return V.makeSpan(["mord","sqrt"],[$,p],t)}else return V.makeSpan(["mord","sqrt"],[p],t)},mathmlBuilder(e,t){var{body:n,index:r}=e;return r?new ie.MathNode("mroot",[et(n,t),et(r,t)]):new ie.MathNode("msqrt",[et(n,t)])}});var X3={display:we.DISPLAY,text:we.TEXT,script:we.SCRIPT,scriptscript:we.SCRIPTSCRIPT};fe({type:"styling",names:["\\displaystyle","\\textstyle","\\scriptstyle","\\scriptscriptstyle"],props:{numArgs:0,allowedInText:!0,primitive:!0},handler(e,t){var{breakOnTokenText:n,funcName:r,parser:i}=e,a=i.parseExpression(!0,n),s=r.slice(1,r.length-5);return{type:"styling",mode:i.mode,style:s,body:a}},htmlBuilder(e,t){var n=X3[e.style],r=t.havingStyle(n).withFont("");return hT(e.body,r,t)},mathmlBuilder(e,t){var n=X3[e.style],r=t.havingStyle(n),i=Un(e.body,r),a=new ie.MathNode("mstyle",i),s={display:["0","true"],text:["0","false"],script:["1","false"],scriptscript:["2","false"]},o=s[e.style];return a.setAttribute("scriptlevel",o[0]),a.setAttribute("displaystyle",o[1]),a}});var qW=function(t,n){var r=t.base;if(r)if(r.type==="op"){var i=r.limits&&(n.style.size===we.DISPLAY.size||r.alwaysHandleSupSub);return i?Zo:null}else if(r.type==="operatorname"){var a=r.alwaysHandleSupSub&&(n.style.size===we.DISPLAY.size||r.limits);return a?mT:null}else{if(r.type==="accent")return ye.isCharacterBox(r.base)?G2:null;if(r.type==="horizBrace"){var s=!t.sub;return s===r.isOver?cT:null}else return null}else return null};ws({type:"supsub",htmlBuilder(e,t){var n=qW(e,t);if(n)return n(e,t);var{base:r,sup:i,sub:a}=e,s=Ge(r,t),o,l,c=t.fontMetrics(),u=0,d=0,m=r&&ye.isCharacterBox(r);if(i){var p=t.havingStyle(t.style.sup());o=Ge(i,p,t),m||(u=s.height-p.fontMetrics().supDrop*p.sizeMultiplier/t.sizeMultiplier)}if(a){var x=t.havingStyle(t.style.sub());l=Ge(a,x,t),m||(d=s.depth+x.fontMetrics().subDrop*x.sizeMultiplier/t.sizeMultiplier)}var g;t.style===we.DISPLAY?g=c.sup1:t.style.cramped?g=c.sup3:g=c.sup2;var w=t.sizeMultiplier,v=me(.5/c.ptPerEm/w),$=null;if(l){var _=e.base&&e.base.type==="op"&&e.base.name&&(e.base.name==="\\oiint"||e.base.name==="\\oiiint");(s instanceof pr||_)&&($=me(-s.italic))}var C;if(o&&l){u=Math.max(u,g,o.depth+.25*c.xHeight),d=Math.max(d,c.sub2);var k=c.defaultRuleThickness,S=4*k;if(u-o.depth-(l.height-d)<S){d=S-(u-o.depth)+l.height;var L=.8*c.xHeight-(u-o.depth);L>0&&(u+=L,d-=L)}var U=[{type:"elem",elem:l,shift:d,marginRight:v,marginLeft:$},{type:"elem",elem:o,shift:-u,marginRight:v}];C=V.makeVList({positionType:"individualShift",children:U},t)}else if(l){d=Math.max(d,c.sub1,l.height-.8*c.xHeight);var F=[{type:"elem",elem:l,marginLeft:$,marginRight:v}];C=V.makeVList({positionType:"shift",positionData:d,children:F},t)}else if(o)u=Math.max(u,g,o.depth+.25*c.xHeight),C=V.makeVList({positionType:"shift",positionData:-u,children:[{type:"elem",elem:o,marginRight:v}]},t);else throw new Error("supsub must have either sup or sub.");var q=ng(s,"right")||"mord";return V.makeSpan([q],[s,V.makeSpan(["msupsub"],[C])],t)},mathmlBuilder(e,t){var n=!1,r,i;e.base&&e.base.type==="horizBrace"&&(i=!!e.sup,i===e.base.isOver&&(n=!0,r=e.base.isOver)),e.base&&(e.base.type==="op"||e.base.type==="operatorname")&&(e.base.parentIsSupSub=!0);var a=[et(e.base,t)];e.sub&&a.push(et(e.sub,t)),e.sup&&a.push(et(e.sup,t));var s;if(n)s=r?"mover":"munder";else if(e.sub)if(e.sup){var c=e.base;c&&c.type==="op"&&c.limits&&t.style===we.DISPLAY||c&&c.type==="operatorname"&&c.alwaysHandleSupSub&&(t.style===we.DISPLAY||c.limits)?s="munderover":s="msubsup"}else{var l=e.base;l&&l.type==="op"&&l.limits&&(t.style===we.DISPLAY||l.alwaysHandleSupSub)||l&&l.type==="operatorname"&&l.alwaysHandleSupSub&&(l.limits||t.style===we.DISPLAY)?s="munder":s="msub"}else{var o=e.base;o&&o.type==="op"&&o.limits&&(t.style===we.DISPLAY||o.alwaysHandleSupSub)||o&&o.type==="operatorname"&&o.alwaysHandleSupSub&&(o.limits||t.style===we.DISPLAY)?s="mover":s="msup"}return new ie.MathNode(s,a)}});ws({type:"atom",htmlBuilder(e,t){return V.mathsym(e.text,e.mode,t,["m"+e.family])},mathmlBuilder(e,t){var n=new ie.MathNode("mo",[fr(e.text,e.mode)]);if(e.family==="bin"){var r=H2(e,t);r==="bold-italic"&&n.setAttribute("mathvariant",r)}else e.family==="punct"?n.setAttribute("separator","true"):(e.family==="open"||e.family==="close")&&n.setAttribute("stretchy","false");return n}});var pT={mi:"italic",mn:"normal",mtext:"normal"};ws({type:"mathord",htmlBuilder(e,t){return V.makeOrd(e,t,"mathord")},mathmlBuilder(e,t){var n=new ie.MathNode("mi",[fr(e.text,e.mode,t)]),r=H2(e,t)||"italic";return r!==pT[n.type]&&n.setAttribute("mathvariant",r),n}});ws({type:"textord",htmlBuilder(e,t){return V.makeOrd(e,t,"textord")},mathmlBuilder(e,t){var n=fr(e.text,e.mode,t),r=H2(e,t)||"normal",i;return e.mode==="text"?i=new ie.MathNode("mtext",[n]):/[0-9]/.test(e.text)?i=new ie.MathNode("mn",[n]):e.text==="\\prime"?i=new ie.MathNode("mo",[n]):i=new ie.MathNode("mi",[n]),r!==pT[i.type]&&i.setAttribute("mathvariant",r),i}});var ep={"\\nobreak":"nobreak","\\allowbreak":"allowbreak"},tp={" ":{},"\\ ":{},"~":{className:"nobreak"},"\\space":{},"\\nobreakspace":{className:"nobreak"}};ws({type:"spacing",htmlBuilder(e,t){if(tp.hasOwnProperty(e.text)){var n=tp[e.text].className||"";if(e.mode==="text"){var r=V.makeOrd(e,t,"textord");return r.classes.push(n),r}else return V.makeSpan(["mspace",n],[V.mathsym(e.text,e.mode,t)],t)}else{if(ep.hasOwnProperty(e.text))return V.makeSpan(["mspace",ep[e.text]],[],t);throw new oe('Unknown type of space "'+e.text+'"')}},mathmlBuilder(e,t){var n;if(tp.hasOwnProperty(e.text))n=new ie.MathNode("mtext",[new ie.TextNode(" ")]);else{if(ep.hasOwnProperty(e.text))return new ie.MathNode("mspace");throw new oe('Unknown type of space "'+e.text+'"')}return n}});var Q3=()=>{var e=new ie.MathNode("mtd",[]);return e.setAttribute("width","50%"),e};ws({type:"tag",mathmlBuilder(e,t){var n=new ie.MathNode("mtable",[new ie.MathNode("mtr",[Q3(),new ie.MathNode("mtd",[Ia(e.body,t)]),Q3(),new ie.MathNode("mtd",[Ia(e.tag,t)])])]);return n.setAttribute("width","100%"),n}});var Z3={"\\text":void 0,"\\textrm":"textrm","\\textsf":"textsf","\\texttt":"texttt","\\textnormal":"textrm"},J3={"\\textbf":"textbf","\\textmd":"textmd"},GW={"\\textit":"textit","\\textup":"textup"},e$=(e,t)=>{var n=e.font;if(n){if(Z3[n])return t.withTextFontFamily(Z3[n]);if(J3[n])return t.withTextFontWeight(J3[n]);if(n==="\\emph")return t.fontShape==="textit"?t.withTextFontShape("textup"):t.withTextFontShape("textit")}else return t;return t.withTextFontShape(GW[n])};fe({type:"text",names:["\\text","\\textrm","\\textsf","\\texttt","\\textnormal","\\textbf","\\textmd","\\textit","\\textup","\\emph"],props:{numArgs:1,argTypes:["text"],allowedInArgument:!0,allowedInText:!0},handler(e,t){var{parser:n,funcName:r}=e,i=t[0];return{type:"text",mode:n.mode,body:At(i),font:r}},htmlBuilder(e,t){var n=e$(e,t),r=Bt(e.body,n,!0);return V.makeSpan(["mord","text"],r,n)},mathmlBuilder(e,t){var n=e$(e,t);return Ia(e.body,n)}});fe({type:"underline",names:["\\underline"],props:{numArgs:1,allowedInText:!0},handler(e,t){var{parser:n}=e;return{type:"underline",mode:n.mode,body:t[0]}},htmlBuilder(e,t){var n=Ge(e.body,t),r=V.makeLineSpan("underline-line",t),i=t.fontMetrics().defaultRuleThickness,a=V.makeVList({positionType:"top",positionData:n.height,children:[{type:"kern",size:i},{type:"elem",elem:r},{type:"kern",size:3*i},{type:"elem",elem:n}]},t);return V.makeSpan(["mord","underline"],[a],t)},mathmlBuilder(e,t){var n=new ie.MathNode("mo",[new ie.TextNode("‾")]);n.setAttribute("stretchy","true");var r=new ie.MathNode("munder",[et(e.body,t),n]);return r.setAttribute("accentunder","true"),r}});fe({type:"vcenter",names:["\\vcenter"],props:{numArgs:1,argTypes:["original"],allowedInText:!1},handler(e,t){var{parser:n}=e;return{type:"vcenter",mode:n.mode,body:t[0]}},htmlBuilder(e,t){var n=Ge(e.body,t),r=t.fontMetrics().axisHeight,i=.5*(n.height-r-(n.depth+r));return V.makeVList({positionType:"shift",positionData:i,children:[{type:"elem",elem:n}]},t)},mathmlBuilder(e,t){return new ie.MathNode("mpadded",[et(e.body,t)],["vcenter"])}});fe({type:"verb",names:["\\verb"],props:{numArgs:0,allowedInText:!0},handler(e,t,n){throw new oe("\\verb ended by end of line instead of matching delimiter")},htmlBuilder(e,t){for(var n=t$(e),r=[],i=t.havingStyle(t.style.text()),a=0;a<n.length;a++){var s=n[a];s==="~"&&(s="\\textasciitilde"),r.push(V.makeSymbol(s,"Typewriter-Regular",e.mode,i,["mord","texttt"]))}return V.makeSpan(["mord","text"].concat(i.sizingClasses(t)),V.tryCombineChars(r),i)},mathmlBuilder(e,t){var n=new ie.TextNode(t$(e)),r=new ie.MathNode("mtext",[n]);return r.setAttribute("mathvariant","monospace"),r}});var t$=e=>e.body.replace(/ /g,e.star?"␣":" "),ha=Rw,fT=`[ \r
	]`,WW="\\\\[a-zA-Z@]+",KW="\\\\[^\uD800-\uDFFF]",YW="("+WW+")"+fT+"*",XW=`\\\\(
|[ \r	]+
?)[ \r	]*`,sg="[̀-ͯ]",QW=new RegExp(sg+"+$"),ZW="("+fT+"+)|"+(XW+"|")+"([!-\\[\\]-‧‪-퟿豈-￿]"+(sg+"*")+"|[\uD800-\uDBFF][\uDC00-\uDFFF]"+(sg+"*")+"|\\\\verb\\*([^]).*?\\4|\\\\verb([^*a-zA-Z]).*?\\5"+("|"+YW)+("|"+KW+")");class n${constructor(t,n){this.input=void 0,this.settings=void 0,this.tokenRegex=void 0,this.catcodes=void 0,this.input=t,this.settings=n,this.tokenRegex=new RegExp(ZW,"g"),this.catcodes={"%":14,"~":13}}setCatcode(t,n){this.catcodes[t]=n}lex(){var t=this.input,n=this.tokenRegex.lastIndex;if(n===t.length)return new dr("EOF",new Fn(this,n,n));var r=this.tokenRegex.exec(t);if(r===null||r.index!==n)throw new oe("Unexpected character: '"+t[n]+"'",new dr(t[n],new Fn(this,n,n+1)));var i=r[6]||r[3]||(r[2]?"\\ ":" ");if(this.catcodes[i]===14){var a=t.indexOf(`
`,this.tokenRegex.lastIndex);return a===-1?(this.tokenRegex.lastIndex=t.length,this.settings.reportNonstrict("commentAtEnd","% comment has no terminating newline; LaTeX would fail because of commenting the end of math mode (e.g. $)")):this.tokenRegex.lastIndex=a+1,this.lex()}return new dr(i,new Fn(this,n,this.tokenRegex.lastIndex))}}class JW{constructor(t,n){t===void 0&&(t={}),n===void 0&&(n={}),this.current=void 0,this.builtins=void 0,this.undefStack=void 0,this.current=n,this.builtins=t,this.undefStack=[]}beginGroup(){this.undefStack.push({})}endGroup(){if(this.undefStack.length===0)throw new oe("Unbalanced namespace destruction: attempt to pop global namespace; please report this as a bug");var t=this.undefStack.pop();for(var n in t)t.hasOwnProperty(n)&&(t[n]==null?delete this.current[n]:this.current[n]=t[n])}endGroups(){for(;this.undefStack.length>0;)this.endGroup()}has(t){return this.current.hasOwnProperty(t)||this.builtins.hasOwnProperty(t)}get(t){return this.current.hasOwnProperty(t)?this.current[t]:this.builtins[t]}set(t,n,r){if(r===void 0&&(r=!1),r){for(var i=0;i<this.undefStack.length;i++)delete this.undefStack[i][t];this.undefStack.length>0&&(this.undefStack[this.undefStack.length-1][t]=n)}else{var a=this.undefStack[this.undefStack.length-1];a&&!a.hasOwnProperty(t)&&(a[t]=this.current[t])}n==null?delete this.current[t]:this.current[t]=n}}var eK=iT;N("\\noexpand",function(e){var t=e.popToken();return e.isExpandable(t.text)&&(t.noexpand=!0,t.treatAsRelax=!0),{tokens:[t],numArgs:0}});N("\\expandafter",function(e){var t=e.popToken();return e.expandOnce(!0),{tokens:[t],numArgs:0}});N("\\@firstoftwo",function(e){var t=e.consumeArgs(2);return{tokens:t[0],numArgs:0}});N("\\@secondoftwo",function(e){var t=e.consumeArgs(2);return{tokens:t[1],numArgs:0}});N("\\@ifnextchar",function(e){var t=e.consumeArgs(3);e.consumeSpaces();var n=e.future();return t[0].length===1&&t[0][0].text===n.text?{tokens:t[1],numArgs:0}:{tokens:t[2],numArgs:0}});N("\\@ifstar","\\@ifnextchar *{\\@firstoftwo{#1}}");N("\\TextOrMath",function(e){var t=e.consumeArgs(2);return e.mode==="text"?{tokens:t[0],numArgs:0}:{tokens:t[1],numArgs:0}});var r$={0:0,1:1,2:2,3:3,4:4,5:5,6:6,7:7,8:8,9:9,a:10,A:10,b:11,B:11,c:12,C:12,d:13,D:13,e:14,E:14,f:15,F:15};N("\\char",function(e){var t=e.popToken(),n,r="";if(t.text==="'")n=8,t=e.popToken();else if(t.text==='"')n=16,t=e.popToken();else if(t.text==="`")if(t=e.popToken(),t.text[0]==="\\")r=t.text.charCodeAt(1);else{if(t.text==="EOF")throw new oe("\\char` missing argument");r=t.text.charCodeAt(0)}else n=10;if(n){if(r=r$[t.text],r==null||r>=n)throw new oe("Invalid base-"+n+" digit "+t.text);for(var i;(i=r$[e.future().text])!=null&&i<n;)r*=n,r+=i,e.popToken()}return"\\@char{"+r+"}"});var eb=(e,t,n,r)=>{var i=e.consumeArg().tokens;if(i.length!==1)throw new oe("\\newcommand's first argument must be a macro name");var a=i[0].text,s=e.isDefined(a);if(s&&!t)throw new oe("\\newcommand{"+a+"} attempting to redefine "+(a+"; use \\renewcommand"));if(!s&&!n)throw new oe("\\renewcommand{"+a+"} when command "+a+" does not yet exist; use \\newcommand");var o=0;if(i=e.consumeArg().tokens,i.length===1&&i[0].text==="["){for(var l="",c=e.expandNextToken();c.text!=="]"&&c.text!=="EOF";)l+=c.text,c=e.expandNextToken();if(!l.match(/^\s*[0-9]+\s*$/))throw new oe("Invalid number of arguments: "+l);o=parseInt(l),i=e.consumeArg().tokens}return s&&r||e.macros.set(a,{tokens:i,numArgs:o}),""};N("\\newcommand",e=>eb(e,!1,!0,!1));N("\\renewcommand",e=>eb(e,!0,!1,!1));N("\\providecommand",e=>eb(e,!0,!0,!0));N("\\message",e=>{var t=e.consumeArgs(1)[0];return console.log(t.reverse().map(n=>n.text).join("")),""});N("\\errmessage",e=>{var t=e.consumeArgs(1)[0];return console.error(t.reverse().map(n=>n.text).join("")),""});N("\\show",e=>{var t=e.popToken(),n=t.text;return console.log(t,e.macros.get(n),ha[n],at.math[n],at.text[n]),""});N("\\bgroup","{");N("\\egroup","}");N("~","\\nobreakspace");N("\\lq","`");N("\\rq","'");N("\\aa","\\r a");N("\\AA","\\r A");N("\\textcopyright","\\html@mathml{\\textcircled{c}}{\\char`©}");N("\\copyright","\\TextOrMath{\\textcopyright}{\\text{\\textcopyright}}");N("\\textregistered","\\html@mathml{\\textcircled{\\scriptsize R}}{\\char`®}");N("ℬ","\\mathscr{B}");N("ℰ","\\mathscr{E}");N("ℱ","\\mathscr{F}");N("ℋ","\\mathscr{H}");N("ℐ","\\mathscr{I}");N("ℒ","\\mathscr{L}");N("ℳ","\\mathscr{M}");N("ℛ","\\mathscr{R}");N("ℭ","\\mathfrak{C}");N("ℌ","\\mathfrak{H}");N("ℨ","\\mathfrak{Z}");N("\\Bbbk","\\Bbb{k}");N("·","\\cdotp");N("\\llap","\\mathllap{\\textrm{#1}}");N("\\rlap","\\mathrlap{\\textrm{#1}}");N("\\clap","\\mathclap{\\textrm{#1}}");N("\\mathstrut","\\vphantom{(}");N("\\underbar","\\underline{\\text{#1}}");N("\\not",'\\html@mathml{\\mathrel{\\mathrlap\\@not}}{\\char"338}');N("\\neq","\\html@mathml{\\mathrel{\\not=}}{\\mathrel{\\char`≠}}");N("\\ne","\\neq");N("≠","\\neq");N("\\notin","\\html@mathml{\\mathrel{{\\in}\\mathllap{/\\mskip1mu}}}{\\mathrel{\\char`∉}}");N("∉","\\notin");N("≘","\\html@mathml{\\mathrel{=\\kern{-1em}\\raisebox{0.4em}{$\\scriptsize\\frown$}}}{\\mathrel{\\char`≘}}");N("≙","\\html@mathml{\\stackrel{\\tiny\\wedge}{=}}{\\mathrel{\\char`≘}}");N("≚","\\html@mathml{\\stackrel{\\tiny\\vee}{=}}{\\mathrel{\\char`≚}}");N("≛","\\html@mathml{\\stackrel{\\scriptsize\\star}{=}}{\\mathrel{\\char`≛}}");N("≝","\\html@mathml{\\stackrel{\\tiny\\mathrm{def}}{=}}{\\mathrel{\\char`≝}}");N("≞","\\html@mathml{\\stackrel{\\tiny\\mathrm{m}}{=}}{\\mathrel{\\char`≞}}");N("≟","\\html@mathml{\\stackrel{\\tiny?}{=}}{\\mathrel{\\char`≟}}");N("⟂","\\perp");N("‼","\\mathclose{!\\mkern-0.8mu!}");N("∌","\\notni");N("⌜","\\ulcorner");N("⌝","\\urcorner");N("⌞","\\llcorner");N("⌟","\\lrcorner");N("©","\\copyright");N("®","\\textregistered");N("️","\\textregistered");N("\\ulcorner",'\\html@mathml{\\@ulcorner}{\\mathop{\\char"231c}}');N("\\urcorner",'\\html@mathml{\\@urcorner}{\\mathop{\\char"231d}}');N("\\llcorner",'\\html@mathml{\\@llcorner}{\\mathop{\\char"231e}}');N("\\lrcorner",'\\html@mathml{\\@lrcorner}{\\mathop{\\char"231f}}');N("\\vdots","{\\varvdots\\rule{0pt}{15pt}}");N("⋮","\\vdots");N("\\varGamma","\\mathit{\\Gamma}");N("\\varDelta","\\mathit{\\Delta}");N("\\varTheta","\\mathit{\\Theta}");N("\\varLambda","\\mathit{\\Lambda}");N("\\varXi","\\mathit{\\Xi}");N("\\varPi","\\mathit{\\Pi}");N("\\varSigma","\\mathit{\\Sigma}");N("\\varUpsilon","\\mathit{\\Upsilon}");N("\\varPhi","\\mathit{\\Phi}");N("\\varPsi","\\mathit{\\Psi}");N("\\varOmega","\\mathit{\\Omega}");N("\\substack","\\begin{subarray}{c}#1\\end{subarray}");N("\\colon","\\nobreak\\mskip2mu\\mathpunct{}\\mathchoice{\\mkern-3mu}{\\mkern-3mu}{}{}{:}\\mskip6mu\\relax");N("\\boxed","\\fbox{$\\displaystyle{#1}$}");N("\\iff","\\DOTSB\\;\\Longleftrightarrow\\;");N("\\implies","\\DOTSB\\;\\Longrightarrow\\;");N("\\impliedby","\\DOTSB\\;\\Longleftarrow\\;");N("\\dddot","{\\overset{\\raisebox{-0.1ex}{\\normalsize ...}}{#1}}");N("\\ddddot","{\\overset{\\raisebox{-0.1ex}{\\normalsize ....}}{#1}}");var i$={",":"\\dotsc","\\not":"\\dotsb","+":"\\dotsb","=":"\\dotsb","<":"\\dotsb",">":"\\dotsb","-":"\\dotsb","*":"\\dotsb",":":"\\dotsb","\\DOTSB":"\\dotsb","\\coprod":"\\dotsb","\\bigvee":"\\dotsb","\\bigwedge":"\\dotsb","\\biguplus":"\\dotsb","\\bigcap":"\\dotsb","\\bigcup":"\\dotsb","\\prod":"\\dotsb","\\sum":"\\dotsb","\\bigotimes":"\\dotsb","\\bigoplus":"\\dotsb","\\bigodot":"\\dotsb","\\bigsqcup":"\\dotsb","\\And":"\\dotsb","\\longrightarrow":"\\dotsb","\\Longrightarrow":"\\dotsb","\\longleftarrow":"\\dotsb","\\Longleftarrow":"\\dotsb","\\longleftrightarrow":"\\dotsb","\\Longleftrightarrow":"\\dotsb","\\mapsto":"\\dotsb","\\longmapsto":"\\dotsb","\\hookrightarrow":"\\dotsb","\\doteq":"\\dotsb","\\mathbin":"\\dotsb","\\mathrel":"\\dotsb","\\relbar":"\\dotsb","\\Relbar":"\\dotsb","\\xrightarrow":"\\dotsb","\\xleftarrow":"\\dotsb","\\DOTSI":"\\dotsi","\\int":"\\dotsi","\\oint":"\\dotsi","\\iint":"\\dotsi","\\iiint":"\\dotsi","\\iiiint":"\\dotsi","\\idotsint":"\\dotsi","\\DOTSX":"\\dotsx"};N("\\dots",function(e){var t="\\dotso",n=e.expandAfterFuture().text;return n in i$?t=i$[n]:(n.slice(0,4)==="\\not"||n in at.math&&ye.contains(["bin","rel"],at.math[n].group))&&(t="\\dotsb"),t});var tb={")":!0,"]":!0,"\\rbrack":!0,"\\}":!0,"\\rbrace":!0,"\\rangle":!0,"\\rceil":!0,"\\rfloor":!0,"\\rgroup":!0,"\\rmoustache":!0,"\\right":!0,"\\bigr":!0,"\\biggr":!0,"\\Bigr":!0,"\\Biggr":!0,$:!0,";":!0,".":!0,",":!0};N("\\dotso",function(e){var t=e.future().text;return t in tb?"\\ldots\\,":"\\ldots"});N("\\dotsc",function(e){var t=e.future().text;return t in tb&&t!==","?"\\ldots\\,":"\\ldots"});N("\\cdots",function(e){var t=e.future().text;return t in tb?"\\@cdots\\,":"\\@cdots"});N("\\dotsb","\\cdots");N("\\dotsm","\\cdots");N("\\dotsi","\\!\\cdots");N("\\dotsx","\\ldots\\,");N("\\DOTSI","\\relax");N("\\DOTSB","\\relax");N("\\DOTSX","\\relax");N("\\tmspace","\\TextOrMath{\\kern#1#3}{\\mskip#1#2}\\relax");N("\\,","\\tmspace+{3mu}{.1667em}");N("\\thinspace","\\,");N("\\>","\\mskip{4mu}");N("\\:","\\tmspace+{4mu}{.2222em}");N("\\medspace","\\:");N("\\;","\\tmspace+{5mu}{.2777em}");N("\\thickspace","\\;");N("\\!","\\tmspace-{3mu}{.1667em}");N("\\negthinspace","\\!");N("\\negmedspace","\\tmspace-{4mu}{.2222em}");N("\\negthickspace","\\tmspace-{5mu}{.277em}");N("\\enspace","\\kern.5em ");N("\\enskip","\\hskip.5em\\relax");N("\\quad","\\hskip1em\\relax");N("\\qquad","\\hskip2em\\relax");N("\\tag","\\@ifstar\\tag@literal\\tag@paren");N("\\tag@paren","\\tag@literal{({#1})}");N("\\tag@literal",e=>{if(e.macros.get("\\df@tag"))throw new oe("Multiple \\tag");return"\\gdef\\df@tag{\\text{#1}}"});N("\\bmod","\\mathchoice{\\mskip1mu}{\\mskip1mu}{\\mskip5mu}{\\mskip5mu}\\mathbin{\\rm mod}\\mathchoice{\\mskip1mu}{\\mskip1mu}{\\mskip5mu}{\\mskip5mu}");N("\\pod","\\allowbreak\\mathchoice{\\mkern18mu}{\\mkern8mu}{\\mkern8mu}{\\mkern8mu}(#1)");N("\\pmod","\\pod{{\\rm mod}\\mkern6mu#1}");N("\\mod","\\allowbreak\\mathchoice{\\mkern18mu}{\\mkern12mu}{\\mkern12mu}{\\mkern12mu}{\\rm mod}\\,\\,#1");N("\\newline","\\\\\\relax");N("\\TeX","\\textrm{\\html@mathml{T\\kern-.1667em\\raisebox{-.5ex}{E}\\kern-.125emX}{TeX}}");var gT=me(Jr["Main-Regular"][84][1]-.7*Jr["Main-Regular"][65][1]);N("\\LaTeX","\\textrm{\\html@mathml{"+("L\\kern-.36em\\raisebox{"+gT+"}{\\scriptstyle A}")+"\\kern-.15em\\TeX}{LaTeX}}");N("\\KaTeX","\\textrm{\\html@mathml{"+("K\\kern-.17em\\raisebox{"+gT+"}{\\scriptstyle A}")+"\\kern-.15em\\TeX}{KaTeX}}");N("\\hspace","\\@ifstar\\@hspacer\\@hspace");N("\\@hspace","\\hskip #1\\relax");N("\\@hspacer","\\rule{0pt}{0pt}\\hskip #1\\relax");N("\\ordinarycolon",":");N("\\vcentcolon","\\mathrel{\\mathop\\ordinarycolon}");N("\\dblcolon",'\\html@mathml{\\mathrel{\\vcentcolon\\mathrel{\\mkern-.9mu}\\vcentcolon}}{\\mathop{\\char"2237}}');N("\\coloneqq",'\\html@mathml{\\mathrel{\\vcentcolon\\mathrel{\\mkern-1.2mu}=}}{\\mathop{\\char"2254}}');N("\\Coloneqq",'\\html@mathml{\\mathrel{\\dblcolon\\mathrel{\\mkern-1.2mu}=}}{\\mathop{\\char"2237\\char"3d}}');N("\\coloneq",'\\html@mathml{\\mathrel{\\vcentcolon\\mathrel{\\mkern-1.2mu}\\mathrel{-}}}{\\mathop{\\char"3a\\char"2212}}');N("\\Coloneq",'\\html@mathml{\\mathrel{\\dblcolon\\mathrel{\\mkern-1.2mu}\\mathrel{-}}}{\\mathop{\\char"2237\\char"2212}}');N("\\eqqcolon",'\\html@mathml{\\mathrel{=\\mathrel{\\mkern-1.2mu}\\vcentcolon}}{\\mathop{\\char"2255}}');N("\\Eqqcolon",'\\html@mathml{\\mathrel{=\\mathrel{\\mkern-1.2mu}\\dblcolon}}{\\mathop{\\char"3d\\char"2237}}');N("\\eqcolon",'\\html@mathml{\\mathrel{\\mathrel{-}\\mathrel{\\mkern-1.2mu}\\vcentcolon}}{\\mathop{\\char"2239}}');N("\\Eqcolon",'\\html@mathml{\\mathrel{\\mathrel{-}\\mathrel{\\mkern-1.2mu}\\dblcolon}}{\\mathop{\\char"2212\\char"2237}}');N("\\colonapprox",'\\html@mathml{\\mathrel{\\vcentcolon\\mathrel{\\mkern-1.2mu}\\approx}}{\\mathop{\\char"3a\\char"2248}}');N("\\Colonapprox",'\\html@mathml{\\mathrel{\\dblcolon\\mathrel{\\mkern-1.2mu}\\approx}}{\\mathop{\\char"2237\\char"2248}}');N("\\colonsim",'\\html@mathml{\\mathrel{\\vcentcolon\\mathrel{\\mkern-1.2mu}\\sim}}{\\mathop{\\char"3a\\char"223c}}');N("\\Colonsim",'\\html@mathml{\\mathrel{\\dblcolon\\mathrel{\\mkern-1.2mu}\\sim}}{\\mathop{\\char"2237\\char"223c}}');N("∷","\\dblcolon");N("∹","\\eqcolon");N("≔","\\coloneqq");N("≕","\\eqqcolon");N("⩴","\\Coloneqq");N("\\ratio","\\vcentcolon");N("\\coloncolon","\\dblcolon");N("\\colonequals","\\coloneqq");N("\\coloncolonequals","\\Coloneqq");N("\\equalscolon","\\eqqcolon");N("\\equalscoloncolon","\\Eqqcolon");N("\\colonminus","\\coloneq");N("\\coloncolonminus","\\Coloneq");N("\\minuscolon","\\eqcolon");N("\\minuscoloncolon","\\Eqcolon");N("\\coloncolonapprox","\\Colonapprox");N("\\coloncolonsim","\\Colonsim");N("\\simcolon","\\mathrel{\\sim\\mathrel{\\mkern-1.2mu}\\vcentcolon}");N("\\simcoloncolon","\\mathrel{\\sim\\mathrel{\\mkern-1.2mu}\\dblcolon}");N("\\approxcolon","\\mathrel{\\approx\\mathrel{\\mkern-1.2mu}\\vcentcolon}");N("\\approxcoloncolon","\\mathrel{\\approx\\mathrel{\\mkern-1.2mu}\\dblcolon}");N("\\notni","\\html@mathml{\\not\\ni}{\\mathrel{\\char`∌}}");N("\\limsup","\\DOTSB\\operatorname*{lim\\,sup}");N("\\liminf","\\DOTSB\\operatorname*{lim\\,inf}");N("\\injlim","\\DOTSB\\operatorname*{inj\\,lim}");N("\\projlim","\\DOTSB\\operatorname*{proj\\,lim}");N("\\varlimsup","\\DOTSB\\operatorname*{\\overline{lim}}");N("\\varliminf","\\DOTSB\\operatorname*{\\underline{lim}}");N("\\varinjlim","\\DOTSB\\operatorname*{\\underrightarrow{lim}}");N("\\varprojlim","\\DOTSB\\operatorname*{\\underleftarrow{lim}}");N("\\gvertneqq","\\html@mathml{\\@gvertneqq}{≩}");N("\\lvertneqq","\\html@mathml{\\@lvertneqq}{≨}");N("\\ngeqq","\\html@mathml{\\@ngeqq}{≱}");N("\\ngeqslant","\\html@mathml{\\@ngeqslant}{≱}");N("\\nleqq","\\html@mathml{\\@nleqq}{≰}");N("\\nleqslant","\\html@mathml{\\@nleqslant}{≰}");N("\\nshortmid","\\html@mathml{\\@nshortmid}{∤}");N("\\nshortparallel","\\html@mathml{\\@nshortparallel}{∦}");N("\\nsubseteqq","\\html@mathml{\\@nsubseteqq}{⊈}");N("\\nsupseteqq","\\html@mathml{\\@nsupseteqq}{⊉}");N("\\varsubsetneq","\\html@mathml{\\@varsubsetneq}{⊊}");N("\\varsubsetneqq","\\html@mathml{\\@varsubsetneqq}{⫋}");N("\\varsupsetneq","\\html@mathml{\\@varsupsetneq}{⊋}");N("\\varsupsetneqq","\\html@mathml{\\@varsupsetneqq}{⫌}");N("\\imath","\\html@mathml{\\@imath}{ı}");N("\\jmath","\\html@mathml{\\@jmath}{ȷ}");N("\\llbracket","\\html@mathml{\\mathopen{[\\mkern-3.2mu[}}{\\mathopen{\\char`⟦}}");N("\\rrbracket","\\html@mathml{\\mathclose{]\\mkern-3.2mu]}}{\\mathclose{\\char`⟧}}");N("⟦","\\llbracket");N("⟧","\\rrbracket");N("\\lBrace","\\html@mathml{\\mathopen{\\{\\mkern-3.2mu[}}{\\mathopen{\\char`⦃}}");N("\\rBrace","\\html@mathml{\\mathclose{]\\mkern-3.2mu\\}}}{\\mathclose{\\char`⦄}}");N("⦃","\\lBrace");N("⦄","\\rBrace");N("\\minuso","\\mathbin{\\html@mathml{{\\mathrlap{\\mathchoice{\\kern{0.145em}}{\\kern{0.145em}}{\\kern{0.1015em}}{\\kern{0.0725em}}\\circ}{-}}}{\\char`⦵}}");N("⦵","\\minuso");N("\\darr","\\downarrow");N("\\dArr","\\Downarrow");N("\\Darr","\\Downarrow");N("\\lang","\\langle");N("\\rang","\\rangle");N("\\uarr","\\uparrow");N("\\uArr","\\Uparrow");N("\\Uarr","\\Uparrow");N("\\N","\\mathbb{N}");N("\\R","\\mathbb{R}");N("\\Z","\\mathbb{Z}");N("\\alef","\\aleph");N("\\alefsym","\\aleph");N("\\Alpha","\\mathrm{A}");N("\\Beta","\\mathrm{B}");N("\\bull","\\bullet");N("\\Chi","\\mathrm{X}");N("\\clubs","\\clubsuit");N("\\cnums","\\mathbb{C}");N("\\Complex","\\mathbb{C}");N("\\Dagger","\\ddagger");N("\\diamonds","\\diamondsuit");N("\\empty","\\emptyset");N("\\Epsilon","\\mathrm{E}");N("\\Eta","\\mathrm{H}");N("\\exist","\\exists");N("\\harr","\\leftrightarrow");N("\\hArr","\\Leftrightarrow");N("\\Harr","\\Leftrightarrow");N("\\hearts","\\heartsuit");N("\\image","\\Im");N("\\infin","\\infty");N("\\Iota","\\mathrm{I}");N("\\isin","\\in");N("\\Kappa","\\mathrm{K}");N("\\larr","\\leftarrow");N("\\lArr","\\Leftarrow");N("\\Larr","\\Leftarrow");N("\\lrarr","\\leftrightarrow");N("\\lrArr","\\Leftrightarrow");N("\\Lrarr","\\Leftrightarrow");N("\\Mu","\\mathrm{M}");N("\\natnums","\\mathbb{N}");N("\\Nu","\\mathrm{N}");N("\\Omicron","\\mathrm{O}");N("\\plusmn","\\pm");N("\\rarr","\\rightarrow");N("\\rArr","\\Rightarrow");N("\\Rarr","\\Rightarrow");N("\\real","\\Re");N("\\reals","\\mathbb{R}");N("\\Reals","\\mathbb{R}");N("\\Rho","\\mathrm{P}");N("\\sdot","\\cdot");N("\\sect","\\S");N("\\spades","\\spadesuit");N("\\sub","\\subset");N("\\sube","\\subseteq");N("\\supe","\\supseteq");N("\\Tau","\\mathrm{T}");N("\\thetasym","\\vartheta");N("\\weierp","\\wp");N("\\Zeta","\\mathrm{Z}");N("\\argmin","\\DOTSB\\operatorname*{arg\\,min}");N("\\argmax","\\DOTSB\\operatorname*{arg\\,max}");N("\\plim","\\DOTSB\\mathop{\\operatorname{plim}}\\limits");N("\\bra","\\mathinner{\\langle{#1}|}");N("\\ket","\\mathinner{|{#1}\\rangle}");N("\\braket","\\mathinner{\\langle{#1}\\rangle}");N("\\Bra","\\left\\langle#1\\right|");N("\\Ket","\\left|#1\\right\\rangle");var bT=e=>t=>{var n=t.consumeArg().tokens,r=t.consumeArg().tokens,i=t.consumeArg().tokens,a=t.consumeArg().tokens,s=t.macros.get("|"),o=t.macros.get("\\|");t.macros.beginGroup();var l=d=>m=>{e&&(m.macros.set("|",s),i.length&&m.macros.set("\\|",o));var p=d;if(!d&&i.length){var x=m.future();x.text==="|"&&(m.popToken(),p=!0)}return{tokens:p?i:r,numArgs:0}};t.macros.set("|",l(!1)),i.length&&t.macros.set("\\|",l(!0));var c=t.consumeArg().tokens,u=t.expandTokens([...a,...c,...n]);return t.macros.endGroup(),{tokens:u.reverse(),numArgs:0}};N("\\bra@ket",bT(!1));N("\\bra@set",bT(!0));N("\\Braket","\\bra@ket{\\left\\langle}{\\,\\middle\\vert\\,}{\\,\\middle\\vert\\,}{\\right\\rangle}");N("\\Set","\\bra@set{\\left\\{\\:}{\\;\\middle\\vert\\;}{\\;\\middle\\Vert\\;}{\\:\\right\\}}");N("\\set","\\bra@set{\\{\\,}{\\mid}{}{\\,\\}}");N("\\angln","{\\angl n}");N("\\blue","\\textcolor{##6495ed}{#1}");N("\\orange","\\textcolor{##ffa500}{#1}");N("\\pink","\\textcolor{##ff00af}{#1}");N("\\red","\\textcolor{##df0030}{#1}");N("\\green","\\textcolor{##28ae7b}{#1}");N("\\gray","\\textcolor{gray}{#1}");N("\\purple","\\textcolor{##9d38bd}{#1}");N("\\blueA","\\textcolor{##ccfaff}{#1}");N("\\blueB","\\textcolor{##80f6ff}{#1}");N("\\blueC","\\textcolor{##63d9ea}{#1}");N("\\blueD","\\textcolor{##11accd}{#1}");N("\\blueE","\\textcolor{##0c7f99}{#1}");N("\\tealA","\\textcolor{##94fff5}{#1}");N("\\tealB","\\textcolor{##26edd5}{#1}");N("\\tealC","\\textcolor{##01d1c1}{#1}");N("\\tealD","\\textcolor{##01a995}{#1}");N("\\tealE","\\textcolor{##208170}{#1}");N("\\greenA","\\textcolor{##b6ffb0}{#1}");N("\\greenB","\\textcolor{##8af281}{#1}");N("\\greenC","\\textcolor{##74cf70}{#1}");N("\\greenD","\\textcolor{##1fab54}{#1}");N("\\greenE","\\textcolor{##0d923f}{#1}");N("\\goldA","\\textcolor{##ffd0a9}{#1}");N("\\goldB","\\textcolor{##ffbb71}{#1}");N("\\goldC","\\textcolor{##ff9c39}{#1}");N("\\goldD","\\textcolor{##e07d10}{#1}");N("\\goldE","\\textcolor{##a75a05}{#1}");N("\\redA","\\textcolor{##fca9a9}{#1}");N("\\redB","\\textcolor{##ff8482}{#1}");N("\\redC","\\textcolor{##f9685d}{#1}");N("\\redD","\\textcolor{##e84d39}{#1}");N("\\redE","\\textcolor{##bc2612}{#1}");N("\\maroonA","\\textcolor{##ffbde0}{#1}");N("\\maroonB","\\textcolor{##ff92c6}{#1}");N("\\maroonC","\\textcolor{##ed5fa6}{#1}");N("\\maroonD","\\textcolor{##ca337c}{#1}");N("\\maroonE","\\textcolor{##9e034e}{#1}");N("\\purpleA","\\textcolor{##ddd7ff}{#1}");N("\\purpleB","\\textcolor{##c6b9fc}{#1}");N("\\purpleC","\\textcolor{##aa87ff}{#1}");N("\\purpleD","\\textcolor{##7854ab}{#1}");N("\\purpleE","\\textcolor{##543b78}{#1}");N("\\mintA","\\textcolor{##f5f9e8}{#1}");N("\\mintB","\\textcolor{##edf2df}{#1}");N("\\mintC","\\textcolor{##e0e5cc}{#1}");N("\\grayA","\\textcolor{##f6f7f7}{#1}");N("\\grayB","\\textcolor{##f0f1f2}{#1}");N("\\grayC","\\textcolor{##e3e5e6}{#1}");N("\\grayD","\\textcolor{##d6d8da}{#1}");N("\\grayE","\\textcolor{##babec2}{#1}");N("\\grayF","\\textcolor{##888d93}{#1}");N("\\grayG","\\textcolor{##626569}{#1}");N("\\grayH","\\textcolor{##3b3e40}{#1}");N("\\grayI","\\textcolor{##21242c}{#1}");N("\\kaBlue","\\textcolor{##314453}{#1}");N("\\kaGreen","\\textcolor{##71B307}{#1}");var vT={"^":!0,_:!0,"\\limits":!0,"\\nolimits":!0};class tK{constructor(t,n,r){this.settings=void 0,this.expansionCount=void 0,this.lexer=void 0,this.macros=void 0,this.stack=void 0,this.mode=void 0,this.settings=n,this.expansionCount=0,this.feed(t),this.macros=new JW(eK,n.macros),this.mode=r,this.stack=[]}feed(t){this.lexer=new n$(t,this.settings)}switchMode(t){this.mode=t}beginGroup(){this.macros.beginGroup()}endGroup(){this.macros.endGroup()}endGroups(){this.macros.endGroups()}future(){return this.stack.length===0&&this.pushToken(this.lexer.lex()),this.stack[this.stack.length-1]}popToken(){return this.future(),this.stack.pop()}pushToken(t){this.stack.push(t)}pushTokens(t){this.stack.push(...t)}scanArgument(t){var n,r,i;if(t){if(this.consumeSpaces(),this.future().text!=="[")return null;n=this.popToken(),{tokens:i,end:r}=this.consumeArg(["]"])}else({tokens:i,start:n,end:r}=this.consumeArg());return this.pushToken(new dr("EOF",r.loc)),this.pushTokens(i),n.range(r,"")}consumeSpaces(){for(;;){var t=this.future();if(t.text===" ")this.stack.pop();else break}}consumeArg(t){var n=[],r=t&&t.length>0;r||this.consumeSpaces();var i=this.future(),a,s=0,o=0;do{if(a=this.popToken(),n.push(a),a.text==="{")++s;else if(a.text==="}"){if(--s,s===-1)throw new oe("Extra }",a)}else if(a.text==="EOF")throw new oe("Unexpected end of input in a macro argument, expected '"+(t&&r?t[o]:"}")+"'",a);if(t&&r)if((s===0||s===1&&t[o]==="{")&&a.text===t[o]){if(++o,o===t.length){n.splice(-o,o);break}}else o=0}while(s!==0||r);return i.text==="{"&&n[n.length-1].text==="}"&&(n.pop(),n.shift()),n.reverse(),{tokens:n,start:i,end:a}}consumeArgs(t,n){if(n){if(n.length!==t+1)throw new oe("The length of delimiters doesn't match the number of args!");for(var r=n[0],i=0;i<r.length;i++){var a=this.popToken();if(r[i]!==a.text)throw new oe("Use of the macro doesn't match its definition",a)}}for(var s=[],o=0;o<t;o++)s.push(this.consumeArg(n&&n[o+1]).tokens);return s}countExpansion(t){if(this.expansionCount+=t,this.expansionCount>this.settings.maxExpand)throw new oe("Too many expansions: infinite loop or need to increase maxExpand setting")}expandOnce(t){var n=this.popToken(),r=n.text,i=n.noexpand?null:this._getExpansion(r);if(i==null||t&&i.unexpandable){if(t&&i==null&&r[0]==="\\"&&!this.isDefined(r))throw new oe("Undefined control sequence: "+r);return this.pushToken(n),!1}this.countExpansion(1);var a=i.tokens,s=this.consumeArgs(i.numArgs,i.delimiters);if(i.numArgs){a=a.slice();for(var o=a.length-1;o>=0;--o){var l=a[o];if(l.text==="#"){if(o===0)throw new oe("Incomplete placeholder at end of macro body",l);if(l=a[--o],l.text==="#")a.splice(o+1,1);else if(/^[1-9]$/.test(l.text))a.splice(o,2,...s[+l.text-1]);else throw new oe("Not a valid argument number",l)}}}return this.pushTokens(a),a.length}expandAfterFuture(){return this.expandOnce(),this.future()}expandNextToken(){for(;;)if(this.expandOnce()===!1){var t=this.stack.pop();return t.treatAsRelax&&(t.text="\\relax"),t}throw new Error}expandMacro(t){return this.macros.has(t)?this.expandTokens([new dr(t)]):void 0}expandTokens(t){var n=[],r=this.stack.length;for(this.pushTokens(t);this.stack.length>r;)if(this.expandOnce(!0)===!1){var i=this.stack.pop();i.treatAsRelax&&(i.noexpand=!1,i.treatAsRelax=!1),n.push(i)}return this.countExpansion(n.length),n}expandMacroAsText(t){var n=this.expandMacro(t);return n&&n.map(r=>r.text).join("")}_getExpansion(t){var n=this.macros.get(t);if(n==null)return n;if(t.length===1){var r=this.lexer.catcodes[t];if(r!=null&&r!==13)return}var i=typeof n=="function"?n(this):n;if(typeof i=="string"){var a=0;if(i.indexOf("#")!==-1)for(var s=i.replace(/##/g,"");s.indexOf("#"+(a+1))!==-1;)++a;for(var o=new n$(i,this.settings),l=[],c=o.lex();c.text!=="EOF";)l.push(c),c=o.lex();l.reverse();var u={tokens:l,numArgs:a};return u}return i}isDefined(t){return this.macros.has(t)||ha.hasOwnProperty(t)||at.math.hasOwnProperty(t)||at.text.hasOwnProperty(t)||vT.hasOwnProperty(t)}isExpandable(t){var n=this.macros.get(t);return n!=null?typeof n=="string"||typeof n=="function"||!n.unexpandable:ha.hasOwnProperty(t)&&!ha[t].primitive}}var a$=/^[₊₋₌₍₎₀₁₂₃₄₅₆₇₈₉ₐₑₕᵢⱼₖₗₘₙₒₚᵣₛₜᵤᵥₓᵦᵧᵨᵩᵪ]/,Xu=Object.freeze({"₊":"+","₋":"-","₌":"=","₍":"(","₎":")","₀":"0","₁":"1","₂":"2","₃":"3","₄":"4","₅":"5","₆":"6","₇":"7","₈":"8","₉":"9","ₐ":"a","ₑ":"e","ₕ":"h","ᵢ":"i","ⱼ":"j","ₖ":"k","ₗ":"l","ₘ":"m","ₙ":"n","ₒ":"o","ₚ":"p","ᵣ":"r","ₛ":"s","ₜ":"t","ᵤ":"u","ᵥ":"v","ₓ":"x","ᵦ":"β","ᵧ":"γ","ᵨ":"ρ","ᵩ":"ϕ","ᵪ":"χ","⁺":"+","⁻":"-","⁼":"=","⁽":"(","⁾":")","⁰":"0","¹":"1","²":"2","³":"3","⁴":"4","⁵":"5","⁶":"6","⁷":"7","⁸":"8","⁹":"9","ᴬ":"A","ᴮ":"B","ᴰ":"D","ᴱ":"E","ᴳ":"G","ᴴ":"H","ᴵ":"I","ᴶ":"J","ᴷ":"K","ᴸ":"L","ᴹ":"M","ᴺ":"N","ᴼ":"O","ᴾ":"P","ᴿ":"R","ᵀ":"T","ᵁ":"U","ⱽ":"V","ᵂ":"W","ᵃ":"a","ᵇ":"b","ᶜ":"c","ᵈ":"d","ᵉ":"e","ᶠ":"f","ᵍ":"g",ʰ:"h","ⁱ":"i",ʲ:"j","ᵏ":"k",ˡ:"l","ᵐ":"m",ⁿ:"n","ᵒ":"o","ᵖ":"p",ʳ:"r",ˢ:"s","ᵗ":"t","ᵘ":"u","ᵛ":"v",ʷ:"w",ˣ:"x",ʸ:"y","ᶻ":"z","ᵝ":"β","ᵞ":"γ","ᵟ":"δ","ᵠ":"ϕ","ᵡ":"χ","ᶿ":"θ"}),np={"́":{text:"\\'",math:"\\acute"},"̀":{text:"\\`",math:"\\grave"},"̈":{text:'\\"',math:"\\ddot"},"̃":{text:"\\~",math:"\\tilde"},"̄":{text:"\\=",math:"\\bar"},"̆":{text:"\\u",math:"\\breve"},"̌":{text:"\\v",math:"\\check"},"̂":{text:"\\^",math:"\\hat"},"̇":{text:"\\.",math:"\\dot"},"̊":{text:"\\r",math:"\\mathring"},"̋":{text:"\\H"},"̧":{text:"\\c"}},s$={á:"á",à:"à",ä:"ä",ǟ:"ǟ",ã:"ã",ā:"ā",ă:"ă",ắ:"ắ",ằ:"ằ",ẵ:"ẵ",ǎ:"ǎ",â:"â",ấ:"ấ",ầ:"ầ",ẫ:"ẫ",ȧ:"ȧ",ǡ:"ǡ",å:"å",ǻ:"ǻ",ḃ:"ḃ",ć:"ć",ḉ:"ḉ",č:"č",ĉ:"ĉ",ċ:"ċ",ç:"ç",ď:"ď",ḋ:"ḋ",ḑ:"ḑ",é:"é",è:"è",ë:"ë",ẽ:"ẽ",ē:"ē",ḗ:"ḗ",ḕ:"ḕ",ĕ:"ĕ",ḝ:"ḝ",ě:"ě",ê:"ê",ế:"ế",ề:"ề",ễ:"ễ",ė:"ė",ȩ:"ȩ",ḟ:"ḟ",ǵ:"ǵ",ḡ:"ḡ",ğ:"ğ",ǧ:"ǧ",ĝ:"ĝ",ġ:"ġ",ģ:"ģ",ḧ:"ḧ",ȟ:"ȟ",ĥ:"ĥ",ḣ:"ḣ",ḩ:"ḩ",í:"í",ì:"ì",ï:"ï",ḯ:"ḯ",ĩ:"ĩ",ī:"ī",ĭ:"ĭ",ǐ:"ǐ",î:"î",ǰ:"ǰ",ĵ:"ĵ",ḱ:"ḱ",ǩ:"ǩ",ķ:"ķ",ĺ:"ĺ",ľ:"ľ",ļ:"ļ",ḿ:"ḿ",ṁ:"ṁ",ń:"ń",ǹ:"ǹ",ñ:"ñ",ň:"ň",ṅ:"ṅ",ņ:"ņ",ó:"ó",ò:"ò",ö:"ö",ȫ:"ȫ",õ:"õ",ṍ:"ṍ",ṏ:"ṏ",ȭ:"ȭ",ō:"ō",ṓ:"ṓ",ṑ:"ṑ",ŏ:"ŏ",ǒ:"ǒ",ô:"ô",ố:"ố",ồ:"ồ",ỗ:"ỗ",ȯ:"ȯ",ȱ:"ȱ",ő:"ő",ṕ:"ṕ",ṗ:"ṗ",ŕ:"ŕ",ř:"ř",ṙ:"ṙ",ŗ:"ŗ",ś:"ś",ṥ:"ṥ",š:"š",ṧ:"ṧ",ŝ:"ŝ",ṡ:"ṡ",ş:"ş",ẗ:"ẗ",ť:"ť",ṫ:"ṫ",ţ:"ţ",ú:"ú",ù:"ù",ü:"ü",ǘ:"ǘ",ǜ:"ǜ",ǖ:"ǖ",ǚ:"ǚ",ũ:"ũ",ṹ:"ṹ",ū:"ū",ṻ:"ṻ",ŭ:"ŭ",ǔ:"ǔ",û:"û",ů:"ů",ű:"ű",ṽ:"ṽ",ẃ:"ẃ",ẁ:"ẁ",ẅ:"ẅ",ŵ:"ŵ",ẇ:"ẇ",ẘ:"ẘ",ẍ:"ẍ",ẋ:"ẋ",ý:"ý",ỳ:"ỳ",ÿ:"ÿ",ỹ:"ỹ",ȳ:"ȳ",ŷ:"ŷ",ẏ:"ẏ",ẙ:"ẙ",ź:"ź",ž:"ž",ẑ:"ẑ",ż:"ż",Á:"Á",À:"À",Ä:"Ä",Ǟ:"Ǟ",Ã:"Ã",Ā:"Ā",Ă:"Ă",Ắ:"Ắ",Ằ:"Ằ",Ẵ:"Ẵ",Ǎ:"Ǎ",Â:"Â",Ấ:"Ấ",Ầ:"Ầ",Ẫ:"Ẫ",Ȧ:"Ȧ",Ǡ:"Ǡ",Å:"Å",Ǻ:"Ǻ",Ḃ:"Ḃ",Ć:"Ć",Ḉ:"Ḉ",Č:"Č",Ĉ:"Ĉ",Ċ:"Ċ",Ç:"Ç",Ď:"Ď",Ḋ:"Ḋ",Ḑ:"Ḑ",É:"É",È:"È",Ë:"Ë",Ẽ:"Ẽ",Ē:"Ē",Ḗ:"Ḗ",Ḕ:"Ḕ",Ĕ:"Ĕ",Ḝ:"Ḝ",Ě:"Ě",Ê:"Ê",Ế:"Ế",Ề:"Ề",Ễ:"Ễ",Ė:"Ė",Ȩ:"Ȩ",Ḟ:"Ḟ",Ǵ:"Ǵ",Ḡ:"Ḡ",Ğ:"Ğ",Ǧ:"Ǧ",Ĝ:"Ĝ",Ġ:"Ġ",Ģ:"Ģ",Ḧ:"Ḧ",Ȟ:"Ȟ",Ĥ:"Ĥ",Ḣ:"Ḣ",Ḩ:"Ḩ",Í:"Í",Ì:"Ì",Ï:"Ï",Ḯ:"Ḯ",Ĩ:"Ĩ",Ī:"Ī",Ĭ:"Ĭ",Ǐ:"Ǐ",Î:"Î",İ:"İ",Ĵ:"Ĵ",Ḱ:"Ḱ",Ǩ:"Ǩ",Ķ:"Ķ",Ĺ:"Ĺ",Ľ:"Ľ",Ļ:"Ļ",Ḿ:"Ḿ",Ṁ:"Ṁ",Ń:"Ń",Ǹ:"Ǹ",Ñ:"Ñ",Ň:"Ň",Ṅ:"Ṅ",Ņ:"Ņ",Ó:"Ó",Ò:"Ò",Ö:"Ö",Ȫ:"Ȫ",Õ:"Õ",Ṍ:"Ṍ",Ṏ:"Ṏ",Ȭ:"Ȭ",Ō:"Ō",Ṓ:"Ṓ",Ṑ:"Ṑ",Ŏ:"Ŏ",Ǒ:"Ǒ",Ô:"Ô",Ố:"Ố",Ồ:"Ồ",Ỗ:"Ỗ",Ȯ:"Ȯ",Ȱ:"Ȱ",Ő:"Ő",Ṕ:"Ṕ",Ṗ:"Ṗ",Ŕ:"Ŕ",Ř:"Ř",Ṙ:"Ṙ",Ŗ:"Ŗ",Ś:"Ś",Ṥ:"Ṥ",Š:"Š",Ṧ:"Ṧ",Ŝ:"Ŝ",Ṡ:"Ṡ",Ş:"Ş",Ť:"Ť",Ṫ:"Ṫ",Ţ:"Ţ",Ú:"Ú",Ù:"Ù",Ü:"Ü",Ǘ:"Ǘ",Ǜ:"Ǜ",Ǖ:"Ǖ",Ǚ:"Ǚ",Ũ:"Ũ",Ṹ:"Ṹ",Ū:"Ū",Ṻ:"Ṻ",Ŭ:"Ŭ",Ǔ:"Ǔ",Û:"Û",Ů:"Ů",Ű:"Ű",Ṽ:"Ṽ",Ẃ:"Ẃ",Ẁ:"Ẁ",Ẅ:"Ẅ",Ŵ:"Ŵ",Ẇ:"Ẇ",Ẍ:"Ẍ",Ẋ:"Ẋ",Ý:"Ý",Ỳ:"Ỳ",Ÿ:"Ÿ",Ỹ:"Ỹ",Ȳ:"Ȳ",Ŷ:"Ŷ",Ẏ:"Ẏ",Ź:"Ź",Ž:"Ž",Ẑ:"Ẑ",Ż:"Ż",ά:"ά",ὰ:"ὰ",ᾱ:"ᾱ",ᾰ:"ᾰ",έ:"έ",ὲ:"ὲ",ή:"ή",ὴ:"ὴ",ί:"ί",ὶ:"ὶ",ϊ:"ϊ",ΐ:"ΐ",ῒ:"ῒ",ῑ:"ῑ",ῐ:"ῐ",ό:"ό",ὸ:"ὸ",ύ:"ύ",ὺ:"ὺ",ϋ:"ϋ",ΰ:"ΰ",ῢ:"ῢ",ῡ:"ῡ",ῠ:"ῠ",ώ:"ώ",ὼ:"ὼ",Ύ:"Ύ",Ὺ:"Ὺ",Ϋ:"Ϋ",Ῡ:"Ῡ",Ῠ:"Ῠ",Ώ:"Ώ",Ὼ:"Ὼ"};let xT=class $T{constructor(t,n){this.mode=void 0,this.gullet=void 0,this.settings=void 0,this.leftrightDepth=void 0,this.nextToken=void 0,this.mode="math",this.gullet=new tK(t,n,this.mode),this.settings=n,this.leftrightDepth=0}expect(t,n){if(n===void 0&&(n=!0),this.fetch().text!==t)throw new oe("Expected '"+t+"', got '"+this.fetch().text+"'",this.fetch());n&&this.consume()}consume(){this.nextToken=null}fetch(){return this.nextToken==null&&(this.nextToken=this.gullet.expandNextToken()),this.nextToken}switchMode(t){this.mode=t,this.gullet.switchMode(t)}parse(){this.settings.globalGroup||this.gullet.beginGroup(),this.settings.colorIsTextColor&&this.gullet.macros.set("\\color","\\textcolor");try{var t=this.parseExpression(!1);return this.expect("EOF"),this.settings.globalGroup||this.gullet.endGroup(),t}finally{this.gullet.endGroups()}}subparse(t){var n=this.nextToken;this.consume(),this.gullet.pushToken(new dr("}")),this.gullet.pushTokens(t);var r=this.parseExpression(!1);return this.expect("}"),this.nextToken=n,r}parseExpression(t,n){for(var r=[];;){this.mode==="math"&&this.consumeSpaces();var i=this.fetch();if($T.endOfExpression.indexOf(i.text)!==-1||n&&i.text===n||t&&ha[i.text]&&ha[i.text].infix)break;var a=this.parseAtom(n);if(a){if(a.type==="internal")continue}else break;r.push(a)}return this.mode==="text"&&this.formLigatures(r),this.handleInfixNodes(r)}handleInfixNodes(t){for(var n=-1,r,i=0;i<t.length;i++)if(t[i].type==="infix"){if(n!==-1)throw new oe("only one infix operator per group",t[i].token);n=i,r=t[i].replaceWith}if(n!==-1&&r){var a,s,o=t.slice(0,n),l=t.slice(n+1);o.length===1&&o[0].type==="ordgroup"?a=o[0]:a={type:"ordgroup",mode:this.mode,body:o},l.length===1&&l[0].type==="ordgroup"?s=l[0]:s={type:"ordgroup",mode:this.mode,body:l};var c;return r==="\\\\abovefrac"?c=this.callFunction(r,[a,t[n],s],[]):c=this.callFunction(r,[a,s],[]),[c]}else return t}handleSupSubscript(t){var n=this.fetch(),r=n.text;this.consume(),this.consumeSpaces();var i;do{var a;i=this.parseGroup(t)}while(((a=i)==null?void 0:a.type)==="internal");if(!i)throw new oe("Expected group after '"+r+"'",n);return i}formatUnsupportedCmd(t){for(var n=[],r=0;r<t.length;r++)n.push({type:"textord",mode:"text",text:t[r]});var i={type:"text",mode:this.mode,body:n},a={type:"color",mode:this.mode,color:this.settings.errorColor,body:[i]};return a}parseAtom(t){var n=this.parseGroup("atom",t);if((n==null?void 0:n.type)==="internal"||this.mode==="text")return n;for(var r,i;;){this.consumeSpaces();var a=this.fetch();if(a.text==="\\limits"||a.text==="\\nolimits"){if(n&&n.type==="op"){var s=a.text==="\\limits";n.limits=s,n.alwaysHandleSupSub=!0}else if(n&&n.type==="operatorname")n.alwaysHandleSupSub&&(n.limits=a.text==="\\limits");else throw new oe("Limit controls must follow a math operator",a);this.consume()}else if(a.text==="^"){if(r)throw new oe("Double superscript",a);r=this.handleSupSubscript("superscript")}else if(a.text==="_"){if(i)throw new oe("Double subscript",a);i=this.handleSupSubscript("subscript")}else if(a.text==="'"){if(r)throw new oe("Double superscript",a);var o={type:"textord",mode:this.mode,text:"\\prime"},l=[o];for(this.consume();this.fetch().text==="'";)l.push(o),this.consume();this.fetch().text==="^"&&l.push(this.handleSupSubscript("superscript")),r={type:"ordgroup",mode:this.mode,body:l}}else if(Xu[a.text]){var c=a$.test(a.text),u=[];for(u.push(new dr(Xu[a.text])),this.consume();;){var d=this.fetch().text;if(!Xu[d]||a$.test(d)!==c)break;u.unshift(new dr(Xu[d])),this.consume()}var m=this.subparse(u);c?i={type:"ordgroup",mode:"math",body:m}:r={type:"ordgroup",mode:"math",body:m}}else break}return r||i?{type:"supsub",mode:this.mode,base:n,sup:r,sub:i}:n}parseFunction(t,n){var r=this.fetch(),i=r.text,a=ha[i];if(!a)return null;if(this.consume(),n&&n!=="atom"&&!a.allowedInArgument)throw new oe("Got function '"+i+"' with no arguments"+(n?" as "+n:""),r);if(this.mode==="text"&&!a.allowedInText)throw new oe("Can't use function '"+i+"' in text mode",r);if(this.mode==="math"&&a.allowedInMath===!1)throw new oe("Can't use function '"+i+"' in math mode",r);var{args:s,optArgs:o}=this.parseArguments(i,a);return this.callFunction(i,s,o,r,t)}callFunction(t,n,r,i,a){var s={funcName:t,parser:this,token:i,breakOnTokenText:a},o=ha[t];if(o&&o.handler)return o.handler(s,n,r);throw new oe("No function handler for "+t)}parseArguments(t,n){var r=n.numArgs+n.numOptionalArgs;if(r===0)return{args:[],optArgs:[]};for(var i=[],a=[],s=0;s<r;s++){var o=n.argTypes&&n.argTypes[s],l=s<n.numOptionalArgs;(n.primitive&&o==null||n.type==="sqrt"&&s===1&&a[0]==null)&&(o="primitive");var c=this.parseGroupOfType("argument to '"+t+"'",o,l);if(l)a.push(c);else if(c!=null)i.push(c);else throw new oe("Null argument, please report this as a bug")}return{args:i,optArgs:a}}parseGroupOfType(t,n,r){switch(n){case"color":return this.parseColorGroup(r);case"size":return this.parseSizeGroup(r);case"url":return this.parseUrlGroup(r);case"math":case"text":return this.parseArgumentGroup(r,n);case"hbox":{var i=this.parseArgumentGroup(r,"text");return i!=null?{type:"styling",mode:i.mode,body:[i],style:"text"}:null}case"raw":{var a=this.parseStringGroup("raw",r);return a!=null?{type:"raw",mode:"text",string:a.text}:null}case"primitive":{if(r)throw new oe("A primitive argument cannot be optional");var s=this.parseGroup(t);if(s==null)throw new oe("Expected group as "+t,this.fetch());return s}case"original":case null:case void 0:return this.parseArgumentGroup(r);default:throw new oe("Unknown group type as "+t,this.fetch())}}consumeSpaces(){for(;this.fetch().text===" ";)this.consume()}parseStringGroup(t,n){var r=this.gullet.scanArgument(n);if(r==null)return null;for(var i="",a;(a=this.fetch()).text!=="EOF";)i+=a.text,this.consume();return this.consume(),r.text=i,r}parseRegexGroup(t,n){for(var r=this.fetch(),i=r,a="",s;(s=this.fetch()).text!=="EOF"&&t.test(a+s.text);)i=s,a+=i.text,this.consume();if(a==="")throw new oe("Invalid "+n+": '"+r.text+"'",r);return r.range(i,a)}parseColorGroup(t){var n=this.parseStringGroup("color",t);if(n==null)return null;var r=/^(#[a-f0-9]{3}|#?[a-f0-9]{6}|[a-z]+)$/i.exec(n.text);if(!r)throw new oe("Invalid color: '"+n.text+"'",n);var i=r[0];return/^[0-9a-f]{6}$/i.test(i)&&(i="#"+i),{type:"color-token",mode:this.mode,color:i}}parseSizeGroup(t){var n,r=!1;if(this.gullet.consumeSpaces(),!t&&this.gullet.future().text!=="{"?n=this.parseRegexGroup(/^[-+]? *(?:$|\d+|\d+\.\d*|\.\d*) *[a-z]{0,2} *$/,"size"):n=this.parseStringGroup("size",t),!n)return null;!t&&n.text.length===0&&(n.text="0pt",r=!0);var i=/([-+]?) *(\d+(?:\.\d*)?|\.\d+) *([a-z]{2})/.exec(n.text);if(!i)throw new oe("Invalid size: '"+n.text+"'",n);var a={number:+(i[1]+i[2]),unit:i[3]};if(!Nw(a))throw new oe("Invalid unit: '"+a.unit+"'",n);return{type:"size",mode:this.mode,value:a,isBlank:r}}parseUrlGroup(t){this.gullet.lexer.setCatcode("%",13),this.gullet.lexer.setCatcode("~",12);var n=this.parseStringGroup("url",t);if(this.gullet.lexer.setCatcode("%",14),this.gullet.lexer.setCatcode("~",13),n==null)return null;var r=n.text.replace(/\\([#$%&~_^{}])/g,"$1");return{type:"url",mode:this.mode,url:r}}parseArgumentGroup(t,n){var r=this.gullet.scanArgument(t);if(r==null)return null;var i=this.mode;n&&this.switchMode(n),this.gullet.beginGroup();var a=this.parseExpression(!1,"EOF");this.expect("EOF"),this.gullet.endGroup();var s={type:"ordgroup",mode:this.mode,loc:r.loc,body:a};return n&&this.switchMode(i),s}parseGroup(t,n){var r=this.fetch(),i=r.text,a;if(i==="{"||i==="\\begingroup"){this.consume();var s=i==="{"?"}":"\\endgroup";this.gullet.beginGroup();var o=this.parseExpression(!1,s),l=this.fetch();this.expect(s),this.gullet.endGroup(),a={type:"ordgroup",mode:this.mode,loc:Fn.range(r,l),body:o,semisimple:i==="\\begingroup"||void 0}}else if(a=this.parseFunction(n,t)||this.parseSymbol(),a==null&&i[0]==="\\"&&!vT.hasOwnProperty(i)){if(this.settings.throwOnError)throw new oe("Undefined control sequence: "+i,r);a=this.formatUnsupportedCmd(i),this.consume()}return a}formLigatures(t){for(var n=t.length-1,r=0;r<n;++r){var i=t[r],a=i.text;a==="-"&&t[r+1].text==="-"&&(r+1<n&&t[r+2].text==="-"?(t.splice(r,3,{type:"textord",mode:"text",loc:Fn.range(i,t[r+2]),text:"---"}),n-=2):(t.splice(r,2,{type:"textord",mode:"text",loc:Fn.range(i,t[r+1]),text:"--"}),n-=1)),(a==="'"||a==="`")&&t[r+1].text===a&&(t.splice(r,2,{type:"textord",mode:"text",loc:Fn.range(i,t[r+1]),text:a+a}),n-=1)}}parseSymbol(){var t=this.fetch(),n=t.text;if(/^\\verb[^a-zA-Z]/.test(n)){this.consume();var r=n.slice(5),i=r.charAt(0)==="*";if(i&&(r=r.slice(1)),r.length<2||r.charAt(0)!==r.slice(-1))throw new oe(`\\verb assertion failed --
                    please report what input caused this bug`);return r=r.slice(1,-1),{type:"verb",mode:"text",body:r,star:i}}s$.hasOwnProperty(n[0])&&!at[this.mode][n[0]]&&(this.settings.strict&&this.mode==="math"&&this.settings.reportNonstrict("unicodeTextInMathMode",'Accented Unicode text character "'+n[0]+'" used in math mode',t),n=s$[n[0]]+n.slice(1));var a=QW.exec(n);a&&(n=n.substring(0,a.index),n==="i"?n="ı":n==="j"&&(n="ȷ"));var s;if(at[this.mode][n]){this.settings.strict&&this.mode==="math"&&tg.indexOf(n)>=0&&this.settings.reportNonstrict("unicodeTextInMathMode",'Latin-1/Unicode text character "'+n[0]+'" used in math mode',t);var o=at[this.mode][n].group,l=Fn.range(t),c;if(VG.hasOwnProperty(o)){var u=o;c={type:"atom",mode:this.mode,family:u,loc:l,text:n}}else c={type:o,mode:this.mode,loc:l,text:n};s=c}else if(n.charCodeAt(0)>=128)this.settings.strict&&(Sw(n.charCodeAt(0))?this.mode==="math"&&this.settings.reportNonstrict("unicodeTextInMathMode",'Unicode text character "'+n[0]+'" used in math mode',t):this.settings.reportNonstrict("unknownSymbol",'Unrecognized Unicode character "'+n[0]+'"'+(" ("+n.charCodeAt(0)+")"),t)),s={type:"textord",mode:"text",loc:Fn.range(t),text:n};else return null;if(this.consume(),a)for(var d=0;d<a[0].length;d++){var m=a[0][d];if(!np[m])throw new oe("Unknown accent ' "+m+"'",t);var p=np[m][this.mode]||np[m].text;if(!p)throw new oe("Accent "+m+" unsupported in "+this.mode+" mode",t);s={type:"accent",mode:this.mode,loc:Fn.range(t),label:p,isStretchy:!1,isShifty:!0,base:s}}return s}};xT.endOfExpression=["}","\\endgroup","\\end","\\right","&"];var nb=function(t,n){if(!(typeof t=="string"||t instanceof String))throw new TypeError("KaTeX can only parse string typed expression");var r=new xT(t,n);delete r.gullet.macros.current["\\df@tag"];var i=r.parse();if(delete r.gullet.macros.current["\\current@color"],delete r.gullet.macros.current["\\color"],r.gullet.macros.get("\\df@tag")){if(!n.displayMode)throw new oe("\\tag works only in display equations");i=[{type:"tag",mode:"text",body:i,tag:r.subparse([new dr("\\df@tag")])}]}return i},yT=function(t,n,r){n.textContent="";var i=rb(t,r).toNode();n.appendChild(i)};typeof document<"u"&&document.compatMode!=="CSS1Compat"&&(typeof console<"u"&&console.warn("Warning: KaTeX doesn't work in quirks mode. Make sure your website has a suitable doctype."),yT=function(){throw new oe("KaTeX doesn't work in quirks mode.")});var nK=function(t,n){var r=rb(t,n).toMarkup();return r},rK=function(t,n){var r=new O2(n);return nb(t,r)},_T=function(t,n,r){if(r.throwOnError||!(t instanceof oe))throw t;var i=V.makeSpan(["katex-error"],[new pr(n)]);return i.setAttribute("title",t.toString()),i.setAttribute("style","color:"+r.errorColor),i},rb=function(t,n){var r=new O2(n);try{var i=nb(t,r);return mW(i,t,r)}catch(a){return _T(a,t,r)}},iK=function(t,n){var r=new O2(n);try{var i=nb(t,r);return hW(i,t,r)}catch(a){return _T(a,t,r)}},aK="0.16.22",sK={Span:eu,Anchor:F2,SymbolNode:pr,SvgNode:Ri,PathNode:Pa,LineNode:eg},o$={version:aK,render:yT,renderToString:nK,ParseError:oe,SETTINGS_SCHEMA:_d,__parse:rK,__renderToDomTree:rb,__renderToHTMLTree:iK,__setFontMetrics:DG,__defineSymbol:b,__defineFunction:fe,__defineMacro:N,__domTree:sK};const oK={};function lK(e){const t=this,n=e||oK,r=t.data(),i=r.micromarkExtensions||(r.micromarkExtensions=[]),a=r.fromMarkdownExtensions||(r.fromMarkdownExtensions=[]),s=r.toMarkdownExtensions||(r.toMarkdownExtensions=[]);i.push(sG(n)),a.push(Zq()),s.push(Jq(n))}const l$=/[#.]/g;function cK(e,t){const n=e||"",r={};let i=0,a,s;for(;i<n.length;){l$.lastIndex=i;const o=l$.exec(n),l=n.slice(i,o?o.index:n.length);l&&(a?a==="#"?r.id=l:Array.isArray(r.className)?r.className.push(l):r.className=[l]:s=l,i+=l.length),o&&(a=o[0],i++)}return{type:"element",tagName:s||t||"div",properties:r,children:[]}}function wT(e,t,n){const r=n?hK(n):void 0;function i(a,s,...o){let l;if(a==null){l={type:"root",children:[]};const c=s;o.unshift(c)}else{l=cK(a,t);const c=l.tagName.toLowerCase(),u=r?r.get(c):void 0;if(l.tagName=u||c,uK(s))o.unshift(s);else for(const[d,m]of Object.entries(s))dK(e,l.properties,d,m)}for(const c of o)og(l.children,c);return l.type==="element"&&l.tagName==="template"&&(l.content={type:"root",children:l.children},l.children=[]),l}return i}function uK(e){if(e===null||typeof e!="object"||Array.isArray(e))return!0;if(typeof e.type!="string")return!1;const t=e,n=Object.keys(e);for(const r of n){const i=t[r];if(i&&typeof i=="object"){if(!Array.isArray(i))return!0;const a=i;for(const s of a)if(typeof s!="number"&&typeof s!="string")return!0}}return!!("children"in e&&Array.isArray(e.children))}function dK(e,t,n,r){const i=w2(e,n);let a;if(r!=null){if(typeof r=="number"){if(Number.isNaN(r))return;a=r}else typeof r=="boolean"?a=r:typeof r=="string"?i.spaceSeparated?a=zx(r):i.commaSeparated?a=Ux(r):i.commaOrSpaceSeparated?a=zx(Ux(r).join(" ")):a=c$(i,i.property,r):Array.isArray(r)?a=[...r]:a=i.property==="style"?mK(r):String(r);if(Array.isArray(a)){const s=[];for(const o of a)s.push(c$(i,i.property,o));a=s}i.property==="className"&&Array.isArray(t.className)&&(a=t.className.concat(a)),t[i.property]=a}}function og(e,t){if(t!=null)if(typeof t=="number"||typeof t=="string")e.push({type:"text",value:String(t)});else if(Array.isArray(t))for(const n of t)og(e,n);else if(typeof t=="object"&&"type"in t)t.type==="root"?og(e,t.children):e.push(t);else throw new Error("Expected node, nodes, or string, got `"+t+"`")}function c$(e,t,n){if(typeof n=="string"){if(e.number&&n&&!Number.isNaN(Number(n)))return Number(n);if((e.boolean||e.overloadedBoolean)&&(n===""||Ac(n)===Ac(t)))return!0}return n}function mK(e){const t=[];for(const[n,r]of Object.entries(e))t.push([n,r].join(": "));return t.join("; ")}function hK(e){const t=new Map;for(const n of e)t.set(n.toLowerCase(),n);return t}const pK=["altGlyph","altGlyphDef","altGlyphItem","animateColor","animateMotion","animateTransform","clipPath","feBlend","feColorMatrix","feComponentTransfer","feComposite","feConvolveMatrix","feDiffuseLighting","feDisplacementMap","feDistantLight","feDropShadow","feFlood","feFuncA","feFuncB","feFuncG","feFuncR","feGaussianBlur","feImage","feMerge","feMergeNode","feMorphology","feOffset","fePointLight","feSpecularLighting","feSpotLight","feTile","feTurbulence","foreignObject","glyphRef","linearGradient","radialGradient","solidColor","textArea","textPath"],TT=wT(um,"div"),ET=wT(Yo,"g",pK),ti={html:"http://www.w3.org/1999/xhtml",mathml:"http://www.w3.org/1998/Math/MathML",svg:"http://www.w3.org/2000/svg",xlink:"http://www.w3.org/1999/xlink",xml:"http://www.w3.org/XML/1998/namespace",xmlns:"http://www.w3.org/2000/xmlns/"};function fK(e,t){return kT(e,{})||{type:"root",children:[]}}function kT(e,t){const n=gK(e,t);return n&&t.afterTransform&&t.afterTransform(e,n),n}function gK(e,t){switch(e.nodeType){case 1:return $K(e,t);case 3:return vK(e);case 8:return xK(e);case 9:return u$(e,t);case 10:return bK();case 11:return u$(e,t);default:return}}function u$(e,t){return{type:"root",children:ST(e,t)}}function bK(){return{type:"doctype"}}function vK(e){return{type:"text",value:e.nodeValue||""}}function xK(e){return{type:"comment",value:e.nodeValue||""}}function $K(e,t){const n=e.namespaceURI,r=n===ti.svg?ET:TT,i=n===ti.html?e.tagName.toLowerCase():e.tagName,a=n===ti.html&&i==="template"?e.content:e,s=e.getAttributeNames(),o={};let l=-1;for(;++l<s.length;)o[s[l]]=e.getAttribute(s[l])||"";return r(i,o,ST(a,t))}function ST(e,t){const n=e.childNodes,r=[];let i=-1;for(;++i<n.length;){const a=kT(n[i],t);a!==void 0&&r.push(a)}return r}new DOMParser;function yK(e,t){const n=_K(e);return fK(n)}function _K(e){const t=document.createElement("template");return t.innerHTML=e,t.content}const d$=function(e,t,n){const r=D2(n);if(!e||!e.type||!e.children)throw new Error("Expected parent node");if(typeof t=="number"){if(t<0||t===Number.POSITIVE_INFINITY)throw new Error("Expected positive finite number as index")}else if(t=e.children.indexOf(t),t<0)throw new Error("Expected child node or index");for(;++t<e.children.length;)if(r(e.children[t],t,e))return e.children[t]},Ts=function(e){if(e==null)return EK;if(typeof e=="string")return TK(e);if(typeof e=="object")return wK(e);if(typeof e=="function")return ib(e);throw new Error("Expected function, string, or array as `test`")};function wK(e){const t=[];let n=-1;for(;++n<e.length;)t[n]=Ts(e[n]);return ib(r);function r(...i){let a=-1;for(;++a<t.length;)if(t[a].apply(this,i))return!0;return!1}}function TK(e){return ib(t);function t(n){return n.tagName===e}}function ib(e){return t;function t(n,r,i){return!!(kK(n)&&e.call(this,n,typeof r=="number"?r:void 0,i||void 0))}}function EK(e){return!!(e&&typeof e=="object"&&"type"in e&&e.type==="element"&&"tagName"in e&&typeof e.tagName=="string")}function kK(e){return e!==null&&typeof e=="object"&&"type"in e&&"tagName"in e}const m$=/\n/g,h$=/[\t ]+/g,lg=Ts("br"),p$=Ts(UK),SK=Ts("p"),f$=Ts("tr"),NK=Ts(["datalist","head","noembed","noframes","noscript","rp","script","style","template","title",IK,DK]),NT=Ts(["address","article","aside","blockquote","body","caption","center","dd","dialog","dir","dl","dt","div","figure","figcaption","footer","form,","h1","h2","h3","h4","h5","h6","header","hgroup","hr","html","legend","li","listing","main","menu","nav","ol","p","plaintext","pre","section","ul","xmp"]);function CK(e,t){const n=t||{},r="children"in e?e.children:[],i=NT(e),a=LT(e,{whitespace:n.whitespace||"normal",breakBefore:!1,breakAfter:!1}),s=[];(e.type==="text"||e.type==="comment")&&s.push(...AT(e,{whitespace:a,breakBefore:!0,breakAfter:!0}));let o=-1;for(;++o<r.length;)s.push(...CT(r[o],e,{whitespace:a,breakBefore:o?void 0:i,breakAfter:o<r.length-1?lg(r[o+1]):i}));const l=[];let c;for(o=-1;++o<s.length;){const u=s[o];typeof u=="number"?c!==void 0&&u>c&&(c=u):u&&(c!==void 0&&c>-1&&l.push(`
`.repeat(c)||" "),c=-1,l.push(u))}return l.join("")}function CT(e,t,n){return e.type==="element"?AK(e,t,n):e.type==="text"?n.whitespace==="normal"?AT(e,n):LK(e):[]}function AK(e,t,n){const r=LT(e,n),i=e.children||[];let a=-1,s=[];if(NK(e))return s;let o,l;for(lg(e)||f$(e)&&d$(t,e,f$)?l=`
`:SK(e)?(o=2,l=2):NT(e)&&(o=1,l=1);++a<i.length;)s=s.concat(CT(i[a],e,{whitespace:r,breakBefore:a?void 0:o,breakAfter:a<i.length-1?lg(i[a+1]):l}));return p$(e)&&d$(t,e,p$)&&s.push("	"),o&&s.unshift(o),l&&s.push(l),s}function AT(e,t){const n=String(e.value),r=[],i=[];let a=0;for(;a<=n.length;){m$.lastIndex=a;const l=m$.exec(n),c=l&&"index"in l?l.index:n.length;r.push(PK(n.slice(a,c).replace(/[\u061C\u200E\u200F\u202A-\u202E\u2066-\u2069]/g,""),a===0?t.breakBefore:!0,c===n.length?t.breakAfter:!0)),a=c+1}let s=-1,o;for(;++s<r.length;)r[s].charCodeAt(r[s].length-1)===8203||s<r.length-1&&r[s+1].charCodeAt(0)===8203?(i.push(r[s]),o=void 0):r[s]?(typeof o=="number"&&i.push(o),i.push(r[s]),o=0):(s===0||s===r.length-1)&&i.push(0);return i}function LK(e){return[String(e.value)]}function PK(e,t,n){const r=[];let i=0,a;for(;i<e.length;){h$.lastIndex=i;const s=h$.exec(e);a=s?s.index:e.length,!i&&!a&&s&&!t&&r.push(""),i!==a&&r.push(e.slice(i,a)),i=s?a+s[0].length:a}return i!==a&&!n&&r.push(""),r.join(" ")}function LT(e,t){if(e.type==="element"){const n=e.properties||{};switch(e.tagName){case"listing":case"plaintext":case"xmp":return"pre";case"nobr":return"nowrap";case"pre":return n.wrap?"pre-wrap":"pre";case"td":case"th":return n.noWrap?"nowrap":t.whitespace;case"textarea":return"pre-wrap"}}return t.whitespace}function IK(e){return!!(e.properties||{}).hidden}function UK(e){return e.tagName==="td"||e.tagName==="th"}function DK(e){return e.tagName==="dialog"&&!(e.properties||{}).open}const MK={},RK=[];function OK(e){const t=e||MK;return function(n,r){ww(n,"element",function(i,a){const s=Array.isArray(i.properties.className)?i.properties.className:RK,o=s.includes("language-math"),l=s.includes("math-display"),c=s.includes("math-inline");let u=l;if(!o&&!l&&!c)return;let d=a[a.length-1],m=i;if(i.tagName==="code"&&o&&d&&d.type==="element"&&d.tagName==="pre"&&(m=d,d=a[a.length-2],u=!0),!d)return;const p=CK(m,{whitespace:"pre"});let x;try{x=o$.renderToString(p,{...t,displayMode:u,throwOnError:!0})}catch(w){const v=w,$=v.name.toLowerCase();r.message("Could not render math with KaTeX",{ancestors:[...a,i],cause:v,place:i.position,ruleId:$,source:"rehype-katex"});try{x=o$.renderToString(p,{...t,displayMode:u,strict:"ignore",throwOnError:!1})}catch{x=[{type:"element",tagName:"span",properties:{className:["katex-error"],style:"color:"+(t.errorColor||"#cc0000"),title:String(w)},children:[{type:"text",value:p}]}]}}typeof x=="string"&&(x=yK(x).children);const g=d.children.indexOf(m);return d.children.splice(g,1,...x),_w})}}function zK(e){const t=String(e),n=[];return{toOffset:i,toPoint:r};function r(a){if(typeof a=="number"&&a>-1&&a<=t.length){let s=0;for(;;){let o=n[s];if(o===void 0){const l=g$(t,n[s-1]);o=l===-1?t.length+1:l+1,n[s]=o}if(o>a)return{line:s+1,column:a-(s>0?n[s-1]:0)+1,offset:a};s++}}}function i(a){if(a&&typeof a.line=="number"&&typeof a.column=="number"&&!Number.isNaN(a.line)&&!Number.isNaN(a.column)){for(;n.length<a.line;){const o=n[n.length-1],l=g$(t,o),c=l===-1?t.length+1:l+1;if(o===c)break;n.push(c)}const s=(a.line>1?n[a.line-2]:0)+a.column-1;if(s<n[a.line-1])return s}}}function g$(e,t){const n=e.indexOf("\r",t),r=e.indexOf(`
`,t);return r===-1?n:n===-1||n+1===r?r:n<r?n:r}const PT={}.hasOwnProperty,BK=Object.prototype;function FK(e,t){const n=t||{};return ab({file:n.file||void 0,location:!1,schema:n.space==="svg"?Yo:um,verbose:n.verbose||!1},e)}function ab(e,t){let n;switch(t.nodeName){case"#comment":{const r=t;return n={type:"comment",value:r.data},Td(e,r,n),n}case"#document":case"#document-fragment":{const r=t,i="mode"in r?r.mode==="quirks"||r.mode==="limited-quirks":!1;if(n={type:"root",children:IT(e,t.childNodes),data:{quirksMode:i}},e.file&&e.location){const a=String(e.file),s=zK(a),o=s.toPoint(0),l=s.toPoint(a.length);n.position={start:o,end:l}}return n}case"#documentType":{const r=t;return n={type:"doctype"},Td(e,r,n),n}case"#text":{const r=t;return n={type:"text",value:r.value},Td(e,r,n),n}default:return n=jK(e,t),n}}function IT(e,t){let n=-1;const r=[];for(;++n<t.length;){const i=ab(e,t[n]);r.push(i)}return r}function jK(e,t){const n=e.schema;e.schema=t.namespaceURI===ti.svg?Yo:um;let r=-1;const i={};for(;++r<t.attrs.length;){const o=t.attrs[r],l=(o.prefix?o.prefix+":":"")+o.name;PT.call(BK,l)||(i[l]=o.value)}const s=(e.schema.space==="svg"?ET:TT)(t.tagName,i,IT(e,t.childNodes));if(Td(e,t,s),s.tagName==="template"){const o=t,l=o.sourceCodeLocation,c=l&&l.startTag&&Zs(l.startTag),u=l&&l.endTag&&Zs(l.endTag),d=ab(e,o.content);c&&u&&e.file&&(d.position={start:c.end,end:u.start}),s.content=d}return e.schema=n,s}function Td(e,t,n){if("sourceCodeLocation"in t&&t.sourceCodeLocation&&e.file){const r=VK(e,n,t.sourceCodeLocation);r&&(e.location=!0,n.position=r)}}function VK(e,t,n){const r=Zs(n);if(t.type==="element"){const i=t.children[t.children.length-1];if(r&&!n.endTag&&i&&i.position&&i.position.end&&(r.end=Object.assign({},i.position.end)),e.verbose){const a={};let s;if(n.attrs)for(s in n.attrs)PT.call(n.attrs,s)&&(a[w2(e.schema,s).property]=Zs(n.attrs[s]));n.startTag;const o=Zs(n.startTag),l=n.endTag?Zs(n.endTag):void 0,c={opening:o};l&&(c.closing=l),c.properties=a,t.data={position:c}}}return r}function Zs(e){const t=b$({line:e.startLine,column:e.startCol,offset:e.startOffset}),n=b$({line:e.endLine,column:e.endCol,offset:e.endOffset});return t||n?{start:t,end:n}:void 0}function b$(e){return e.line&&e.column?e:void 0}class nu{constructor(t,n,r){this.property=t,this.normal=n,r&&(this.space=r)}}nu.prototype.property={};nu.prototype.normal={};nu.prototype.space=null;function UT(e,t){const n={},r={};let i=-1;for(;++i<e.length;)Object.assign(n,e[i].property),Object.assign(r,e[i].normal);return new nu(n,r,t)}function cg(e){return e.toLowerCase()}class gr{constructor(t,n){this.property=t,this.attribute=n}}gr.prototype.space=null;gr.prototype.boolean=!1;gr.prototype.booleanish=!1;gr.prototype.overloadedBoolean=!1;gr.prototype.number=!1;gr.prototype.commaSeparated=!1;gr.prototype.spaceSeparated=!1;gr.prototype.commaOrSpaceSeparated=!1;gr.prototype.mustUseProperty=!1;gr.prototype.defined=!1;let HK=0;const Ie=Es(),Pt=Es(),DT=Es(),ue=Es(),rt=Es(),mo=Es(),Rn=Es();function Es(){return 2**++HK}const ug=Object.freeze(Object.defineProperty({__proto__:null,boolean:Ie,booleanish:Pt,commaOrSpaceSeparated:Rn,commaSeparated:mo,number:ue,overloadedBoolean:DT,spaceSeparated:rt},Symbol.toStringTag,{value:"Module"})),rp=Object.keys(ug);class sb extends gr{constructor(t,n,r,i){let a=-1;if(super(t,n),v$(this,"space",i),typeof r=="number")for(;++a<rp.length;){const s=rp[a];v$(this,rp[a],(r&ug[s])===ug[s])}}}sb.prototype.defined=!0;function v$(e,t,n){n&&(e[t]=n)}const qK={}.hasOwnProperty;function Jo(e){const t={},n={};let r;for(r in e.properties)if(qK.call(e.properties,r)){const i=e.properties[r],a=new sb(r,e.transform(e.attributes||{},r),i,e.space);e.mustUseProperty&&e.mustUseProperty.includes(r)&&(a.mustUseProperty=!0),t[r]=a,n[cg(r)]=r,n[cg(a.attribute)]=r}return new nu(t,n,e.space)}const MT=Jo({space:"xlink",transform(e,t){return"xlink:"+t.slice(5).toLowerCase()},properties:{xLinkActuate:null,xLinkArcRole:null,xLinkHref:null,xLinkRole:null,xLinkShow:null,xLinkTitle:null,xLinkType:null}}),RT=Jo({space:"xml",transform(e,t){return"xml:"+t.slice(3).toLowerCase()},properties:{xmlLang:null,xmlBase:null,xmlSpace:null}});function OT(e,t){return t in e?e[t]:t}function zT(e,t){return OT(e,t.toLowerCase())}const BT=Jo({space:"xmlns",attributes:{xmlnsxlink:"xmlns:xlink"},transform:zT,properties:{xmlns:null,xmlnsXLink:null}}),FT=Jo({transform(e,t){return t==="role"?t:"aria-"+t.slice(4).toLowerCase()},properties:{ariaActiveDescendant:null,ariaAtomic:Pt,ariaAutoComplete:null,ariaBusy:Pt,ariaChecked:Pt,ariaColCount:ue,ariaColIndex:ue,ariaColSpan:ue,ariaControls:rt,ariaCurrent:null,ariaDescribedBy:rt,ariaDetails:null,ariaDisabled:Pt,ariaDropEffect:rt,ariaErrorMessage:null,ariaExpanded:Pt,ariaFlowTo:rt,ariaGrabbed:Pt,ariaHasPopup:null,ariaHidden:Pt,ariaInvalid:null,ariaKeyShortcuts:null,ariaLabel:null,ariaLabelledBy:rt,ariaLevel:ue,ariaLive:null,ariaModal:Pt,ariaMultiLine:Pt,ariaMultiSelectable:Pt,ariaOrientation:null,ariaOwns:rt,ariaPlaceholder:null,ariaPosInSet:ue,ariaPressed:Pt,ariaReadOnly:Pt,ariaRelevant:null,ariaRequired:Pt,ariaRoleDescription:rt,ariaRowCount:ue,ariaRowIndex:ue,ariaRowSpan:ue,ariaSelected:Pt,ariaSetSize:ue,ariaSort:null,ariaValueMax:ue,ariaValueMin:ue,ariaValueNow:ue,ariaValueText:null,role:null}}),GK=Jo({space:"html",attributes:{acceptcharset:"accept-charset",classname:"class",htmlfor:"for",httpequiv:"http-equiv"},transform:zT,mustUseProperty:["checked","multiple","muted","selected"],properties:{abbr:null,accept:mo,acceptCharset:rt,accessKey:rt,action:null,allow:null,allowFullScreen:Ie,allowPaymentRequest:Ie,allowUserMedia:Ie,alt:null,as:null,async:Ie,autoCapitalize:null,autoComplete:rt,autoFocus:Ie,autoPlay:Ie,blocking:rt,capture:null,charSet:null,checked:Ie,cite:null,className:rt,cols:ue,colSpan:null,content:null,contentEditable:Pt,controls:Ie,controlsList:rt,coords:ue|mo,crossOrigin:null,data:null,dateTime:null,decoding:null,default:Ie,defer:Ie,dir:null,dirName:null,disabled:Ie,download:DT,draggable:Pt,encType:null,enterKeyHint:null,fetchPriority:null,form:null,formAction:null,formEncType:null,formMethod:null,formNoValidate:Ie,formTarget:null,headers:rt,height:ue,hidden:Ie,high:ue,href:null,hrefLang:null,htmlFor:rt,httpEquiv:rt,id:null,imageSizes:null,imageSrcSet:null,inert:Ie,inputMode:null,integrity:null,is:null,isMap:Ie,itemId:null,itemProp:rt,itemRef:rt,itemScope:Ie,itemType:rt,kind:null,label:null,lang:null,language:null,list:null,loading:null,loop:Ie,low:ue,manifest:null,max:null,maxLength:ue,media:null,method:null,min:null,minLength:ue,multiple:Ie,muted:Ie,name:null,nonce:null,noModule:Ie,noValidate:Ie,onAbort:null,onAfterPrint:null,onAuxClick:null,onBeforeMatch:null,onBeforePrint:null,onBeforeToggle:null,onBeforeUnload:null,onBlur:null,onCancel:null,onCanPlay:null,onCanPlayThrough:null,onChange:null,onClick:null,onClose:null,onContextLost:null,onContextMenu:null,onContextRestored:null,onCopy:null,onCueChange:null,onCut:null,onDblClick:null,onDrag:null,onDragEnd:null,onDragEnter:null,onDragExit:null,onDragLeave:null,onDragOver:null,onDragStart:null,onDrop:null,onDurationChange:null,onEmptied:null,onEnded:null,onError:null,onFocus:null,onFormData:null,onHashChange:null,onInput:null,onInvalid:null,onKeyDown:null,onKeyPress:null,onKeyUp:null,onLanguageChange:null,onLoad:null,onLoadedData:null,onLoadedMetadata:null,onLoadEnd:null,onLoadStart:null,onMessage:null,onMessageError:null,onMouseDown:null,onMouseEnter:null,onMouseLeave:null,onMouseMove:null,onMouseOut:null,onMouseOver:null,onMouseUp:null,onOffline:null,onOnline:null,onPageHide:null,onPageShow:null,onPaste:null,onPause:null,onPlay:null,onPlaying:null,onPopState:null,onProgress:null,onRateChange:null,onRejectionHandled:null,onReset:null,onResize:null,onScroll:null,onScrollEnd:null,onSecurityPolicyViolation:null,onSeeked:null,onSeeking:null,onSelect:null,onSlotChange:null,onStalled:null,onStorage:null,onSubmit:null,onSuspend:null,onTimeUpdate:null,onToggle:null,onUnhandledRejection:null,onUnload:null,onVolumeChange:null,onWaiting:null,onWheel:null,open:Ie,optimum:ue,pattern:null,ping:rt,placeholder:null,playsInline:Ie,popover:null,popoverTarget:null,popoverTargetAction:null,poster:null,preload:null,readOnly:Ie,referrerPolicy:null,rel:rt,required:Ie,reversed:Ie,rows:ue,rowSpan:ue,sandbox:rt,scope:null,scoped:Ie,seamless:Ie,selected:Ie,shadowRootClonable:Ie,shadowRootDelegatesFocus:Ie,shadowRootMode:null,shape:null,size:ue,sizes:null,slot:null,span:ue,spellCheck:Pt,src:null,srcDoc:null,srcLang:null,srcSet:null,start:ue,step:null,style:null,tabIndex:ue,target:null,title:null,translate:null,type:null,typeMustMatch:Ie,useMap:null,value:Pt,width:ue,wrap:null,writingSuggestions:null,align:null,aLink:null,archive:rt,axis:null,background:null,bgColor:null,border:ue,borderColor:null,bottomMargin:ue,cellPadding:null,cellSpacing:null,char:null,charOff:null,classId:null,clear:null,code:null,codeBase:null,codeType:null,color:null,compact:Ie,declare:Ie,event:null,face:null,frame:null,frameBorder:null,hSpace:ue,leftMargin:ue,link:null,longDesc:null,lowSrc:null,marginHeight:ue,marginWidth:ue,noResize:Ie,noHref:Ie,noShade:Ie,noWrap:Ie,object:null,profile:null,prompt:null,rev:null,rightMargin:ue,rules:null,scheme:null,scrolling:Pt,standby:null,summary:null,text:null,topMargin:ue,valueType:null,version:null,vAlign:null,vLink:null,vSpace:ue,allowTransparency:null,autoCorrect:null,autoSave:null,disablePictureInPicture:Ie,disableRemotePlayback:Ie,prefix:null,property:null,results:ue,security:null,unselectable:null}}),WK=Jo({space:"svg",attributes:{accentHeight:"accent-height",alignmentBaseline:"alignment-baseline",arabicForm:"arabic-form",baselineShift:"baseline-shift",capHeight:"cap-height",className:"class",clipPath:"clip-path",clipRule:"clip-rule",colorInterpolation:"color-interpolation",colorInterpolationFilters:"color-interpolation-filters",colorProfile:"color-profile",colorRendering:"color-rendering",crossOrigin:"crossorigin",dataType:"datatype",dominantBaseline:"dominant-baseline",enableBackground:"enable-background",fillOpacity:"fill-opacity",fillRule:"fill-rule",floodColor:"flood-color",floodOpacity:"flood-opacity",fontFamily:"font-family",fontSize:"font-size",fontSizeAdjust:"font-size-adjust",fontStretch:"font-stretch",fontStyle:"font-style",fontVariant:"font-variant",fontWeight:"font-weight",glyphName:"glyph-name",glyphOrientationHorizontal:"glyph-orientation-horizontal",glyphOrientationVertical:"glyph-orientation-vertical",hrefLang:"hreflang",horizAdvX:"horiz-adv-x",horizOriginX:"horiz-origin-x",horizOriginY:"horiz-origin-y",imageRendering:"image-rendering",letterSpacing:"letter-spacing",lightingColor:"lighting-color",markerEnd:"marker-end",markerMid:"marker-mid",markerStart:"marker-start",navDown:"nav-down",navDownLeft:"nav-down-left",navDownRight:"nav-down-right",navLeft:"nav-left",navNext:"nav-next",navPrev:"nav-prev",navRight:"nav-right",navUp:"nav-up",navUpLeft:"nav-up-left",navUpRight:"nav-up-right",onAbort:"onabort",onActivate:"onactivate",onAfterPrint:"onafterprint",onBeforePrint:"onbeforeprint",onBegin:"onbegin",onCancel:"oncancel",onCanPlay:"oncanplay",onCanPlayThrough:"oncanplaythrough",onChange:"onchange",onClick:"onclick",onClose:"onclose",onCopy:"oncopy",onCueChange:"oncuechange",onCut:"oncut",onDblClick:"ondblclick",onDrag:"ondrag",onDragEnd:"ondragend",onDragEnter:"ondragenter",onDragExit:"ondragexit",onDragLeave:"ondragleave",onDragOver:"ondragover",onDragStart:"ondragstart",onDrop:"ondrop",onDurationChange:"ondurationchange",onEmptied:"onemptied",onEnd:"onend",onEnded:"onended",onError:"onerror",onFocus:"onfocus",onFocusIn:"onfocusin",onFocusOut:"onfocusout",onHashChange:"onhashchange",onInput:"oninput",onInvalid:"oninvalid",onKeyDown:"onkeydown",onKeyPress:"onkeypress",onKeyUp:"onkeyup",onLoad:"onload",onLoadedData:"onloadeddata",onLoadedMetadata:"onloadedmetadata",onLoadStart:"onloadstart",onMessage:"onmessage",onMouseDown:"onmousedown",onMouseEnter:"onmouseenter",onMouseLeave:"onmouseleave",onMouseMove:"onmousemove",onMouseOut:"onmouseout",onMouseOver:"onmouseover",onMouseUp:"onmouseup",onMouseWheel:"onmousewheel",onOffline:"onoffline",onOnline:"ononline",onPageHide:"onpagehide",onPageShow:"onpageshow",onPaste:"onpaste",onPause:"onpause",onPlay:"onplay",onPlaying:"onplaying",onPopState:"onpopstate",onProgress:"onprogress",onRateChange:"onratechange",onRepeat:"onrepeat",onReset:"onreset",onResize:"onresize",onScroll:"onscroll",onSeeked:"onseeked",onSeeking:"onseeking",onSelect:"onselect",onShow:"onshow",onStalled:"onstalled",onStorage:"onstorage",onSubmit:"onsubmit",onSuspend:"onsuspend",onTimeUpdate:"ontimeupdate",onToggle:"ontoggle",onUnload:"onunload",onVolumeChange:"onvolumechange",onWaiting:"onwaiting",onZoom:"onzoom",overlinePosition:"overline-position",overlineThickness:"overline-thickness",paintOrder:"paint-order",panose1:"panose-1",pointerEvents:"pointer-events",referrerPolicy:"referrerpolicy",renderingIntent:"rendering-intent",shapeRendering:"shape-rendering",stopColor:"stop-color",stopOpacity:"stop-opacity",strikethroughPosition:"strikethrough-position",strikethroughThickness:"strikethrough-thickness",strokeDashArray:"stroke-dasharray",strokeDashOffset:"stroke-dashoffset",strokeLineCap:"stroke-linecap",strokeLineJoin:"stroke-linejoin",strokeMiterLimit:"stroke-miterlimit",strokeOpacity:"stroke-opacity",strokeWidth:"stroke-width",tabIndex:"tabindex",textAnchor:"text-anchor",textDecoration:"text-decoration",textRendering:"text-rendering",transformOrigin:"transform-origin",typeOf:"typeof",underlinePosition:"underline-position",underlineThickness:"underline-thickness",unicodeBidi:"unicode-bidi",unicodeRange:"unicode-range",unitsPerEm:"units-per-em",vAlphabetic:"v-alphabetic",vHanging:"v-hanging",vIdeographic:"v-ideographic",vMathematical:"v-mathematical",vectorEffect:"vector-effect",vertAdvY:"vert-adv-y",vertOriginX:"vert-origin-x",vertOriginY:"vert-origin-y",wordSpacing:"word-spacing",writingMode:"writing-mode",xHeight:"x-height",playbackOrder:"playbackorder",timelineBegin:"timelinebegin"},transform:OT,properties:{about:Rn,accentHeight:ue,accumulate:null,additive:null,alignmentBaseline:null,alphabetic:ue,amplitude:ue,arabicForm:null,ascent:ue,attributeName:null,attributeType:null,azimuth:ue,bandwidth:null,baselineShift:null,baseFrequency:null,baseProfile:null,bbox:null,begin:null,bias:ue,by:null,calcMode:null,capHeight:ue,className:rt,clip:null,clipPath:null,clipPathUnits:null,clipRule:null,color:null,colorInterpolation:null,colorInterpolationFilters:null,colorProfile:null,colorRendering:null,content:null,contentScriptType:null,contentStyleType:null,crossOrigin:null,cursor:null,cx:null,cy:null,d:null,dataType:null,defaultAction:null,descent:ue,diffuseConstant:ue,direction:null,display:null,dur:null,divisor:ue,dominantBaseline:null,download:Ie,dx:null,dy:null,edgeMode:null,editable:null,elevation:ue,enableBackground:null,end:null,event:null,exponent:ue,externalResourcesRequired:null,fill:null,fillOpacity:ue,fillRule:null,filter:null,filterRes:null,filterUnits:null,floodColor:null,floodOpacity:null,focusable:null,focusHighlight:null,fontFamily:null,fontSize:null,fontSizeAdjust:null,fontStretch:null,fontStyle:null,fontVariant:null,fontWeight:null,format:null,fr:null,from:null,fx:null,fy:null,g1:mo,g2:mo,glyphName:mo,glyphOrientationHorizontal:null,glyphOrientationVertical:null,glyphRef:null,gradientTransform:null,gradientUnits:null,handler:null,hanging:ue,hatchContentUnits:null,hatchUnits:null,height:null,href:null,hrefLang:null,horizAdvX:ue,horizOriginX:ue,horizOriginY:ue,id:null,ideographic:ue,imageRendering:null,initialVisibility:null,in:null,in2:null,intercept:ue,k:ue,k1:ue,k2:ue,k3:ue,k4:ue,kernelMatrix:Rn,kernelUnitLength:null,keyPoints:null,keySplines:null,keyTimes:null,kerning:null,lang:null,lengthAdjust:null,letterSpacing:null,lightingColor:null,limitingConeAngle:ue,local:null,markerEnd:null,markerMid:null,markerStart:null,markerHeight:null,markerUnits:null,markerWidth:null,mask:null,maskContentUnits:null,maskUnits:null,mathematical:null,max:null,media:null,mediaCharacterEncoding:null,mediaContentEncodings:null,mediaSize:ue,mediaTime:null,method:null,min:null,mode:null,name:null,navDown:null,navDownLeft:null,navDownRight:null,navLeft:null,navNext:null,navPrev:null,navRight:null,navUp:null,navUpLeft:null,navUpRight:null,numOctaves:null,observer:null,offset:null,onAbort:null,onActivate:null,onAfterPrint:null,onBeforePrint:null,onBegin:null,onCancel:null,onCanPlay:null,onCanPlayThrough:null,onChange:null,onClick:null,onClose:null,onCopy:null,onCueChange:null,onCut:null,onDblClick:null,onDrag:null,onDragEnd:null,onDragEnter:null,onDragExit:null,onDragLeave:null,onDragOver:null,onDragStart:null,onDrop:null,onDurationChange:null,onEmptied:null,onEnd:null,onEnded:null,onError:null,onFocus:null,onFocusIn:null,onFocusOut:null,onHashChange:null,onInput:null,onInvalid:null,onKeyDown:null,onKeyPress:null,onKeyUp:null,onLoad:null,onLoadedData:null,onLoadedMetadata:null,onLoadStart:null,onMessage:null,onMouseDown:null,onMouseEnter:null,onMouseLeave:null,onMouseMove:null,onMouseOut:null,onMouseOver:null,onMouseUp:null,onMouseWheel:null,onOffline:null,onOnline:null,onPageHide:null,onPageShow:null,onPaste:null,onPause:null,onPlay:null,onPlaying:null,onPopState:null,onProgress:null,onRateChange:null,onRepeat:null,onReset:null,onResize:null,onScroll:null,onSeeked:null,onSeeking:null,onSelect:null,onShow:null,onStalled:null,onStorage:null,onSubmit:null,onSuspend:null,onTimeUpdate:null,onToggle:null,onUnload:null,onVolumeChange:null,onWaiting:null,onZoom:null,opacity:null,operator:null,order:null,orient:null,orientation:null,origin:null,overflow:null,overlay:null,overlinePosition:ue,overlineThickness:ue,paintOrder:null,panose1:null,path:null,pathLength:ue,patternContentUnits:null,patternTransform:null,patternUnits:null,phase:null,ping:rt,pitch:null,playbackOrder:null,pointerEvents:null,points:null,pointsAtX:ue,pointsAtY:ue,pointsAtZ:ue,preserveAlpha:null,preserveAspectRatio:null,primitiveUnits:null,propagate:null,property:Rn,r:null,radius:null,referrerPolicy:null,refX:null,refY:null,rel:Rn,rev:Rn,renderingIntent:null,repeatCount:null,repeatDur:null,requiredExtensions:Rn,requiredFeatures:Rn,requiredFonts:Rn,requiredFormats:Rn,resource:null,restart:null,result:null,rotate:null,rx:null,ry:null,scale:null,seed:null,shapeRendering:null,side:null,slope:null,snapshotTime:null,specularConstant:ue,specularExponent:ue,spreadMethod:null,spacing:null,startOffset:null,stdDeviation:null,stemh:null,stemv:null,stitchTiles:null,stopColor:null,stopOpacity:null,strikethroughPosition:ue,strikethroughThickness:ue,string:null,stroke:null,strokeDashArray:Rn,strokeDashOffset:null,strokeLineCap:null,strokeLineJoin:null,strokeMiterLimit:ue,strokeOpacity:ue,strokeWidth:null,style:null,surfaceScale:ue,syncBehavior:null,syncBehaviorDefault:null,syncMaster:null,syncTolerance:null,syncToleranceDefault:null,systemLanguage:Rn,tabIndex:ue,tableValues:null,target:null,targetX:ue,targetY:ue,textAnchor:null,textDecoration:null,textRendering:null,textLength:null,timelineBegin:null,title:null,transformBehavior:null,type:null,typeOf:Rn,to:null,transform:null,transformOrigin:null,u1:null,u2:null,underlinePosition:ue,underlineThickness:ue,unicode:null,unicodeBidi:null,unicodeRange:null,unitsPerEm:ue,values:null,vAlphabetic:ue,vMathematical:ue,vectorEffect:null,vHanging:ue,vIdeographic:ue,version:null,vertAdvY:ue,vertOriginX:ue,vertOriginY:ue,viewBox:null,viewTarget:null,visibility:null,width:null,widths:null,wordSpacing:null,writingMode:null,x:null,x1:null,x2:null,xChannelSelector:null,xHeight:ue,y:null,y1:null,y2:null,yChannelSelector:null,z:null,zoomAndPan:null}}),KK=/^data[-\w.:]+$/i,x$=/-[a-z]/g,YK=/[A-Z]/g;function XK(e,t){const n=cg(t);let r=t,i=gr;if(n in e.normal)return e.property[e.normal[n]];if(n.length>4&&n.slice(0,4)==="data"&&KK.test(t)){if(t.charAt(4)==="-"){const a=t.slice(5).replace(x$,ZK);r="data"+a.charAt(0).toUpperCase()+a.slice(1)}else{const a=t.slice(4);if(!x$.test(a)){let s=a.replace(YK,QK);s.charAt(0)!=="-"&&(s="-"+s),t="data"+s}}i=sb}return new i(r,t)}function QK(e){return"-"+e.toLowerCase()}function ZK(e){return e.charAt(1).toUpperCase()}const JK=UT([RT,MT,BT,FT,GK],"html"),jT=UT([RT,MT,BT,FT,WK],"svg"),$$={}.hasOwnProperty;function VT(e,t){const n=t||{};function r(i,...a){let s=r.invalid;const o=r.handlers;if(i&&$$.call(i,e)){const l=String(i[e]);s=$$.call(o,l)?o[l]:r.unknown}if(s)return s.call(this,i,...a)}return r.handlers=n.handlers||{},r.invalid=n.invalid,r.unknown=n.unknown,r}const eY={},tY={}.hasOwnProperty,HT=VT("type",{handlers:{root:rY,element:lY,text:sY,comment:oY,doctype:aY}});function nY(e,t){const r=(t||eY).space;return HT(e,r==="svg"?jT:JK)}function rY(e,t){const n={nodeName:"#document",mode:(e.data||{}).quirksMode?"quirks":"no-quirks",childNodes:[]};return n.childNodes=ob(e.children,n,t),el(e,n),n}function iY(e,t){const n={nodeName:"#document-fragment",childNodes:[]};return n.childNodes=ob(e.children,n,t),el(e,n),n}function aY(e){const t={nodeName:"#documentType",name:"html",publicId:"",systemId:"",parentNode:null};return el(e,t),t}function sY(e){const t={nodeName:"#text",value:e.value,parentNode:null};return el(e,t),t}function oY(e){const t={nodeName:"#comment",data:e.value,parentNode:null};return el(e,t),t}function lY(e,t){const n=t;let r=n;e.type==="element"&&e.tagName.toLowerCase()==="svg"&&n.space==="html"&&(r=jT);const i=[];let a;if(e.properties){for(a in e.properties)if(a!=="children"&&tY.call(e.properties,a)){const l=cY(r,a,e.properties[a]);l&&i.push(l)}}const s=r.space,o={nodeName:e.tagName,tagName:e.tagName,attrs:i,namespaceURI:ti[s],childNodes:[],parentNode:null};return o.childNodes=ob(e.children,o,r),el(e,o),e.tagName==="template"&&e.content&&(o.content=iY(e.content,r)),o}function cY(e,t,n){const r=XK(e,t);if(n===!1||n===null||n===void 0||typeof n=="number"&&Number.isNaN(n)||!n&&r.boolean)return;Array.isArray(n)&&(n=r.commaSeparated?V_(n):Q_(n));const i={name:r.attribute,value:n===!0?"":String(n)};if(r.space&&r.space!=="html"&&r.space!=="svg"){const a=i.name.indexOf(":");a<0?i.prefix="":(i.name=i.name.slice(a+1),i.prefix=r.attribute.slice(0,a)),i.namespace=ti[r.space]}return i}function ob(e,t,n){let r=-1;const i=[];if(e)for(;++r<e.length;){const a=HT(e[r],n);a.parentNode=t,i.push(a)}return i}function el(e,t){const n=e.position;n&&n.start&&n.end&&(n.start.offset,n.end.offset,t.sourceCodeLocation={startLine:n.start.line,startCol:n.start.column,startOffset:n.start.offset,endLine:n.end.line,endCol:n.end.column,endOffset:n.end.offset})}const uY=["area","base","basefont","bgsound","br","col","command","embed","frame","hr","image","img","input","keygen","link","meta","param","source","track","wbr"],dY=new Set([65534,65535,131070,131071,196606,196607,262142,262143,327678,327679,393214,393215,458750,458751,524286,524287,589822,589823,655358,655359,720894,720895,786430,786431,851966,851967,917502,917503,983038,983039,1048574,1048575,1114110,1114111]),dt="�";var A;(function(e){e[e.EOF=-1]="EOF",e[e.NULL=0]="NULL",e[e.TABULATION=9]="TABULATION",e[e.CARRIAGE_RETURN=13]="CARRIAGE_RETURN",e[e.LINE_FEED=10]="LINE_FEED",e[e.FORM_FEED=12]="FORM_FEED",e[e.SPACE=32]="SPACE",e[e.EXCLAMATION_MARK=33]="EXCLAMATION_MARK",e[e.QUOTATION_MARK=34]="QUOTATION_MARK",e[e.AMPERSAND=38]="AMPERSAND",e[e.APOSTROPHE=39]="APOSTROPHE",e[e.HYPHEN_MINUS=45]="HYPHEN_MINUS",e[e.SOLIDUS=47]="SOLIDUS",e[e.DIGIT_0=48]="DIGIT_0",e[e.DIGIT_9=57]="DIGIT_9",e[e.SEMICOLON=59]="SEMICOLON",e[e.LESS_THAN_SIGN=60]="LESS_THAN_SIGN",e[e.EQUALS_SIGN=61]="EQUALS_SIGN",e[e.GREATER_THAN_SIGN=62]="GREATER_THAN_SIGN",e[e.QUESTION_MARK=63]="QUESTION_MARK",e[e.LATIN_CAPITAL_A=65]="LATIN_CAPITAL_A",e[e.LATIN_CAPITAL_Z=90]="LATIN_CAPITAL_Z",e[e.RIGHT_SQUARE_BRACKET=93]="RIGHT_SQUARE_BRACKET",e[e.GRAVE_ACCENT=96]="GRAVE_ACCENT",e[e.LATIN_SMALL_A=97]="LATIN_SMALL_A",e[e.LATIN_SMALL_Z=122]="LATIN_SMALL_Z"})(A||(A={}));const _n={DASH_DASH:"--",CDATA_START:"[CDATA[",DOCTYPE:"doctype",SCRIPT:"script",PUBLIC:"public",SYSTEM:"system"};function qT(e){return e>=55296&&e<=57343}function mY(e){return e>=56320&&e<=57343}function hY(e,t){return(e-55296)*1024+9216+t}function GT(e){return e!==32&&e!==10&&e!==13&&e!==9&&e!==12&&e>=1&&e<=31||e>=127&&e<=159}function WT(e){return e>=64976&&e<=65007||dY.has(e)}var W;(function(e){e.controlCharacterInInputStream="control-character-in-input-stream",e.noncharacterInInputStream="noncharacter-in-input-stream",e.surrogateInInputStream="surrogate-in-input-stream",e.nonVoidHtmlElementStartTagWithTrailingSolidus="non-void-html-element-start-tag-with-trailing-solidus",e.endTagWithAttributes="end-tag-with-attributes",e.endTagWithTrailingSolidus="end-tag-with-trailing-solidus",e.unexpectedSolidusInTag="unexpected-solidus-in-tag",e.unexpectedNullCharacter="unexpected-null-character",e.unexpectedQuestionMarkInsteadOfTagName="unexpected-question-mark-instead-of-tag-name",e.invalidFirstCharacterOfTagName="invalid-first-character-of-tag-name",e.unexpectedEqualsSignBeforeAttributeName="unexpected-equals-sign-before-attribute-name",e.missingEndTagName="missing-end-tag-name",e.unexpectedCharacterInAttributeName="unexpected-character-in-attribute-name",e.unknownNamedCharacterReference="unknown-named-character-reference",e.missingSemicolonAfterCharacterReference="missing-semicolon-after-character-reference",e.unexpectedCharacterAfterDoctypeSystemIdentifier="unexpected-character-after-doctype-system-identifier",e.unexpectedCharacterInUnquotedAttributeValue="unexpected-character-in-unquoted-attribute-value",e.eofBeforeTagName="eof-before-tag-name",e.eofInTag="eof-in-tag",e.missingAttributeValue="missing-attribute-value",e.missingWhitespaceBetweenAttributes="missing-whitespace-between-attributes",e.missingWhitespaceAfterDoctypePublicKeyword="missing-whitespace-after-doctype-public-keyword",e.missingWhitespaceBetweenDoctypePublicAndSystemIdentifiers="missing-whitespace-between-doctype-public-and-system-identifiers",e.missingWhitespaceAfterDoctypeSystemKeyword="missing-whitespace-after-doctype-system-keyword",e.missingQuoteBeforeDoctypePublicIdentifier="missing-quote-before-doctype-public-identifier",e.missingQuoteBeforeDoctypeSystemIdentifier="missing-quote-before-doctype-system-identifier",e.missingDoctypePublicIdentifier="missing-doctype-public-identifier",e.missingDoctypeSystemIdentifier="missing-doctype-system-identifier",e.abruptDoctypePublicIdentifier="abrupt-doctype-public-identifier",e.abruptDoctypeSystemIdentifier="abrupt-doctype-system-identifier",e.cdataInHtmlContent="cdata-in-html-content",e.incorrectlyOpenedComment="incorrectly-opened-comment",e.eofInScriptHtmlCommentLikeText="eof-in-script-html-comment-like-text",e.eofInDoctype="eof-in-doctype",e.nestedComment="nested-comment",e.abruptClosingOfEmptyComment="abrupt-closing-of-empty-comment",e.eofInComment="eof-in-comment",e.incorrectlyClosedComment="incorrectly-closed-comment",e.eofInCdata="eof-in-cdata",e.absenceOfDigitsInNumericCharacterReference="absence-of-digits-in-numeric-character-reference",e.nullCharacterReference="null-character-reference",e.surrogateCharacterReference="surrogate-character-reference",e.characterReferenceOutsideUnicodeRange="character-reference-outside-unicode-range",e.controlCharacterReference="control-character-reference",e.noncharacterCharacterReference="noncharacter-character-reference",e.missingWhitespaceBeforeDoctypeName="missing-whitespace-before-doctype-name",e.missingDoctypeName="missing-doctype-name",e.invalidCharacterSequenceAfterDoctypeName="invalid-character-sequence-after-doctype-name",e.duplicateAttribute="duplicate-attribute",e.nonConformingDoctype="non-conforming-doctype",e.missingDoctype="missing-doctype",e.misplacedDoctype="misplaced-doctype",e.endTagWithoutMatchingOpenElement="end-tag-without-matching-open-element",e.closingOfElementWithOpenChildElements="closing-of-element-with-open-child-elements",e.disallowedContentInNoscriptInHead="disallowed-content-in-noscript-in-head",e.openElementsLeftAfterEof="open-elements-left-after-eof",e.abandonedHeadElementChild="abandoned-head-element-child",e.misplacedStartTagForHeadElement="misplaced-start-tag-for-head-element",e.nestedNoscriptInHead="nested-noscript-in-head",e.eofInElementThatCanContainOnlyText="eof-in-element-that-can-contain-only-text"})(W||(W={}));const pY=65536;class fY{constructor(t){this.handler=t,this.html="",this.pos=-1,this.lastGapPos=-2,this.gapStack=[],this.skipNextNewLine=!1,this.lastChunkWritten=!1,this.endOfChunkHit=!1,this.bufferWaterline=pY,this.isEol=!1,this.lineStartPos=0,this.droppedBufferSize=0,this.line=1,this.lastErrOffset=-1}get col(){return this.pos-this.lineStartPos+ +(this.lastGapPos!==this.pos)}get offset(){return this.droppedBufferSize+this.pos}getError(t,n){const{line:r,col:i,offset:a}=this,s=i+n,o=a+n;return{code:t,startLine:r,endLine:r,startCol:s,endCol:s,startOffset:o,endOffset:o}}_err(t){this.handler.onParseError&&this.lastErrOffset!==this.offset&&(this.lastErrOffset=this.offset,this.handler.onParseError(this.getError(t,0)))}_addGap(){this.gapStack.push(this.lastGapPos),this.lastGapPos=this.pos}_processSurrogate(t){if(this.pos!==this.html.length-1){const n=this.html.charCodeAt(this.pos+1);if(mY(n))return this.pos++,this._addGap(),hY(t,n)}else if(!this.lastChunkWritten)return this.endOfChunkHit=!0,A.EOF;return this._err(W.surrogateInInputStream),t}willDropParsedChunk(){return this.pos>this.bufferWaterline}dropParsedChunk(){this.willDropParsedChunk()&&(this.html=this.html.substring(this.pos),this.lineStartPos-=this.pos,this.droppedBufferSize+=this.pos,this.pos=0,this.lastGapPos=-2,this.gapStack.length=0)}write(t,n){this.html.length>0?this.html+=t:this.html=t,this.endOfChunkHit=!1,this.lastChunkWritten=n}insertHtmlAtCurrentPos(t){this.html=this.html.substring(0,this.pos+1)+t+this.html.substring(this.pos+1),this.endOfChunkHit=!1}startsWith(t,n){if(this.pos+t.length>this.html.length)return this.endOfChunkHit=!this.lastChunkWritten,!1;if(n)return this.html.startsWith(t,this.pos);for(let r=0;r<t.length;r++)if((this.html.charCodeAt(this.pos+r)|32)!==t.charCodeAt(r))return!1;return!0}peek(t){const n=this.pos+t;if(n>=this.html.length)return this.endOfChunkHit=!this.lastChunkWritten,A.EOF;const r=this.html.charCodeAt(n);return r===A.CARRIAGE_RETURN?A.LINE_FEED:r}advance(){if(this.pos++,this.isEol&&(this.isEol=!1,this.line++,this.lineStartPos=this.pos),this.pos>=this.html.length)return this.endOfChunkHit=!this.lastChunkWritten,A.EOF;let t=this.html.charCodeAt(this.pos);return t===A.CARRIAGE_RETURN?(this.isEol=!0,this.skipNextNewLine=!0,A.LINE_FEED):t===A.LINE_FEED&&(this.isEol=!0,this.skipNextNewLine)?(this.line--,this.skipNextNewLine=!1,this._addGap(),this.advance()):(this.skipNextNewLine=!1,qT(t)&&(t=this._processSurrogate(t)),this.handler.onParseError===null||t>31&&t<127||t===A.LINE_FEED||t===A.CARRIAGE_RETURN||t>159&&t<64976||this._checkForProblematicCharacters(t),t)}_checkForProblematicCharacters(t){GT(t)?this._err(W.controlCharacterInInputStream):WT(t)&&this._err(W.noncharacterInInputStream)}retreat(t){for(this.pos-=t;this.pos<this.lastGapPos;)this.lastGapPos=this.gapStack.pop(),this.pos--;this.isEol=!1}}var ze;(function(e){e[e.CHARACTER=0]="CHARACTER",e[e.NULL_CHARACTER=1]="NULL_CHARACTER",e[e.WHITESPACE_CHARACTER=2]="WHITESPACE_CHARACTER",e[e.START_TAG=3]="START_TAG",e[e.END_TAG=4]="END_TAG",e[e.COMMENT=5]="COMMENT",e[e.DOCTYPE=6]="DOCTYPE",e[e.EOF=7]="EOF",e[e.HIBERNATION=8]="HIBERNATION"})(ze||(ze={}));function KT(e,t){for(let n=e.attrs.length-1;n>=0;n--)if(e.attrs[n].name===t)return e.attrs[n].value;return null}const gY=new Uint16Array('ᵁ<Õıʊҝջאٵ۞ޢߖࠏ੊ઑඡ๭༉༦჊ረዡᐕᒝᓃᓟᔥ\0\0\0\0\0\0ᕫᛍᦍᰒᷝ὾⁠↰⊍⏀⏻⑂⠤⤒ⴈ⹈⿎〖㊺㘹㞬㣾㨨㩱㫠㬮ࠀEMabcfglmnoprstu\\bfms¦³¹ÈÏlig耻Æ䃆P耻&䀦cute耻Á䃁reve;䄂Āiyx}rc耻Â䃂;䐐r;쀀𝔄rave耻À䃀pha;䎑acr;䄀d;橓Āgp¡on;䄄f;쀀𝔸plyFunction;恡ing耻Å䃅Ācs¾Ãr;쀀𝒜ign;扔ilde耻Ã䃃ml耻Ä䃄ЀaceforsuåûþėĜĢħĪĀcrêòkslash;或Ŷöø;櫧ed;挆y;䐑ƀcrtąċĔause;戵noullis;愬a;䎒r;쀀𝔅pf;쀀𝔹eve;䋘còēmpeq;扎܀HOacdefhilorsuōőŖƀƞƢƵƷƺǜȕɳɸɾcy;䐧PY耻©䂩ƀcpyŝŢźute;䄆Ā;iŧŨ拒talDifferentialD;慅leys;愭ȀaeioƉƎƔƘron;䄌dil耻Ç䃇rc;䄈nint;戰ot;䄊ĀdnƧƭilla;䂸terDot;䂷òſi;䎧rcleȀDMPTǇǋǑǖot;抙inus;抖lus;投imes;抗oĀcsǢǸkwiseContourIntegral;戲eCurlyĀDQȃȏoubleQuote;思uote;怙ȀlnpuȞȨɇɕonĀ;eȥȦ户;橴ƀgitȯȶȺruent;扡nt;戯ourIntegral;戮ĀfrɌɎ;愂oduct;成nterClockwiseContourIntegral;戳oss;樯cr;쀀𝒞pĀ;Cʄʅ拓ap;才րDJSZacefiosʠʬʰʴʸˋ˗ˡ˦̳ҍĀ;oŹʥtrahd;椑cy;䐂cy;䐅cy;䐏ƀgrsʿ˄ˇger;怡r;憡hv;櫤Āayː˕ron;䄎;䐔lĀ;t˝˞戇a;䎔r;쀀𝔇Āaf˫̧Ācm˰̢riticalȀADGT̖̜̀̆cute;䂴oŴ̋̍;䋙bleAcute;䋝rave;䁠ilde;䋜ond;拄ferentialD;慆Ѱ̽\0\0\0͔͂\0Ѕf;쀀𝔻ƀ;DE͈͉͍䂨ot;惜qual;扐blèCDLRUVͣͲ΂ϏϢϸontourIntegraìȹoɴ͹\0\0ͻ»͉nArrow;懓Āeo·ΤftƀARTΐΖΡrrow;懐ightArrow;懔eåˊngĀLRΫτeftĀARγιrrow;柸ightArrow;柺ightArrow;柹ightĀATϘϞrrow;懒ee;抨pɁϩ\0\0ϯrrow;懑ownArrow;懕erticalBar;戥ǹABLRTaВЪаўѿͼrrowƀ;BUНОТ憓ar;椓pArrow;懵reve;䌑eft˒к\0ц\0ѐightVector;楐eeVector;楞ectorĀ;Bљњ憽ar;楖ightǔѧ\0ѱeeVector;楟ectorĀ;BѺѻ懁ar;楗eeĀ;A҆҇护rrow;憧ĀctҒҗr;쀀𝒟rok;䄐ࠀNTacdfglmopqstuxҽӀӄӋӞӢӧӮӵԡԯԶՒ՝ՠեG;䅊H耻Ð䃐cute耻É䃉ƀaiyӒӗӜron;䄚rc耻Ê䃊;䐭ot;䄖r;쀀𝔈rave耻È䃈ement;戈ĀapӺӾcr;䄒tyɓԆ\0\0ԒmallSquare;旻erySmallSquare;斫ĀgpԦԪon;䄘f;쀀𝔼silon;䎕uĀaiԼՉlĀ;TՂՃ橵ilde;扂librium;懌Āci՗՚r;愰m;橳a;䎗ml耻Ë䃋Āipժկsts;戃onentialE;慇ʀcfiosօֈ֍ֲ׌y;䐤r;쀀𝔉lledɓ֗\0\0֣mallSquare;旼erySmallSquare;斪Ͱֺ\0ֿ\0\0ׄf;쀀𝔽All;戀riertrf;愱cò׋؀JTabcdfgorstר׬ׯ׺؀ؒؖ؛؝أ٬ٲcy;䐃耻>䀾mmaĀ;d׷׸䎓;䏜reve;䄞ƀeiy؇،ؐdil;䄢rc;䄜;䐓ot;䄠r;쀀𝔊;拙pf;쀀𝔾eater̀EFGLSTصلَٖٛ٦qualĀ;Lؾؿ扥ess;招ullEqual;执reater;檢ess;扷lantEqual;橾ilde;扳cr;쀀𝒢;扫ЀAacfiosuڅڋږڛڞڪھۊRDcy;䐪Āctڐڔek;䋇;䁞irc;䄤r;愌lbertSpace;愋ǰگ\0ڲf;愍izontalLine;攀Āctۃۅòکrok;䄦mpńېۘownHumðįqual;扏܀EJOacdfgmnostuۺ۾܃܇܎ܚܞܡܨ݄ݸދޏޕcy;䐕lig;䄲cy;䐁cute耻Í䃍Āiyܓܘrc耻Î䃎;䐘ot;䄰r;愑rave耻Ì䃌ƀ;apܠܯܿĀcgܴܷr;䄪inaryI;慈lieóϝǴ݉\0ݢĀ;eݍݎ戬Āgrݓݘral;戫section;拂isibleĀCTݬݲomma;恣imes;恢ƀgptݿރވon;䄮f;쀀𝕀a;䎙cr;愐ilde;䄨ǫޚ\0ޞcy;䐆l耻Ï䃏ʀcfosuެ޷޼߂ߐĀiyޱ޵rc;䄴;䐙r;쀀𝔍pf;쀀𝕁ǣ߇\0ߌr;쀀𝒥rcy;䐈kcy;䐄΀HJacfosߤߨ߽߬߱ࠂࠈcy;䐥cy;䐌ppa;䎚Āey߶߻dil;䄶;䐚r;쀀𝔎pf;쀀𝕂cr;쀀𝒦րJTaceflmostࠥࠩࠬࡐࡣ঳সে্਷ੇcy;䐉耻<䀼ʀcmnpr࠷࠼ࡁࡄࡍute;䄹bda;䎛g;柪lacetrf;愒r;憞ƀaeyࡗ࡜ࡡron;䄽dil;䄻;䐛Āfsࡨ॰tԀACDFRTUVarࡾࢩࢱࣦ࣠ࣼयज़ΐ४Ānrࢃ࢏gleBracket;柨rowƀ;BR࢙࢚࢞憐ar;懤ightArrow;懆eiling;挈oǵࢷ\0ࣃbleBracket;柦nǔࣈ\0࣒eeVector;楡ectorĀ;Bࣛࣜ懃ar;楙loor;挊ightĀAV࣯ࣵrrow;憔ector;楎Āerँगeƀ;AVउऊऐ抣rrow;憤ector;楚iangleƀ;BEतथऩ抲ar;槏qual;抴pƀDTVषूौownVector;楑eeVector;楠ectorĀ;Bॖॗ憿ar;楘ectorĀ;B॥०憼ar;楒ightáΜs̀EFGLSTॾঋকঝঢভqualGreater;拚ullEqual;扦reater;扶ess;檡lantEqual;橽ilde;扲r;쀀𝔏Ā;eঽা拘ftarrow;懚idot;䄿ƀnpw৔ਖਛgȀLRlr৞৷ਂਐeftĀAR০৬rrow;柵ightArrow;柷ightArrow;柶eftĀarγਊightáοightáϊf;쀀𝕃erĀLRਢਬeftArrow;憙ightArrow;憘ƀchtਾੀੂòࡌ;憰rok;䅁;扪Ѐacefiosuਗ਼੝੠੷੼અઋ઎p;椅y;䐜Ādl੥੯iumSpace;恟lintrf;愳r;쀀𝔐nusPlus;戓pf;쀀𝕄cò੶;䎜ҀJacefostuણધભીଔଙඑ඗ඞcy;䐊cute;䅃ƀaey઴હાron;䅇dil;䅅;䐝ƀgswે૰଎ativeƀMTV૓૟૨ediumSpace;怋hiĀcn૦૘ë૙eryThiî૙tedĀGL૸ଆreaterGreateòٳessLesóੈLine;䀊r;쀀𝔑ȀBnptଢନଷ଺reak;恠BreakingSpace;䂠f;愕ڀ;CDEGHLNPRSTV୕ୖ୪୼஡௫ఄ౞಄ದ೘ൡඅ櫬Āou୛୤ngruent;扢pCap;扭oubleVerticalBar;戦ƀlqxஃஊ஛ement;戉ualĀ;Tஒஓ扠ilde;쀀≂̸ists;戄reater΀;EFGLSTஶஷ஽௉௓௘௥扯qual;扱ullEqual;쀀≧̸reater;쀀≫̸ess;批lantEqual;쀀⩾̸ilde;扵umpń௲௽ownHump;쀀≎̸qual;쀀≏̸eĀfsఊధtTriangleƀ;BEచఛడ拪ar;쀀⧏̸qual;括s̀;EGLSTవశ఼ౄోౘ扮qual;扰reater;扸ess;쀀≪̸lantEqual;쀀⩽̸ilde;扴estedĀGL౨౹reaterGreater;쀀⪢̸essLess;쀀⪡̸recedesƀ;ESಒಓಛ技qual;쀀⪯̸lantEqual;拠ĀeiಫಹverseElement;戌ghtTriangleƀ;BEೋೌ೒拫ar;쀀⧐̸qual;拭ĀquೝഌuareSuĀbp೨೹setĀ;E೰ೳ쀀⊏̸qual;拢ersetĀ;Eഃആ쀀⊐̸qual;拣ƀbcpഓതൎsetĀ;Eഛഞ쀀⊂⃒qual;抈ceedsȀ;ESTലള഻െ抁qual;쀀⪰̸lantEqual;拡ilde;쀀≿̸ersetĀ;E൘൛쀀⊃⃒qual;抉ildeȀ;EFT൮൯൵ൿ扁qual;扄ullEqual;扇ilde;扉erticalBar;戤cr;쀀𝒩ilde耻Ñ䃑;䎝܀Eacdfgmoprstuvලෂ෉෕ෛ෠෧෼ขภยา฿ไlig;䅒cute耻Ó䃓Āiy෎ීrc耻Ô䃔;䐞blac;䅐r;쀀𝔒rave耻Ò䃒ƀaei෮ෲ෶cr;䅌ga;䎩cron;䎟pf;쀀𝕆enCurlyĀDQฎบoubleQuote;怜uote;怘;橔Āclวฬr;쀀𝒪ash耻Ø䃘iŬื฼de耻Õ䃕es;樷ml耻Ö䃖erĀBP๋๠Āar๐๓r;怾acĀek๚๜;揞et;掴arenthesis;揜Ҁacfhilors๿ງຊຏຒດຝະ໼rtialD;戂y;䐟r;쀀𝔓i;䎦;䎠usMinus;䂱Āipຢອncareplanåڝf;愙Ȁ;eio຺ູ໠໤檻cedesȀ;EST່້໏໚扺qual;檯lantEqual;扼ilde;找me;怳Ādp໩໮uct;戏ortionĀ;aȥ໹l;戝Āci༁༆r;쀀𝒫;䎨ȀUfos༑༖༛༟OT耻"䀢r;쀀𝔔pf;愚cr;쀀𝒬؀BEacefhiorsu༾གྷཇའཱིྦྷྪྭ႖ႩႴႾarr;椐G耻®䂮ƀcnrཎནབute;䅔g;柫rĀ;tཛྷཝ憠l;椖ƀaeyཧཬཱron;䅘dil;䅖;䐠Ā;vླྀཹ愜erseĀEUྂྙĀlq྇ྎement;戋uilibrium;懋pEquilibrium;楯r»ཹo;䎡ghtЀACDFTUVa࿁࿫࿳ဢဨၛႇϘĀnr࿆࿒gleBracket;柩rowƀ;BL࿜࿝࿡憒ar;懥eftArrow;懄eiling;按oǵ࿹\0စbleBracket;柧nǔည\0နeeVector;楝ectorĀ;Bဝသ懂ar;楕loor;挋Āerိ၃eƀ;AVဵံြ抢rrow;憦ector;楛iangleƀ;BEၐၑၕ抳ar;槐qual;抵pƀDTVၣၮၸownVector;楏eeVector;楜ectorĀ;Bႂႃ憾ar;楔ectorĀ;B႑႒懀ar;楓Āpuႛ႞f;愝ndImplies;楰ightarrow;懛ĀchႹႼr;愛;憱leDelayed;槴ڀHOacfhimoqstuფჱჷჽᄙᄞᅑᅖᅡᅧᆵᆻᆿĀCcჩხHcy;䐩y;䐨FTcy;䐬cute;䅚ʀ;aeiyᄈᄉᄎᄓᄗ檼ron;䅠dil;䅞rc;䅜;䐡r;쀀𝔖ortȀDLRUᄪᄴᄾᅉownArrow»ОeftArrow»࢚ightArrow»࿝pArrow;憑gma;䎣allCircle;战pf;쀀𝕊ɲᅭ\0\0ᅰt;戚areȀ;ISUᅻᅼᆉᆯ斡ntersection;抓uĀbpᆏᆞsetĀ;Eᆗᆘ抏qual;抑ersetĀ;Eᆨᆩ抐qual;抒nion;抔cr;쀀𝒮ar;拆ȀbcmpᇈᇛሉላĀ;sᇍᇎ拐etĀ;Eᇍᇕqual;抆ĀchᇠህeedsȀ;ESTᇭᇮᇴᇿ扻qual;檰lantEqual;扽ilde;承Tháྌ;我ƀ;esሒሓሣ拑rsetĀ;Eሜም抃qual;抇et»ሓրHRSacfhiorsሾቄ቉ቕ቞ቱቶኟዂወዑORN耻Þ䃞ADE;愢ĀHc቎ቒcy;䐋y;䐦Ābuቚቜ;䀉;䎤ƀaeyብቪቯron;䅤dil;䅢;䐢r;쀀𝔗Āeiቻ኉ǲኀ\0ኇefore;戴a;䎘Ācn኎ኘkSpace;쀀  Space;怉ldeȀ;EFTካኬኲኼ戼qual;扃ullEqual;扅ilde;扈pf;쀀𝕋ipleDot;惛Āctዖዛr;쀀𝒯rok;䅦ૡዷጎጚጦ\0ጬጱ\0\0\0\0\0ጸጽ፷ᎅ\0᏿ᐄᐊᐐĀcrዻጁute耻Ú䃚rĀ;oጇገ憟cir;楉rǣጓ\0጖y;䐎ve;䅬Āiyጞጣrc耻Û䃛;䐣blac;䅰r;쀀𝔘rave耻Ù䃙acr;䅪Ādiፁ፩erĀBPፈ፝Āarፍፐr;䁟acĀekፗፙ;揟et;掵arenthesis;揝onĀ;P፰፱拃lus;抎Āgp፻፿on;䅲f;쀀𝕌ЀADETadps᎕ᎮᎸᏄϨᏒᏗᏳrrowƀ;BDᅐᎠᎤar;椒ownArrow;懅ownArrow;憕quilibrium;楮eeĀ;AᏋᏌ报rrow;憥ownáϳerĀLRᏞᏨeftArrow;憖ightArrow;憗iĀ;lᏹᏺ䏒on;䎥ing;䅮cr;쀀𝒰ilde;䅨ml耻Ü䃜ҀDbcdefosvᐧᐬᐰᐳᐾᒅᒊᒐᒖash;披ar;櫫y;䐒ashĀ;lᐻᐼ抩;櫦Āerᑃᑅ;拁ƀbtyᑌᑐᑺar;怖Ā;iᑏᑕcalȀBLSTᑡᑥᑪᑴar;戣ine;䁼eparator;杘ilde;所ThinSpace;怊r;쀀𝔙pf;쀀𝕍cr;쀀𝒱dash;抪ʀcefosᒧᒬᒱᒶᒼirc;䅴dge;拀r;쀀𝔚pf;쀀𝕎cr;쀀𝒲Ȁfiosᓋᓐᓒᓘr;쀀𝔛;䎞pf;쀀𝕏cr;쀀𝒳ҀAIUacfosuᓱᓵᓹᓽᔄᔏᔔᔚᔠcy;䐯cy;䐇cy;䐮cute耻Ý䃝Āiyᔉᔍrc;䅶;䐫r;쀀𝔜pf;쀀𝕐cr;쀀𝒴ml;䅸ЀHacdefosᔵᔹᔿᕋᕏᕝᕠᕤcy;䐖cute;䅹Āayᕄᕉron;䅽;䐗ot;䅻ǲᕔ\0ᕛoWidtè૙a;䎖r;愨pf;愤cr;쀀𝒵௡ᖃᖊᖐ\0ᖰᖶᖿ\0\0\0\0ᗆᗛᗫᙟ᙭\0ᚕ᚛ᚲᚹ\0ᚾcute耻á䃡reve;䄃̀;Ediuyᖜᖝᖡᖣᖨᖭ戾;쀀∾̳;房rc耻â䃢te肻´̆;䐰lig耻æ䃦Ā;r²ᖺ;쀀𝔞rave耻à䃠ĀepᗊᗖĀfpᗏᗔsym;愵èᗓha;䎱ĀapᗟcĀclᗤᗧr;䄁g;樿ɤᗰ\0\0ᘊʀ;adsvᗺᗻᗿᘁᘇ戧nd;橕;橜lope;橘;橚΀;elmrszᘘᘙᘛᘞᘿᙏᙙ戠;榤e»ᘙsdĀ;aᘥᘦ戡ѡᘰᘲᘴᘶᘸᘺᘼᘾ;榨;榩;榪;榫;榬;榭;榮;榯tĀ;vᙅᙆ戟bĀ;dᙌᙍ抾;榝Āptᙔᙗh;戢»¹arr;捼Āgpᙣᙧon;䄅f;쀀𝕒΀;Eaeiop዁ᙻᙽᚂᚄᚇᚊ;橰cir;橯;扊d;手s;䀧roxĀ;e዁ᚒñᚃing耻å䃥ƀctyᚡᚦᚨr;쀀𝒶;䀪mpĀ;e዁ᚯñʈilde耻ã䃣ml耻ä䃤Āciᛂᛈoninôɲnt;樑ࠀNabcdefiklnoprsu᛭ᛱᜰ᜼ᝃᝈ᝸᝽០៦ᠹᡐᜍ᤽᥈ᥰot;櫭Ācrᛶ᜞kȀcepsᜀᜅᜍᜓong;扌psilon;䏶rime;怵imĀ;e᜚᜛戽q;拍Ŷᜢᜦee;抽edĀ;gᜬᜭ挅e»ᜭrkĀ;t፜᜷brk;掶Āoyᜁᝁ;䐱quo;怞ʀcmprtᝓ᝛ᝡᝤᝨausĀ;eĊĉptyv;榰séᜌnoõēƀahwᝯ᝱ᝳ;䎲;愶een;扬r;쀀𝔟g΀costuvwឍឝឳេ៕៛៞ƀaiuបពរðݠrc;旯p»፱ƀdptឤឨឭot;樀lus;樁imes;樂ɱឹ\0\0ើcup;樆ar;昅riangleĀdu៍្own;施p;斳plus;樄eåᑄåᒭarow;植ƀako៭ᠦᠵĀcn៲ᠣkƀlst៺֫᠂ozenge;槫riangleȀ;dlr᠒᠓᠘᠝斴own;斾eft;旂ight;斸k;搣Ʊᠫ\0ᠳƲᠯ\0ᠱ;斒;斑4;斓ck;斈ĀeoᠾᡍĀ;qᡃᡆ쀀=⃥uiv;쀀≡⃥t;挐Ȁptwxᡙᡞᡧᡬf;쀀𝕓Ā;tᏋᡣom»Ꮜtie;拈؀DHUVbdhmptuvᢅᢖᢪᢻᣗᣛᣬ᣿ᤅᤊᤐᤡȀLRlrᢎᢐᢒᢔ;敗;敔;敖;敓ʀ;DUduᢡᢢᢤᢦᢨ敐;敦;敩;敤;敧ȀLRlrᢳᢵᢷᢹ;敝;敚;敜;教΀;HLRhlrᣊᣋᣍᣏᣑᣓᣕ救;敬;散;敠;敫;敢;敟ox;槉ȀLRlrᣤᣦᣨᣪ;敕;敒;攐;攌ʀ;DUduڽ᣷᣹᣻᣽;敥;敨;攬;攴inus;抟lus;択imes;抠ȀLRlrᤙᤛᤝ᤟;敛;敘;攘;攔΀;HLRhlrᤰᤱᤳᤵᤷ᤻᤹攂;敪;敡;敞;攼;攤;攜Āevģ᥂bar耻¦䂦Ȁceioᥑᥖᥚᥠr;쀀𝒷mi;恏mĀ;e᜚᜜lƀ;bhᥨᥩᥫ䁜;槅sub;柈Ŭᥴ᥾lĀ;e᥹᥺怢t»᥺pƀ;Eeįᦅᦇ;檮Ā;qۜۛೡᦧ\0᧨ᨑᨕᨲ\0ᨷᩐ\0\0᪴\0\0᫁\0\0ᬡᬮ᭍᭒\0᯽\0ᰌƀcpr᦭ᦲ᧝ute;䄇̀;abcdsᦿᧀᧄ᧊᧕᧙戩nd;橄rcup;橉Āau᧏᧒p;橋p;橇ot;橀;쀀∩︀Āeo᧢᧥t;恁îړȀaeiu᧰᧻ᨁᨅǰ᧵\0᧸s;橍on;䄍dil耻ç䃧rc;䄉psĀ;sᨌᨍ橌m;橐ot;䄋ƀdmnᨛᨠᨦil肻¸ƭptyv;榲t脀¢;eᨭᨮ䂢räƲr;쀀𝔠ƀceiᨽᩀᩍy;䑇ckĀ;mᩇᩈ朓ark»ᩈ;䏇r΀;Ecefms᩟᩠ᩢᩫ᪤᪪᪮旋;槃ƀ;elᩩᩪᩭ䋆q;扗eɡᩴ\0\0᪈rrowĀlr᩼᪁eft;憺ight;憻ʀRSacd᪒᪔᪖᪚᪟»ཇ;擈st;抛irc;抚ash;抝nint;樐id;櫯cir;槂ubsĀ;u᪻᪼晣it»᪼ˬ᫇᫔᫺\0ᬊonĀ;eᫍᫎ䀺Ā;qÇÆɭ᫙\0\0᫢aĀ;t᫞᫟䀬;䁀ƀ;fl᫨᫩᫫戁îᅠeĀmx᫱᫶ent»᫩eóɍǧ᫾\0ᬇĀ;dኻᬂot;橭nôɆƀfryᬐᬔᬗ;쀀𝕔oäɔ脀©;sŕᬝr;愗Āaoᬥᬩrr;憵ss;朗Ācuᬲᬷr;쀀𝒸Ābpᬼ᭄Ā;eᭁᭂ櫏;櫑Ā;eᭉᭊ櫐;櫒dot;拯΀delprvw᭠᭬᭷ᮂᮬᯔ᯹arrĀlr᭨᭪;椸;椵ɰ᭲\0\0᭵r;拞c;拟arrĀ;p᭿ᮀ憶;椽̀;bcdosᮏᮐᮖᮡᮥᮨ截rcap;橈Āauᮛᮞp;橆p;橊ot;抍r;橅;쀀∪︀Ȁalrv᮵ᮿᯞᯣrrĀ;mᮼᮽ憷;椼yƀevwᯇᯔᯘqɰᯎ\0\0ᯒreã᭳uã᭵ee;拎edge;拏en耻¤䂤earrowĀlrᯮ᯳eft»ᮀight»ᮽeäᯝĀciᰁᰇoninôǷnt;戱lcty;挭ঀAHabcdefhijlorstuwz᰸᰻᰿ᱝᱩᱵᲊᲞᲬᲷ᳻᳿ᴍᵻᶑᶫᶻ᷆᷍rò΁ar;楥Ȁglrs᱈ᱍ᱒᱔ger;怠eth;愸òᄳhĀ;vᱚᱛ怐»ऊūᱡᱧarow;椏aã̕Āayᱮᱳron;䄏;䐴ƀ;ao̲ᱼᲄĀgrʿᲁr;懊tseq;橷ƀglmᲑᲔᲘ耻°䂰ta;䎴ptyv;榱ĀirᲣᲨsht;楿;쀀𝔡arĀlrᲳᲵ»ࣜ»သʀaegsv᳂͸᳖᳜᳠mƀ;oș᳊᳔ndĀ;ș᳑uit;晦amma;䏝in;拲ƀ;io᳧᳨᳸䃷de脀÷;o᳧ᳰntimes;拇nø᳷cy;䑒cɯᴆ\0\0ᴊrn;挞op;挍ʀlptuwᴘᴝᴢᵉᵕlar;䀤f;쀀𝕕ʀ;emps̋ᴭᴷᴽᵂqĀ;d͒ᴳot;扑inus;戸lus;戔quare;抡blebarwedgåúnƀadhᄮᵝᵧownarrowóᲃarpoonĀlrᵲᵶefôᲴighôᲶŢᵿᶅkaro÷གɯᶊ\0\0ᶎrn;挟op;挌ƀcotᶘᶣᶦĀryᶝᶡ;쀀𝒹;䑕l;槶rok;䄑Ādrᶰᶴot;拱iĀ;fᶺ᠖斿Āah᷀᷃ròЩaòྦangle;榦Āci᷒ᷕy;䑟grarr;柿ऀDacdefglmnopqrstuxḁḉḙḸոḼṉṡṾấắẽỡἪἷὄ὎὚ĀDoḆᴴoôᲉĀcsḎḔute耻é䃩ter;橮ȀaioyḢḧḱḶron;䄛rĀ;cḭḮ扖耻ê䃪lon;払;䑍ot;䄗ĀDrṁṅot;扒;쀀𝔢ƀ;rsṐṑṗ檚ave耻è䃨Ā;dṜṝ檖ot;檘Ȁ;ilsṪṫṲṴ檙nters;揧;愓Ā;dṹṺ檕ot;檗ƀapsẅẉẗcr;䄓tyƀ;svẒẓẕ戅et»ẓpĀ1;ẝẤĳạả;怄;怅怃ĀgsẪẬ;䅋p;怂ĀgpẴẸon;䄙f;쀀𝕖ƀalsỄỎỒrĀ;sỊị拕l;槣us;橱iƀ;lvỚớở䎵on»ớ;䏵ȀcsuvỪỳἋἣĀioữḱrc»Ḯɩỹ\0\0ỻíՈantĀglἂἆtr»ṝess»Ṻƀaeiἒ἖Ἒls;䀽st;扟vĀ;DȵἠD;橸parsl;槥ĀDaἯἳot;打rr;楱ƀcdiἾὁỸr;愯oô͒ĀahὉὋ;䎷耻ð䃰Āmrὓὗl耻ë䃫o;悬ƀcipὡὤὧl;䀡sôծĀeoὬὴctatioîՙnentialåչৡᾒ\0ᾞ\0ᾡᾧ\0\0ῆῌ\0ΐ\0ῦῪ \0 ⁚llingdotseñṄy;䑄male;晀ƀilrᾭᾳ῁lig;耀ﬃɩᾹ\0\0᾽g;耀ﬀig;耀ﬄ;쀀𝔣lig;耀ﬁlig;쀀fjƀaltῙ῜ῡt;晭ig;耀ﬂns;斱of;䆒ǰ΅\0ῳf;쀀𝕗ĀakֿῷĀ;vῼ´拔;櫙artint;樍Āao‌⁕Ācs‑⁒α‚‰‸⁅⁈\0⁐β•‥‧‪‬\0‮耻½䂽;慓耻¼䂼;慕;慙;慛Ƴ‴\0‶;慔;慖ʴ‾⁁\0\0⁃耻¾䂾;慗;慜5;慘ƶ⁌\0⁎;慚;慝8;慞l;恄wn;挢cr;쀀𝒻ࢀEabcdefgijlnorstv₂₉₟₥₰₴⃰⃵⃺⃿℃ℒℸ̗ℾ⅒↞Ā;lٍ₇;檌ƀcmpₐₕ₝ute;䇵maĀ;dₜ᳚䎳;檆reve;䄟Āiy₪₮rc;䄝;䐳ot;䄡Ȁ;lqsؾق₽⃉ƀ;qsؾٌ⃄lanô٥Ȁ;cdl٥⃒⃥⃕c;檩otĀ;o⃜⃝檀Ā;l⃢⃣檂;檄Ā;e⃪⃭쀀⋛︀s;檔r;쀀𝔤Ā;gٳ؛mel;愷cy;䑓Ȁ;Eajٚℌℎℐ;檒;檥;檤ȀEaesℛℝ℩ℴ;扩pĀ;p℣ℤ檊rox»ℤĀ;q℮ℯ檈Ā;q℮ℛim;拧pf;쀀𝕘Āci⅃ⅆr;愊mƀ;el٫ⅎ⅐;檎;檐茀>;cdlqr׮ⅠⅪⅮⅳⅹĀciⅥⅧ;檧r;橺ot;拗Par;榕uest;橼ʀadelsↄⅪ←ٖ↛ǰ↉\0↎proø₞r;楸qĀlqؿ↖lesó₈ií٫Āen↣↭rtneqq;쀀≩︀Å↪ԀAabcefkosy⇄⇇⇱⇵⇺∘∝∯≨≽ròΠȀilmr⇐⇔⇗⇛rsðᒄf»․ilôکĀdr⇠⇤cy;䑊ƀ;cwࣴ⇫⇯ir;楈;憭ar;意irc;䄥ƀalr∁∎∓rtsĀ;u∉∊晥it»∊lip;怦con;抹r;쀀𝔥sĀew∣∩arow;椥arow;椦ʀamopr∺∾≃≞≣rr;懿tht;戻kĀlr≉≓eftarrow;憩ightarrow;憪f;쀀𝕙bar;怕ƀclt≯≴≸r;쀀𝒽asè⇴rok;䄧Ābp⊂⊇ull;恃hen»ᱛૡ⊣\0⊪\0⊸⋅⋎\0⋕⋳\0\0⋸⌢⍧⍢⍿\0⎆⎪⎴cute耻í䃭ƀ;iyݱ⊰⊵rc耻î䃮;䐸Ācx⊼⊿y;䐵cl耻¡䂡ĀfrΟ⋉;쀀𝔦rave耻ì䃬Ȁ;inoܾ⋝⋩⋮Āin⋢⋦nt;樌t;戭fin;槜ta;愩lig;䄳ƀaop⋾⌚⌝ƀcgt⌅⌈⌗r;䄫ƀelpܟ⌏⌓inåގarôܠh;䄱f;抷ed;䆵ʀ;cfotӴ⌬⌱⌽⍁are;愅inĀ;t⌸⌹戞ie;槝doô⌙ʀ;celpݗ⍌⍐⍛⍡al;抺Āgr⍕⍙eróᕣã⍍arhk;樗rod;樼Ȁcgpt⍯⍲⍶⍻y;䑑on;䄯f;쀀𝕚a;䎹uest耻¿䂿Āci⎊⎏r;쀀𝒾nʀ;EdsvӴ⎛⎝⎡ӳ;拹ot;拵Ā;v⎦⎧拴;拳Ā;iݷ⎮lde;䄩ǫ⎸\0⎼cy;䑖l耻ï䃯̀cfmosu⏌⏗⏜⏡⏧⏵Āiy⏑⏕rc;䄵;䐹r;쀀𝔧ath;䈷pf;쀀𝕛ǣ⏬\0⏱r;쀀𝒿rcy;䑘kcy;䑔Ѐacfghjos␋␖␢␧␭␱␵␻ppaĀ;v␓␔䎺;䏰Āey␛␠dil;䄷;䐺r;쀀𝔨reen;䄸cy;䑅cy;䑜pf;쀀𝕜cr;쀀𝓀஀ABEHabcdefghjlmnoprstuv⑰⒁⒆⒍⒑┎┽╚▀♎♞♥♹♽⚚⚲⛘❝❨➋⟀⠁⠒ƀart⑷⑺⑼rò৆òΕail;椛arr;椎Ā;gঔ⒋;檋ar;楢ॣ⒥\0⒪\0⒱\0\0\0\0\0⒵Ⓔ\0ⓆⓈⓍ\0⓹ute;䄺mptyv;榴raîࡌbda;䎻gƀ;dlࢎⓁⓃ;榑åࢎ;檅uo耻«䂫rЀ;bfhlpst࢙ⓞⓦⓩ⓫⓮⓱⓵Ā;f࢝ⓣs;椟s;椝ë≒p;憫l;椹im;楳l;憢ƀ;ae⓿─┄檫il;椙Ā;s┉┊檭;쀀⪭︀ƀabr┕┙┝rr;椌rk;杲Āak┢┬cĀek┨┪;䁻;䁛Āes┱┳;榋lĀdu┹┻;榏;榍Ȁaeuy╆╋╖╘ron;䄾Ādi═╔il;䄼ìࢰâ┩;䐻Ȁcqrs╣╦╭╽a;椶uoĀ;rนᝆĀdu╲╷har;楧shar;楋h;憲ʀ;fgqs▋▌উ◳◿扤tʀahlrt▘▤▷◂◨rrowĀ;t࢙□aé⓶arpoonĀdu▯▴own»њp»०eftarrows;懇ightƀahs◍◖◞rrowĀ;sࣴࢧarpoonó྘quigarro÷⇰hreetimes;拋ƀ;qs▋ও◺lanôবʀ;cdgsব☊☍☝☨c;檨otĀ;o☔☕橿Ā;r☚☛檁;檃Ā;e☢☥쀀⋚︀s;檓ʀadegs☳☹☽♉♋pproøⓆot;拖qĀgq♃♅ôউgtò⒌ôছiíলƀilr♕࣡♚sht;楼;쀀𝔩Ā;Eজ♣;檑š♩♶rĀdu▲♮Ā;l॥♳;楪lk;斄cy;䑙ʀ;achtੈ⚈⚋⚑⚖rò◁orneòᴈard;楫ri;旺Āio⚟⚤dot;䅀ustĀ;a⚬⚭掰che»⚭ȀEaes⚻⚽⛉⛔;扨pĀ;p⛃⛄檉rox»⛄Ā;q⛎⛏檇Ā;q⛎⚻im;拦Ѐabnoptwz⛩⛴⛷✚✯❁❇❐Ānr⛮⛱g;柬r;懽rëࣁgƀlmr⛿✍✔eftĀar০✇ightá৲apsto;柼ightá৽parrowĀlr✥✩efô⓭ight;憬ƀafl✶✹✽r;榅;쀀𝕝us;樭imes;樴š❋❏st;戗áፎƀ;ef❗❘᠀旊nge»❘arĀ;l❤❥䀨t;榓ʀachmt❳❶❼➅➇ròࢨorneòᶌarĀ;d྘➃;業;怎ri;抿̀achiqt➘➝ੀ➢➮➻quo;怹r;쀀𝓁mƀ;egল➪➬;檍;檏Ābu┪➳oĀ;rฟ➹;怚rok;䅂萀<;cdhilqrࠫ⟒☹⟜⟠⟥⟪⟰Āci⟗⟙;檦r;橹reå◲mes;拉arr;楶uest;橻ĀPi⟵⟹ar;榖ƀ;ef⠀भ᠛旃rĀdu⠇⠍shar;楊har;楦Āen⠗⠡rtneqq;쀀≨︀Å⠞܀Dacdefhilnopsu⡀⡅⢂⢎⢓⢠⢥⢨⣚⣢⣤ઃ⣳⤂Dot;戺Ȁclpr⡎⡒⡣⡽r耻¯䂯Āet⡗⡙;時Ā;e⡞⡟朠se»⡟Ā;sျ⡨toȀ;dluျ⡳⡷⡻owîҌefôएðᏑker;斮Āoy⢇⢌mma;権;䐼ash;怔asuredangle»ᘦr;쀀𝔪o;愧ƀcdn⢯⢴⣉ro耻µ䂵Ȁ;acdᑤ⢽⣀⣄sôᚧir;櫰ot肻·Ƶusƀ;bd⣒ᤃ⣓戒Ā;uᴼ⣘;横ţ⣞⣡p;櫛ò−ðઁĀdp⣩⣮els;抧f;쀀𝕞Āct⣸⣽r;쀀𝓂pos»ᖝƀ;lm⤉⤊⤍䎼timap;抸ఀGLRVabcdefghijlmoprstuvw⥂⥓⥾⦉⦘⧚⧩⨕⨚⩘⩝⪃⪕⪤⪨⬄⬇⭄⭿⮮ⰴⱧⱼ⳩Āgt⥇⥋;쀀⋙̸Ā;v⥐௏쀀≫⃒ƀelt⥚⥲⥶ftĀar⥡⥧rrow;懍ightarrow;懎;쀀⋘̸Ā;v⥻ే쀀≪⃒ightarrow;懏ĀDd⦎⦓ash;抯ash;抮ʀbcnpt⦣⦧⦬⦱⧌la»˞ute;䅄g;쀀∠⃒ʀ;Eiop඄⦼⧀⧅⧈;쀀⩰̸d;쀀≋̸s;䅉roø඄urĀ;a⧓⧔普lĀ;s⧓ସǳ⧟\0⧣p肻 ଷmpĀ;e௹ఀʀaeouy⧴⧾⨃⨐⨓ǰ⧹\0⧻;橃on;䅈dil;䅆ngĀ;dൾ⨊ot;쀀⩭̸p;橂;䐽ash;怓΀;Aadqsxஒ⨩⨭⨻⩁⩅⩐rr;懗rĀhr⨳⨶k;椤Ā;oᏲᏰot;쀀≐̸uiöୣĀei⩊⩎ar;椨í஘istĀ;s஠டr;쀀𝔫ȀEest௅⩦⩹⩼ƀ;qs஼⩭௡ƀ;qs஼௅⩴lanô௢ií௪Ā;rஶ⪁»ஷƀAap⪊⪍⪑rò⥱rr;憮ar;櫲ƀ;svྍ⪜ྌĀ;d⪡⪢拼;拺cy;䑚΀AEadest⪷⪺⪾⫂⫅⫶⫹rò⥦;쀀≦̸rr;憚r;急Ȁ;fqs఻⫎⫣⫯tĀar⫔⫙rro÷⫁ightarro÷⪐ƀ;qs఻⪺⫪lanôౕĀ;sౕ⫴»శiíౝĀ;rవ⫾iĀ;eచథiäඐĀpt⬌⬑f;쀀𝕟膀¬;in⬙⬚⬶䂬nȀ;Edvஉ⬤⬨⬮;쀀⋹̸ot;쀀⋵̸ǡஉ⬳⬵;拷;拶iĀ;vಸ⬼ǡಸ⭁⭃;拾;拽ƀaor⭋⭣⭩rȀ;ast୻⭕⭚⭟lleì୻l;쀀⫽⃥;쀀∂̸lint;樔ƀ;ceಒ⭰⭳uåಥĀ;cಘ⭸Ā;eಒ⭽ñಘȀAait⮈⮋⮝⮧rò⦈rrƀ;cw⮔⮕⮙憛;쀀⤳̸;쀀↝̸ghtarrow»⮕riĀ;eೋೖ΀chimpqu⮽⯍⯙⬄୸⯤⯯Ȁ;cerല⯆ഷ⯉uå൅;쀀𝓃ortɭ⬅\0\0⯖ará⭖mĀ;e൮⯟Ā;q൴൳suĀbp⯫⯭å೸åഋƀbcp⯶ⰑⰙȀ;Ees⯿ⰀഢⰄ抄;쀀⫅̸etĀ;eഛⰋqĀ;qണⰀcĀ;eലⰗñസȀ;EesⰢⰣൟⰧ抅;쀀⫆̸etĀ;e൘ⰮqĀ;qൠⰣȀgilrⰽⰿⱅⱇìௗlde耻ñ䃱çృiangleĀlrⱒⱜeftĀ;eచⱚñదightĀ;eೋⱥñ೗Ā;mⱬⱭ䎽ƀ;esⱴⱵⱹ䀣ro;愖p;怇ҀDHadgilrsⲏⲔⲙⲞⲣⲰⲶⳓⳣash;抭arr;椄p;쀀≍⃒ash;抬ĀetⲨⲬ;쀀≥⃒;쀀>⃒nfin;槞ƀAetⲽⳁⳅrr;椂;쀀≤⃒Ā;rⳊⳍ쀀<⃒ie;쀀⊴⃒ĀAtⳘⳜrr;椃rie;쀀⊵⃒im;쀀∼⃒ƀAan⳰⳴ⴂrr;懖rĀhr⳺⳽k;椣Ā;oᏧᏥear;椧ቓ᪕\0\0\0\0\0\0\0\0\0\0\0\0\0ⴭ\0ⴸⵈⵠⵥ⵲ⶄᬇ\0\0ⶍⶫ\0ⷈⷎ\0ⷜ⸙⸫⸾⹃Ācsⴱ᪗ute耻ó䃳ĀiyⴼⵅrĀ;c᪞ⵂ耻ô䃴;䐾ʀabios᪠ⵒⵗǈⵚlac;䅑v;樸old;榼lig;䅓Ācr⵩⵭ir;榿;쀀𝔬ͯ⵹\0\0⵼\0ⶂn;䋛ave耻ò䃲;槁Ābmⶈ෴ar;榵Ȁacitⶕ⶘ⶥⶨrò᪀Āir⶝ⶠr;榾oss;榻nå๒;槀ƀaeiⶱⶵⶹcr;䅍ga;䏉ƀcdnⷀⷅǍron;䎿;榶pf;쀀𝕠ƀaelⷔ⷗ǒr;榷rp;榹΀;adiosvⷪⷫⷮ⸈⸍⸐⸖戨rò᪆Ȁ;efmⷷⷸ⸂⸅橝rĀ;oⷾⷿ愴f»ⷿ耻ª䂪耻º䂺gof;抶r;橖lope;橗;橛ƀclo⸟⸡⸧ò⸁ash耻ø䃸l;折iŬⸯ⸴de耻õ䃵esĀ;aǛ⸺s;樶ml耻ö䃶bar;挽ૡ⹞\0⹽\0⺀⺝\0⺢⺹\0\0⻋ຜ\0⼓\0\0⼫⾼\0⿈rȀ;astЃ⹧⹲຅脀¶;l⹭⹮䂶leìЃɩ⹸\0\0⹻m;櫳;櫽y;䐿rʀcimpt⺋⺏⺓ᡥ⺗nt;䀥od;䀮il;怰enk;怱r;쀀𝔭ƀimo⺨⺰⺴Ā;v⺭⺮䏆;䏕maô੶ne;明ƀ;tv⺿⻀⻈䏀chfork»´;䏖Āau⻏⻟nĀck⻕⻝kĀ;h⇴⻛;愎ö⇴sҀ;abcdemst⻳⻴ᤈ⻹⻽⼄⼆⼊⼎䀫cir;樣ir;樢Āouᵀ⼂;樥;橲n肻±ຝim;樦wo;樧ƀipu⼙⼠⼥ntint;樕f;쀀𝕡nd耻£䂣Ԁ;Eaceinosu່⼿⽁⽄⽇⾁⾉⾒⽾⾶;檳p;檷uå໙Ā;c໎⽌̀;acens່⽙⽟⽦⽨⽾pproø⽃urlyeñ໙ñ໎ƀaes⽯⽶⽺pprox;檹qq;檵im;拨iíໟmeĀ;s⾈ຮ怲ƀEas⽸⾐⽺ð⽵ƀdfp໬⾙⾯ƀals⾠⾥⾪lar;挮ine;挒urf;挓Ā;t໻⾴ï໻rel;抰Āci⿀⿅r;쀀𝓅;䏈ncsp;怈̀fiopsu⿚⋢⿟⿥⿫⿱r;쀀𝔮pf;쀀𝕢rime;恗cr;쀀𝓆ƀaeo⿸〉〓tĀei⿾々rnionóڰnt;樖stĀ;e【】䀿ñἙô༔઀ABHabcdefhilmnoprstux぀けさすムㄎㄫㅇㅢㅲㆎ㈆㈕㈤㈩㉘㉮㉲㊐㊰㊷ƀartぇおがròႳòϝail;検aròᱥar;楤΀cdenqrtとふへみわゔヌĀeuねぱ;쀀∽̱te;䅕iãᅮmptyv;榳gȀ;del࿑らるろ;榒;榥å࿑uo耻»䂻rր;abcfhlpstw࿜ガクシスゼゾダッデナp;極Ā;f࿠ゴs;椠;椳s;椞ë≝ð✮l;楅im;楴l;憣;憝Āaiパフil;椚oĀ;nホボ戶aló༞ƀabrョリヮrò៥rk;杳ĀakンヽcĀekヹ・;䁽;䁝Āes㄂㄄;榌lĀduㄊㄌ;榎;榐Ȁaeuyㄗㄜㄧㄩron;䅙Ādiㄡㄥil;䅗ì࿲âヺ;䑀Ȁclqsㄴㄷㄽㅄa;椷dhar;楩uoĀ;rȎȍh;憳ƀacgㅎㅟངlȀ;ipsླྀㅘㅛႜnåႻarôྩt;断ƀilrㅩဣㅮsht;楽;쀀𝔯ĀaoㅷㆆrĀduㅽㅿ»ѻĀ;l႑ㆄ;楬Ā;vㆋㆌ䏁;䏱ƀgns㆕ㇹㇼht̀ahlrstㆤㆰ㇂㇘㇤㇮rrowĀ;t࿜ㆭaéトarpoonĀduㆻㆿowîㅾp»႒eftĀah㇊㇐rrowó࿪arpoonóՑightarrows;應quigarro÷ニhreetimes;拌g;䋚ingdotseñἲƀahm㈍㈐㈓rò࿪aòՑ;怏oustĀ;a㈞㈟掱che»㈟mid;櫮Ȁabpt㈲㈽㉀㉒Ānr㈷㈺g;柭r;懾rëဃƀafl㉇㉊㉎r;榆;쀀𝕣us;樮imes;樵Āap㉝㉧rĀ;g㉣㉤䀩t;榔olint;樒arò㇣Ȁachq㉻㊀Ⴜ㊅quo;怺r;쀀𝓇Ābu・㊊oĀ;rȔȓƀhir㊗㊛㊠reåㇸmes;拊iȀ;efl㊪ၙᠡ㊫方tri;槎luhar;楨;愞ൡ㋕㋛㋟㌬㌸㍱\0㍺㎤\0\0㏬㏰\0㐨㑈㑚㒭㒱㓊㓱\0㘖\0\0㘳cute;䅛quï➺Ԁ;Eaceinpsyᇭ㋳㋵㋿㌂㌋㌏㌟㌦㌩;檴ǰ㋺\0㋼;檸on;䅡uåᇾĀ;dᇳ㌇il;䅟rc;䅝ƀEas㌖㌘㌛;檶p;檺im;择olint;樓iíሄ;䑁otƀ;be㌴ᵇ㌵担;橦΀Aacmstx㍆㍊㍗㍛㍞㍣㍭rr;懘rĀhr㍐㍒ë∨Ā;oਸ਼਴t耻§䂧i;䀻war;椩mĀin㍩ðnuóñt;朶rĀ;o㍶⁕쀀𝔰Ȁacoy㎂㎆㎑㎠rp;景Āhy㎋㎏cy;䑉;䑈rtɭ㎙\0\0㎜iäᑤaraì⹯耻­䂭Āgm㎨㎴maƀ;fv㎱㎲㎲䏃;䏂Ѐ;deglnprካ㏅㏉㏎㏖㏞㏡㏦ot;橪Ā;q኱ኰĀ;E㏓㏔檞;檠Ā;E㏛㏜檝;檟e;扆lus;樤arr;楲aròᄽȀaeit㏸㐈㐏㐗Āls㏽㐄lsetmé㍪hp;樳parsl;槤Ādlᑣ㐔e;挣Ā;e㐜㐝檪Ā;s㐢㐣檬;쀀⪬︀ƀflp㐮㐳㑂tcy;䑌Ā;b㐸㐹䀯Ā;a㐾㐿槄r;挿f;쀀𝕤aĀdr㑍ЂesĀ;u㑔㑕晠it»㑕ƀcsu㑠㑹㒟Āau㑥㑯pĀ;sᆈ㑫;쀀⊓︀pĀ;sᆴ㑵;쀀⊔︀uĀbp㑿㒏ƀ;esᆗᆜ㒆etĀ;eᆗ㒍ñᆝƀ;esᆨᆭ㒖etĀ;eᆨ㒝ñᆮƀ;afᅻ㒦ְrť㒫ֱ»ᅼaròᅈȀcemt㒹㒾㓂㓅r;쀀𝓈tmîñiì㐕aræᆾĀar㓎㓕rĀ;f㓔ឿ昆Āan㓚㓭ightĀep㓣㓪psiloîỠhé⺯s»⡒ʀbcmnp㓻㕞ሉ㖋㖎Ҁ;Edemnprs㔎㔏㔑㔕㔞㔣㔬㔱㔶抂;櫅ot;檽Ā;dᇚ㔚ot;櫃ult;櫁ĀEe㔨㔪;櫋;把lus;檿arr;楹ƀeiu㔽㕒㕕tƀ;en㔎㕅㕋qĀ;qᇚ㔏eqĀ;q㔫㔨m;櫇Ābp㕚㕜;櫕;櫓c̀;acensᇭ㕬㕲㕹㕻㌦pproø㋺urlyeñᇾñᇳƀaes㖂㖈㌛pproø㌚qñ㌗g;晪ڀ123;Edehlmnps㖩㖬㖯ሜ㖲㖴㗀㗉㗕㗚㗟㗨㗭耻¹䂹耻²䂲耻³䂳;櫆Āos㖹㖼t;檾ub;櫘Ā;dሢ㗅ot;櫄sĀou㗏㗒l;柉b;櫗arr;楻ult;櫂ĀEe㗤㗦;櫌;抋lus;櫀ƀeiu㗴㘉㘌tƀ;enሜ㗼㘂qĀ;qሢ㖲eqĀ;q㗧㗤m;櫈Ābp㘑㘓;櫔;櫖ƀAan㘜㘠㘭rr;懙rĀhr㘦㘨ë∮Ā;oਫ਩war;椪lig耻ß䃟௡㙑㙝㙠ዎ㙳㙹\0㙾㛂\0\0\0\0\0㛛㜃\0㜉㝬\0\0\0㞇ɲ㙖\0\0㙛get;挖;䏄rë๟ƀaey㙦㙫㙰ron;䅥dil;䅣;䑂lrec;挕r;쀀𝔱Ȁeiko㚆㚝㚵㚼ǲ㚋\0㚑eĀ4fኄኁaƀ;sv㚘㚙㚛䎸ym;䏑Ācn㚢㚲kĀas㚨㚮pproø዁im»ኬsðኞĀas㚺㚮ð዁rn耻þ䃾Ǭ̟㛆⋧es膀×;bd㛏㛐㛘䃗Ā;aᤏ㛕r;樱;樰ƀeps㛡㛣㜀á⩍Ȁ;bcf҆㛬㛰㛴ot;挶ir;櫱Ā;o㛹㛼쀀𝕥rk;櫚á㍢rime;怴ƀaip㜏㜒㝤dåቈ΀adempst㜡㝍㝀㝑㝗㝜㝟ngleʀ;dlqr㜰㜱㜶㝀㝂斵own»ᶻeftĀ;e⠀㜾ñम;扜ightĀ;e㊪㝋ñၚot;旬inus;樺lus;樹b;槍ime;樻ezium;揢ƀcht㝲㝽㞁Āry㝷㝻;쀀𝓉;䑆cy;䑛rok;䅧Āio㞋㞎xô᝷headĀlr㞗㞠eftarro÷ࡏightarrow»ཝऀAHabcdfghlmoprstuw㟐㟓㟗㟤㟰㟼㠎㠜㠣㠴㡑㡝㡫㢩㣌㣒㣪㣶ròϭar;楣Ācr㟜㟢ute耻ú䃺òᅐrǣ㟪\0㟭y;䑞ve;䅭Āiy㟵㟺rc耻û䃻;䑃ƀabh㠃㠆㠋ròᎭlac;䅱aòᏃĀir㠓㠘sht;楾;쀀𝔲rave耻ù䃹š㠧㠱rĀlr㠬㠮»ॗ»ႃlk;斀Āct㠹㡍ɯ㠿\0\0㡊rnĀ;e㡅㡆挜r»㡆op;挏ri;旸Āal㡖㡚cr;䅫肻¨͉Āgp㡢㡦on;䅳f;쀀𝕦̀adhlsuᅋ㡸㡽፲㢑㢠ownáᎳarpoonĀlr㢈㢌efô㠭ighô㠯iƀ;hl㢙㢚㢜䏅»ᏺon»㢚parrows;懈ƀcit㢰㣄㣈ɯ㢶\0\0㣁rnĀ;e㢼㢽挝r»㢽op;挎ng;䅯ri;旹cr;쀀𝓊ƀdir㣙㣝㣢ot;拰lde;䅩iĀ;f㜰㣨»᠓Āam㣯㣲rò㢨l耻ü䃼angle;榧ހABDacdeflnoprsz㤜㤟㤩㤭㦵㦸㦽㧟㧤㧨㧳㧹㧽㨁㨠ròϷarĀ;v㤦㤧櫨;櫩asèϡĀnr㤲㤷grt;榜΀eknprst㓣㥆㥋㥒㥝㥤㦖appá␕othinçẖƀhir㓫⻈㥙opô⾵Ā;hᎷ㥢ïㆍĀiu㥩㥭gmá㎳Ābp㥲㦄setneqĀ;q㥽㦀쀀⊊︀;쀀⫋︀setneqĀ;q㦏㦒쀀⊋︀;쀀⫌︀Āhr㦛㦟etá㚜iangleĀlr㦪㦯eft»थight»ၑy;䐲ash»ံƀelr㧄㧒㧗ƀ;beⷪ㧋㧏ar;抻q;扚lip;拮Ābt㧜ᑨaòᑩr;쀀𝔳tré㦮suĀbp㧯㧱»ജ»൙pf;쀀𝕧roð໻tré㦴Ācu㨆㨋r;쀀𝓋Ābp㨐㨘nĀEe㦀㨖»㥾nĀEe㦒㨞»㦐igzag;榚΀cefoprs㨶㨻㩖㩛㩔㩡㩪irc;䅵Ādi㩀㩑Ābg㩅㩉ar;機eĀ;qᗺ㩏;扙erp;愘r;쀀𝔴pf;쀀𝕨Ā;eᑹ㩦atèᑹcr;쀀𝓌ૣណ㪇\0㪋\0㪐㪛\0\0㪝㪨㪫㪯\0\0㫃㫎\0㫘ៜ៟tré៑r;쀀𝔵ĀAa㪔㪗ròσrò৶;䎾ĀAa㪡㪤ròθrò৫að✓is;拻ƀdptឤ㪵㪾Āfl㪺ឩ;쀀𝕩imåឲĀAa㫇㫊ròώròਁĀcq㫒ីr;쀀𝓍Āpt៖㫜ré។Ѐacefiosu㫰㫽㬈㬌㬑㬕㬛㬡cĀuy㫶㫻te耻ý䃽;䑏Āiy㬂㬆rc;䅷;䑋n耻¥䂥r;쀀𝔶cy;䑗pf;쀀𝕪cr;쀀𝓎Ācm㬦㬩y;䑎l耻ÿ䃿Ԁacdefhiosw㭂㭈㭔㭘㭤㭩㭭㭴㭺㮀cute;䅺Āay㭍㭒ron;䅾;䐷ot;䅼Āet㭝㭡træᕟa;䎶r;쀀𝔷cy;䐶grarr;懝pf;쀀𝕫cr;쀀𝓏Ājn㮅㮇;怍j;怌'.split("").map(e=>e.charCodeAt(0))),bY=new Map([[0,65533],[128,8364],[130,8218],[131,402],[132,8222],[133,8230],[134,8224],[135,8225],[136,710],[137,8240],[138,352],[139,8249],[140,338],[142,381],[145,8216],[146,8217],[147,8220],[148,8221],[149,8226],[150,8211],[151,8212],[152,732],[153,8482],[154,353],[155,8250],[156,339],[158,382],[159,376]]);function vY(e){var t;return e>=55296&&e<=57343||e>1114111?65533:(t=bY.get(e))!==null&&t!==void 0?t:e}var Ht;(function(e){e[e.NUM=35]="NUM",e[e.SEMI=59]="SEMI",e[e.EQUALS=61]="EQUALS",e[e.ZERO=48]="ZERO",e[e.NINE=57]="NINE",e[e.LOWER_A=97]="LOWER_A",e[e.LOWER_F=102]="LOWER_F",e[e.LOWER_X=120]="LOWER_X",e[e.LOWER_Z=122]="LOWER_Z",e[e.UPPER_A=65]="UPPER_A",e[e.UPPER_F=70]="UPPER_F",e[e.UPPER_Z=90]="UPPER_Z"})(Ht||(Ht={}));const xY=32;var pa;(function(e){e[e.VALUE_LENGTH=49152]="VALUE_LENGTH",e[e.BRANCH_LENGTH=16256]="BRANCH_LENGTH",e[e.JUMP_TABLE=127]="JUMP_TABLE"})(pa||(pa={}));function dg(e){return e>=Ht.ZERO&&e<=Ht.NINE}function $Y(e){return e>=Ht.UPPER_A&&e<=Ht.UPPER_F||e>=Ht.LOWER_A&&e<=Ht.LOWER_F}function yY(e){return e>=Ht.UPPER_A&&e<=Ht.UPPER_Z||e>=Ht.LOWER_A&&e<=Ht.LOWER_Z||dg(e)}function _Y(e){return e===Ht.EQUALS||yY(e)}var jt;(function(e){e[e.EntityStart=0]="EntityStart",e[e.NumericStart=1]="NumericStart",e[e.NumericDecimal=2]="NumericDecimal",e[e.NumericHex=3]="NumericHex",e[e.NamedEntity=4]="NamedEntity"})(jt||(jt={}));var _i;(function(e){e[e.Legacy=0]="Legacy",e[e.Strict=1]="Strict",e[e.Attribute=2]="Attribute"})(_i||(_i={}));class wY{constructor(t,n,r){this.decodeTree=t,this.emitCodePoint=n,this.errors=r,this.state=jt.EntityStart,this.consumed=1,this.result=0,this.treeIndex=0,this.excess=1,this.decodeMode=_i.Strict}startEntity(t){this.decodeMode=t,this.state=jt.EntityStart,this.result=0,this.treeIndex=0,this.excess=1,this.consumed=1}write(t,n){switch(this.state){case jt.EntityStart:return t.charCodeAt(n)===Ht.NUM?(this.state=jt.NumericStart,this.consumed+=1,this.stateNumericStart(t,n+1)):(this.state=jt.NamedEntity,this.stateNamedEntity(t,n));case jt.NumericStart:return this.stateNumericStart(t,n);case jt.NumericDecimal:return this.stateNumericDecimal(t,n);case jt.NumericHex:return this.stateNumericHex(t,n);case jt.NamedEntity:return this.stateNamedEntity(t,n)}}stateNumericStart(t,n){return n>=t.length?-1:(t.charCodeAt(n)|xY)===Ht.LOWER_X?(this.state=jt.NumericHex,this.consumed+=1,this.stateNumericHex(t,n+1)):(this.state=jt.NumericDecimal,this.stateNumericDecimal(t,n))}addToNumericResult(t,n,r,i){if(n!==r){const a=r-n;this.result=this.result*Math.pow(i,a)+Number.parseInt(t.substr(n,a),i),this.consumed+=a}}stateNumericHex(t,n){const r=n;for(;n<t.length;){const i=t.charCodeAt(n);if(dg(i)||$Y(i))n+=1;else return this.addToNumericResult(t,r,n,16),this.emitNumericEntity(i,3)}return this.addToNumericResult(t,r,n,16),-1}stateNumericDecimal(t,n){const r=n;for(;n<t.length;){const i=t.charCodeAt(n);if(dg(i))n+=1;else return this.addToNumericResult(t,r,n,10),this.emitNumericEntity(i,2)}return this.addToNumericResult(t,r,n,10),-1}emitNumericEntity(t,n){var r;if(this.consumed<=n)return(r=this.errors)===null||r===void 0||r.absenceOfDigitsInNumericCharacterReference(this.consumed),0;if(t===Ht.SEMI)this.consumed+=1;else if(this.decodeMode===_i.Strict)return 0;return this.emitCodePoint(vY(this.result),this.consumed),this.errors&&(t!==Ht.SEMI&&this.errors.missingSemicolonAfterCharacterReference(),this.errors.validateNumericCharacterReference(this.result)),this.consumed}stateNamedEntity(t,n){const{decodeTree:r}=this;let i=r[this.treeIndex],a=(i&pa.VALUE_LENGTH)>>14;for(;n<t.length;n++,this.excess++){const s=t.charCodeAt(n);if(this.treeIndex=TY(r,i,this.treeIndex+Math.max(1,a),s),this.treeIndex<0)return this.result===0||this.decodeMode===_i.Attribute&&(a===0||_Y(s))?0:this.emitNotTerminatedNamedEntity();if(i=r[this.treeIndex],a=(i&pa.VALUE_LENGTH)>>14,a!==0){if(s===Ht.SEMI)return this.emitNamedEntityData(this.treeIndex,a,this.consumed+this.excess);this.decodeMode!==_i.Strict&&(this.result=this.treeIndex,this.consumed+=this.excess,this.excess=0)}}return-1}emitNotTerminatedNamedEntity(){var t;const{result:n,decodeTree:r}=this,i=(r[n]&pa.VALUE_LENGTH)>>14;return this.emitNamedEntityData(n,i,this.consumed),(t=this.errors)===null||t===void 0||t.missingSemicolonAfterCharacterReference(),this.consumed}emitNamedEntityData(t,n,r){const{decodeTree:i}=this;return this.emitCodePoint(n===1?i[t]&~pa.VALUE_LENGTH:i[t+1],r),n===3&&this.emitCodePoint(i[t+2],r),r}end(){var t;switch(this.state){case jt.NamedEntity:return this.result!==0&&(this.decodeMode!==_i.Attribute||this.result===this.treeIndex)?this.emitNotTerminatedNamedEntity():0;case jt.NumericDecimal:return this.emitNumericEntity(0,2);case jt.NumericHex:return this.emitNumericEntity(0,3);case jt.NumericStart:return(t=this.errors)===null||t===void 0||t.absenceOfDigitsInNumericCharacterReference(this.consumed),0;case jt.EntityStart:return 0}}}function TY(e,t,n,r){const i=(t&pa.BRANCH_LENGTH)>>7,a=t&pa.JUMP_TABLE;if(i===0)return a!==0&&r===a?n:-1;if(a){const l=r-a;return l<0||l>=i?-1:e[n+l]-1}let s=n,o=s+i-1;for(;s<=o;){const l=s+o>>>1,c=e[l];if(c<r)s=l+1;else if(c>r)o=l-1;else return e[l+i]}return-1}var ee;(function(e){e.HTML="http://www.w3.org/1999/xhtml",e.MATHML="http://www.w3.org/1998/Math/MathML",e.SVG="http://www.w3.org/2000/svg",e.XLINK="http://www.w3.org/1999/xlink",e.XML="http://www.w3.org/XML/1998/namespace",e.XMLNS="http://www.w3.org/2000/xmlns/"})(ee||(ee={}));var us;(function(e){e.TYPE="type",e.ACTION="action",e.ENCODING="encoding",e.PROMPT="prompt",e.NAME="name",e.COLOR="color",e.FACE="face",e.SIZE="size"})(us||(us={}));var ar;(function(e){e.NO_QUIRKS="no-quirks",e.QUIRKS="quirks",e.LIMITED_QUIRKS="limited-quirks"})(ar||(ar={}));var j;(function(e){e.A="a",e.ADDRESS="address",e.ANNOTATION_XML="annotation-xml",e.APPLET="applet",e.AREA="area",e.ARTICLE="article",e.ASIDE="aside",e.B="b",e.BASE="base",e.BASEFONT="basefont",e.BGSOUND="bgsound",e.BIG="big",e.BLOCKQUOTE="blockquote",e.BODY="body",e.BR="br",e.BUTTON="button",e.CAPTION="caption",e.CENTER="center",e.CODE="code",e.COL="col",e.COLGROUP="colgroup",e.DD="dd",e.DESC="desc",e.DETAILS="details",e.DIALOG="dialog",e.DIR="dir",e.DIV="div",e.DL="dl",e.DT="dt",e.EM="em",e.EMBED="embed",e.FIELDSET="fieldset",e.FIGCAPTION="figcaption",e.FIGURE="figure",e.FONT="font",e.FOOTER="footer",e.FOREIGN_OBJECT="foreignObject",e.FORM="form",e.FRAME="frame",e.FRAMESET="frameset",e.H1="h1",e.H2="h2",e.H3="h3",e.H4="h4",e.H5="h5",e.H6="h6",e.HEAD="head",e.HEADER="header",e.HGROUP="hgroup",e.HR="hr",e.HTML="html",e.I="i",e.IMG="img",e.IMAGE="image",e.INPUT="input",e.IFRAME="iframe",e.KEYGEN="keygen",e.LABEL="label",e.LI="li",e.LINK="link",e.LISTING="listing",e.MAIN="main",e.MALIGNMARK="malignmark",e.MARQUEE="marquee",e.MATH="math",e.MENU="menu",e.META="meta",e.MGLYPH="mglyph",e.MI="mi",e.MO="mo",e.MN="mn",e.MS="ms",e.MTEXT="mtext",e.NAV="nav",e.NOBR="nobr",e.NOFRAMES="noframes",e.NOEMBED="noembed",e.NOSCRIPT="noscript",e.OBJECT="object",e.OL="ol",e.OPTGROUP="optgroup",e.OPTION="option",e.P="p",e.PARAM="param",e.PLAINTEXT="plaintext",e.PRE="pre",e.RB="rb",e.RP="rp",e.RT="rt",e.RTC="rtc",e.RUBY="ruby",e.S="s",e.SCRIPT="script",e.SEARCH="search",e.SECTION="section",e.SELECT="select",e.SOURCE="source",e.SMALL="small",e.SPAN="span",e.STRIKE="strike",e.STRONG="strong",e.STYLE="style",e.SUB="sub",e.SUMMARY="summary",e.SUP="sup",e.TABLE="table",e.TBODY="tbody",e.TEMPLATE="template",e.TEXTAREA="textarea",e.TFOOT="tfoot",e.TD="td",e.TH="th",e.THEAD="thead",e.TITLE="title",e.TR="tr",e.TRACK="track",e.TT="tt",e.U="u",e.UL="ul",e.SVG="svg",e.VAR="var",e.WBR="wbr",e.XMP="xmp"})(j||(j={}));var f;(function(e){e[e.UNKNOWN=0]="UNKNOWN",e[e.A=1]="A",e[e.ADDRESS=2]="ADDRESS",e[e.ANNOTATION_XML=3]="ANNOTATION_XML",e[e.APPLET=4]="APPLET",e[e.AREA=5]="AREA",e[e.ARTICLE=6]="ARTICLE",e[e.ASIDE=7]="ASIDE",e[e.B=8]="B",e[e.BASE=9]="BASE",e[e.BASEFONT=10]="BASEFONT",e[e.BGSOUND=11]="BGSOUND",e[e.BIG=12]="BIG",e[e.BLOCKQUOTE=13]="BLOCKQUOTE",e[e.BODY=14]="BODY",e[e.BR=15]="BR",e[e.BUTTON=16]="BUTTON",e[e.CAPTION=17]="CAPTION",e[e.CENTER=18]="CENTER",e[e.CODE=19]="CODE",e[e.COL=20]="COL",e[e.COLGROUP=21]="COLGROUP",e[e.DD=22]="DD",e[e.DESC=23]="DESC",e[e.DETAILS=24]="DETAILS",e[e.DIALOG=25]="DIALOG",e[e.DIR=26]="DIR",e[e.DIV=27]="DIV",e[e.DL=28]="DL",e[e.DT=29]="DT",e[e.EM=30]="EM",e[e.EMBED=31]="EMBED",e[e.FIELDSET=32]="FIELDSET",e[e.FIGCAPTION=33]="FIGCAPTION",e[e.FIGURE=34]="FIGURE",e[e.FONT=35]="FONT",e[e.FOOTER=36]="FOOTER",e[e.FOREIGN_OBJECT=37]="FOREIGN_OBJECT",e[e.FORM=38]="FORM",e[e.FRAME=39]="FRAME",e[e.FRAMESET=40]="FRAMESET",e[e.H1=41]="H1",e[e.H2=42]="H2",e[e.H3=43]="H3",e[e.H4=44]="H4",e[e.H5=45]="H5",e[e.H6=46]="H6",e[e.HEAD=47]="HEAD",e[e.HEADER=48]="HEADER",e[e.HGROUP=49]="HGROUP",e[e.HR=50]="HR",e[e.HTML=51]="HTML",e[e.I=52]="I",e[e.IMG=53]="IMG",e[e.IMAGE=54]="IMAGE",e[e.INPUT=55]="INPUT",e[e.IFRAME=56]="IFRAME",e[e.KEYGEN=57]="KEYGEN",e[e.LABEL=58]="LABEL",e[e.LI=59]="LI",e[e.LINK=60]="LINK",e[e.LISTING=61]="LISTING",e[e.MAIN=62]="MAIN",e[e.MALIGNMARK=63]="MALIGNMARK",e[e.MARQUEE=64]="MARQUEE",e[e.MATH=65]="MATH",e[e.MENU=66]="MENU",e[e.META=67]="META",e[e.MGLYPH=68]="MGLYPH",e[e.MI=69]="MI",e[e.MO=70]="MO",e[e.MN=71]="MN",e[e.MS=72]="MS",e[e.MTEXT=73]="MTEXT",e[e.NAV=74]="NAV",e[e.NOBR=75]="NOBR",e[e.NOFRAMES=76]="NOFRAMES",e[e.NOEMBED=77]="NOEMBED",e[e.NOSCRIPT=78]="NOSCRIPT",e[e.OBJECT=79]="OBJECT",e[e.OL=80]="OL",e[e.OPTGROUP=81]="OPTGROUP",e[e.OPTION=82]="OPTION",e[e.P=83]="P",e[e.PARAM=84]="PARAM",e[e.PLAINTEXT=85]="PLAINTEXT",e[e.PRE=86]="PRE",e[e.RB=87]="RB",e[e.RP=88]="RP",e[e.RT=89]="RT",e[e.RTC=90]="RTC",e[e.RUBY=91]="RUBY",e[e.S=92]="S",e[e.SCRIPT=93]="SCRIPT",e[e.SEARCH=94]="SEARCH",e[e.SECTION=95]="SECTION",e[e.SELECT=96]="SELECT",e[e.SOURCE=97]="SOURCE",e[e.SMALL=98]="SMALL",e[e.SPAN=99]="SPAN",e[e.STRIKE=100]="STRIKE",e[e.STRONG=101]="STRONG",e[e.STYLE=102]="STYLE",e[e.SUB=103]="SUB",e[e.SUMMARY=104]="SUMMARY",e[e.SUP=105]="SUP",e[e.TABLE=106]="TABLE",e[e.TBODY=107]="TBODY",e[e.TEMPLATE=108]="TEMPLATE",e[e.TEXTAREA=109]="TEXTAREA",e[e.TFOOT=110]="TFOOT",e[e.TD=111]="TD",e[e.TH=112]="TH",e[e.THEAD=113]="THEAD",e[e.TITLE=114]="TITLE",e[e.TR=115]="TR",e[e.TRACK=116]="TRACK",e[e.TT=117]="TT",e[e.U=118]="U",e[e.UL=119]="UL",e[e.SVG=120]="SVG",e[e.VAR=121]="VAR",e[e.WBR=122]="WBR",e[e.XMP=123]="XMP"})(f||(f={}));const EY=new Map([[j.A,f.A],[j.ADDRESS,f.ADDRESS],[j.ANNOTATION_XML,f.ANNOTATION_XML],[j.APPLET,f.APPLET],[j.AREA,f.AREA],[j.ARTICLE,f.ARTICLE],[j.ASIDE,f.ASIDE],[j.B,f.B],[j.BASE,f.BASE],[j.BASEFONT,f.BASEFONT],[j.BGSOUND,f.BGSOUND],[j.BIG,f.BIG],[j.BLOCKQUOTE,f.BLOCKQUOTE],[j.BODY,f.BODY],[j.BR,f.BR],[j.BUTTON,f.BUTTON],[j.CAPTION,f.CAPTION],[j.CENTER,f.CENTER],[j.CODE,f.CODE],[j.COL,f.COL],[j.COLGROUP,f.COLGROUP],[j.DD,f.DD],[j.DESC,f.DESC],[j.DETAILS,f.DETAILS],[j.DIALOG,f.DIALOG],[j.DIR,f.DIR],[j.DIV,f.DIV],[j.DL,f.DL],[j.DT,f.DT],[j.EM,f.EM],[j.EMBED,f.EMBED],[j.FIELDSET,f.FIELDSET],[j.FIGCAPTION,f.FIGCAPTION],[j.FIGURE,f.FIGURE],[j.FONT,f.FONT],[j.FOOTER,f.FOOTER],[j.FOREIGN_OBJECT,f.FOREIGN_OBJECT],[j.FORM,f.FORM],[j.FRAME,f.FRAME],[j.FRAMESET,f.FRAMESET],[j.H1,f.H1],[j.H2,f.H2],[j.H3,f.H3],[j.H4,f.H4],[j.H5,f.H5],[j.H6,f.H6],[j.HEAD,f.HEAD],[j.HEADER,f.HEADER],[j.HGROUP,f.HGROUP],[j.HR,f.HR],[j.HTML,f.HTML],[j.I,f.I],[j.IMG,f.IMG],[j.IMAGE,f.IMAGE],[j.INPUT,f.INPUT],[j.IFRAME,f.IFRAME],[j.KEYGEN,f.KEYGEN],[j.LABEL,f.LABEL],[j.LI,f.LI],[j.LINK,f.LINK],[j.LISTING,f.LISTING],[j.MAIN,f.MAIN],[j.MALIGNMARK,f.MALIGNMARK],[j.MARQUEE,f.MARQUEE],[j.MATH,f.MATH],[j.MENU,f.MENU],[j.META,f.META],[j.MGLYPH,f.MGLYPH],[j.MI,f.MI],[j.MO,f.MO],[j.MN,f.MN],[j.MS,f.MS],[j.MTEXT,f.MTEXT],[j.NAV,f.NAV],[j.NOBR,f.NOBR],[j.NOFRAMES,f.NOFRAMES],[j.NOEMBED,f.NOEMBED],[j.NOSCRIPT,f.NOSCRIPT],[j.OBJECT,f.OBJECT],[j.OL,f.OL],[j.OPTGROUP,f.OPTGROUP],[j.OPTION,f.OPTION],[j.P,f.P],[j.PARAM,f.PARAM],[j.PLAINTEXT,f.PLAINTEXT],[j.PRE,f.PRE],[j.RB,f.RB],[j.RP,f.RP],[j.RT,f.RT],[j.RTC,f.RTC],[j.RUBY,f.RUBY],[j.S,f.S],[j.SCRIPT,f.SCRIPT],[j.SEARCH,f.SEARCH],[j.SECTION,f.SECTION],[j.SELECT,f.SELECT],[j.SOURCE,f.SOURCE],[j.SMALL,f.SMALL],[j.SPAN,f.SPAN],[j.STRIKE,f.STRIKE],[j.STRONG,f.STRONG],[j.STYLE,f.STYLE],[j.SUB,f.SUB],[j.SUMMARY,f.SUMMARY],[j.SUP,f.SUP],[j.TABLE,f.TABLE],[j.TBODY,f.TBODY],[j.TEMPLATE,f.TEMPLATE],[j.TEXTAREA,f.TEXTAREA],[j.TFOOT,f.TFOOT],[j.TD,f.TD],[j.TH,f.TH],[j.THEAD,f.THEAD],[j.TITLE,f.TITLE],[j.TR,f.TR],[j.TRACK,f.TRACK],[j.TT,f.TT],[j.U,f.U],[j.UL,f.UL],[j.SVG,f.SVG],[j.VAR,f.VAR],[j.WBR,f.WBR],[j.XMP,f.XMP]]);function tl(e){var t;return(t=EY.get(e))!==null&&t!==void 0?t:f.UNKNOWN}const re=f,kY={[ee.HTML]:new Set([re.ADDRESS,re.APPLET,re.AREA,re.ARTICLE,re.ASIDE,re.BASE,re.BASEFONT,re.BGSOUND,re.BLOCKQUOTE,re.BODY,re.BR,re.BUTTON,re.CAPTION,re.CENTER,re.COL,re.COLGROUP,re.DD,re.DETAILS,re.DIR,re.DIV,re.DL,re.DT,re.EMBED,re.FIELDSET,re.FIGCAPTION,re.FIGURE,re.FOOTER,re.FORM,re.FRAME,re.FRAMESET,re.H1,re.H2,re.H3,re.H4,re.H5,re.H6,re.HEAD,re.HEADER,re.HGROUP,re.HR,re.HTML,re.IFRAME,re.IMG,re.INPUT,re.LI,re.LINK,re.LISTING,re.MAIN,re.MARQUEE,re.MENU,re.META,re.NAV,re.NOEMBED,re.NOFRAMES,re.NOSCRIPT,re.OBJECT,re.OL,re.P,re.PARAM,re.PLAINTEXT,re.PRE,re.SCRIPT,re.SECTION,re.SELECT,re.SOURCE,re.STYLE,re.SUMMARY,re.TABLE,re.TBODY,re.TD,re.TEMPLATE,re.TEXTAREA,re.TFOOT,re.TH,re.THEAD,re.TITLE,re.TR,re.TRACK,re.UL,re.WBR,re.XMP]),[ee.MATHML]:new Set([re.MI,re.MO,re.MN,re.MS,re.MTEXT,re.ANNOTATION_XML]),[ee.SVG]:new Set([re.TITLE,re.FOREIGN_OBJECT,re.DESC]),[ee.XLINK]:new Set,[ee.XML]:new Set,[ee.XMLNS]:new Set},mg=new Set([re.H1,re.H2,re.H3,re.H4,re.H5,re.H6]);j.STYLE,j.SCRIPT,j.XMP,j.IFRAME,j.NOEMBED,j.NOFRAMES,j.PLAINTEXT;var I;(function(e){e[e.DATA=0]="DATA",e[e.RCDATA=1]="RCDATA",e[e.RAWTEXT=2]="RAWTEXT",e[e.SCRIPT_DATA=3]="SCRIPT_DATA",e[e.PLAINTEXT=4]="PLAINTEXT",e[e.TAG_OPEN=5]="TAG_OPEN",e[e.END_TAG_OPEN=6]="END_TAG_OPEN",e[e.TAG_NAME=7]="TAG_NAME",e[e.RCDATA_LESS_THAN_SIGN=8]="RCDATA_LESS_THAN_SIGN",e[e.RCDATA_END_TAG_OPEN=9]="RCDATA_END_TAG_OPEN",e[e.RCDATA_END_TAG_NAME=10]="RCDATA_END_TAG_NAME",e[e.RAWTEXT_LESS_THAN_SIGN=11]="RAWTEXT_LESS_THAN_SIGN",e[e.RAWTEXT_END_TAG_OPEN=12]="RAWTEXT_END_TAG_OPEN",e[e.RAWTEXT_END_TAG_NAME=13]="RAWTEXT_END_TAG_NAME",e[e.SCRIPT_DATA_LESS_THAN_SIGN=14]="SCRIPT_DATA_LESS_THAN_SIGN",e[e.SCRIPT_DATA_END_TAG_OPEN=15]="SCRIPT_DATA_END_TAG_OPEN",e[e.SCRIPT_DATA_END_TAG_NAME=16]="SCRIPT_DATA_END_TAG_NAME",e[e.SCRIPT_DATA_ESCAPE_START=17]="SCRIPT_DATA_ESCAPE_START",e[e.SCRIPT_DATA_ESCAPE_START_DASH=18]="SCRIPT_DATA_ESCAPE_START_DASH",e[e.SCRIPT_DATA_ESCAPED=19]="SCRIPT_DATA_ESCAPED",e[e.SCRIPT_DATA_ESCAPED_DASH=20]="SCRIPT_DATA_ESCAPED_DASH",e[e.SCRIPT_DATA_ESCAPED_DASH_DASH=21]="SCRIPT_DATA_ESCAPED_DASH_DASH",e[e.SCRIPT_DATA_ESCAPED_LESS_THAN_SIGN=22]="SCRIPT_DATA_ESCAPED_LESS_THAN_SIGN",e[e.SCRIPT_DATA_ESCAPED_END_TAG_OPEN=23]="SCRIPT_DATA_ESCAPED_END_TAG_OPEN",e[e.SCRIPT_DATA_ESCAPED_END_TAG_NAME=24]="SCRIPT_DATA_ESCAPED_END_TAG_NAME",e[e.SCRIPT_DATA_DOUBLE_ESCAPE_START=25]="SCRIPT_DATA_DOUBLE_ESCAPE_START",e[e.SCRIPT_DATA_DOUBLE_ESCAPED=26]="SCRIPT_DATA_DOUBLE_ESCAPED",e[e.SCRIPT_DATA_DOUBLE_ESCAPED_DASH=27]="SCRIPT_DATA_DOUBLE_ESCAPED_DASH",e[e.SCRIPT_DATA_DOUBLE_ESCAPED_DASH_DASH=28]="SCRIPT_DATA_DOUBLE_ESCAPED_DASH_DASH",e[e.SCRIPT_DATA_DOUBLE_ESCAPED_LESS_THAN_SIGN=29]="SCRIPT_DATA_DOUBLE_ESCAPED_LESS_THAN_SIGN",e[e.SCRIPT_DATA_DOUBLE_ESCAPE_END=30]="SCRIPT_DATA_DOUBLE_ESCAPE_END",e[e.BEFORE_ATTRIBUTE_NAME=31]="BEFORE_ATTRIBUTE_NAME",e[e.ATTRIBUTE_NAME=32]="ATTRIBUTE_NAME",e[e.AFTER_ATTRIBUTE_NAME=33]="AFTER_ATTRIBUTE_NAME",e[e.BEFORE_ATTRIBUTE_VALUE=34]="BEFORE_ATTRIBUTE_VALUE",e[e.ATTRIBUTE_VALUE_DOUBLE_QUOTED=35]="ATTRIBUTE_VALUE_DOUBLE_QUOTED",e[e.ATTRIBUTE_VALUE_SINGLE_QUOTED=36]="ATTRIBUTE_VALUE_SINGLE_QUOTED",e[e.ATTRIBUTE_VALUE_UNQUOTED=37]="ATTRIBUTE_VALUE_UNQUOTED",e[e.AFTER_ATTRIBUTE_VALUE_QUOTED=38]="AFTER_ATTRIBUTE_VALUE_QUOTED",e[e.SELF_CLOSING_START_TAG=39]="SELF_CLOSING_START_TAG",e[e.BOGUS_COMMENT=40]="BOGUS_COMMENT",e[e.MARKUP_DECLARATION_OPEN=41]="MARKUP_DECLARATION_OPEN",e[e.COMMENT_START=42]="COMMENT_START",e[e.COMMENT_START_DASH=43]="COMMENT_START_DASH",e[e.COMMENT=44]="COMMENT",e[e.COMMENT_LESS_THAN_SIGN=45]="COMMENT_LESS_THAN_SIGN",e[e.COMMENT_LESS_THAN_SIGN_BANG=46]="COMMENT_LESS_THAN_SIGN_BANG",e[e.COMMENT_LESS_THAN_SIGN_BANG_DASH=47]="COMMENT_LESS_THAN_SIGN_BANG_DASH",e[e.COMMENT_LESS_THAN_SIGN_BANG_DASH_DASH=48]="COMMENT_LESS_THAN_SIGN_BANG_DASH_DASH",e[e.COMMENT_END_DASH=49]="COMMENT_END_DASH",e[e.COMMENT_END=50]="COMMENT_END",e[e.COMMENT_END_BANG=51]="COMMENT_END_BANG",e[e.DOCTYPE=52]="DOCTYPE",e[e.BEFORE_DOCTYPE_NAME=53]="BEFORE_DOCTYPE_NAME",e[e.DOCTYPE_NAME=54]="DOCTYPE_NAME",e[e.AFTER_DOCTYPE_NAME=55]="AFTER_DOCTYPE_NAME",e[e.AFTER_DOCTYPE_PUBLIC_KEYWORD=56]="AFTER_DOCTYPE_PUBLIC_KEYWORD",e[e.BEFORE_DOCTYPE_PUBLIC_IDENTIFIER=57]="BEFORE_DOCTYPE_PUBLIC_IDENTIFIER",e[e.DOCTYPE_PUBLIC_IDENTIFIER_DOUBLE_QUOTED=58]="DOCTYPE_PUBLIC_IDENTIFIER_DOUBLE_QUOTED",e[e.DOCTYPE_PUBLIC_IDENTIFIER_SINGLE_QUOTED=59]="DOCTYPE_PUBLIC_IDENTIFIER_SINGLE_QUOTED",e[e.AFTER_DOCTYPE_PUBLIC_IDENTIFIER=60]="AFTER_DOCTYPE_PUBLIC_IDENTIFIER",e[e.BETWEEN_DOCTYPE_PUBLIC_AND_SYSTEM_IDENTIFIERS=61]="BETWEEN_DOCTYPE_PUBLIC_AND_SYSTEM_IDENTIFIERS",e[e.AFTER_DOCTYPE_SYSTEM_KEYWORD=62]="AFTER_DOCTYPE_SYSTEM_KEYWORD",e[e.BEFORE_DOCTYPE_SYSTEM_IDENTIFIER=63]="BEFORE_DOCTYPE_SYSTEM_IDENTIFIER",e[e.DOCTYPE_SYSTEM_IDENTIFIER_DOUBLE_QUOTED=64]="DOCTYPE_SYSTEM_IDENTIFIER_DOUBLE_QUOTED",e[e.DOCTYPE_SYSTEM_IDENTIFIER_SINGLE_QUOTED=65]="DOCTYPE_SYSTEM_IDENTIFIER_SINGLE_QUOTED",e[e.AFTER_DOCTYPE_SYSTEM_IDENTIFIER=66]="AFTER_DOCTYPE_SYSTEM_IDENTIFIER",e[e.BOGUS_DOCTYPE=67]="BOGUS_DOCTYPE",e[e.CDATA_SECTION=68]="CDATA_SECTION",e[e.CDATA_SECTION_BRACKET=69]="CDATA_SECTION_BRACKET",e[e.CDATA_SECTION_END=70]="CDATA_SECTION_END",e[e.CHARACTER_REFERENCE=71]="CHARACTER_REFERENCE",e[e.AMBIGUOUS_AMPERSAND=72]="AMBIGUOUS_AMPERSAND"})(I||(I={}));const Ct={DATA:I.DATA,RCDATA:I.RCDATA,RAWTEXT:I.RAWTEXT,SCRIPT_DATA:I.SCRIPT_DATA,PLAINTEXT:I.PLAINTEXT,CDATA_SECTION:I.CDATA_SECTION};function SY(e){return e>=A.DIGIT_0&&e<=A.DIGIT_9}function El(e){return e>=A.LATIN_CAPITAL_A&&e<=A.LATIN_CAPITAL_Z}function NY(e){return e>=A.LATIN_SMALL_A&&e<=A.LATIN_SMALL_Z}function Zi(e){return NY(e)||El(e)}function y$(e){return Zi(e)||SY(e)}function Qu(e){return e+32}function YT(e){return e===A.SPACE||e===A.LINE_FEED||e===A.TABULATION||e===A.FORM_FEED}function _$(e){return YT(e)||e===A.SOLIDUS||e===A.GREATER_THAN_SIGN}function CY(e){return e===A.NULL?W.nullCharacterReference:e>1114111?W.characterReferenceOutsideUnicodeRange:qT(e)?W.surrogateCharacterReference:WT(e)?W.noncharacterCharacterReference:GT(e)||e===A.CARRIAGE_RETURN?W.controlCharacterReference:null}class AY{constructor(t,n){this.options=t,this.handler=n,this.paused=!1,this.inLoop=!1,this.inForeignNode=!1,this.lastStartTagName="",this.active=!1,this.state=I.DATA,this.returnState=I.DATA,this.entityStartPos=0,this.consumedAfterSnapshot=-1,this.currentCharacterToken=null,this.currentToken=null,this.currentAttr={name:"",value:""},this.preprocessor=new fY(n),this.currentLocation=this.getCurrentLocation(-1),this.entityDecoder=new wY(gY,(r,i)=>{this.preprocessor.pos=this.entityStartPos+i-1,this._flushCodePointConsumedAsCharacterReference(r)},n.onParseError?{missingSemicolonAfterCharacterReference:()=>{this._err(W.missingSemicolonAfterCharacterReference,1)},absenceOfDigitsInNumericCharacterReference:r=>{this._err(W.absenceOfDigitsInNumericCharacterReference,this.entityStartPos-this.preprocessor.pos+r)},validateNumericCharacterReference:r=>{const i=CY(r);i&&this._err(i,1)}}:void 0)}_err(t,n=0){var r,i;(i=(r=this.handler).onParseError)===null||i===void 0||i.call(r,this.preprocessor.getError(t,n))}getCurrentLocation(t){return this.options.sourceCodeLocationInfo?{startLine:this.preprocessor.line,startCol:this.preprocessor.col-t,startOffset:this.preprocessor.offset-t,endLine:-1,endCol:-1,endOffset:-1}:null}_runParsingLoop(){if(!this.inLoop){for(this.inLoop=!0;this.active&&!this.paused;){this.consumedAfterSnapshot=0;const t=this._consume();this._ensureHibernation()||this._callState(t)}this.inLoop=!1}}pause(){this.paused=!0}resume(t){if(!this.paused)throw new Error("Parser was already resumed");this.paused=!1,!this.inLoop&&(this._runParsingLoop(),this.paused||t==null||t())}write(t,n,r){this.active=!0,this.preprocessor.write(t,n),this._runParsingLoop(),this.paused||r==null||r()}insertHtmlAtCurrentPos(t){this.active=!0,this.preprocessor.insertHtmlAtCurrentPos(t),this._runParsingLoop()}_ensureHibernation(){return this.preprocessor.endOfChunkHit?(this.preprocessor.retreat(this.consumedAfterSnapshot),this.consumedAfterSnapshot=0,this.active=!1,!0):!1}_consume(){return this.consumedAfterSnapshot++,this.preprocessor.advance()}_advanceBy(t){this.consumedAfterSnapshot+=t;for(let n=0;n<t;n++)this.preprocessor.advance()}_consumeSequenceIfMatch(t,n){return this.preprocessor.startsWith(t,n)?(this._advanceBy(t.length-1),!0):!1}_createStartTagToken(){this.currentToken={type:ze.START_TAG,tagName:"",tagID:f.UNKNOWN,selfClosing:!1,ackSelfClosing:!1,attrs:[],location:this.getCurrentLocation(1)}}_createEndTagToken(){this.currentToken={type:ze.END_TAG,tagName:"",tagID:f.UNKNOWN,selfClosing:!1,ackSelfClosing:!1,attrs:[],location:this.getCurrentLocation(2)}}_createCommentToken(t){this.currentToken={type:ze.COMMENT,data:"",location:this.getCurrentLocation(t)}}_createDoctypeToken(t){this.currentToken={type:ze.DOCTYPE,name:t,forceQuirks:!1,publicId:null,systemId:null,location:this.currentLocation}}_createCharacterToken(t,n){this.currentCharacterToken={type:t,chars:n,location:this.currentLocation}}_createAttr(t){this.currentAttr={name:t,value:""},this.currentLocation=this.getCurrentLocation(0)}_leaveAttrName(){var t,n;const r=this.currentToken;if(KT(r,this.currentAttr.name)===null){if(r.attrs.push(this.currentAttr),r.location&&this.currentLocation){const i=(t=(n=r.location).attrs)!==null&&t!==void 0?t:n.attrs=Object.create(null);i[this.currentAttr.name]=this.currentLocation,this._leaveAttrValue()}}else this._err(W.duplicateAttribute)}_leaveAttrValue(){this.currentLocation&&(this.currentLocation.endLine=this.preprocessor.line,this.currentLocation.endCol=this.preprocessor.col,this.currentLocation.endOffset=this.preprocessor.offset)}prepareToken(t){this._emitCurrentCharacterToken(t.location),this.currentToken=null,t.location&&(t.location.endLine=this.preprocessor.line,t.location.endCol=this.preprocessor.col+1,t.location.endOffset=this.preprocessor.offset+1),this.currentLocation=this.getCurrentLocation(-1)}emitCurrentTagToken(){const t=this.currentToken;this.prepareToken(t),t.tagID=tl(t.tagName),t.type===ze.START_TAG?(this.lastStartTagName=t.tagName,this.handler.onStartTag(t)):(t.attrs.length>0&&this._err(W.endTagWithAttributes),t.selfClosing&&this._err(W.endTagWithTrailingSolidus),this.handler.onEndTag(t)),this.preprocessor.dropParsedChunk()}emitCurrentComment(t){this.prepareToken(t),this.handler.onComment(t),this.preprocessor.dropParsedChunk()}emitCurrentDoctype(t){this.prepareToken(t),this.handler.onDoctype(t),this.preprocessor.dropParsedChunk()}_emitCurrentCharacterToken(t){if(this.currentCharacterToken){switch(t&&this.currentCharacterToken.location&&(this.currentCharacterToken.location.endLine=t.startLine,this.currentCharacterToken.location.endCol=t.startCol,this.currentCharacterToken.location.endOffset=t.startOffset),this.currentCharacterToken.type){case ze.CHARACTER:{this.handler.onCharacter(this.currentCharacterToken);break}case ze.NULL_CHARACTER:{this.handler.onNullCharacter(this.currentCharacterToken);break}case ze.WHITESPACE_CHARACTER:{this.handler.onWhitespaceCharacter(this.currentCharacterToken);break}}this.currentCharacterToken=null}}_emitEOFToken(){const t=this.getCurrentLocation(0);t&&(t.endLine=t.startLine,t.endCol=t.startCol,t.endOffset=t.startOffset),this._emitCurrentCharacterToken(t),this.handler.onEof({type:ze.EOF,location:t}),this.active=!1}_appendCharToCurrentCharacterToken(t,n){if(this.currentCharacterToken)if(this.currentCharacterToken.type===t){this.currentCharacterToken.chars+=n;return}else this.currentLocation=this.getCurrentLocation(0),this._emitCurrentCharacterToken(this.currentLocation),this.preprocessor.dropParsedChunk();this._createCharacterToken(t,n)}_emitCodePoint(t){const n=YT(t)?ze.WHITESPACE_CHARACTER:t===A.NULL?ze.NULL_CHARACTER:ze.CHARACTER;this._appendCharToCurrentCharacterToken(n,String.fromCodePoint(t))}_emitChars(t){this._appendCharToCurrentCharacterToken(ze.CHARACTER,t)}_startCharacterReference(){this.returnState=this.state,this.state=I.CHARACTER_REFERENCE,this.entityStartPos=this.preprocessor.pos,this.entityDecoder.startEntity(this._isCharacterReferenceInAttribute()?_i.Attribute:_i.Legacy)}_isCharacterReferenceInAttribute(){return this.returnState===I.ATTRIBUTE_VALUE_DOUBLE_QUOTED||this.returnState===I.ATTRIBUTE_VALUE_SINGLE_QUOTED||this.returnState===I.ATTRIBUTE_VALUE_UNQUOTED}_flushCodePointConsumedAsCharacterReference(t){this._isCharacterReferenceInAttribute()?this.currentAttr.value+=String.fromCodePoint(t):this._emitCodePoint(t)}_callState(t){switch(this.state){case I.DATA:{this._stateData(t);break}case I.RCDATA:{this._stateRcdata(t);break}case I.RAWTEXT:{this._stateRawtext(t);break}case I.SCRIPT_DATA:{this._stateScriptData(t);break}case I.PLAINTEXT:{this._statePlaintext(t);break}case I.TAG_OPEN:{this._stateTagOpen(t);break}case I.END_TAG_OPEN:{this._stateEndTagOpen(t);break}case I.TAG_NAME:{this._stateTagName(t);break}case I.RCDATA_LESS_THAN_SIGN:{this._stateRcdataLessThanSign(t);break}case I.RCDATA_END_TAG_OPEN:{this._stateRcdataEndTagOpen(t);break}case I.RCDATA_END_TAG_NAME:{this._stateRcdataEndTagName(t);break}case I.RAWTEXT_LESS_THAN_SIGN:{this._stateRawtextLessThanSign(t);break}case I.RAWTEXT_END_TAG_OPEN:{this._stateRawtextEndTagOpen(t);break}case I.RAWTEXT_END_TAG_NAME:{this._stateRawtextEndTagName(t);break}case I.SCRIPT_DATA_LESS_THAN_SIGN:{this._stateScriptDataLessThanSign(t);break}case I.SCRIPT_DATA_END_TAG_OPEN:{this._stateScriptDataEndTagOpen(t);break}case I.SCRIPT_DATA_END_TAG_NAME:{this._stateScriptDataEndTagName(t);break}case I.SCRIPT_DATA_ESCAPE_START:{this._stateScriptDataEscapeStart(t);break}case I.SCRIPT_DATA_ESCAPE_START_DASH:{this._stateScriptDataEscapeStartDash(t);break}case I.SCRIPT_DATA_ESCAPED:{this._stateScriptDataEscaped(t);break}case I.SCRIPT_DATA_ESCAPED_DASH:{this._stateScriptDataEscapedDash(t);break}case I.SCRIPT_DATA_ESCAPED_DASH_DASH:{this._stateScriptDataEscapedDashDash(t);break}case I.SCRIPT_DATA_ESCAPED_LESS_THAN_SIGN:{this._stateScriptDataEscapedLessThanSign(t);break}case I.SCRIPT_DATA_ESCAPED_END_TAG_OPEN:{this._stateScriptDataEscapedEndTagOpen(t);break}case I.SCRIPT_DATA_ESCAPED_END_TAG_NAME:{this._stateScriptDataEscapedEndTagName(t);break}case I.SCRIPT_DATA_DOUBLE_ESCAPE_START:{this._stateScriptDataDoubleEscapeStart(t);break}case I.SCRIPT_DATA_DOUBLE_ESCAPED:{this._stateScriptDataDoubleEscaped(t);break}case I.SCRIPT_DATA_DOUBLE_ESCAPED_DASH:{this._stateScriptDataDoubleEscapedDash(t);break}case I.SCRIPT_DATA_DOUBLE_ESCAPED_DASH_DASH:{this._stateScriptDataDoubleEscapedDashDash(t);break}case I.SCRIPT_DATA_DOUBLE_ESCAPED_LESS_THAN_SIGN:{this._stateScriptDataDoubleEscapedLessThanSign(t);break}case I.SCRIPT_DATA_DOUBLE_ESCAPE_END:{this._stateScriptDataDoubleEscapeEnd(t);break}case I.BEFORE_ATTRIBUTE_NAME:{this._stateBeforeAttributeName(t);break}case I.ATTRIBUTE_NAME:{this._stateAttributeName(t);break}case I.AFTER_ATTRIBUTE_NAME:{this._stateAfterAttributeName(t);break}case I.BEFORE_ATTRIBUTE_VALUE:{this._stateBeforeAttributeValue(t);break}case I.ATTRIBUTE_VALUE_DOUBLE_QUOTED:{this._stateAttributeValueDoubleQuoted(t);break}case I.ATTRIBUTE_VALUE_SINGLE_QUOTED:{this._stateAttributeValueSingleQuoted(t);break}case I.ATTRIBUTE_VALUE_UNQUOTED:{this._stateAttributeValueUnquoted(t);break}case I.AFTER_ATTRIBUTE_VALUE_QUOTED:{this._stateAfterAttributeValueQuoted(t);break}case I.SELF_CLOSING_START_TAG:{this._stateSelfClosingStartTag(t);break}case I.BOGUS_COMMENT:{this._stateBogusComment(t);break}case I.MARKUP_DECLARATION_OPEN:{this._stateMarkupDeclarationOpen(t);break}case I.COMMENT_START:{this._stateCommentStart(t);break}case I.COMMENT_START_DASH:{this._stateCommentStartDash(t);break}case I.COMMENT:{this._stateComment(t);break}case I.COMMENT_LESS_THAN_SIGN:{this._stateCommentLessThanSign(t);break}case I.COMMENT_LESS_THAN_SIGN_BANG:{this._stateCommentLessThanSignBang(t);break}case I.COMMENT_LESS_THAN_SIGN_BANG_DASH:{this._stateCommentLessThanSignBangDash(t);break}case I.COMMENT_LESS_THAN_SIGN_BANG_DASH_DASH:{this._stateCommentLessThanSignBangDashDash(t);break}case I.COMMENT_END_DASH:{this._stateCommentEndDash(t);break}case I.COMMENT_END:{this._stateCommentEnd(t);break}case I.COMMENT_END_BANG:{this._stateCommentEndBang(t);break}case I.DOCTYPE:{this._stateDoctype(t);break}case I.BEFORE_DOCTYPE_NAME:{this._stateBeforeDoctypeName(t);break}case I.DOCTYPE_NAME:{this._stateDoctypeName(t);break}case I.AFTER_DOCTYPE_NAME:{this._stateAfterDoctypeName(t);break}case I.AFTER_DOCTYPE_PUBLIC_KEYWORD:{this._stateAfterDoctypePublicKeyword(t);break}case I.BEFORE_DOCTYPE_PUBLIC_IDENTIFIER:{this._stateBeforeDoctypePublicIdentifier(t);break}case I.DOCTYPE_PUBLIC_IDENTIFIER_DOUBLE_QUOTED:{this._stateDoctypePublicIdentifierDoubleQuoted(t);break}case I.DOCTYPE_PUBLIC_IDENTIFIER_SINGLE_QUOTED:{this._stateDoctypePublicIdentifierSingleQuoted(t);break}case I.AFTER_DOCTYPE_PUBLIC_IDENTIFIER:{this._stateAfterDoctypePublicIdentifier(t);break}case I.BETWEEN_DOCTYPE_PUBLIC_AND_SYSTEM_IDENTIFIERS:{this._stateBetweenDoctypePublicAndSystemIdentifiers(t);break}case I.AFTER_DOCTYPE_SYSTEM_KEYWORD:{this._stateAfterDoctypeSystemKeyword(t);break}case I.BEFORE_DOCTYPE_SYSTEM_IDENTIFIER:{this._stateBeforeDoctypeSystemIdentifier(t);break}case I.DOCTYPE_SYSTEM_IDENTIFIER_DOUBLE_QUOTED:{this._stateDoctypeSystemIdentifierDoubleQuoted(t);break}case I.DOCTYPE_SYSTEM_IDENTIFIER_SINGLE_QUOTED:{this._stateDoctypeSystemIdentifierSingleQuoted(t);break}case I.AFTER_DOCTYPE_SYSTEM_IDENTIFIER:{this._stateAfterDoctypeSystemIdentifier(t);break}case I.BOGUS_DOCTYPE:{this._stateBogusDoctype(t);break}case I.CDATA_SECTION:{this._stateCdataSection(t);break}case I.CDATA_SECTION_BRACKET:{this._stateCdataSectionBracket(t);break}case I.CDATA_SECTION_END:{this._stateCdataSectionEnd(t);break}case I.CHARACTER_REFERENCE:{this._stateCharacterReference();break}case I.AMBIGUOUS_AMPERSAND:{this._stateAmbiguousAmpersand(t);break}default:throw new Error("Unknown state")}}_stateData(t){switch(t){case A.LESS_THAN_SIGN:{this.state=I.TAG_OPEN;break}case A.AMPERSAND:{this._startCharacterReference();break}case A.NULL:{this._err(W.unexpectedNullCharacter),this._emitCodePoint(t);break}case A.EOF:{this._emitEOFToken();break}default:this._emitCodePoint(t)}}_stateRcdata(t){switch(t){case A.AMPERSAND:{this._startCharacterReference();break}case A.LESS_THAN_SIGN:{this.state=I.RCDATA_LESS_THAN_SIGN;break}case A.NULL:{this._err(W.unexpectedNullCharacter),this._emitChars(dt);break}case A.EOF:{this._emitEOFToken();break}default:this._emitCodePoint(t)}}_stateRawtext(t){switch(t){case A.LESS_THAN_SIGN:{this.state=I.RAWTEXT_LESS_THAN_SIGN;break}case A.NULL:{this._err(W.unexpectedNullCharacter),this._emitChars(dt);break}case A.EOF:{this._emitEOFToken();break}default:this._emitCodePoint(t)}}_stateScriptData(t){switch(t){case A.LESS_THAN_SIGN:{this.state=I.SCRIPT_DATA_LESS_THAN_SIGN;break}case A.NULL:{this._err(W.unexpectedNullCharacter),this._emitChars(dt);break}case A.EOF:{this._emitEOFToken();break}default:this._emitCodePoint(t)}}_statePlaintext(t){switch(t){case A.NULL:{this._err(W.unexpectedNullCharacter),this._emitChars(dt);break}case A.EOF:{this._emitEOFToken();break}default:this._emitCodePoint(t)}}_stateTagOpen(t){if(Zi(t))this._createStartTagToken(),this.state=I.TAG_NAME,this._stateTagName(t);else switch(t){case A.EXCLAMATION_MARK:{this.state=I.MARKUP_DECLARATION_OPEN;break}case A.SOLIDUS:{this.state=I.END_TAG_OPEN;break}case A.QUESTION_MARK:{this._err(W.unexpectedQuestionMarkInsteadOfTagName),this._createCommentToken(1),this.state=I.BOGUS_COMMENT,this._stateBogusComment(t);break}case A.EOF:{this._err(W.eofBeforeTagName),this._emitChars("<"),this._emitEOFToken();break}default:this._err(W.invalidFirstCharacterOfTagName),this._emitChars("<"),this.state=I.DATA,this._stateData(t)}}_stateEndTagOpen(t){if(Zi(t))this._createEndTagToken(),this.state=I.TAG_NAME,this._stateTagName(t);else switch(t){case A.GREATER_THAN_SIGN:{this._err(W.missingEndTagName),this.state=I.DATA;break}case A.EOF:{this._err(W.eofBeforeTagName),this._emitChars("</"),this._emitEOFToken();break}default:this._err(W.invalidFirstCharacterOfTagName),this._createCommentToken(2),this.state=I.BOGUS_COMMENT,this._stateBogusComment(t)}}_stateTagName(t){const n=this.currentToken;switch(t){case A.SPACE:case A.LINE_FEED:case A.TABULATION:case A.FORM_FEED:{this.state=I.BEFORE_ATTRIBUTE_NAME;break}case A.SOLIDUS:{this.state=I.SELF_CLOSING_START_TAG;break}case A.GREATER_THAN_SIGN:{this.state=I.DATA,this.emitCurrentTagToken();break}case A.NULL:{this._err(W.unexpectedNullCharacter),n.tagName+=dt;break}case A.EOF:{this._err(W.eofInTag),this._emitEOFToken();break}default:n.tagName+=String.fromCodePoint(El(t)?Qu(t):t)}}_stateRcdataLessThanSign(t){t===A.SOLIDUS?this.state=I.RCDATA_END_TAG_OPEN:(this._emitChars("<"),this.state=I.RCDATA,this._stateRcdata(t))}_stateRcdataEndTagOpen(t){Zi(t)?(this.state=I.RCDATA_END_TAG_NAME,this._stateRcdataEndTagName(t)):(this._emitChars("</"),this.state=I.RCDATA,this._stateRcdata(t))}handleSpecialEndTag(t){if(!this.preprocessor.startsWith(this.lastStartTagName,!1))return!this._ensureHibernation();this._createEndTagToken();const n=this.currentToken;switch(n.tagName=this.lastStartTagName,this.preprocessor.peek(this.lastStartTagName.length)){case A.SPACE:case A.LINE_FEED:case A.TABULATION:case A.FORM_FEED:return this._advanceBy(this.lastStartTagName.length),this.state=I.BEFORE_ATTRIBUTE_NAME,!1;case A.SOLIDUS:return this._advanceBy(this.lastStartTagName.length),this.state=I.SELF_CLOSING_START_TAG,!1;case A.GREATER_THAN_SIGN:return this._advanceBy(this.lastStartTagName.length),this.emitCurrentTagToken(),this.state=I.DATA,!1;default:return!this._ensureHibernation()}}_stateRcdataEndTagName(t){this.handleSpecialEndTag(t)&&(this._emitChars("</"),this.state=I.RCDATA,this._stateRcdata(t))}_stateRawtextLessThanSign(t){t===A.SOLIDUS?this.state=I.RAWTEXT_END_TAG_OPEN:(this._emitChars("<"),this.state=I.RAWTEXT,this._stateRawtext(t))}_stateRawtextEndTagOpen(t){Zi(t)?(this.state=I.RAWTEXT_END_TAG_NAME,this._stateRawtextEndTagName(t)):(this._emitChars("</"),this.state=I.RAWTEXT,this._stateRawtext(t))}_stateRawtextEndTagName(t){this.handleSpecialEndTag(t)&&(this._emitChars("</"),this.state=I.RAWTEXT,this._stateRawtext(t))}_stateScriptDataLessThanSign(t){switch(t){case A.SOLIDUS:{this.state=I.SCRIPT_DATA_END_TAG_OPEN;break}case A.EXCLAMATION_MARK:{this.state=I.SCRIPT_DATA_ESCAPE_START,this._emitChars("<!");break}default:this._emitChars("<"),this.state=I.SCRIPT_DATA,this._stateScriptData(t)}}_stateScriptDataEndTagOpen(t){Zi(t)?(this.state=I.SCRIPT_DATA_END_TAG_NAME,this._stateScriptDataEndTagName(t)):(this._emitChars("</"),this.state=I.SCRIPT_DATA,this._stateScriptData(t))}_stateScriptDataEndTagName(t){this.handleSpecialEndTag(t)&&(this._emitChars("</"),this.state=I.SCRIPT_DATA,this._stateScriptData(t))}_stateScriptDataEscapeStart(t){t===A.HYPHEN_MINUS?(this.state=I.SCRIPT_DATA_ESCAPE_START_DASH,this._emitChars("-")):(this.state=I.SCRIPT_DATA,this._stateScriptData(t))}_stateScriptDataEscapeStartDash(t){t===A.HYPHEN_MINUS?(this.state=I.SCRIPT_DATA_ESCAPED_DASH_DASH,this._emitChars("-")):(this.state=I.SCRIPT_DATA,this._stateScriptData(t))}_stateScriptDataEscaped(t){switch(t){case A.HYPHEN_MINUS:{this.state=I.SCRIPT_DATA_ESCAPED_DASH,this._emitChars("-");break}case A.LESS_THAN_SIGN:{this.state=I.SCRIPT_DATA_ESCAPED_LESS_THAN_SIGN;break}case A.NULL:{this._err(W.unexpectedNullCharacter),this._emitChars(dt);break}case A.EOF:{this._err(W.eofInScriptHtmlCommentLikeText),this._emitEOFToken();break}default:this._emitCodePoint(t)}}_stateScriptDataEscapedDash(t){switch(t){case A.HYPHEN_MINUS:{this.state=I.SCRIPT_DATA_ESCAPED_DASH_DASH,this._emitChars("-");break}case A.LESS_THAN_SIGN:{this.state=I.SCRIPT_DATA_ESCAPED_LESS_THAN_SIGN;break}case A.NULL:{this._err(W.unexpectedNullCharacter),this.state=I.SCRIPT_DATA_ESCAPED,this._emitChars(dt);break}case A.EOF:{this._err(W.eofInScriptHtmlCommentLikeText),this._emitEOFToken();break}default:this.state=I.SCRIPT_DATA_ESCAPED,this._emitCodePoint(t)}}_stateScriptDataEscapedDashDash(t){switch(t){case A.HYPHEN_MINUS:{this._emitChars("-");break}case A.LESS_THAN_SIGN:{this.state=I.SCRIPT_DATA_ESCAPED_LESS_THAN_SIGN;break}case A.GREATER_THAN_SIGN:{this.state=I.SCRIPT_DATA,this._emitChars(">");break}case A.NULL:{this._err(W.unexpectedNullCharacter),this.state=I.SCRIPT_DATA_ESCAPED,this._emitChars(dt);break}case A.EOF:{this._err(W.eofInScriptHtmlCommentLikeText),this._emitEOFToken();break}default:this.state=I.SCRIPT_DATA_ESCAPED,this._emitCodePoint(t)}}_stateScriptDataEscapedLessThanSign(t){t===A.SOLIDUS?this.state=I.SCRIPT_DATA_ESCAPED_END_TAG_OPEN:Zi(t)?(this._emitChars("<"),this.state=I.SCRIPT_DATA_DOUBLE_ESCAPE_START,this._stateScriptDataDoubleEscapeStart(t)):(this._emitChars("<"),this.state=I.SCRIPT_DATA_ESCAPED,this._stateScriptDataEscaped(t))}_stateScriptDataEscapedEndTagOpen(t){Zi(t)?(this.state=I.SCRIPT_DATA_ESCAPED_END_TAG_NAME,this._stateScriptDataEscapedEndTagName(t)):(this._emitChars("</"),this.state=I.SCRIPT_DATA_ESCAPED,this._stateScriptDataEscaped(t))}_stateScriptDataEscapedEndTagName(t){this.handleSpecialEndTag(t)&&(this._emitChars("</"),this.state=I.SCRIPT_DATA_ESCAPED,this._stateScriptDataEscaped(t))}_stateScriptDataDoubleEscapeStart(t){if(this.preprocessor.startsWith(_n.SCRIPT,!1)&&_$(this.preprocessor.peek(_n.SCRIPT.length))){this._emitCodePoint(t);for(let n=0;n<_n.SCRIPT.length;n++)this._emitCodePoint(this._consume());this.state=I.SCRIPT_DATA_DOUBLE_ESCAPED}else this._ensureHibernation()||(this.state=I.SCRIPT_DATA_ESCAPED,this._stateScriptDataEscaped(t))}_stateScriptDataDoubleEscaped(t){switch(t){case A.HYPHEN_MINUS:{this.state=I.SCRIPT_DATA_DOUBLE_ESCAPED_DASH,this._emitChars("-");break}case A.LESS_THAN_SIGN:{this.state=I.SCRIPT_DATA_DOUBLE_ESCAPED_LESS_THAN_SIGN,this._emitChars("<");break}case A.NULL:{this._err(W.unexpectedNullCharacter),this._emitChars(dt);break}case A.EOF:{this._err(W.eofInScriptHtmlCommentLikeText),this._emitEOFToken();break}default:this._emitCodePoint(t)}}_stateScriptDataDoubleEscapedDash(t){switch(t){case A.HYPHEN_MINUS:{this.state=I.SCRIPT_DATA_DOUBLE_ESCAPED_DASH_DASH,this._emitChars("-");break}case A.LESS_THAN_SIGN:{this.state=I.SCRIPT_DATA_DOUBLE_ESCAPED_LESS_THAN_SIGN,this._emitChars("<");break}case A.NULL:{this._err(W.unexpectedNullCharacter),this.state=I.SCRIPT_DATA_DOUBLE_ESCAPED,this._emitChars(dt);break}case A.EOF:{this._err(W.eofInScriptHtmlCommentLikeText),this._emitEOFToken();break}default:this.state=I.SCRIPT_DATA_DOUBLE_ESCAPED,this._emitCodePoint(t)}}_stateScriptDataDoubleEscapedDashDash(t){switch(t){case A.HYPHEN_MINUS:{this._emitChars("-");break}case A.LESS_THAN_SIGN:{this.state=I.SCRIPT_DATA_DOUBLE_ESCAPED_LESS_THAN_SIGN,this._emitChars("<");break}case A.GREATER_THAN_SIGN:{this.state=I.SCRIPT_DATA,this._emitChars(">");break}case A.NULL:{this._err(W.unexpectedNullCharacter),this.state=I.SCRIPT_DATA_DOUBLE_ESCAPED,this._emitChars(dt);break}case A.EOF:{this._err(W.eofInScriptHtmlCommentLikeText),this._emitEOFToken();break}default:this.state=I.SCRIPT_DATA_DOUBLE_ESCAPED,this._emitCodePoint(t)}}_stateScriptDataDoubleEscapedLessThanSign(t){t===A.SOLIDUS?(this.state=I.SCRIPT_DATA_DOUBLE_ESCAPE_END,this._emitChars("/")):(this.state=I.SCRIPT_DATA_DOUBLE_ESCAPED,this._stateScriptDataDoubleEscaped(t))}_stateScriptDataDoubleEscapeEnd(t){if(this.preprocessor.startsWith(_n.SCRIPT,!1)&&_$(this.preprocessor.peek(_n.SCRIPT.length))){this._emitCodePoint(t);for(let n=0;n<_n.SCRIPT.length;n++)this._emitCodePoint(this._consume());this.state=I.SCRIPT_DATA_ESCAPED}else this._ensureHibernation()||(this.state=I.SCRIPT_DATA_DOUBLE_ESCAPED,this._stateScriptDataDoubleEscaped(t))}_stateBeforeAttributeName(t){switch(t){case A.SPACE:case A.LINE_FEED:case A.TABULATION:case A.FORM_FEED:break;case A.SOLIDUS:case A.GREATER_THAN_SIGN:case A.EOF:{this.state=I.AFTER_ATTRIBUTE_NAME,this._stateAfterAttributeName(t);break}case A.EQUALS_SIGN:{this._err(W.unexpectedEqualsSignBeforeAttributeName),this._createAttr("="),this.state=I.ATTRIBUTE_NAME;break}default:this._createAttr(""),this.state=I.ATTRIBUTE_NAME,this._stateAttributeName(t)}}_stateAttributeName(t){switch(t){case A.SPACE:case A.LINE_FEED:case A.TABULATION:case A.FORM_FEED:case A.SOLIDUS:case A.GREATER_THAN_SIGN:case A.EOF:{this._leaveAttrName(),this.state=I.AFTER_ATTRIBUTE_NAME,this._stateAfterAttributeName(t);break}case A.EQUALS_SIGN:{this._leaveAttrName(),this.state=I.BEFORE_ATTRIBUTE_VALUE;break}case A.QUOTATION_MARK:case A.APOSTROPHE:case A.LESS_THAN_SIGN:{this._err(W.unexpectedCharacterInAttributeName),this.currentAttr.name+=String.fromCodePoint(t);break}case A.NULL:{this._err(W.unexpectedNullCharacter),this.currentAttr.name+=dt;break}default:this.currentAttr.name+=String.fromCodePoint(El(t)?Qu(t):t)}}_stateAfterAttributeName(t){switch(t){case A.SPACE:case A.LINE_FEED:case A.TABULATION:case A.FORM_FEED:break;case A.SOLIDUS:{this.state=I.SELF_CLOSING_START_TAG;break}case A.EQUALS_SIGN:{this.state=I.BEFORE_ATTRIBUTE_VALUE;break}case A.GREATER_THAN_SIGN:{this.state=I.DATA,this.emitCurrentTagToken();break}case A.EOF:{this._err(W.eofInTag),this._emitEOFToken();break}default:this._createAttr(""),this.state=I.ATTRIBUTE_NAME,this._stateAttributeName(t)}}_stateBeforeAttributeValue(t){switch(t){case A.SPACE:case A.LINE_FEED:case A.TABULATION:case A.FORM_FEED:break;case A.QUOTATION_MARK:{this.state=I.ATTRIBUTE_VALUE_DOUBLE_QUOTED;break}case A.APOSTROPHE:{this.state=I.ATTRIBUTE_VALUE_SINGLE_QUOTED;break}case A.GREATER_THAN_SIGN:{this._err(W.missingAttributeValue),this.state=I.DATA,this.emitCurrentTagToken();break}default:this.state=I.ATTRIBUTE_VALUE_UNQUOTED,this._stateAttributeValueUnquoted(t)}}_stateAttributeValueDoubleQuoted(t){switch(t){case A.QUOTATION_MARK:{this.state=I.AFTER_ATTRIBUTE_VALUE_QUOTED;break}case A.AMPERSAND:{this._startCharacterReference();break}case A.NULL:{this._err(W.unexpectedNullCharacter),this.currentAttr.value+=dt;break}case A.EOF:{this._err(W.eofInTag),this._emitEOFToken();break}default:this.currentAttr.value+=String.fromCodePoint(t)}}_stateAttributeValueSingleQuoted(t){switch(t){case A.APOSTROPHE:{this.state=I.AFTER_ATTRIBUTE_VALUE_QUOTED;break}case A.AMPERSAND:{this._startCharacterReference();break}case A.NULL:{this._err(W.unexpectedNullCharacter),this.currentAttr.value+=dt;break}case A.EOF:{this._err(W.eofInTag),this._emitEOFToken();break}default:this.currentAttr.value+=String.fromCodePoint(t)}}_stateAttributeValueUnquoted(t){switch(t){case A.SPACE:case A.LINE_FEED:case A.TABULATION:case A.FORM_FEED:{this._leaveAttrValue(),this.state=I.BEFORE_ATTRIBUTE_NAME;break}case A.AMPERSAND:{this._startCharacterReference();break}case A.GREATER_THAN_SIGN:{this._leaveAttrValue(),this.state=I.DATA,this.emitCurrentTagToken();break}case A.NULL:{this._err(W.unexpectedNullCharacter),this.currentAttr.value+=dt;break}case A.QUOTATION_MARK:case A.APOSTROPHE:case A.LESS_THAN_SIGN:case A.EQUALS_SIGN:case A.GRAVE_ACCENT:{this._err(W.unexpectedCharacterInUnquotedAttributeValue),this.currentAttr.value+=String.fromCodePoint(t);break}case A.EOF:{this._err(W.eofInTag),this._emitEOFToken();break}default:this.currentAttr.value+=String.fromCodePoint(t)}}_stateAfterAttributeValueQuoted(t){switch(t){case A.SPACE:case A.LINE_FEED:case A.TABULATION:case A.FORM_FEED:{this._leaveAttrValue(),this.state=I.BEFORE_ATTRIBUTE_NAME;break}case A.SOLIDUS:{this._leaveAttrValue(),this.state=I.SELF_CLOSING_START_TAG;break}case A.GREATER_THAN_SIGN:{this._leaveAttrValue(),this.state=I.DATA,this.emitCurrentTagToken();break}case A.EOF:{this._err(W.eofInTag),this._emitEOFToken();break}default:this._err(W.missingWhitespaceBetweenAttributes),this.state=I.BEFORE_ATTRIBUTE_NAME,this._stateBeforeAttributeName(t)}}_stateSelfClosingStartTag(t){switch(t){case A.GREATER_THAN_SIGN:{const n=this.currentToken;n.selfClosing=!0,this.state=I.DATA,this.emitCurrentTagToken();break}case A.EOF:{this._err(W.eofInTag),this._emitEOFToken();break}default:this._err(W.unexpectedSolidusInTag),this.state=I.BEFORE_ATTRIBUTE_NAME,this._stateBeforeAttributeName(t)}}_stateBogusComment(t){const n=this.currentToken;switch(t){case A.GREATER_THAN_SIGN:{this.state=I.DATA,this.emitCurrentComment(n);break}case A.EOF:{this.emitCurrentComment(n),this._emitEOFToken();break}case A.NULL:{this._err(W.unexpectedNullCharacter),n.data+=dt;break}default:n.data+=String.fromCodePoint(t)}}_stateMarkupDeclarationOpen(t){this._consumeSequenceIfMatch(_n.DASH_DASH,!0)?(this._createCommentToken(_n.DASH_DASH.length+1),this.state=I.COMMENT_START):this._consumeSequenceIfMatch(_n.DOCTYPE,!1)?(this.currentLocation=this.getCurrentLocation(_n.DOCTYPE.length+1),this.state=I.DOCTYPE):this._consumeSequenceIfMatch(_n.CDATA_START,!0)?this.inForeignNode?this.state=I.CDATA_SECTION:(this._err(W.cdataInHtmlContent),this._createCommentToken(_n.CDATA_START.length+1),this.currentToken.data="[CDATA[",this.state=I.BOGUS_COMMENT):this._ensureHibernation()||(this._err(W.incorrectlyOpenedComment),this._createCommentToken(2),this.state=I.BOGUS_COMMENT,this._stateBogusComment(t))}_stateCommentStart(t){switch(t){case A.HYPHEN_MINUS:{this.state=I.COMMENT_START_DASH;break}case A.GREATER_THAN_SIGN:{this._err(W.abruptClosingOfEmptyComment),this.state=I.DATA;const n=this.currentToken;this.emitCurrentComment(n);break}default:this.state=I.COMMENT,this._stateComment(t)}}_stateCommentStartDash(t){const n=this.currentToken;switch(t){case A.HYPHEN_MINUS:{this.state=I.COMMENT_END;break}case A.GREATER_THAN_SIGN:{this._err(W.abruptClosingOfEmptyComment),this.state=I.DATA,this.emitCurrentComment(n);break}case A.EOF:{this._err(W.eofInComment),this.emitCurrentComment(n),this._emitEOFToken();break}default:n.data+="-",this.state=I.COMMENT,this._stateComment(t)}}_stateComment(t){const n=this.currentToken;switch(t){case A.HYPHEN_MINUS:{this.state=I.COMMENT_END_DASH;break}case A.LESS_THAN_SIGN:{n.data+="<",this.state=I.COMMENT_LESS_THAN_SIGN;break}case A.NULL:{this._err(W.unexpectedNullCharacter),n.data+=dt;break}case A.EOF:{this._err(W.eofInComment),this.emitCurrentComment(n),this._emitEOFToken();break}default:n.data+=String.fromCodePoint(t)}}_stateCommentLessThanSign(t){const n=this.currentToken;switch(t){case A.EXCLAMATION_MARK:{n.data+="!",this.state=I.COMMENT_LESS_THAN_SIGN_BANG;break}case A.LESS_THAN_SIGN:{n.data+="<";break}default:this.state=I.COMMENT,this._stateComment(t)}}_stateCommentLessThanSignBang(t){t===A.HYPHEN_MINUS?this.state=I.COMMENT_LESS_THAN_SIGN_BANG_DASH:(this.state=I.COMMENT,this._stateComment(t))}_stateCommentLessThanSignBangDash(t){t===A.HYPHEN_MINUS?this.state=I.COMMENT_LESS_THAN_SIGN_BANG_DASH_DASH:(this.state=I.COMMENT_END_DASH,this._stateCommentEndDash(t))}_stateCommentLessThanSignBangDashDash(t){t!==A.GREATER_THAN_SIGN&&t!==A.EOF&&this._err(W.nestedComment),this.state=I.COMMENT_END,this._stateCommentEnd(t)}_stateCommentEndDash(t){const n=this.currentToken;switch(t){case A.HYPHEN_MINUS:{this.state=I.COMMENT_END;break}case A.EOF:{this._err(W.eofInComment),this.emitCurrentComment(n),this._emitEOFToken();break}default:n.data+="-",this.state=I.COMMENT,this._stateComment(t)}}_stateCommentEnd(t){const n=this.currentToken;switch(t){case A.GREATER_THAN_SIGN:{this.state=I.DATA,this.emitCurrentComment(n);break}case A.EXCLAMATION_MARK:{this.state=I.COMMENT_END_BANG;break}case A.HYPHEN_MINUS:{n.data+="-";break}case A.EOF:{this._err(W.eofInComment),this.emitCurrentComment(n),this._emitEOFToken();break}default:n.data+="--",this.state=I.COMMENT,this._stateComment(t)}}_stateCommentEndBang(t){const n=this.currentToken;switch(t){case A.HYPHEN_MINUS:{n.data+="--!",this.state=I.COMMENT_END_DASH;break}case A.GREATER_THAN_SIGN:{this._err(W.incorrectlyClosedComment),this.state=I.DATA,this.emitCurrentComment(n);break}case A.EOF:{this._err(W.eofInComment),this.emitCurrentComment(n),this._emitEOFToken();break}default:n.data+="--!",this.state=I.COMMENT,this._stateComment(t)}}_stateDoctype(t){switch(t){case A.SPACE:case A.LINE_FEED:case A.TABULATION:case A.FORM_FEED:{this.state=I.BEFORE_DOCTYPE_NAME;break}case A.GREATER_THAN_SIGN:{this.state=I.BEFORE_DOCTYPE_NAME,this._stateBeforeDoctypeName(t);break}case A.EOF:{this._err(W.eofInDoctype),this._createDoctypeToken(null);const n=this.currentToken;n.forceQuirks=!0,this.emitCurrentDoctype(n),this._emitEOFToken();break}default:this._err(W.missingWhitespaceBeforeDoctypeName),this.state=I.BEFORE_DOCTYPE_NAME,this._stateBeforeDoctypeName(t)}}_stateBeforeDoctypeName(t){if(El(t))this._createDoctypeToken(String.fromCharCode(Qu(t))),this.state=I.DOCTYPE_NAME;else switch(t){case A.SPACE:case A.LINE_FEED:case A.TABULATION:case A.FORM_FEED:break;case A.NULL:{this._err(W.unexpectedNullCharacter),this._createDoctypeToken(dt),this.state=I.DOCTYPE_NAME;break}case A.GREATER_THAN_SIGN:{this._err(W.missingDoctypeName),this._createDoctypeToken(null);const n=this.currentToken;n.forceQuirks=!0,this.emitCurrentDoctype(n),this.state=I.DATA;break}case A.EOF:{this._err(W.eofInDoctype),this._createDoctypeToken(null);const n=this.currentToken;n.forceQuirks=!0,this.emitCurrentDoctype(n),this._emitEOFToken();break}default:this._createDoctypeToken(String.fromCodePoint(t)),this.state=I.DOCTYPE_NAME}}_stateDoctypeName(t){const n=this.currentToken;switch(t){case A.SPACE:case A.LINE_FEED:case A.TABULATION:case A.FORM_FEED:{this.state=I.AFTER_DOCTYPE_NAME;break}case A.GREATER_THAN_SIGN:{this.state=I.DATA,this.emitCurrentDoctype(n);break}case A.NULL:{this._err(W.unexpectedNullCharacter),n.name+=dt;break}case A.EOF:{this._err(W.eofInDoctype),n.forceQuirks=!0,this.emitCurrentDoctype(n),this._emitEOFToken();break}default:n.name+=String.fromCodePoint(El(t)?Qu(t):t)}}_stateAfterDoctypeName(t){const n=this.currentToken;switch(t){case A.SPACE:case A.LINE_FEED:case A.TABULATION:case A.FORM_FEED:break;case A.GREATER_THAN_SIGN:{this.state=I.DATA,this.emitCurrentDoctype(n);break}case A.EOF:{this._err(W.eofInDoctype),n.forceQuirks=!0,this.emitCurrentDoctype(n),this._emitEOFToken();break}default:this._consumeSequenceIfMatch(_n.PUBLIC,!1)?this.state=I.AFTER_DOCTYPE_PUBLIC_KEYWORD:this._consumeSequenceIfMatch(_n.SYSTEM,!1)?this.state=I.AFTER_DOCTYPE_SYSTEM_KEYWORD:this._ensureHibernation()||(this._err(W.invalidCharacterSequenceAfterDoctypeName),n.forceQuirks=!0,this.state=I.BOGUS_DOCTYPE,this._stateBogusDoctype(t))}}_stateAfterDoctypePublicKeyword(t){const n=this.currentToken;switch(t){case A.SPACE:case A.LINE_FEED:case A.TABULATION:case A.FORM_FEED:{this.state=I.BEFORE_DOCTYPE_PUBLIC_IDENTIFIER;break}case A.QUOTATION_MARK:{this._err(W.missingWhitespaceAfterDoctypePublicKeyword),n.publicId="",this.state=I.DOCTYPE_PUBLIC_IDENTIFIER_DOUBLE_QUOTED;break}case A.APOSTROPHE:{this._err(W.missingWhitespaceAfterDoctypePublicKeyword),n.publicId="",this.state=I.DOCTYPE_PUBLIC_IDENTIFIER_SINGLE_QUOTED;break}case A.GREATER_THAN_SIGN:{this._err(W.missingDoctypePublicIdentifier),n.forceQuirks=!0,this.state=I.DATA,this.emitCurrentDoctype(n);break}case A.EOF:{this._err(W.eofInDoctype),n.forceQuirks=!0,this.emitCurrentDoctype(n),this._emitEOFToken();break}default:this._err(W.missingQuoteBeforeDoctypePublicIdentifier),n.forceQuirks=!0,this.state=I.BOGUS_DOCTYPE,this._stateBogusDoctype(t)}}_stateBeforeDoctypePublicIdentifier(t){const n=this.currentToken;switch(t){case A.SPACE:case A.LINE_FEED:case A.TABULATION:case A.FORM_FEED:break;case A.QUOTATION_MARK:{n.publicId="",this.state=I.DOCTYPE_PUBLIC_IDENTIFIER_DOUBLE_QUOTED;break}case A.APOSTROPHE:{n.publicId="",this.state=I.DOCTYPE_PUBLIC_IDENTIFIER_SINGLE_QUOTED;break}case A.GREATER_THAN_SIGN:{this._err(W.missingDoctypePublicIdentifier),n.forceQuirks=!0,this.state=I.DATA,this.emitCurrentDoctype(n);break}case A.EOF:{this._err(W.eofInDoctype),n.forceQuirks=!0,this.emitCurrentDoctype(n),this._emitEOFToken();break}default:this._err(W.missingQuoteBeforeDoctypePublicIdentifier),n.forceQuirks=!0,this.state=I.BOGUS_DOCTYPE,this._stateBogusDoctype(t)}}_stateDoctypePublicIdentifierDoubleQuoted(t){const n=this.currentToken;switch(t){case A.QUOTATION_MARK:{this.state=I.AFTER_DOCTYPE_PUBLIC_IDENTIFIER;break}case A.NULL:{this._err(W.unexpectedNullCharacter),n.publicId+=dt;break}case A.GREATER_THAN_SIGN:{this._err(W.abruptDoctypePublicIdentifier),n.forceQuirks=!0,this.emitCurrentDoctype(n),this.state=I.DATA;break}case A.EOF:{this._err(W.eofInDoctype),n.forceQuirks=!0,this.emitCurrentDoctype(n),this._emitEOFToken();break}default:n.publicId+=String.fromCodePoint(t)}}_stateDoctypePublicIdentifierSingleQuoted(t){const n=this.currentToken;switch(t){case A.APOSTROPHE:{this.state=I.AFTER_DOCTYPE_PUBLIC_IDENTIFIER;break}case A.NULL:{this._err(W.unexpectedNullCharacter),n.publicId+=dt;break}case A.GREATER_THAN_SIGN:{this._err(W.abruptDoctypePublicIdentifier),n.forceQuirks=!0,this.emitCurrentDoctype(n),this.state=I.DATA;break}case A.EOF:{this._err(W.eofInDoctype),n.forceQuirks=!0,this.emitCurrentDoctype(n),this._emitEOFToken();break}default:n.publicId+=String.fromCodePoint(t)}}_stateAfterDoctypePublicIdentifier(t){const n=this.currentToken;switch(t){case A.SPACE:case A.LINE_FEED:case A.TABULATION:case A.FORM_FEED:{this.state=I.BETWEEN_DOCTYPE_PUBLIC_AND_SYSTEM_IDENTIFIERS;break}case A.GREATER_THAN_SIGN:{this.state=I.DATA,this.emitCurrentDoctype(n);break}case A.QUOTATION_MARK:{this._err(W.missingWhitespaceBetweenDoctypePublicAndSystemIdentifiers),n.systemId="",this.state=I.DOCTYPE_SYSTEM_IDENTIFIER_DOUBLE_QUOTED;break}case A.APOSTROPHE:{this._err(W.missingWhitespaceBetweenDoctypePublicAndSystemIdentifiers),n.systemId="",this.state=I.DOCTYPE_SYSTEM_IDENTIFIER_SINGLE_QUOTED;break}case A.EOF:{this._err(W.eofInDoctype),n.forceQuirks=!0,this.emitCurrentDoctype(n),this._emitEOFToken();break}default:this._err(W.missingQuoteBeforeDoctypeSystemIdentifier),n.forceQuirks=!0,this.state=I.BOGUS_DOCTYPE,this._stateBogusDoctype(t)}}_stateBetweenDoctypePublicAndSystemIdentifiers(t){const n=this.currentToken;switch(t){case A.SPACE:case A.LINE_FEED:case A.TABULATION:case A.FORM_FEED:break;case A.GREATER_THAN_SIGN:{this.emitCurrentDoctype(n),this.state=I.DATA;break}case A.QUOTATION_MARK:{n.systemId="",this.state=I.DOCTYPE_SYSTEM_IDENTIFIER_DOUBLE_QUOTED;break}case A.APOSTROPHE:{n.systemId="",this.state=I.DOCTYPE_SYSTEM_IDENTIFIER_SINGLE_QUOTED;break}case A.EOF:{this._err(W.eofInDoctype),n.forceQuirks=!0,this.emitCurrentDoctype(n),this._emitEOFToken();break}default:this._err(W.missingQuoteBeforeDoctypeSystemIdentifier),n.forceQuirks=!0,this.state=I.BOGUS_DOCTYPE,this._stateBogusDoctype(t)}}_stateAfterDoctypeSystemKeyword(t){const n=this.currentToken;switch(t){case A.SPACE:case A.LINE_FEED:case A.TABULATION:case A.FORM_FEED:{this.state=I.BEFORE_DOCTYPE_SYSTEM_IDENTIFIER;break}case A.QUOTATION_MARK:{this._err(W.missingWhitespaceAfterDoctypeSystemKeyword),n.systemId="",this.state=I.DOCTYPE_SYSTEM_IDENTIFIER_DOUBLE_QUOTED;break}case A.APOSTROPHE:{this._err(W.missingWhitespaceAfterDoctypeSystemKeyword),n.systemId="",this.state=I.DOCTYPE_SYSTEM_IDENTIFIER_SINGLE_QUOTED;break}case A.GREATER_THAN_SIGN:{this._err(W.missingDoctypeSystemIdentifier),n.forceQuirks=!0,this.state=I.DATA,this.emitCurrentDoctype(n);break}case A.EOF:{this._err(W.eofInDoctype),n.forceQuirks=!0,this.emitCurrentDoctype(n),this._emitEOFToken();break}default:this._err(W.missingQuoteBeforeDoctypeSystemIdentifier),n.forceQuirks=!0,this.state=I.BOGUS_DOCTYPE,this._stateBogusDoctype(t)}}_stateBeforeDoctypeSystemIdentifier(t){const n=this.currentToken;switch(t){case A.SPACE:case A.LINE_FEED:case A.TABULATION:case A.FORM_FEED:break;case A.QUOTATION_MARK:{n.systemId="",this.state=I.DOCTYPE_SYSTEM_IDENTIFIER_DOUBLE_QUOTED;break}case A.APOSTROPHE:{n.systemId="",this.state=I.DOCTYPE_SYSTEM_IDENTIFIER_SINGLE_QUOTED;break}case A.GREATER_THAN_SIGN:{this._err(W.missingDoctypeSystemIdentifier),n.forceQuirks=!0,this.state=I.DATA,this.emitCurrentDoctype(n);break}case A.EOF:{this._err(W.eofInDoctype),n.forceQuirks=!0,this.emitCurrentDoctype(n),this._emitEOFToken();break}default:this._err(W.missingQuoteBeforeDoctypeSystemIdentifier),n.forceQuirks=!0,this.state=I.BOGUS_DOCTYPE,this._stateBogusDoctype(t)}}_stateDoctypeSystemIdentifierDoubleQuoted(t){const n=this.currentToken;switch(t){case A.QUOTATION_MARK:{this.state=I.AFTER_DOCTYPE_SYSTEM_IDENTIFIER;break}case A.NULL:{this._err(W.unexpectedNullCharacter),n.systemId+=dt;break}case A.GREATER_THAN_SIGN:{this._err(W.abruptDoctypeSystemIdentifier),n.forceQuirks=!0,this.emitCurrentDoctype(n),this.state=I.DATA;break}case A.EOF:{this._err(W.eofInDoctype),n.forceQuirks=!0,this.emitCurrentDoctype(n),this._emitEOFToken();break}default:n.systemId+=String.fromCodePoint(t)}}_stateDoctypeSystemIdentifierSingleQuoted(t){const n=this.currentToken;switch(t){case A.APOSTROPHE:{this.state=I.AFTER_DOCTYPE_SYSTEM_IDENTIFIER;break}case A.NULL:{this._err(W.unexpectedNullCharacter),n.systemId+=dt;break}case A.GREATER_THAN_SIGN:{this._err(W.abruptDoctypeSystemIdentifier),n.forceQuirks=!0,this.emitCurrentDoctype(n),this.state=I.DATA;break}case A.EOF:{this._err(W.eofInDoctype),n.forceQuirks=!0,this.emitCurrentDoctype(n),this._emitEOFToken();break}default:n.systemId+=String.fromCodePoint(t)}}_stateAfterDoctypeSystemIdentifier(t){const n=this.currentToken;switch(t){case A.SPACE:case A.LINE_FEED:case A.TABULATION:case A.FORM_FEED:break;case A.GREATER_THAN_SIGN:{this.emitCurrentDoctype(n),this.state=I.DATA;break}case A.EOF:{this._err(W.eofInDoctype),n.forceQuirks=!0,this.emitCurrentDoctype(n),this._emitEOFToken();break}default:this._err(W.unexpectedCharacterAfterDoctypeSystemIdentifier),this.state=I.BOGUS_DOCTYPE,this._stateBogusDoctype(t)}}_stateBogusDoctype(t){const n=this.currentToken;switch(t){case A.GREATER_THAN_SIGN:{this.emitCurrentDoctype(n),this.state=I.DATA;break}case A.NULL:{this._err(W.unexpectedNullCharacter);break}case A.EOF:{this.emitCurrentDoctype(n),this._emitEOFToken();break}}}_stateCdataSection(t){switch(t){case A.RIGHT_SQUARE_BRACKET:{this.state=I.CDATA_SECTION_BRACKET;break}case A.EOF:{this._err(W.eofInCdata),this._emitEOFToken();break}default:this._emitCodePoint(t)}}_stateCdataSectionBracket(t){t===A.RIGHT_SQUARE_BRACKET?this.state=I.CDATA_SECTION_END:(this._emitChars("]"),this.state=I.CDATA_SECTION,this._stateCdataSection(t))}_stateCdataSectionEnd(t){switch(t){case A.GREATER_THAN_SIGN:{this.state=I.DATA;break}case A.RIGHT_SQUARE_BRACKET:{this._emitChars("]");break}default:this._emitChars("]]"),this.state=I.CDATA_SECTION,this._stateCdataSection(t)}}_stateCharacterReference(){let t=this.entityDecoder.write(this.preprocessor.html,this.preprocessor.pos);if(t<0)if(this.preprocessor.lastChunkWritten)t=this.entityDecoder.end();else{this.active=!1,this.preprocessor.pos=this.preprocessor.html.length-1,this.consumedAfterSnapshot=0,this.preprocessor.endOfChunkHit=!0;return}t===0?(this.preprocessor.pos=this.entityStartPos,this._flushCodePointConsumedAsCharacterReference(A.AMPERSAND),this.state=!this._isCharacterReferenceInAttribute()&&y$(this.preprocessor.peek(1))?I.AMBIGUOUS_AMPERSAND:this.returnState):this.state=this.returnState}_stateAmbiguousAmpersand(t){y$(t)?this._flushCodePointConsumedAsCharacterReference(t):(t===A.SEMICOLON&&this._err(W.unknownNamedCharacterReference),this.state=this.returnState,this._callState(t))}}const XT=new Set([f.DD,f.DT,f.LI,f.OPTGROUP,f.OPTION,f.P,f.RB,f.RP,f.RT,f.RTC]),w$=new Set([...XT,f.CAPTION,f.COLGROUP,f.TBODY,f.TD,f.TFOOT,f.TH,f.THEAD,f.TR]),g0=new Set([f.APPLET,f.CAPTION,f.HTML,f.MARQUEE,f.OBJECT,f.TABLE,f.TD,f.TEMPLATE,f.TH]),LY=new Set([...g0,f.OL,f.UL]),PY=new Set([...g0,f.BUTTON]),T$=new Set([f.ANNOTATION_XML,f.MI,f.MN,f.MO,f.MS,f.MTEXT]),E$=new Set([f.DESC,f.FOREIGN_OBJECT,f.TITLE]),IY=new Set([f.TR,f.TEMPLATE,f.HTML]),UY=new Set([f.TBODY,f.TFOOT,f.THEAD,f.TEMPLATE,f.HTML]),DY=new Set([f.TABLE,f.TEMPLATE,f.HTML]),MY=new Set([f.TD,f.TH]);class RY{get currentTmplContentOrNode(){return this._isInTemplate()?this.treeAdapter.getTemplateContent(this.current):this.current}constructor(t,n,r){this.treeAdapter=n,this.handler=r,this.items=[],this.tagIDs=[],this.stackTop=-1,this.tmplCount=0,this.currentTagId=f.UNKNOWN,this.current=t}_indexOf(t){return this.items.lastIndexOf(t,this.stackTop)}_isInTemplate(){return this.currentTagId===f.TEMPLATE&&this.treeAdapter.getNamespaceURI(this.current)===ee.HTML}_updateCurrentElement(){this.current=this.items[this.stackTop],this.currentTagId=this.tagIDs[this.stackTop]}push(t,n){this.stackTop++,this.items[this.stackTop]=t,this.current=t,this.tagIDs[this.stackTop]=n,this.currentTagId=n,this._isInTemplate()&&this.tmplCount++,this.handler.onItemPush(t,n,!0)}pop(){const t=this.current;this.tmplCount>0&&this._isInTemplate()&&this.tmplCount--,this.stackTop--,this._updateCurrentElement(),this.handler.onItemPop(t,!0)}replace(t,n){const r=this._indexOf(t);this.items[r]=n,r===this.stackTop&&(this.current=n)}insertAfter(t,n,r){const i=this._indexOf(t)+1;this.items.splice(i,0,n),this.tagIDs.splice(i,0,r),this.stackTop++,i===this.stackTop&&this._updateCurrentElement(),this.current&&this.currentTagId!==void 0&&this.handler.onItemPush(this.current,this.currentTagId,i===this.stackTop)}popUntilTagNamePopped(t){let n=this.stackTop+1;do n=this.tagIDs.lastIndexOf(t,n-1);while(n>0&&this.treeAdapter.getNamespaceURI(this.items[n])!==ee.HTML);this.shortenToLength(Math.max(n,0))}shortenToLength(t){for(;this.stackTop>=t;){const n=this.current;this.tmplCount>0&&this._isInTemplate()&&(this.tmplCount-=1),this.stackTop--,this._updateCurrentElement(),this.handler.onItemPop(n,this.stackTop<t)}}popUntilElementPopped(t){const n=this._indexOf(t);this.shortenToLength(Math.max(n,0))}popUntilPopped(t,n){const r=this._indexOfTagNames(t,n);this.shortenToLength(Math.max(r,0))}popUntilNumberedHeaderPopped(){this.popUntilPopped(mg,ee.HTML)}popUntilTableCellPopped(){this.popUntilPopped(MY,ee.HTML)}popAllUpToHtmlElement(){this.tmplCount=0,this.shortenToLength(1)}_indexOfTagNames(t,n){for(let r=this.stackTop;r>=0;r--)if(t.has(this.tagIDs[r])&&this.treeAdapter.getNamespaceURI(this.items[r])===n)return r;return-1}clearBackTo(t,n){const r=this._indexOfTagNames(t,n);this.shortenToLength(r+1)}clearBackToTableContext(){this.clearBackTo(DY,ee.HTML)}clearBackToTableBodyContext(){this.clearBackTo(UY,ee.HTML)}clearBackToTableRowContext(){this.clearBackTo(IY,ee.HTML)}remove(t){const n=this._indexOf(t);n>=0&&(n===this.stackTop?this.pop():(this.items.splice(n,1),this.tagIDs.splice(n,1),this.stackTop--,this._updateCurrentElement(),this.handler.onItemPop(t,!1)))}tryPeekProperlyNestedBodyElement(){return this.stackTop>=1&&this.tagIDs[1]===f.BODY?this.items[1]:null}contains(t){return this._indexOf(t)>-1}getCommonAncestor(t){const n=this._indexOf(t)-1;return n>=0?this.items[n]:null}isRootHtmlElementCurrent(){return this.stackTop===0&&this.tagIDs[0]===f.HTML}hasInDynamicScope(t,n){for(let r=this.stackTop;r>=0;r--){const i=this.tagIDs[r];switch(this.treeAdapter.getNamespaceURI(this.items[r])){case ee.HTML:{if(i===t)return!0;if(n.has(i))return!1;break}case ee.SVG:{if(E$.has(i))return!1;break}case ee.MATHML:{if(T$.has(i))return!1;break}}}return!0}hasInScope(t){return this.hasInDynamicScope(t,g0)}hasInListItemScope(t){return this.hasInDynamicScope(t,LY)}hasInButtonScope(t){return this.hasInDynamicScope(t,PY)}hasNumberedHeaderInScope(){for(let t=this.stackTop;t>=0;t--){const n=this.tagIDs[t];switch(this.treeAdapter.getNamespaceURI(this.items[t])){case ee.HTML:{if(mg.has(n))return!0;if(g0.has(n))return!1;break}case ee.SVG:{if(E$.has(n))return!1;break}case ee.MATHML:{if(T$.has(n))return!1;break}}}return!0}hasInTableScope(t){for(let n=this.stackTop;n>=0;n--)if(this.treeAdapter.getNamespaceURI(this.items[n])===ee.HTML)switch(this.tagIDs[n]){case t:return!0;case f.TABLE:case f.HTML:return!1}return!0}hasTableBodyContextInTableScope(){for(let t=this.stackTop;t>=0;t--)if(this.treeAdapter.getNamespaceURI(this.items[t])===ee.HTML)switch(this.tagIDs[t]){case f.TBODY:case f.THEAD:case f.TFOOT:return!0;case f.TABLE:case f.HTML:return!1}return!0}hasInSelectScope(t){for(let n=this.stackTop;n>=0;n--)if(this.treeAdapter.getNamespaceURI(this.items[n])===ee.HTML)switch(this.tagIDs[n]){case t:return!0;case f.OPTION:case f.OPTGROUP:break;default:return!1}return!0}generateImpliedEndTags(){for(;this.currentTagId!==void 0&&XT.has(this.currentTagId);)this.pop()}generateImpliedEndTagsThoroughly(){for(;this.currentTagId!==void 0&&w$.has(this.currentTagId);)this.pop()}generateImpliedEndTagsWithExclusion(t){for(;this.currentTagId!==void 0&&this.currentTagId!==t&&w$.has(this.currentTagId);)this.pop()}}const ip=3;var Yr;(function(e){e[e.Marker=0]="Marker",e[e.Element=1]="Element"})(Yr||(Yr={}));const k$={type:Yr.Marker};class OY{constructor(t){this.treeAdapter=t,this.entries=[],this.bookmark=null}_getNoahArkConditionCandidates(t,n){const r=[],i=n.length,a=this.treeAdapter.getTagName(t),s=this.treeAdapter.getNamespaceURI(t);for(let o=0;o<this.entries.length;o++){const l=this.entries[o];if(l.type===Yr.Marker)break;const{element:c}=l;if(this.treeAdapter.getTagName(c)===a&&this.treeAdapter.getNamespaceURI(c)===s){const u=this.treeAdapter.getAttrList(c);u.length===i&&r.push({idx:o,attrs:u})}}return r}_ensureNoahArkCondition(t){if(this.entries.length<ip)return;const n=this.treeAdapter.getAttrList(t),r=this._getNoahArkConditionCandidates(t,n);if(r.length<ip)return;const i=new Map(n.map(s=>[s.name,s.value]));let a=0;for(let s=0;s<r.length;s++){const o=r[s];o.attrs.every(l=>i.get(l.name)===l.value)&&(a+=1,a>=ip&&this.entries.splice(o.idx,1))}}insertMarker(){this.entries.unshift(k$)}pushElement(t,n){this._ensureNoahArkCondition(t),this.entries.unshift({type:Yr.Element,element:t,token:n})}insertElementAfterBookmark(t,n){const r=this.entries.indexOf(this.bookmark);this.entries.splice(r,0,{type:Yr.Element,element:t,token:n})}removeEntry(t){const n=this.entries.indexOf(t);n!==-1&&this.entries.splice(n,1)}clearToLastMarker(){const t=this.entries.indexOf(k$);t===-1?this.entries.length=0:this.entries.splice(0,t+1)}getElementEntryInScopeWithTagName(t){const n=this.entries.find(r=>r.type===Yr.Marker||this.treeAdapter.getTagName(r.element)===t);return n&&n.type===Yr.Element?n:null}getElementEntry(t){return this.entries.find(n=>n.type===Yr.Element&&n.element===t)}}const Ji={createDocument(){return{nodeName:"#document",mode:ar.NO_QUIRKS,childNodes:[]}},createDocumentFragment(){return{nodeName:"#document-fragment",childNodes:[]}},createElement(e,t,n){return{nodeName:e,tagName:e,attrs:n,namespaceURI:t,childNodes:[],parentNode:null}},createCommentNode(e){return{nodeName:"#comment",data:e,parentNode:null}},createTextNode(e){return{nodeName:"#text",value:e,parentNode:null}},appendChild(e,t){e.childNodes.push(t),t.parentNode=e},insertBefore(e,t,n){const r=e.childNodes.indexOf(n);e.childNodes.splice(r,0,t),t.parentNode=e},setTemplateContent(e,t){e.content=t},getTemplateContent(e){return e.content},setDocumentType(e,t,n,r){const i=e.childNodes.find(a=>a.nodeName==="#documentType");if(i)i.name=t,i.publicId=n,i.systemId=r;else{const a={nodeName:"#documentType",name:t,publicId:n,systemId:r,parentNode:null};Ji.appendChild(e,a)}},setDocumentMode(e,t){e.mode=t},getDocumentMode(e){return e.mode},detachNode(e){if(e.parentNode){const t=e.parentNode.childNodes.indexOf(e);e.parentNode.childNodes.splice(t,1),e.parentNode=null}},insertText(e,t){if(e.childNodes.length>0){const n=e.childNodes[e.childNodes.length-1];if(Ji.isTextNode(n)){n.value+=t;return}}Ji.appendChild(e,Ji.createTextNode(t))},insertTextBefore(e,t,n){const r=e.childNodes[e.childNodes.indexOf(n)-1];r&&Ji.isTextNode(r)?r.value+=t:Ji.insertBefore(e,Ji.createTextNode(t),n)},adoptAttributes(e,t){const n=new Set(e.attrs.map(r=>r.name));for(let r=0;r<t.length;r++)n.has(t[r].name)||e.attrs.push(t[r])},getFirstChild(e){return e.childNodes[0]},getChildNodes(e){return e.childNodes},getParentNode(e){return e.parentNode},getAttrList(e){return e.attrs},getTagName(e){return e.tagName},getNamespaceURI(e){return e.namespaceURI},getTextNodeContent(e){return e.value},getCommentNodeContent(e){return e.data},getDocumentTypeNodeName(e){return e.name},getDocumentTypeNodePublicId(e){return e.publicId},getDocumentTypeNodeSystemId(e){return e.systemId},isTextNode(e){return e.nodeName==="#text"},isCommentNode(e){return e.nodeName==="#comment"},isDocumentTypeNode(e){return e.nodeName==="#documentType"},isElementNode(e){return Object.prototype.hasOwnProperty.call(e,"tagName")},setNodeSourceCodeLocation(e,t){e.sourceCodeLocation=t},getNodeSourceCodeLocation(e){return e.sourceCodeLocation},updateNodeSourceCodeLocation(e,t){e.sourceCodeLocation={...e.sourceCodeLocation,...t}}},QT="html",zY="about:legacy-compat",BY="http://www.ibm.com/data/dtd/v11/ibmxhtml1-transitional.dtd",ZT=["+//silmaril//dtd html pro v0r11 19970101//","-//as//dtd html 3.0 aswedit + extensions//","-//advasoft ltd//dtd html 3.0 aswedit + extensions//","-//ietf//dtd html 2.0 level 1//","-//ietf//dtd html 2.0 level 2//","-//ietf//dtd html 2.0 strict level 1//","-//ietf//dtd html 2.0 strict level 2//","-//ietf//dtd html 2.0 strict//","-//ietf//dtd html 2.0//","-//ietf//dtd html 2.1e//","-//ietf//dtd html 3.0//","-//ietf//dtd html 3.2 final//","-//ietf//dtd html 3.2//","-//ietf//dtd html 3//","-//ietf//dtd html level 0//","-//ietf//dtd html level 1//","-//ietf//dtd html level 2//","-//ietf//dtd html level 3//","-//ietf//dtd html strict level 0//","-//ietf//dtd html strict level 1//","-//ietf//dtd html strict level 2//","-//ietf//dtd html strict level 3//","-//ietf//dtd html strict//","-//ietf//dtd html//","-//metrius//dtd metrius presentational//","-//microsoft//dtd internet explorer 2.0 html strict//","-//microsoft//dtd internet explorer 2.0 html//","-//microsoft//dtd internet explorer 2.0 tables//","-//microsoft//dtd internet explorer 3.0 html strict//","-//microsoft//dtd internet explorer 3.0 html//","-//microsoft//dtd internet explorer 3.0 tables//","-//netscape comm. corp.//dtd html//","-//netscape comm. corp.//dtd strict html//","-//o'reilly and associates//dtd html 2.0//","-//o'reilly and associates//dtd html extended 1.0//","-//o'reilly and associates//dtd html extended relaxed 1.0//","-//sq//dtd html 2.0 hotmetal + extensions//","-//softquad software//dtd hotmetal pro 6.0::19990601::extensions to html 4.0//","-//softquad//dtd hotmetal pro 4.0::19971010::extensions to html 4.0//","-//spyglass//dtd html 2.0 extended//","-//sun microsystems corp.//dtd hotjava html//","-//sun microsystems corp.//dtd hotjava strict html//","-//w3c//dtd html 3 1995-03-24//","-//w3c//dtd html 3.2 draft//","-//w3c//dtd html 3.2 final//","-//w3c//dtd html 3.2//","-//w3c//dtd html 3.2s draft//","-//w3c//dtd html 4.0 frameset//","-//w3c//dtd html 4.0 transitional//","-//w3c//dtd html experimental 19960712//","-//w3c//dtd html experimental 970421//","-//w3c//dtd w3 html//","-//w3o//dtd w3 html 3.0//","-//webtechs//dtd mozilla html 2.0//","-//webtechs//dtd mozilla html//"],FY=[...ZT,"-//w3c//dtd html 4.01 frameset//","-//w3c//dtd html 4.01 transitional//"],jY=new Set(["-//w3o//dtd w3 html strict 3.0//en//","-/w3c/dtd html 4.0 transitional/en","html"]),JT=["-//w3c//dtd xhtml 1.0 frameset//","-//w3c//dtd xhtml 1.0 transitional//"],VY=[...JT,"-//w3c//dtd html 4.01 frameset//","-//w3c//dtd html 4.01 transitional//"];function S$(e,t){return t.some(n=>e.startsWith(n))}function HY(e){return e.name===QT&&e.publicId===null&&(e.systemId===null||e.systemId===zY)}function qY(e){if(e.name!==QT)return ar.QUIRKS;const{systemId:t}=e;if(t&&t.toLowerCase()===BY)return ar.QUIRKS;let{publicId:n}=e;if(n!==null){if(n=n.toLowerCase(),jY.has(n))return ar.QUIRKS;let r=t===null?FY:ZT;if(S$(n,r))return ar.QUIRKS;if(r=t===null?JT:VY,S$(n,r))return ar.LIMITED_QUIRKS}return ar.NO_QUIRKS}const N$={TEXT_HTML:"text/html",APPLICATION_XML:"application/xhtml+xml"},GY="definitionurl",WY="definitionURL",KY=new Map(["attributeName","attributeType","baseFrequency","baseProfile","calcMode","clipPathUnits","diffuseConstant","edgeMode","filterUnits","glyphRef","gradientTransform","gradientUnits","kernelMatrix","kernelUnitLength","keyPoints","keySplines","keyTimes","lengthAdjust","limitingConeAngle","markerHeight","markerUnits","markerWidth","maskContentUnits","maskUnits","numOctaves","pathLength","patternContentUnits","patternTransform","patternUnits","pointsAtX","pointsAtY","pointsAtZ","preserveAlpha","preserveAspectRatio","primitiveUnits","refX","refY","repeatCount","repeatDur","requiredExtensions","requiredFeatures","specularConstant","specularExponent","spreadMethod","startOffset","stdDeviation","stitchTiles","surfaceScale","systemLanguage","tableValues","targetX","targetY","textLength","viewBox","viewTarget","xChannelSelector","yChannelSelector","zoomAndPan"].map(e=>[e.toLowerCase(),e])),YY=new Map([["xlink:actuate",{prefix:"xlink",name:"actuate",namespace:ee.XLINK}],["xlink:arcrole",{prefix:"xlink",name:"arcrole",namespace:ee.XLINK}],["xlink:href",{prefix:"xlink",name:"href",namespace:ee.XLINK}],["xlink:role",{prefix:"xlink",name:"role",namespace:ee.XLINK}],["xlink:show",{prefix:"xlink",name:"show",namespace:ee.XLINK}],["xlink:title",{prefix:"xlink",name:"title",namespace:ee.XLINK}],["xlink:type",{prefix:"xlink",name:"type",namespace:ee.XLINK}],["xml:lang",{prefix:"xml",name:"lang",namespace:ee.XML}],["xml:space",{prefix:"xml",name:"space",namespace:ee.XML}],["xmlns",{prefix:"",name:"xmlns",namespace:ee.XMLNS}],["xmlns:xlink",{prefix:"xmlns",name:"xlink",namespace:ee.XMLNS}]]),XY=new Map(["altGlyph","altGlyphDef","altGlyphItem","animateColor","animateMotion","animateTransform","clipPath","feBlend","feColorMatrix","feComponentTransfer","feComposite","feConvolveMatrix","feDiffuseLighting","feDisplacementMap","feDistantLight","feFlood","feFuncA","feFuncB","feFuncG","feFuncR","feGaussianBlur","feImage","feMerge","feMergeNode","feMorphology","feOffset","fePointLight","feSpecularLighting","feSpotLight","feTile","feTurbulence","foreignObject","glyphRef","linearGradient","radialGradient","textPath"].map(e=>[e.toLowerCase(),e])),QY=new Set([f.B,f.BIG,f.BLOCKQUOTE,f.BODY,f.BR,f.CENTER,f.CODE,f.DD,f.DIV,f.DL,f.DT,f.EM,f.EMBED,f.H1,f.H2,f.H3,f.H4,f.H5,f.H6,f.HEAD,f.HR,f.I,f.IMG,f.LI,f.LISTING,f.MENU,f.META,f.NOBR,f.OL,f.P,f.PRE,f.RUBY,f.S,f.SMALL,f.SPAN,f.STRONG,f.STRIKE,f.SUB,f.SUP,f.TABLE,f.TT,f.U,f.UL,f.VAR]);function ZY(e){const t=e.tagID;return t===f.FONT&&e.attrs.some(({name:r})=>r===us.COLOR||r===us.SIZE||r===us.FACE)||QY.has(t)}function eE(e){for(let t=0;t<e.attrs.length;t++)if(e.attrs[t].name===GY){e.attrs[t].name=WY;break}}function tE(e){for(let t=0;t<e.attrs.length;t++){const n=KY.get(e.attrs[t].name);n!=null&&(e.attrs[t].name=n)}}function lb(e){for(let t=0;t<e.attrs.length;t++){const n=YY.get(e.attrs[t].name);n&&(e.attrs[t].prefix=n.prefix,e.attrs[t].name=n.name,e.attrs[t].namespace=n.namespace)}}function JY(e){const t=XY.get(e.tagName);t!=null&&(e.tagName=t,e.tagID=tl(e.tagName))}function eX(e,t){return t===ee.MATHML&&(e===f.MI||e===f.MO||e===f.MN||e===f.MS||e===f.MTEXT)}function tX(e,t,n){if(t===ee.MATHML&&e===f.ANNOTATION_XML){for(let r=0;r<n.length;r++)if(n[r].name===us.ENCODING){const i=n[r].value.toLowerCase();return i===N$.TEXT_HTML||i===N$.APPLICATION_XML}}return t===ee.SVG&&(e===f.FOREIGN_OBJECT||e===f.DESC||e===f.TITLE)}function nX(e,t,n,r){return(!r||r===ee.HTML)&&tX(e,t,n)||(!r||r===ee.MATHML)&&eX(e,t)}const rX="hidden",iX=8,aX=3;var O;(function(e){e[e.INITIAL=0]="INITIAL",e[e.BEFORE_HTML=1]="BEFORE_HTML",e[e.BEFORE_HEAD=2]="BEFORE_HEAD",e[e.IN_HEAD=3]="IN_HEAD",e[e.IN_HEAD_NO_SCRIPT=4]="IN_HEAD_NO_SCRIPT",e[e.AFTER_HEAD=5]="AFTER_HEAD",e[e.IN_BODY=6]="IN_BODY",e[e.TEXT=7]="TEXT",e[e.IN_TABLE=8]="IN_TABLE",e[e.IN_TABLE_TEXT=9]="IN_TABLE_TEXT",e[e.IN_CAPTION=10]="IN_CAPTION",e[e.IN_COLUMN_GROUP=11]="IN_COLUMN_GROUP",e[e.IN_TABLE_BODY=12]="IN_TABLE_BODY",e[e.IN_ROW=13]="IN_ROW",e[e.IN_CELL=14]="IN_CELL",e[e.IN_SELECT=15]="IN_SELECT",e[e.IN_SELECT_IN_TABLE=16]="IN_SELECT_IN_TABLE",e[e.IN_TEMPLATE=17]="IN_TEMPLATE",e[e.AFTER_BODY=18]="AFTER_BODY",e[e.IN_FRAMESET=19]="IN_FRAMESET",e[e.AFTER_FRAMESET=20]="AFTER_FRAMESET",e[e.AFTER_AFTER_BODY=21]="AFTER_AFTER_BODY",e[e.AFTER_AFTER_FRAMESET=22]="AFTER_AFTER_FRAMESET"})(O||(O={}));const sX={startLine:-1,startCol:-1,startOffset:-1,endLine:-1,endCol:-1,endOffset:-1},nE=new Set([f.TABLE,f.TBODY,f.TFOOT,f.THEAD,f.TR]),C$={scriptingEnabled:!0,sourceCodeLocationInfo:!1,treeAdapter:Ji,onParseError:null};class A${constructor(t,n,r=null,i=null){this.fragmentContext=r,this.scriptHandler=i,this.currentToken=null,this.stopped=!1,this.insertionMode=O.INITIAL,this.originalInsertionMode=O.INITIAL,this.headElement=null,this.formElement=null,this.currentNotInHTML=!1,this.tmplInsertionModeStack=[],this.pendingCharacterTokens=[],this.hasNonWhitespacePendingCharacterToken=!1,this.framesetOk=!0,this.skipNextNewLine=!1,this.fosterParentingEnabled=!1,this.options={...C$,...t},this.treeAdapter=this.options.treeAdapter,this.onParseError=this.options.onParseError,this.onParseError&&(this.options.sourceCodeLocationInfo=!0),this.document=n??this.treeAdapter.createDocument(),this.tokenizer=new AY(this.options,this),this.activeFormattingElements=new OY(this.treeAdapter),this.fragmentContextID=r?tl(this.treeAdapter.getTagName(r)):f.UNKNOWN,this._setContextModes(r??this.document,this.fragmentContextID),this.openElements=new RY(this.document,this.treeAdapter,this)}static parse(t,n){const r=new this(n);return r.tokenizer.write(t,!0),r.document}static getFragmentParser(t,n){const r={...C$,...n};t??(t=r.treeAdapter.createElement(j.TEMPLATE,ee.HTML,[]));const i=r.treeAdapter.createElement("documentmock",ee.HTML,[]),a=new this(r,i,t);return a.fragmentContextID===f.TEMPLATE&&a.tmplInsertionModeStack.unshift(O.IN_TEMPLATE),a._initTokenizerForFragmentParsing(),a._insertFakeRootElement(),a._resetInsertionMode(),a._findFormInFragmentContext(),a}getFragment(){const t=this.treeAdapter.getFirstChild(this.document),n=this.treeAdapter.createDocumentFragment();return this._adoptNodes(t,n),n}_err(t,n,r){var i;if(!this.onParseError)return;const a=(i=t.location)!==null&&i!==void 0?i:sX,s={code:n,startLine:a.startLine,startCol:a.startCol,startOffset:a.startOffset,endLine:r?a.startLine:a.endLine,endCol:r?a.startCol:a.endCol,endOffset:r?a.startOffset:a.endOffset};this.onParseError(s)}onItemPush(t,n,r){var i,a;(a=(i=this.treeAdapter).onItemPush)===null||a===void 0||a.call(i,t),r&&this.openElements.stackTop>0&&this._setContextModes(t,n)}onItemPop(t,n){var r,i;if(this.options.sourceCodeLocationInfo&&this._setEndLocation(t,this.currentToken),(i=(r=this.treeAdapter).onItemPop)===null||i===void 0||i.call(r,t,this.openElements.current),n){let a,s;this.openElements.stackTop===0&&this.fragmentContext?(a=this.fragmentContext,s=this.fragmentContextID):{current:a,currentTagId:s}=this.openElements,this._setContextModes(a,s)}}_setContextModes(t,n){const r=t===this.document||t&&this.treeAdapter.getNamespaceURI(t)===ee.HTML;this.currentNotInHTML=!r,this.tokenizer.inForeignNode=!r&&t!==void 0&&n!==void 0&&!this._isIntegrationPoint(n,t)}_switchToTextParsing(t,n){this._insertElement(t,ee.HTML),this.tokenizer.state=n,this.originalInsertionMode=this.insertionMode,this.insertionMode=O.TEXT}switchToPlaintextParsing(){this.insertionMode=O.TEXT,this.originalInsertionMode=O.IN_BODY,this.tokenizer.state=Ct.PLAINTEXT}_getAdjustedCurrentElement(){return this.openElements.stackTop===0&&this.fragmentContext?this.fragmentContext:this.openElements.current}_findFormInFragmentContext(){let t=this.fragmentContext;for(;t;){if(this.treeAdapter.getTagName(t)===j.FORM){this.formElement=t;break}t=this.treeAdapter.getParentNode(t)}}_initTokenizerForFragmentParsing(){if(!(!this.fragmentContext||this.treeAdapter.getNamespaceURI(this.fragmentContext)!==ee.HTML))switch(this.fragmentContextID){case f.TITLE:case f.TEXTAREA:{this.tokenizer.state=Ct.RCDATA;break}case f.STYLE:case f.XMP:case f.IFRAME:case f.NOEMBED:case f.NOFRAMES:case f.NOSCRIPT:{this.tokenizer.state=Ct.RAWTEXT;break}case f.SCRIPT:{this.tokenizer.state=Ct.SCRIPT_DATA;break}case f.PLAINTEXT:{this.tokenizer.state=Ct.PLAINTEXT;break}}}_setDocumentType(t){const n=t.name||"",r=t.publicId||"",i=t.systemId||"";if(this.treeAdapter.setDocumentType(this.document,n,r,i),t.location){const s=this.treeAdapter.getChildNodes(this.document).find(o=>this.treeAdapter.isDocumentTypeNode(o));s&&this.treeAdapter.setNodeSourceCodeLocation(s,t.location)}}_attachElementToTree(t,n){if(this.options.sourceCodeLocationInfo){const r=n&&{...n,startTag:n};this.treeAdapter.setNodeSourceCodeLocation(t,r)}if(this._shouldFosterParentOnInsertion())this._fosterParentElement(t);else{const r=this.openElements.currentTmplContentOrNode;this.treeAdapter.appendChild(r??this.document,t)}}_appendElement(t,n){const r=this.treeAdapter.createElement(t.tagName,n,t.attrs);this._attachElementToTree(r,t.location)}_insertElement(t,n){const r=this.treeAdapter.createElement(t.tagName,n,t.attrs);this._attachElementToTree(r,t.location),this.openElements.push(r,t.tagID)}_insertFakeElement(t,n){const r=this.treeAdapter.createElement(t,ee.HTML,[]);this._attachElementToTree(r,null),this.openElements.push(r,n)}_insertTemplate(t){const n=this.treeAdapter.createElement(t.tagName,ee.HTML,t.attrs),r=this.treeAdapter.createDocumentFragment();this.treeAdapter.setTemplateContent(n,r),this._attachElementToTree(n,t.location),this.openElements.push(n,t.tagID),this.options.sourceCodeLocationInfo&&this.treeAdapter.setNodeSourceCodeLocation(r,null)}_insertFakeRootElement(){const t=this.treeAdapter.createElement(j.HTML,ee.HTML,[]);this.options.sourceCodeLocationInfo&&this.treeAdapter.setNodeSourceCodeLocation(t,null),this.treeAdapter.appendChild(this.openElements.current,t),this.openElements.push(t,f.HTML)}_appendCommentNode(t,n){const r=this.treeAdapter.createCommentNode(t.data);this.treeAdapter.appendChild(n,r),this.options.sourceCodeLocationInfo&&this.treeAdapter.setNodeSourceCodeLocation(r,t.location)}_insertCharacters(t){let n,r;if(this._shouldFosterParentOnInsertion()?({parent:n,beforeElement:r}=this._findFosterParentingLocation(),r?this.treeAdapter.insertTextBefore(n,t.chars,r):this.treeAdapter.insertText(n,t.chars)):(n=this.openElements.currentTmplContentOrNode,this.treeAdapter.insertText(n,t.chars)),!t.location)return;const i=this.treeAdapter.getChildNodes(n),a=r?i.lastIndexOf(r):i.length,s=i[a-1];if(this.treeAdapter.getNodeSourceCodeLocation(s)){const{endLine:l,endCol:c,endOffset:u}=t.location;this.treeAdapter.updateNodeSourceCodeLocation(s,{endLine:l,endCol:c,endOffset:u})}else this.options.sourceCodeLocationInfo&&this.treeAdapter.setNodeSourceCodeLocation(s,t.location)}_adoptNodes(t,n){for(let r=this.treeAdapter.getFirstChild(t);r;r=this.treeAdapter.getFirstChild(t))this.treeAdapter.detachNode(r),this.treeAdapter.appendChild(n,r)}_setEndLocation(t,n){if(this.treeAdapter.getNodeSourceCodeLocation(t)&&n.location){const r=n.location,i=this.treeAdapter.getTagName(t),a=n.type===ze.END_TAG&&i===n.tagName?{endTag:{...r},endLine:r.endLine,endCol:r.endCol,endOffset:r.endOffset}:{endLine:r.startLine,endCol:r.startCol,endOffset:r.startOffset};this.treeAdapter.updateNodeSourceCodeLocation(t,a)}}shouldProcessStartTagTokenInForeignContent(t){if(!this.currentNotInHTML)return!1;let n,r;return this.openElements.stackTop===0&&this.fragmentContext?(n=this.fragmentContext,r=this.fragmentContextID):{current:n,currentTagId:r}=this.openElements,t.tagID===f.SVG&&this.treeAdapter.getTagName(n)===j.ANNOTATION_XML&&this.treeAdapter.getNamespaceURI(n)===ee.MATHML?!1:this.tokenizer.inForeignNode||(t.tagID===f.MGLYPH||t.tagID===f.MALIGNMARK)&&r!==void 0&&!this._isIntegrationPoint(r,n,ee.HTML)}_processToken(t){switch(t.type){case ze.CHARACTER:{this.onCharacter(t);break}case ze.NULL_CHARACTER:{this.onNullCharacter(t);break}case ze.COMMENT:{this.onComment(t);break}case ze.DOCTYPE:{this.onDoctype(t);break}case ze.START_TAG:{this._processStartTag(t);break}case ze.END_TAG:{this.onEndTag(t);break}case ze.EOF:{this.onEof(t);break}case ze.WHITESPACE_CHARACTER:{this.onWhitespaceCharacter(t);break}}}_isIntegrationPoint(t,n,r){const i=this.treeAdapter.getNamespaceURI(n),a=this.treeAdapter.getAttrList(n);return nX(t,i,a,r)}_reconstructActiveFormattingElements(){const t=this.activeFormattingElements.entries.length;if(t){const n=this.activeFormattingElements.entries.findIndex(i=>i.type===Yr.Marker||this.openElements.contains(i.element)),r=n===-1?t-1:n-1;for(let i=r;i>=0;i--){const a=this.activeFormattingElements.entries[i];this._insertElement(a.token,this.treeAdapter.getNamespaceURI(a.element)),a.element=this.openElements.current}}}_closeTableCell(){this.openElements.generateImpliedEndTags(),this.openElements.popUntilTableCellPopped(),this.activeFormattingElements.clearToLastMarker(),this.insertionMode=O.IN_ROW}_closePElement(){this.openElements.generateImpliedEndTagsWithExclusion(f.P),this.openElements.popUntilTagNamePopped(f.P)}_resetInsertionMode(){for(let t=this.openElements.stackTop;t>=0;t--)switch(t===0&&this.fragmentContext?this.fragmentContextID:this.openElements.tagIDs[t]){case f.TR:{this.insertionMode=O.IN_ROW;return}case f.TBODY:case f.THEAD:case f.TFOOT:{this.insertionMode=O.IN_TABLE_BODY;return}case f.CAPTION:{this.insertionMode=O.IN_CAPTION;return}case f.COLGROUP:{this.insertionMode=O.IN_COLUMN_GROUP;return}case f.TABLE:{this.insertionMode=O.IN_TABLE;return}case f.BODY:{this.insertionMode=O.IN_BODY;return}case f.FRAMESET:{this.insertionMode=O.IN_FRAMESET;return}case f.SELECT:{this._resetInsertionModeForSelect(t);return}case f.TEMPLATE:{this.insertionMode=this.tmplInsertionModeStack[0];return}case f.HTML:{this.insertionMode=this.headElement?O.AFTER_HEAD:O.BEFORE_HEAD;return}case f.TD:case f.TH:{if(t>0){this.insertionMode=O.IN_CELL;return}break}case f.HEAD:{if(t>0){this.insertionMode=O.IN_HEAD;return}break}}this.insertionMode=O.IN_BODY}_resetInsertionModeForSelect(t){if(t>0)for(let n=t-1;n>0;n--){const r=this.openElements.tagIDs[n];if(r===f.TEMPLATE)break;if(r===f.TABLE){this.insertionMode=O.IN_SELECT_IN_TABLE;return}}this.insertionMode=O.IN_SELECT}_isElementCausesFosterParenting(t){return nE.has(t)}_shouldFosterParentOnInsertion(){return this.fosterParentingEnabled&&this.openElements.currentTagId!==void 0&&this._isElementCausesFosterParenting(this.openElements.currentTagId)}_findFosterParentingLocation(){for(let t=this.openElements.stackTop;t>=0;t--){const n=this.openElements.items[t];switch(this.openElements.tagIDs[t]){case f.TEMPLATE:{if(this.treeAdapter.getNamespaceURI(n)===ee.HTML)return{parent:this.treeAdapter.getTemplateContent(n),beforeElement:null};break}case f.TABLE:{const r=this.treeAdapter.getParentNode(n);return r?{parent:r,beforeElement:n}:{parent:this.openElements.items[t-1],beforeElement:null}}}}return{parent:this.openElements.items[0],beforeElement:null}}_fosterParentElement(t){const n=this._findFosterParentingLocation();n.beforeElement?this.treeAdapter.insertBefore(n.parent,t,n.beforeElement):this.treeAdapter.appendChild(n.parent,t)}_isSpecialElement(t,n){const r=this.treeAdapter.getNamespaceURI(t);return kY[r].has(n)}onCharacter(t){if(this.skipNextNewLine=!1,this.tokenizer.inForeignNode){MQ(this,t);return}switch(this.insertionMode){case O.INITIAL:{gl(this,t);break}case O.BEFORE_HTML:{Gl(this,t);break}case O.BEFORE_HEAD:{Wl(this,t);break}case O.IN_HEAD:{Kl(this,t);break}case O.IN_HEAD_NO_SCRIPT:{Yl(this,t);break}case O.AFTER_HEAD:{Xl(this,t);break}case O.IN_BODY:case O.IN_CAPTION:case O.IN_CELL:case O.IN_TEMPLATE:{iE(this,t);break}case O.TEXT:case O.IN_SELECT:case O.IN_SELECT_IN_TABLE:{this._insertCharacters(t);break}case O.IN_TABLE:case O.IN_TABLE_BODY:case O.IN_ROW:{ap(this,t);break}case O.IN_TABLE_TEXT:{uE(this,t);break}case O.IN_COLUMN_GROUP:{b0(this,t);break}case O.AFTER_BODY:{v0(this,t);break}case O.AFTER_AFTER_BODY:{Ed(this,t);break}}}onNullCharacter(t){if(this.skipNextNewLine=!1,this.tokenizer.inForeignNode){DQ(this,t);return}switch(this.insertionMode){case O.INITIAL:{gl(this,t);break}case O.BEFORE_HTML:{Gl(this,t);break}case O.BEFORE_HEAD:{Wl(this,t);break}case O.IN_HEAD:{Kl(this,t);break}case O.IN_HEAD_NO_SCRIPT:{Yl(this,t);break}case O.AFTER_HEAD:{Xl(this,t);break}case O.TEXT:{this._insertCharacters(t);break}case O.IN_TABLE:case O.IN_TABLE_BODY:case O.IN_ROW:{ap(this,t);break}case O.IN_COLUMN_GROUP:{b0(this,t);break}case O.AFTER_BODY:{v0(this,t);break}case O.AFTER_AFTER_BODY:{Ed(this,t);break}}}onComment(t){if(this.skipNextNewLine=!1,this.currentNotInHTML){hg(this,t);return}switch(this.insertionMode){case O.INITIAL:case O.BEFORE_HTML:case O.BEFORE_HEAD:case O.IN_HEAD:case O.IN_HEAD_NO_SCRIPT:case O.AFTER_HEAD:case O.IN_BODY:case O.IN_TABLE:case O.IN_CAPTION:case O.IN_COLUMN_GROUP:case O.IN_TABLE_BODY:case O.IN_ROW:case O.IN_CELL:case O.IN_SELECT:case O.IN_SELECT_IN_TABLE:case O.IN_TEMPLATE:case O.IN_FRAMESET:case O.AFTER_FRAMESET:{hg(this,t);break}case O.IN_TABLE_TEXT:{bl(this,t);break}case O.AFTER_BODY:{hX(this,t);break}case O.AFTER_AFTER_BODY:case O.AFTER_AFTER_FRAMESET:{pX(this,t);break}}}onDoctype(t){switch(this.skipNextNewLine=!1,this.insertionMode){case O.INITIAL:{fX(this,t);break}case O.BEFORE_HEAD:case O.IN_HEAD:case O.IN_HEAD_NO_SCRIPT:case O.AFTER_HEAD:{this._err(t,W.misplacedDoctype);break}case O.IN_TABLE_TEXT:{bl(this,t);break}}}onStartTag(t){this.skipNextNewLine=!1,this.currentToken=t,this._processStartTag(t),t.selfClosing&&!t.ackSelfClosing&&this._err(t,W.nonVoidHtmlElementStartTagWithTrailingSolidus)}_processStartTag(t){this.shouldProcessStartTagTokenInForeignContent(t)?RQ(this,t):this._startTagOutsideForeignContent(t)}_startTagOutsideForeignContent(t){switch(this.insertionMode){case O.INITIAL:{gl(this,t);break}case O.BEFORE_HTML:{gX(this,t);break}case O.BEFORE_HEAD:{vX(this,t);break}case O.IN_HEAD:{Ir(this,t);break}case O.IN_HEAD_NO_SCRIPT:{yX(this,t);break}case O.AFTER_HEAD:{wX(this,t);break}case O.IN_BODY:{ln(this,t);break}case O.IN_TABLE:{Mo(this,t);break}case O.IN_TABLE_TEXT:{bl(this,t);break}case O.IN_CAPTION:{xQ(this,t);break}case O.IN_COLUMN_GROUP:{db(this,t);break}case O.IN_TABLE_BODY:{Tm(this,t);break}case O.IN_ROW:{Em(this,t);break}case O.IN_CELL:{_Q(this,t);break}case O.IN_SELECT:{hE(this,t);break}case O.IN_SELECT_IN_TABLE:{TQ(this,t);break}case O.IN_TEMPLATE:{kQ(this,t);break}case O.AFTER_BODY:{NQ(this,t);break}case O.IN_FRAMESET:{CQ(this,t);break}case O.AFTER_FRAMESET:{LQ(this,t);break}case O.AFTER_AFTER_BODY:{IQ(this,t);break}case O.AFTER_AFTER_FRAMESET:{UQ(this,t);break}}}onEndTag(t){this.skipNextNewLine=!1,this.currentToken=t,this.currentNotInHTML?OQ(this,t):this._endTagOutsideForeignContent(t)}_endTagOutsideForeignContent(t){switch(this.insertionMode){case O.INITIAL:{gl(this,t);break}case O.BEFORE_HTML:{bX(this,t);break}case O.BEFORE_HEAD:{xX(this,t);break}case O.IN_HEAD:{$X(this,t);break}case O.IN_HEAD_NO_SCRIPT:{_X(this,t);break}case O.AFTER_HEAD:{TX(this,t);break}case O.IN_BODY:{wm(this,t);break}case O.TEXT:{cQ(this,t);break}case O.IN_TABLE:{Uc(this,t);break}case O.IN_TABLE_TEXT:{bl(this,t);break}case O.IN_CAPTION:{$Q(this,t);break}case O.IN_COLUMN_GROUP:{yQ(this,t);break}case O.IN_TABLE_BODY:{pg(this,t);break}case O.IN_ROW:{mE(this,t);break}case O.IN_CELL:{wQ(this,t);break}case O.IN_SELECT:{pE(this,t);break}case O.IN_SELECT_IN_TABLE:{EQ(this,t);break}case O.IN_TEMPLATE:{SQ(this,t);break}case O.AFTER_BODY:{gE(this,t);break}case O.IN_FRAMESET:{AQ(this,t);break}case O.AFTER_FRAMESET:{PQ(this,t);break}case O.AFTER_AFTER_BODY:{Ed(this,t);break}}}onEof(t){switch(this.insertionMode){case O.INITIAL:{gl(this,t);break}case O.BEFORE_HTML:{Gl(this,t);break}case O.BEFORE_HEAD:{Wl(this,t);break}case O.IN_HEAD:{Kl(this,t);break}case O.IN_HEAD_NO_SCRIPT:{Yl(this,t);break}case O.AFTER_HEAD:{Xl(this,t);break}case O.IN_BODY:case O.IN_TABLE:case O.IN_CAPTION:case O.IN_COLUMN_GROUP:case O.IN_TABLE_BODY:case O.IN_ROW:case O.IN_CELL:case O.IN_SELECT:case O.IN_SELECT_IN_TABLE:{lE(this,t);break}case O.TEXT:{uQ(this,t);break}case O.IN_TABLE_TEXT:{bl(this,t);break}case O.IN_TEMPLATE:{fE(this,t);break}case O.AFTER_BODY:case O.IN_FRAMESET:case O.AFTER_FRAMESET:case O.AFTER_AFTER_BODY:case O.AFTER_AFTER_FRAMESET:{ub(this,t);break}}}onWhitespaceCharacter(t){if(this.skipNextNewLine&&(this.skipNextNewLine=!1,t.chars.charCodeAt(0)===A.LINE_FEED)){if(t.chars.length===1)return;t.chars=t.chars.substr(1)}if(this.tokenizer.inForeignNode){this._insertCharacters(t);return}switch(this.insertionMode){case O.IN_HEAD:case O.IN_HEAD_NO_SCRIPT:case O.AFTER_HEAD:case O.TEXT:case O.IN_COLUMN_GROUP:case O.IN_SELECT:case O.IN_SELECT_IN_TABLE:case O.IN_FRAMESET:case O.AFTER_FRAMESET:{this._insertCharacters(t);break}case O.IN_BODY:case O.IN_CAPTION:case O.IN_CELL:case O.IN_TEMPLATE:case O.AFTER_BODY:case O.AFTER_AFTER_BODY:case O.AFTER_AFTER_FRAMESET:{rE(this,t);break}case O.IN_TABLE:case O.IN_TABLE_BODY:case O.IN_ROW:{ap(this,t);break}case O.IN_TABLE_TEXT:{cE(this,t);break}}}}function oX(e,t){let n=e.activeFormattingElements.getElementEntryInScopeWithTagName(t.tagName);return n?e.openElements.contains(n.element)?e.openElements.hasInScope(t.tagID)||(n=null):(e.activeFormattingElements.removeEntry(n),n=null):oE(e,t),n}function lX(e,t){let n=null,r=e.openElements.stackTop;for(;r>=0;r--){const i=e.openElements.items[r];if(i===t.element)break;e._isSpecialElement(i,e.openElements.tagIDs[r])&&(n=i)}return n||(e.openElements.shortenToLength(Math.max(r,0)),e.activeFormattingElements.removeEntry(t)),n}function cX(e,t,n){let r=t,i=e.openElements.getCommonAncestor(t);for(let a=0,s=i;s!==n;a++,s=i){i=e.openElements.getCommonAncestor(s);const o=e.activeFormattingElements.getElementEntry(s),l=o&&a>=aX;!o||l?(l&&e.activeFormattingElements.removeEntry(o),e.openElements.remove(s)):(s=uX(e,o),r===t&&(e.activeFormattingElements.bookmark=o),e.treeAdapter.detachNode(r),e.treeAdapter.appendChild(s,r),r=s)}return r}function uX(e,t){const n=e.treeAdapter.getNamespaceURI(t.element),r=e.treeAdapter.createElement(t.token.tagName,n,t.token.attrs);return e.openElements.replace(t.element,r),t.element=r,r}function dX(e,t,n){const r=e.treeAdapter.getTagName(t),i=tl(r);if(e._isElementCausesFosterParenting(i))e._fosterParentElement(n);else{const a=e.treeAdapter.getNamespaceURI(t);i===f.TEMPLATE&&a===ee.HTML&&(t=e.treeAdapter.getTemplateContent(t)),e.treeAdapter.appendChild(t,n)}}function mX(e,t,n){const r=e.treeAdapter.getNamespaceURI(n.element),{token:i}=n,a=e.treeAdapter.createElement(i.tagName,r,i.attrs);e._adoptNodes(t,a),e.treeAdapter.appendChild(t,a),e.activeFormattingElements.insertElementAfterBookmark(a,i),e.activeFormattingElements.removeEntry(n),e.openElements.remove(n.element),e.openElements.insertAfter(t,a,i.tagID)}function cb(e,t){for(let n=0;n<iX;n++){const r=oX(e,t);if(!r)break;const i=lX(e,r);if(!i)break;e.activeFormattingElements.bookmark=r;const a=cX(e,i,r.element),s=e.openElements.getCommonAncestor(r.element);e.treeAdapter.detachNode(a),s&&dX(e,s,a),mX(e,i,r)}}function hg(e,t){e._appendCommentNode(t,e.openElements.currentTmplContentOrNode)}function hX(e,t){e._appendCommentNode(t,e.openElements.items[0])}function pX(e,t){e._appendCommentNode(t,e.document)}function ub(e,t){if(e.stopped=!0,t.location){const n=e.fragmentContext?0:2;for(let r=e.openElements.stackTop;r>=n;r--)e._setEndLocation(e.openElements.items[r],t);if(!e.fragmentContext&&e.openElements.stackTop>=0){const r=e.openElements.items[0],i=e.treeAdapter.getNodeSourceCodeLocation(r);if(i&&!i.endTag&&(e._setEndLocation(r,t),e.openElements.stackTop>=1)){const a=e.openElements.items[1],s=e.treeAdapter.getNodeSourceCodeLocation(a);s&&!s.endTag&&e._setEndLocation(a,t)}}}}function fX(e,t){e._setDocumentType(t);const n=t.forceQuirks?ar.QUIRKS:qY(t);HY(t)||e._err(t,W.nonConformingDoctype),e.treeAdapter.setDocumentMode(e.document,n),e.insertionMode=O.BEFORE_HTML}function gl(e,t){e._err(t,W.missingDoctype,!0),e.treeAdapter.setDocumentMode(e.document,ar.QUIRKS),e.insertionMode=O.BEFORE_HTML,e._processToken(t)}function gX(e,t){t.tagID===f.HTML?(e._insertElement(t,ee.HTML),e.insertionMode=O.BEFORE_HEAD):Gl(e,t)}function bX(e,t){const n=t.tagID;(n===f.HTML||n===f.HEAD||n===f.BODY||n===f.BR)&&Gl(e,t)}function Gl(e,t){e._insertFakeRootElement(),e.insertionMode=O.BEFORE_HEAD,e._processToken(t)}function vX(e,t){switch(t.tagID){case f.HTML:{ln(e,t);break}case f.HEAD:{e._insertElement(t,ee.HTML),e.headElement=e.openElements.current,e.insertionMode=O.IN_HEAD;break}default:Wl(e,t)}}function xX(e,t){const n=t.tagID;n===f.HEAD||n===f.BODY||n===f.HTML||n===f.BR?Wl(e,t):e._err(t,W.endTagWithoutMatchingOpenElement)}function Wl(e,t){e._insertFakeElement(j.HEAD,f.HEAD),e.headElement=e.openElements.current,e.insertionMode=O.IN_HEAD,e._processToken(t)}function Ir(e,t){switch(t.tagID){case f.HTML:{ln(e,t);break}case f.BASE:case f.BASEFONT:case f.BGSOUND:case f.LINK:case f.META:{e._appendElement(t,ee.HTML),t.ackSelfClosing=!0;break}case f.TITLE:{e._switchToTextParsing(t,Ct.RCDATA);break}case f.NOSCRIPT:{e.options.scriptingEnabled?e._switchToTextParsing(t,Ct.RAWTEXT):(e._insertElement(t,ee.HTML),e.insertionMode=O.IN_HEAD_NO_SCRIPT);break}case f.NOFRAMES:case f.STYLE:{e._switchToTextParsing(t,Ct.RAWTEXT);break}case f.SCRIPT:{e._switchToTextParsing(t,Ct.SCRIPT_DATA);break}case f.TEMPLATE:{e._insertTemplate(t),e.activeFormattingElements.insertMarker(),e.framesetOk=!1,e.insertionMode=O.IN_TEMPLATE,e.tmplInsertionModeStack.unshift(O.IN_TEMPLATE);break}case f.HEAD:{e._err(t,W.misplacedStartTagForHeadElement);break}default:Kl(e,t)}}function $X(e,t){switch(t.tagID){case f.HEAD:{e.openElements.pop(),e.insertionMode=O.AFTER_HEAD;break}case f.BODY:case f.BR:case f.HTML:{Kl(e,t);break}case f.TEMPLATE:{ks(e,t);break}default:e._err(t,W.endTagWithoutMatchingOpenElement)}}function ks(e,t){e.openElements.tmplCount>0?(e.openElements.generateImpliedEndTagsThoroughly(),e.openElements.currentTagId!==f.TEMPLATE&&e._err(t,W.closingOfElementWithOpenChildElements),e.openElements.popUntilTagNamePopped(f.TEMPLATE),e.activeFormattingElements.clearToLastMarker(),e.tmplInsertionModeStack.shift(),e._resetInsertionMode()):e._err(t,W.endTagWithoutMatchingOpenElement)}function Kl(e,t){e.openElements.pop(),e.insertionMode=O.AFTER_HEAD,e._processToken(t)}function yX(e,t){switch(t.tagID){case f.HTML:{ln(e,t);break}case f.BASEFONT:case f.BGSOUND:case f.HEAD:case f.LINK:case f.META:case f.NOFRAMES:case f.STYLE:{Ir(e,t);break}case f.NOSCRIPT:{e._err(t,W.nestedNoscriptInHead);break}default:Yl(e,t)}}function _X(e,t){switch(t.tagID){case f.NOSCRIPT:{e.openElements.pop(),e.insertionMode=O.IN_HEAD;break}case f.BR:{Yl(e,t);break}default:e._err(t,W.endTagWithoutMatchingOpenElement)}}function Yl(e,t){const n=t.type===ze.EOF?W.openElementsLeftAfterEof:W.disallowedContentInNoscriptInHead;e._err(t,n),e.openElements.pop(),e.insertionMode=O.IN_HEAD,e._processToken(t)}function wX(e,t){switch(t.tagID){case f.HTML:{ln(e,t);break}case f.BODY:{e._insertElement(t,ee.HTML),e.framesetOk=!1,e.insertionMode=O.IN_BODY;break}case f.FRAMESET:{e._insertElement(t,ee.HTML),e.insertionMode=O.IN_FRAMESET;break}case f.BASE:case f.BASEFONT:case f.BGSOUND:case f.LINK:case f.META:case f.NOFRAMES:case f.SCRIPT:case f.STYLE:case f.TEMPLATE:case f.TITLE:{e._err(t,W.abandonedHeadElementChild),e.openElements.push(e.headElement,f.HEAD),Ir(e,t),e.openElements.remove(e.headElement);break}case f.HEAD:{e._err(t,W.misplacedStartTagForHeadElement);break}default:Xl(e,t)}}function TX(e,t){switch(t.tagID){case f.BODY:case f.HTML:case f.BR:{Xl(e,t);break}case f.TEMPLATE:{ks(e,t);break}default:e._err(t,W.endTagWithoutMatchingOpenElement)}}function Xl(e,t){e._insertFakeElement(j.BODY,f.BODY),e.insertionMode=O.IN_BODY,_m(e,t)}function _m(e,t){switch(t.type){case ze.CHARACTER:{iE(e,t);break}case ze.WHITESPACE_CHARACTER:{rE(e,t);break}case ze.COMMENT:{hg(e,t);break}case ze.START_TAG:{ln(e,t);break}case ze.END_TAG:{wm(e,t);break}case ze.EOF:{lE(e,t);break}}}function rE(e,t){e._reconstructActiveFormattingElements(),e._insertCharacters(t)}function iE(e,t){e._reconstructActiveFormattingElements(),e._insertCharacters(t),e.framesetOk=!1}function EX(e,t){e.openElements.tmplCount===0&&e.treeAdapter.adoptAttributes(e.openElements.items[0],t.attrs)}function kX(e,t){const n=e.openElements.tryPeekProperlyNestedBodyElement();n&&e.openElements.tmplCount===0&&(e.framesetOk=!1,e.treeAdapter.adoptAttributes(n,t.attrs))}function SX(e,t){const n=e.openElements.tryPeekProperlyNestedBodyElement();e.framesetOk&&n&&(e.treeAdapter.detachNode(n),e.openElements.popAllUpToHtmlElement(),e._insertElement(t,ee.HTML),e.insertionMode=O.IN_FRAMESET)}function NX(e,t){e.openElements.hasInButtonScope(f.P)&&e._closePElement(),e._insertElement(t,ee.HTML)}function CX(e,t){e.openElements.hasInButtonScope(f.P)&&e._closePElement(),e.openElements.currentTagId!==void 0&&mg.has(e.openElements.currentTagId)&&e.openElements.pop(),e._insertElement(t,ee.HTML)}function AX(e,t){e.openElements.hasInButtonScope(f.P)&&e._closePElement(),e._insertElement(t,ee.HTML),e.skipNextNewLine=!0,e.framesetOk=!1}function LX(e,t){const n=e.openElements.tmplCount>0;(!e.formElement||n)&&(e.openElements.hasInButtonScope(f.P)&&e._closePElement(),e._insertElement(t,ee.HTML),n||(e.formElement=e.openElements.current))}function PX(e,t){e.framesetOk=!1;const n=t.tagID;for(let r=e.openElements.stackTop;r>=0;r--){const i=e.openElements.tagIDs[r];if(n===f.LI&&i===f.LI||(n===f.DD||n===f.DT)&&(i===f.DD||i===f.DT)){e.openElements.generateImpliedEndTagsWithExclusion(i),e.openElements.popUntilTagNamePopped(i);break}if(i!==f.ADDRESS&&i!==f.DIV&&i!==f.P&&e._isSpecialElement(e.openElements.items[r],i))break}e.openElements.hasInButtonScope(f.P)&&e._closePElement(),e._insertElement(t,ee.HTML)}function IX(e,t){e.openElements.hasInButtonScope(f.P)&&e._closePElement(),e._insertElement(t,ee.HTML),e.tokenizer.state=Ct.PLAINTEXT}function UX(e,t){e.openElements.hasInScope(f.BUTTON)&&(e.openElements.generateImpliedEndTags(),e.openElements.popUntilTagNamePopped(f.BUTTON)),e._reconstructActiveFormattingElements(),e._insertElement(t,ee.HTML),e.framesetOk=!1}function DX(e,t){const n=e.activeFormattingElements.getElementEntryInScopeWithTagName(j.A);n&&(cb(e,t),e.openElements.remove(n.element),e.activeFormattingElements.removeEntry(n)),e._reconstructActiveFormattingElements(),e._insertElement(t,ee.HTML),e.activeFormattingElements.pushElement(e.openElements.current,t)}function MX(e,t){e._reconstructActiveFormattingElements(),e._insertElement(t,ee.HTML),e.activeFormattingElements.pushElement(e.openElements.current,t)}function RX(e,t){e._reconstructActiveFormattingElements(),e.openElements.hasInScope(f.NOBR)&&(cb(e,t),e._reconstructActiveFormattingElements()),e._insertElement(t,ee.HTML),e.activeFormattingElements.pushElement(e.openElements.current,t)}function OX(e,t){e._reconstructActiveFormattingElements(),e._insertElement(t,ee.HTML),e.activeFormattingElements.insertMarker(),e.framesetOk=!1}function zX(e,t){e.treeAdapter.getDocumentMode(e.document)!==ar.QUIRKS&&e.openElements.hasInButtonScope(f.P)&&e._closePElement(),e._insertElement(t,ee.HTML),e.framesetOk=!1,e.insertionMode=O.IN_TABLE}function aE(e,t){e._reconstructActiveFormattingElements(),e._appendElement(t,ee.HTML),e.framesetOk=!1,t.ackSelfClosing=!0}function sE(e){const t=KT(e,us.TYPE);return t!=null&&t.toLowerCase()===rX}function BX(e,t){e._reconstructActiveFormattingElements(),e._appendElement(t,ee.HTML),sE(t)||(e.framesetOk=!1),t.ackSelfClosing=!0}function FX(e,t){e._appendElement(t,ee.HTML),t.ackSelfClosing=!0}function jX(e,t){e.openElements.hasInButtonScope(f.P)&&e._closePElement(),e._appendElement(t,ee.HTML),e.framesetOk=!1,t.ackSelfClosing=!0}function VX(e,t){t.tagName=j.IMG,t.tagID=f.IMG,aE(e,t)}function HX(e,t){e._insertElement(t,ee.HTML),e.skipNextNewLine=!0,e.tokenizer.state=Ct.RCDATA,e.originalInsertionMode=e.insertionMode,e.framesetOk=!1,e.insertionMode=O.TEXT}function qX(e,t){e.openElements.hasInButtonScope(f.P)&&e._closePElement(),e._reconstructActiveFormattingElements(),e.framesetOk=!1,e._switchToTextParsing(t,Ct.RAWTEXT)}function GX(e,t){e.framesetOk=!1,e._switchToTextParsing(t,Ct.RAWTEXT)}function L$(e,t){e._switchToTextParsing(t,Ct.RAWTEXT)}function WX(e,t){e._reconstructActiveFormattingElements(),e._insertElement(t,ee.HTML),e.framesetOk=!1,e.insertionMode=e.insertionMode===O.IN_TABLE||e.insertionMode===O.IN_CAPTION||e.insertionMode===O.IN_TABLE_BODY||e.insertionMode===O.IN_ROW||e.insertionMode===O.IN_CELL?O.IN_SELECT_IN_TABLE:O.IN_SELECT}function KX(e,t){e.openElements.currentTagId===f.OPTION&&e.openElements.pop(),e._reconstructActiveFormattingElements(),e._insertElement(t,ee.HTML)}function YX(e,t){e.openElements.hasInScope(f.RUBY)&&e.openElements.generateImpliedEndTags(),e._insertElement(t,ee.HTML)}function XX(e,t){e.openElements.hasInScope(f.RUBY)&&e.openElements.generateImpliedEndTagsWithExclusion(f.RTC),e._insertElement(t,ee.HTML)}function QX(e,t){e._reconstructActiveFormattingElements(),eE(t),lb(t),t.selfClosing?e._appendElement(t,ee.MATHML):e._insertElement(t,ee.MATHML),t.ackSelfClosing=!0}function ZX(e,t){e._reconstructActiveFormattingElements(),tE(t),lb(t),t.selfClosing?e._appendElement(t,ee.SVG):e._insertElement(t,ee.SVG),t.ackSelfClosing=!0}function P$(e,t){e._reconstructActiveFormattingElements(),e._insertElement(t,ee.HTML)}function ln(e,t){switch(t.tagID){case f.I:case f.S:case f.B:case f.U:case f.EM:case f.TT:case f.BIG:case f.CODE:case f.FONT:case f.SMALL:case f.STRIKE:case f.STRONG:{MX(e,t);break}case f.A:{DX(e,t);break}case f.H1:case f.H2:case f.H3:case f.H4:case f.H5:case f.H6:{CX(e,t);break}case f.P:case f.DL:case f.OL:case f.UL:case f.DIV:case f.DIR:case f.NAV:case f.MAIN:case f.MENU:case f.ASIDE:case f.CENTER:case f.FIGURE:case f.FOOTER:case f.HEADER:case f.HGROUP:case f.DIALOG:case f.DETAILS:case f.ADDRESS:case f.ARTICLE:case f.SEARCH:case f.SECTION:case f.SUMMARY:case f.FIELDSET:case f.BLOCKQUOTE:case f.FIGCAPTION:{NX(e,t);break}case f.LI:case f.DD:case f.DT:{PX(e,t);break}case f.BR:case f.IMG:case f.WBR:case f.AREA:case f.EMBED:case f.KEYGEN:{aE(e,t);break}case f.HR:{jX(e,t);break}case f.RB:case f.RTC:{YX(e,t);break}case f.RT:case f.RP:{XX(e,t);break}case f.PRE:case f.LISTING:{AX(e,t);break}case f.XMP:{qX(e,t);break}case f.SVG:{ZX(e,t);break}case f.HTML:{EX(e,t);break}case f.BASE:case f.LINK:case f.META:case f.STYLE:case f.TITLE:case f.SCRIPT:case f.BGSOUND:case f.BASEFONT:case f.TEMPLATE:{Ir(e,t);break}case f.BODY:{kX(e,t);break}case f.FORM:{LX(e,t);break}case f.NOBR:{RX(e,t);break}case f.MATH:{QX(e,t);break}case f.TABLE:{zX(e,t);break}case f.INPUT:{BX(e,t);break}case f.PARAM:case f.TRACK:case f.SOURCE:{FX(e,t);break}case f.IMAGE:{VX(e,t);break}case f.BUTTON:{UX(e,t);break}case f.APPLET:case f.OBJECT:case f.MARQUEE:{OX(e,t);break}case f.IFRAME:{GX(e,t);break}case f.SELECT:{WX(e,t);break}case f.OPTION:case f.OPTGROUP:{KX(e,t);break}case f.NOEMBED:case f.NOFRAMES:{L$(e,t);break}case f.FRAMESET:{SX(e,t);break}case f.TEXTAREA:{HX(e,t);break}case f.NOSCRIPT:{e.options.scriptingEnabled?L$(e,t):P$(e,t);break}case f.PLAINTEXT:{IX(e,t);break}case f.COL:case f.TH:case f.TD:case f.TR:case f.HEAD:case f.FRAME:case f.TBODY:case f.TFOOT:case f.THEAD:case f.CAPTION:case f.COLGROUP:break;default:P$(e,t)}}function JX(e,t){if(e.openElements.hasInScope(f.BODY)&&(e.insertionMode=O.AFTER_BODY,e.options.sourceCodeLocationInfo)){const n=e.openElements.tryPeekProperlyNestedBodyElement();n&&e._setEndLocation(n,t)}}function eQ(e,t){e.openElements.hasInScope(f.BODY)&&(e.insertionMode=O.AFTER_BODY,gE(e,t))}function tQ(e,t){const n=t.tagID;e.openElements.hasInScope(n)&&(e.openElements.generateImpliedEndTags(),e.openElements.popUntilTagNamePopped(n))}function nQ(e){const t=e.openElements.tmplCount>0,{formElement:n}=e;t||(e.formElement=null),(n||t)&&e.openElements.hasInScope(f.FORM)&&(e.openElements.generateImpliedEndTags(),t?e.openElements.popUntilTagNamePopped(f.FORM):n&&e.openElements.remove(n))}function rQ(e){e.openElements.hasInButtonScope(f.P)||e._insertFakeElement(j.P,f.P),e._closePElement()}function iQ(e){e.openElements.hasInListItemScope(f.LI)&&(e.openElements.generateImpliedEndTagsWithExclusion(f.LI),e.openElements.popUntilTagNamePopped(f.LI))}function aQ(e,t){const n=t.tagID;e.openElements.hasInScope(n)&&(e.openElements.generateImpliedEndTagsWithExclusion(n),e.openElements.popUntilTagNamePopped(n))}function sQ(e){e.openElements.hasNumberedHeaderInScope()&&(e.openElements.generateImpliedEndTags(),e.openElements.popUntilNumberedHeaderPopped())}function oQ(e,t){const n=t.tagID;e.openElements.hasInScope(n)&&(e.openElements.generateImpliedEndTags(),e.openElements.popUntilTagNamePopped(n),e.activeFormattingElements.clearToLastMarker())}function lQ(e){e._reconstructActiveFormattingElements(),e._insertFakeElement(j.BR,f.BR),e.openElements.pop(),e.framesetOk=!1}function oE(e,t){const n=t.tagName,r=t.tagID;for(let i=e.openElements.stackTop;i>0;i--){const a=e.openElements.items[i],s=e.openElements.tagIDs[i];if(r===s&&(r!==f.UNKNOWN||e.treeAdapter.getTagName(a)===n)){e.openElements.generateImpliedEndTagsWithExclusion(r),e.openElements.stackTop>=i&&e.openElements.shortenToLength(i);break}if(e._isSpecialElement(a,s))break}}function wm(e,t){switch(t.tagID){case f.A:case f.B:case f.I:case f.S:case f.U:case f.EM:case f.TT:case f.BIG:case f.CODE:case f.FONT:case f.NOBR:case f.SMALL:case f.STRIKE:case f.STRONG:{cb(e,t);break}case f.P:{rQ(e);break}case f.DL:case f.UL:case f.OL:case f.DIR:case f.DIV:case f.NAV:case f.PRE:case f.MAIN:case f.MENU:case f.ASIDE:case f.BUTTON:case f.CENTER:case f.FIGURE:case f.FOOTER:case f.HEADER:case f.HGROUP:case f.DIALOG:case f.ADDRESS:case f.ARTICLE:case f.DETAILS:case f.SEARCH:case f.SECTION:case f.SUMMARY:case f.LISTING:case f.FIELDSET:case f.BLOCKQUOTE:case f.FIGCAPTION:{tQ(e,t);break}case f.LI:{iQ(e);break}case f.DD:case f.DT:{aQ(e,t);break}case f.H1:case f.H2:case f.H3:case f.H4:case f.H5:case f.H6:{sQ(e);break}case f.BR:{lQ(e);break}case f.BODY:{JX(e,t);break}case f.HTML:{eQ(e,t);break}case f.FORM:{nQ(e);break}case f.APPLET:case f.OBJECT:case f.MARQUEE:{oQ(e,t);break}case f.TEMPLATE:{ks(e,t);break}default:oE(e,t)}}function lE(e,t){e.tmplInsertionModeStack.length>0?fE(e,t):ub(e,t)}function cQ(e,t){var n;t.tagID===f.SCRIPT&&((n=e.scriptHandler)===null||n===void 0||n.call(e,e.openElements.current)),e.openElements.pop(),e.insertionMode=e.originalInsertionMode}function uQ(e,t){e._err(t,W.eofInElementThatCanContainOnlyText),e.openElements.pop(),e.insertionMode=e.originalInsertionMode,e.onEof(t)}function ap(e,t){if(e.openElements.currentTagId!==void 0&&nE.has(e.openElements.currentTagId))switch(e.pendingCharacterTokens.length=0,e.hasNonWhitespacePendingCharacterToken=!1,e.originalInsertionMode=e.insertionMode,e.insertionMode=O.IN_TABLE_TEXT,t.type){case ze.CHARACTER:{uE(e,t);break}case ze.WHITESPACE_CHARACTER:{cE(e,t);break}}else ru(e,t)}function dQ(e,t){e.openElements.clearBackToTableContext(),e.activeFormattingElements.insertMarker(),e._insertElement(t,ee.HTML),e.insertionMode=O.IN_CAPTION}function mQ(e,t){e.openElements.clearBackToTableContext(),e._insertElement(t,ee.HTML),e.insertionMode=O.IN_COLUMN_GROUP}function hQ(e,t){e.openElements.clearBackToTableContext(),e._insertFakeElement(j.COLGROUP,f.COLGROUP),e.insertionMode=O.IN_COLUMN_GROUP,db(e,t)}function pQ(e,t){e.openElements.clearBackToTableContext(),e._insertElement(t,ee.HTML),e.insertionMode=O.IN_TABLE_BODY}function fQ(e,t){e.openElements.clearBackToTableContext(),e._insertFakeElement(j.TBODY,f.TBODY),e.insertionMode=O.IN_TABLE_BODY,Tm(e,t)}function gQ(e,t){e.openElements.hasInTableScope(f.TABLE)&&(e.openElements.popUntilTagNamePopped(f.TABLE),e._resetInsertionMode(),e._processStartTag(t))}function bQ(e,t){sE(t)?e._appendElement(t,ee.HTML):ru(e,t),t.ackSelfClosing=!0}function vQ(e,t){!e.formElement&&e.openElements.tmplCount===0&&(e._insertElement(t,ee.HTML),e.formElement=e.openElements.current,e.openElements.pop())}function Mo(e,t){switch(t.tagID){case f.TD:case f.TH:case f.TR:{fQ(e,t);break}case f.STYLE:case f.SCRIPT:case f.TEMPLATE:{Ir(e,t);break}case f.COL:{hQ(e,t);break}case f.FORM:{vQ(e,t);break}case f.TABLE:{gQ(e,t);break}case f.TBODY:case f.TFOOT:case f.THEAD:{pQ(e,t);break}case f.INPUT:{bQ(e,t);break}case f.CAPTION:{dQ(e,t);break}case f.COLGROUP:{mQ(e,t);break}default:ru(e,t)}}function Uc(e,t){switch(t.tagID){case f.TABLE:{e.openElements.hasInTableScope(f.TABLE)&&(e.openElements.popUntilTagNamePopped(f.TABLE),e._resetInsertionMode());break}case f.TEMPLATE:{ks(e,t);break}case f.BODY:case f.CAPTION:case f.COL:case f.COLGROUP:case f.HTML:case f.TBODY:case f.TD:case f.TFOOT:case f.TH:case f.THEAD:case f.TR:break;default:ru(e,t)}}function ru(e,t){const n=e.fosterParentingEnabled;e.fosterParentingEnabled=!0,_m(e,t),e.fosterParentingEnabled=n}function cE(e,t){e.pendingCharacterTokens.push(t)}function uE(e,t){e.pendingCharacterTokens.push(t),e.hasNonWhitespacePendingCharacterToken=!0}function bl(e,t){let n=0;if(e.hasNonWhitespacePendingCharacterToken)for(;n<e.pendingCharacterTokens.length;n++)ru(e,e.pendingCharacterTokens[n]);else for(;n<e.pendingCharacterTokens.length;n++)e._insertCharacters(e.pendingCharacterTokens[n]);e.insertionMode=e.originalInsertionMode,e._processToken(t)}const dE=new Set([f.CAPTION,f.COL,f.COLGROUP,f.TBODY,f.TD,f.TFOOT,f.TH,f.THEAD,f.TR]);function xQ(e,t){const n=t.tagID;dE.has(n)?e.openElements.hasInTableScope(f.CAPTION)&&(e.openElements.generateImpliedEndTags(),e.openElements.popUntilTagNamePopped(f.CAPTION),e.activeFormattingElements.clearToLastMarker(),e.insertionMode=O.IN_TABLE,Mo(e,t)):ln(e,t)}function $Q(e,t){const n=t.tagID;switch(n){case f.CAPTION:case f.TABLE:{e.openElements.hasInTableScope(f.CAPTION)&&(e.openElements.generateImpliedEndTags(),e.openElements.popUntilTagNamePopped(f.CAPTION),e.activeFormattingElements.clearToLastMarker(),e.insertionMode=O.IN_TABLE,n===f.TABLE&&Uc(e,t));break}case f.BODY:case f.COL:case f.COLGROUP:case f.HTML:case f.TBODY:case f.TD:case f.TFOOT:case f.TH:case f.THEAD:case f.TR:break;default:wm(e,t)}}function db(e,t){switch(t.tagID){case f.HTML:{ln(e,t);break}case f.COL:{e._appendElement(t,ee.HTML),t.ackSelfClosing=!0;break}case f.TEMPLATE:{Ir(e,t);break}default:b0(e,t)}}function yQ(e,t){switch(t.tagID){case f.COLGROUP:{e.openElements.currentTagId===f.COLGROUP&&(e.openElements.pop(),e.insertionMode=O.IN_TABLE);break}case f.TEMPLATE:{ks(e,t);break}case f.COL:break;default:b0(e,t)}}function b0(e,t){e.openElements.currentTagId===f.COLGROUP&&(e.openElements.pop(),e.insertionMode=O.IN_TABLE,e._processToken(t))}function Tm(e,t){switch(t.tagID){case f.TR:{e.openElements.clearBackToTableBodyContext(),e._insertElement(t,ee.HTML),e.insertionMode=O.IN_ROW;break}case f.TH:case f.TD:{e.openElements.clearBackToTableBodyContext(),e._insertFakeElement(j.TR,f.TR),e.insertionMode=O.IN_ROW,Em(e,t);break}case f.CAPTION:case f.COL:case f.COLGROUP:case f.TBODY:case f.TFOOT:case f.THEAD:{e.openElements.hasTableBodyContextInTableScope()&&(e.openElements.clearBackToTableBodyContext(),e.openElements.pop(),e.insertionMode=O.IN_TABLE,Mo(e,t));break}default:Mo(e,t)}}function pg(e,t){const n=t.tagID;switch(t.tagID){case f.TBODY:case f.TFOOT:case f.THEAD:{e.openElements.hasInTableScope(n)&&(e.openElements.clearBackToTableBodyContext(),e.openElements.pop(),e.insertionMode=O.IN_TABLE);break}case f.TABLE:{e.openElements.hasTableBodyContextInTableScope()&&(e.openElements.clearBackToTableBodyContext(),e.openElements.pop(),e.insertionMode=O.IN_TABLE,Uc(e,t));break}case f.BODY:case f.CAPTION:case f.COL:case f.COLGROUP:case f.HTML:case f.TD:case f.TH:case f.TR:break;default:Uc(e,t)}}function Em(e,t){switch(t.tagID){case f.TH:case f.TD:{e.openElements.clearBackToTableRowContext(),e._insertElement(t,ee.HTML),e.insertionMode=O.IN_CELL,e.activeFormattingElements.insertMarker();break}case f.CAPTION:case f.COL:case f.COLGROUP:case f.TBODY:case f.TFOOT:case f.THEAD:case f.TR:{e.openElements.hasInTableScope(f.TR)&&(e.openElements.clearBackToTableRowContext(),e.openElements.pop(),e.insertionMode=O.IN_TABLE_BODY,Tm(e,t));break}default:Mo(e,t)}}function mE(e,t){switch(t.tagID){case f.TR:{e.openElements.hasInTableScope(f.TR)&&(e.openElements.clearBackToTableRowContext(),e.openElements.pop(),e.insertionMode=O.IN_TABLE_BODY);break}case f.TABLE:{e.openElements.hasInTableScope(f.TR)&&(e.openElements.clearBackToTableRowContext(),e.openElements.pop(),e.insertionMode=O.IN_TABLE_BODY,pg(e,t));break}case f.TBODY:case f.TFOOT:case f.THEAD:{(e.openElements.hasInTableScope(t.tagID)||e.openElements.hasInTableScope(f.TR))&&(e.openElements.clearBackToTableRowContext(),e.openElements.pop(),e.insertionMode=O.IN_TABLE_BODY,pg(e,t));break}case f.BODY:case f.CAPTION:case f.COL:case f.COLGROUP:case f.HTML:case f.TD:case f.TH:break;default:Uc(e,t)}}function _Q(e,t){const n=t.tagID;dE.has(n)?(e.openElements.hasInTableScope(f.TD)||e.openElements.hasInTableScope(f.TH))&&(e._closeTableCell(),Em(e,t)):ln(e,t)}function wQ(e,t){const n=t.tagID;switch(n){case f.TD:case f.TH:{e.openElements.hasInTableScope(n)&&(e.openElements.generateImpliedEndTags(),e.openElements.popUntilTagNamePopped(n),e.activeFormattingElements.clearToLastMarker(),e.insertionMode=O.IN_ROW);break}case f.TABLE:case f.TBODY:case f.TFOOT:case f.THEAD:case f.TR:{e.openElements.hasInTableScope(n)&&(e._closeTableCell(),mE(e,t));break}case f.BODY:case f.CAPTION:case f.COL:case f.COLGROUP:case f.HTML:break;default:wm(e,t)}}function hE(e,t){switch(t.tagID){case f.HTML:{ln(e,t);break}case f.OPTION:{e.openElements.currentTagId===f.OPTION&&e.openElements.pop(),e._insertElement(t,ee.HTML);break}case f.OPTGROUP:{e.openElements.currentTagId===f.OPTION&&e.openElements.pop(),e.openElements.currentTagId===f.OPTGROUP&&e.openElements.pop(),e._insertElement(t,ee.HTML);break}case f.HR:{e.openElements.currentTagId===f.OPTION&&e.openElements.pop(),e.openElements.currentTagId===f.OPTGROUP&&e.openElements.pop(),e._appendElement(t,ee.HTML),t.ackSelfClosing=!0;break}case f.INPUT:case f.KEYGEN:case f.TEXTAREA:case f.SELECT:{e.openElements.hasInSelectScope(f.SELECT)&&(e.openElements.popUntilTagNamePopped(f.SELECT),e._resetInsertionMode(),t.tagID!==f.SELECT&&e._processStartTag(t));break}case f.SCRIPT:case f.TEMPLATE:{Ir(e,t);break}}}function pE(e,t){switch(t.tagID){case f.OPTGROUP:{e.openElements.stackTop>0&&e.openElements.currentTagId===f.OPTION&&e.openElements.tagIDs[e.openElements.stackTop-1]===f.OPTGROUP&&e.openElements.pop(),e.openElements.currentTagId===f.OPTGROUP&&e.openElements.pop();break}case f.OPTION:{e.openElements.currentTagId===f.OPTION&&e.openElements.pop();break}case f.SELECT:{e.openElements.hasInSelectScope(f.SELECT)&&(e.openElements.popUntilTagNamePopped(f.SELECT),e._resetInsertionMode());break}case f.TEMPLATE:{ks(e,t);break}}}function TQ(e,t){const n=t.tagID;n===f.CAPTION||n===f.TABLE||n===f.TBODY||n===f.TFOOT||n===f.THEAD||n===f.TR||n===f.TD||n===f.TH?(e.openElements.popUntilTagNamePopped(f.SELECT),e._resetInsertionMode(),e._processStartTag(t)):hE(e,t)}function EQ(e,t){const n=t.tagID;n===f.CAPTION||n===f.TABLE||n===f.TBODY||n===f.TFOOT||n===f.THEAD||n===f.TR||n===f.TD||n===f.TH?e.openElements.hasInTableScope(n)&&(e.openElements.popUntilTagNamePopped(f.SELECT),e._resetInsertionMode(),e.onEndTag(t)):pE(e,t)}function kQ(e,t){switch(t.tagID){case f.BASE:case f.BASEFONT:case f.BGSOUND:case f.LINK:case f.META:case f.NOFRAMES:case f.SCRIPT:case f.STYLE:case f.TEMPLATE:case f.TITLE:{Ir(e,t);break}case f.CAPTION:case f.COLGROUP:case f.TBODY:case f.TFOOT:case f.THEAD:{e.tmplInsertionModeStack[0]=O.IN_TABLE,e.insertionMode=O.IN_TABLE,Mo(e,t);break}case f.COL:{e.tmplInsertionModeStack[0]=O.IN_COLUMN_GROUP,e.insertionMode=O.IN_COLUMN_GROUP,db(e,t);break}case f.TR:{e.tmplInsertionModeStack[0]=O.IN_TABLE_BODY,e.insertionMode=O.IN_TABLE_BODY,Tm(e,t);break}case f.TD:case f.TH:{e.tmplInsertionModeStack[0]=O.IN_ROW,e.insertionMode=O.IN_ROW,Em(e,t);break}default:e.tmplInsertionModeStack[0]=O.IN_BODY,e.insertionMode=O.IN_BODY,ln(e,t)}}function SQ(e,t){t.tagID===f.TEMPLATE&&ks(e,t)}function fE(e,t){e.openElements.tmplCount>0?(e.openElements.popUntilTagNamePopped(f.TEMPLATE),e.activeFormattingElements.clearToLastMarker(),e.tmplInsertionModeStack.shift(),e._resetInsertionMode(),e.onEof(t)):ub(e,t)}function NQ(e,t){t.tagID===f.HTML?ln(e,t):v0(e,t)}function gE(e,t){var n;if(t.tagID===f.HTML){if(e.fragmentContext||(e.insertionMode=O.AFTER_AFTER_BODY),e.options.sourceCodeLocationInfo&&e.openElements.tagIDs[0]===f.HTML){e._setEndLocation(e.openElements.items[0],t);const r=e.openElements.items[1];r&&!(!((n=e.treeAdapter.getNodeSourceCodeLocation(r))===null||n===void 0)&&n.endTag)&&e._setEndLocation(r,t)}}else v0(e,t)}function v0(e,t){e.insertionMode=O.IN_BODY,_m(e,t)}function CQ(e,t){switch(t.tagID){case f.HTML:{ln(e,t);break}case f.FRAMESET:{e._insertElement(t,ee.HTML);break}case f.FRAME:{e._appendElement(t,ee.HTML),t.ackSelfClosing=!0;break}case f.NOFRAMES:{Ir(e,t);break}}}function AQ(e,t){t.tagID===f.FRAMESET&&!e.openElements.isRootHtmlElementCurrent()&&(e.openElements.pop(),!e.fragmentContext&&e.openElements.currentTagId!==f.FRAMESET&&(e.insertionMode=O.AFTER_FRAMESET))}function LQ(e,t){switch(t.tagID){case f.HTML:{ln(e,t);break}case f.NOFRAMES:{Ir(e,t);break}}}function PQ(e,t){t.tagID===f.HTML&&(e.insertionMode=O.AFTER_AFTER_FRAMESET)}function IQ(e,t){t.tagID===f.HTML?ln(e,t):Ed(e,t)}function Ed(e,t){e.insertionMode=O.IN_BODY,_m(e,t)}function UQ(e,t){switch(t.tagID){case f.HTML:{ln(e,t);break}case f.NOFRAMES:{Ir(e,t);break}}}function DQ(e,t){t.chars=dt,e._insertCharacters(t)}function MQ(e,t){e._insertCharacters(t),e.framesetOk=!1}function bE(e){for(;e.treeAdapter.getNamespaceURI(e.openElements.current)!==ee.HTML&&e.openElements.currentTagId!==void 0&&!e._isIntegrationPoint(e.openElements.currentTagId,e.openElements.current);)e.openElements.pop()}function RQ(e,t){if(ZY(t))bE(e),e._startTagOutsideForeignContent(t);else{const n=e._getAdjustedCurrentElement(),r=e.treeAdapter.getNamespaceURI(n);r===ee.MATHML?eE(t):r===ee.SVG&&(JY(t),tE(t)),lb(t),t.selfClosing?e._appendElement(t,r):e._insertElement(t,r),t.ackSelfClosing=!0}}function OQ(e,t){if(t.tagID===f.P||t.tagID===f.BR){bE(e),e._endTagOutsideForeignContent(t);return}for(let n=e.openElements.stackTop;n>0;n--){const r=e.openElements.items[n];if(e.treeAdapter.getNamespaceURI(r)===ee.HTML){e._endTagOutsideForeignContent(t);break}const i=e.treeAdapter.getTagName(r);if(i.toLowerCase()===t.tagName){t.tagName=i,e.openElements.shortenToLength(n);break}}}j.AREA,j.BASE,j.BASEFONT,j.BGSOUND,j.BR,j.COL,j.EMBED,j.FRAME,j.HR,j.IMG,j.INPUT,j.KEYGEN,j.LINK,j.META,j.PARAM,j.SOURCE,j.TRACK,j.WBR;const zQ=/<(\/?)(iframe|noembed|noframes|plaintext|script|style|textarea|title|xmp)(?=[\t\n\f\r />])/gi,BQ=new Set(["mdxFlowExpression","mdxJsxFlowElement","mdxJsxTextElement","mdxTextExpression","mdxjsEsm"]),I$={sourceCodeLocationInfo:!0,scriptingEnabled:!1};function vE(e,t){const n=XQ(e),r=VT("type",{handlers:{root:FQ,element:jQ,text:VQ,comment:$E,doctype:HQ,raw:GQ},unknown:WQ}),i={parser:n?new A$(I$):A$.getFragmentParser(void 0,I$),handle(o){r(o,i)},stitches:!1,options:t||{}};r(e,i),nl(i,ui());const a=n?i.parser.document:i.parser.getFragment(),s=FK(a,{file:i.options.file});return i.stitches&&M2(s,"comment",function(o,l,c){const u=o;if(u.value.stitch&&c&&l!==void 0){const d=c.children;return d[l]=u.value.stitch,l}}),s.type==="root"&&s.children.length===1&&s.children[0].type===e.type?s.children[0]:s}function xE(e,t){let n=-1;if(e)for(;++n<e.length;)t.handle(e[n])}function FQ(e,t){xE(e.children,t)}function jQ(e,t){KQ(e,t),xE(e.children,t),YQ(e,t)}function VQ(e,t){t.parser.tokenizer.state>4&&(t.parser.tokenizer.state=0);const n={type:ze.CHARACTER,chars:e.value,location:iu(e)};nl(t,ui(e)),t.parser.currentToken=n,t.parser._processToken(t.parser.currentToken)}function HQ(e,t){const n={type:ze.DOCTYPE,name:"html",forceQuirks:!1,publicId:"",systemId:"",location:iu(e)};nl(t,ui(e)),t.parser.currentToken=n,t.parser._processToken(t.parser.currentToken)}function qQ(e,t){t.stitches=!0;const n=QQ(e);if("children"in e&&"children"in n){const r=vE({type:"root",children:e.children},t.options);n.children=r.children}$E({type:"comment",value:{stitch:n}},t)}function $E(e,t){const n=e.value,r={type:ze.COMMENT,data:n,location:iu(e)};nl(t,ui(e)),t.parser.currentToken=r,t.parser._processToken(t.parser.currentToken)}function GQ(e,t){if(t.parser.tokenizer.preprocessor.html="",t.parser.tokenizer.preprocessor.pos=-1,t.parser.tokenizer.preprocessor.lastGapPos=-2,t.parser.tokenizer.preprocessor.gapStack=[],t.parser.tokenizer.preprocessor.skipNextNewLine=!1,t.parser.tokenizer.preprocessor.lastChunkWritten=!1,t.parser.tokenizer.preprocessor.endOfChunkHit=!1,t.parser.tokenizer.preprocessor.isEol=!1,yE(t,ui(e)),t.parser.tokenizer.write(t.options.tagfilter?e.value.replace(zQ,"&lt;$1$2"):e.value,!1),t.parser.tokenizer._runParsingLoop(),t.parser.tokenizer.state===72||t.parser.tokenizer.state===78){t.parser.tokenizer.preprocessor.lastChunkWritten=!0;const n=t.parser.tokenizer._consume();t.parser.tokenizer._callState(n)}}function WQ(e,t){const n=e;if(t.options.passThrough&&t.options.passThrough.includes(n.type))qQ(n,t);else{let r="";throw BQ.has(n.type)&&(r=". It looks like you are using MDX nodes with `hast-util-raw` (or `rehype-raw`). If you use this because you are using remark or rehype plugins that inject `'html'` nodes, then please raise an issue with that plugin, as its a bad and slow idea. If you use this because you are using markdown syntax, then you have to configure this utility (or plugin) to pass through these nodes (see `passThrough` in docs), but you can also migrate to use the MDX syntax"),new Error("Cannot compile `"+n.type+"` node"+r)}}function nl(e,t){yE(e,t);const n=e.parser.tokenizer.currentCharacterToken;n&&n.location&&(n.location.endLine=e.parser.tokenizer.preprocessor.line,n.location.endCol=e.parser.tokenizer.preprocessor.col+1,n.location.endOffset=e.parser.tokenizer.preprocessor.offset+1,e.parser.currentToken=n,e.parser._processToken(e.parser.currentToken)),e.parser.tokenizer.paused=!1,e.parser.tokenizer.inLoop=!1,e.parser.tokenizer.active=!1,e.parser.tokenizer.returnState=Ct.DATA,e.parser.tokenizer.charRefCode=-1,e.parser.tokenizer.consumedAfterSnapshot=-1,e.parser.tokenizer.currentLocation=null,e.parser.tokenizer.currentCharacterToken=null,e.parser.tokenizer.currentToken=null,e.parser.tokenizer.currentAttr={name:"",value:""}}function yE(e,t){if(t&&t.offset!==void 0){const n={startLine:t.line,startCol:t.column,startOffset:t.offset,endLine:-1,endCol:-1,endOffset:-1};e.parser.tokenizer.preprocessor.lineStartPos=-t.column+1,e.parser.tokenizer.preprocessor.droppedBufferSize=t.offset,e.parser.tokenizer.preprocessor.line=t.line,e.parser.tokenizer.currentLocation=n}}function KQ(e,t){const n=e.tagName.toLowerCase();if(t.parser.tokenizer.state===Ct.PLAINTEXT)return;nl(t,ui(e));const r=t.parser.openElements.current;let i="namespaceURI"in r?r.namespaceURI:ti.html;i===ti.html&&n==="svg"&&(i=ti.svg);const a=nY({...e,children:[]},{space:i===ti.svg?"svg":"html"}),s={type:ze.START_TAG,tagName:n,tagID:tl(n),selfClosing:!1,ackSelfClosing:!1,attrs:"attrs"in a?a.attrs:[],location:iu(e)};t.parser.currentToken=s,t.parser._processToken(t.parser.currentToken),t.parser.tokenizer.lastStartTagName=n}function YQ(e,t){const n=e.tagName.toLowerCase();if(!t.parser.tokenizer.inForeignNode&&uY.includes(n)||t.parser.tokenizer.state===Ct.PLAINTEXT)return;nl(t,mm(e));const r={type:ze.END_TAG,tagName:n,tagID:tl(n),selfClosing:!1,ackSelfClosing:!1,attrs:[],location:iu(e)};t.parser.currentToken=r,t.parser._processToken(t.parser.currentToken),n===t.parser.tokenizer.lastStartTagName&&(t.parser.tokenizer.state===Ct.RCDATA||t.parser.tokenizer.state===Ct.RAWTEXT||t.parser.tokenizer.state===Ct.SCRIPT_DATA)&&(t.parser.tokenizer.state=Ct.DATA)}function XQ(e){const t=e.type==="root"?e.children[0]:e;return!!(t&&(t.type==="doctype"||t.type==="element"&&t.tagName.toLowerCase()==="html"))}function iu(e){const t=ui(e)||{line:void 0,column:void 0,offset:void 0},n=mm(e)||{line:void 0,column:void 0,offset:void 0};return{startLine:t.line,startCol:t.column,startOffset:t.offset,endLine:n.line,endCol:n.column,endOffset:n.offset}}function QQ(e){return"children"in e?Uo({...e,children:[]}):Uo(e)}function ZQ(e){return function(t,n){return vE(t,{...e,file:n})}}const JQ=({content:e})=>h.jsx("div",{className:"prose prose-lg max-w-none",children:h.jsx(Gq,{remarkPlugins:[lK],rehypePlugins:[OK,ZQ],components:{h1:({children:t})=>h.jsx("h1",{className:"text-3xl font-bold mb-6 gradient-text",children:t}),h2:({children:t})=>h.jsx("h2",{className:"text-2xl font-bold mb-4 text-foreground",children:t}),h3:({children:t})=>h.jsx("h3",{className:"text-xl font-bold mb-3 text-foreground",children:t}),p:({children:t})=>h.jsx("p",{className:"mb-4 text-foreground/90 leading-relaxed",children:t}),code:({children:t})=>h.jsxs("code",{className:"relative inline-block bg-gradient-to-r from-research-500/10 to-purple-500/10 text-research-700 dark:text-research-300 px-2 py-1 rounded text-sm font-mono border border-research-500/20 dark:border-research-400/30 shadow-sm",children:[h.jsx("span",{className:"relative z-10",children:t}),h.jsx("div",{className:"absolute inset-0 bg-gradient-to-r from-research-500/5 to-purple-500/5 rounded animate-pulse opacity-50"})]}),pre:({children:t})=>h.jsx("pre",{className:"p-4 rounded-lg overflow-x-auto mb-6",children:t}),ul:({children:t})=>h.jsx("ul",{className:"mb-4 pl-6 space-y-2",children:t}),ol:({children:t})=>h.jsx("ol",{className:"mb-4 pl-6 space-y-2 list-decimal",children:t}),li:({children:t})=>h.jsx("li",{className:"text-foreground/90",children:t}),blockquote:({children:t})=>h.jsx("blockquote",{className:"border-l-4 border-research-500 pl-4 italic text-foreground/80 mb-4",children:t}),strong:({children:t})=>h.jsx("strong",{className:"font-bold text-blue-700 dark:text-blue-300",children:t}),em:({children:t})=>h.jsx("em",{className:"italic text-foreground/90",children:t}),a:({children:t,href:n})=>h.jsx("a",{href:n,className:"text-blue-600 dark:text-blue-400 hover:text-blue-700 dark:hover:text-blue-300 underline decoration-blue-600/30 dark:decoration-blue-400/30 hover:decoration-blue-700/50 dark:hover:decoration-blue-300/50 underline-offset-2 transition-all duration-300 font-medium",target:"_blank",rel:"noopener noreferrer",children:t}),img:({src:t,alt:n,...r})=>h.jsx("img",{src:t,alt:n||"",className:"bg-white dark:bg-white rounded-md mx-auto",style:{backgroundColor:"white",display:"block",marginLeft:"auto",marginRight:"auto",marginTop:"1rem",marginBottom:"1rem"},...r})},children:e})}),eZ=()=>{const[e,t]=T.useState(0);return T.useEffect(()=>{const n=()=>{const r=window.scrollY,i=document.documentElement.scrollHeight-window.innerHeight,a=r/i;t(Math.min(a*100,100))};return window.addEventListener("scroll",n),()=>window.removeEventListener("scroll",n)},[]),e},tZ=()=>{const e=eZ();return h.jsx("div",{className:"fixed bottom-0 left-0 right-0 h-1 bg-muted/30 z-50",children:h.jsx("div",{className:"h-full bg-gradient-to-r from-research-500 to-purple-500 transition-all duration-150 ease-out",style:{width:`${e}%`}})})},nZ=()=>{const{slug:e}=KP(),t=em(),{posts:n,loading:r,error:i}=d2();if(r)return h.jsxs("div",{className:"min-h-screen bg-background",children:[h.jsx(Ei,{}),h.jsx("div",{className:"pt-24 flex items-center justify-center",children:h.jsxs("div",{className:"text-center",children:[h.jsx("div",{className:"animate-spin rounded-full h-12 w-12 border-b-2 border-research-500 mx-auto"}),h.jsx("p",{className:"mt-4 text-foreground/60",children:"Loading blog post..."})]})})]});if(i)return h.jsxs("div",{className:"min-h-screen bg-background",children:[h.jsx(Ei,{}),h.jsx("div",{className:"pt-24 flex items-center justify-center",children:h.jsxs("div",{className:"text-center",children:[h.jsx("h1",{className:"text-4xl font-bold mb-4",children:"Error Loading Post"}),h.jsx("p",{className:"text-red-500 mb-4",children:i}),h.jsx(sr,{onClick:()=>t(-1),children:"Go Back"})]})})]});const a=n.find(p=>p.slug===e);if(!a)return h.jsxs("div",{className:"min-h-screen bg-background",children:[h.jsx(Ei,{}),h.jsx("div",{className:"pt-24 flex items-center justify-center",children:h.jsxs("div",{className:"text-center",children:[h.jsx("h1",{className:"text-4xl font-bold mb-4",children:"Post Not Found"}),h.jsx(sr,{onClick:()=>t(-1),children:"Go Back"})]})})]});const s=ma.find(p=>p.slug===a.category),o=n.filter(p=>p.category===a.category).sort((p,x)=>new Date(p.publishedAt).getTime()-new Date(x.publishedAt).getTime()),l=o.findIndex(p=>p.slug===a.slug),c=l>0?o[l-1]:null,u=l<o.length-1?o[l+1]:null,d=p=>{t(`/blog/${p}`)},m=p=>({"ai theory":"bg-gradient-to-r from-sky-500 to-sky-600","ai papers":"bg-gradient-to-r from-purple-500 to-purple-600","ai technology":"bg-gradient-to-r from-emerald-500 to-emerald-600","personal insights":"bg-gradient-to-r from-rose-500 to-rose-600"})[p]||"bg-gradient-to-r from-gray-500 to-gray-600";return h.jsxs("div",{className:"min-h-screen bg-background",children:[h.jsx(Ei,{}),h.jsx(tZ,{}),h.jsxs("div",{className:"container mx-auto px-6 py-8 pt-24",children:[h.jsxs(sr,{variant:"ghost",onClick:()=>t(-1),className:"mb-8 hover:bg-accent",children:[h.jsx(P6,{className:"w-4 h-4 mr-2"}),"Back"]}),h.jsxs("div",{className:"max-w-4xl mx-auto",children:[h.jsxs("div",{className:"mb-6",children:[h.jsx(m2,{variant:"category",className:`${m(a.category)} mb-4 px-4 py-2 text-sm`,children:s==null?void 0:s.name}),h.jsx("h1",{className:"text-4xl md:text-5xl font-bold mb-4 gradient-text",children:a.title}),h.jsxs("div",{className:"flex items-center gap-6 text-foreground/60 mb-8",children:[h.jsxs("div",{className:"flex items-center gap-2",children:[h.jsx(g1,{className:"w-4 h-4"}),h.jsx("span",{children:a.author})]}),h.jsxs("div",{className:"flex items-center gap-2",children:[h.jsx(f1,{className:"w-4 h-4"}),h.jsxs("span",{children:[a.readTime," min read"]})]}),h.jsx("span",{children:new Date(a.publishedAt).toLocaleDateString()})]})]}),a.thumbnail?h.jsx("div",{className:"mb-12 rounded-xl overflow-hidden shadow-lg",children:h.jsx("img",{src:a.thumbnail,alt:a.title,className:"w-full h-64 md:h-96 object-cover bg-white dark:bg-white",style:{backgroundColor:"white"}})}):h.jsx("div",{className:"mb-12 rounded-xl overflow-hidden shadow-lg",children:h.jsx("div",{className:"w-full h-64 md:h-96 bg-gradient-to-br from-research-500/20 to-purple-500/20 flex items-center justify-center",children:h.jsx("div",{className:"text-6xl font-bold text-white/40",children:a.title.charAt(0)})})}),h.jsx("div",{className:"mb-12",children:h.jsx(JQ,{content:a.content})}),h.jsx("div",{className:"border-t pt-8",children:h.jsxs("div",{className:"flex justify-between items-center gap-4",children:[h.jsx("div",{className:"flex-1 min-w-0",children:c&&h.jsxs(sr,{variant:"ghost",onClick:()=>d(c.slug),className:"liquid-glass-button flex items-center gap-2 p-4 h-auto group w-full",children:[h.jsx(U6,{className:"w-5 h-5 group-hover:translate-x-[-2px] transition-transform flex-shrink-0"}),h.jsxs("div",{className:"text-left min-w-0 flex-1",children:[h.jsx("div",{className:"text-sm text-foreground/60",children:"Previous"}),h.jsx("div",{className:"font-medium text-foreground truncate",children:c.title})]})]})}),h.jsx("div",{className:"flex-1 min-w-0",children:u&&h.jsxs(sr,{variant:"ghost",onClick:()=>d(u.slug),className:"liquid-glass-button flex items-center gap-2 p-4 h-auto group w-full",children:[h.jsxs("div",{className:"text-right min-w-0 flex-1",children:[h.jsx("div",{className:"text-sm text-foreground/60",children:"Next"}),h.jsx("div",{className:"font-medium text-foreground truncate",children:u.title})]}),h.jsx(D6,{className:"w-5 h-5 group-hover:translate-x-[2px] transition-transform flex-shrink-0"})]})})]})})]})]})]})},_E=({className:e,...t})=>h.jsx("nav",{role:"navigation","aria-label":"pagination",className:Ue("mx-auto flex w-full justify-center",e),...t});_E.displayName="Pagination";const wE=T.forwardRef(({className:e,...t},n)=>h.jsx("ul",{ref:n,className:Ue("flex flex-row items-center gap-1",e),...t}));wE.displayName="PaginationContent";const vr=T.forwardRef(({className:e,...t},n)=>h.jsx("li",{ref:n,className:Ue("",e),...t}));vr.displayName="PaginationItem";const wr=({className:e,isActive:t,size:n="icon",...r})=>h.jsx("a",{"aria-current":t?"page":void 0,className:Ue(j5({variant:t?"outline":"ghost",size:n}),e),...r});wr.displayName="PaginationLink";const TE=({className:e,...t})=>h.jsxs(wr,{"aria-label":"Go to previous page",size:"default",className:Ue("gap-1 pl-2.5",e),...t,children:[h.jsx(U6,{className:"h-4 w-4"}),h.jsx("span",{children:"Previous"})]});TE.displayName="PaginationPrevious";const EE=({className:e,...t})=>h.jsxs(wr,{"aria-label":"Go to next page",size:"default",className:Ue("gap-1 pr-2.5",e),...t,children:[h.jsx("span",{children:"Next"}),h.jsx(D6,{className:"h-4 w-4"})]});EE.displayName="PaginationNext";const kl=({className:e,...t})=>h.jsxs("span",{"aria-hidden":!0,className:Ue("flex h-9 w-9 items-center justify-center",e),...t,children:[h.jsx(nC,{className:"h-4 w-4"}),h.jsx("span",{className:"sr-only",children:"More pages"})]});kl.displayName="PaginationEllipsis";const rZ=(e={})=>{const t=T.useRef(null),n=T.useRef(null);return T.useEffect(()=>{if(!t.current)return;const i=setTimeout(()=>{if(!t.current)return;const a=t.current.getBoundingClientRect(),s=a.width,o=a.height;if(s<=0||o<=0){console.log("Liquid glass: Element dimensions not ready, skipping initialization");return}const{width:l=Math.max(s,100),height:c=Math.max(o,50),borderRadius:u=12,displacement:d=15,interactive:m=!0}=e,p=Math.max(l,10),x=Math.max(c,10);function g(C,k,S){return S=Math.max(0,Math.min(1,(S-C)/(k-C))),S*S*(3-2*S)}function w(C,k){return Math.sqrt(C*C+k*k)}function v(C,k,S,L,U){const F=Math.abs(C)-S+U,q=Math.abs(k)-L+U;return Math.min(Math.max(F,q),0)+w(Math.max(F,0),Math.max(q,0))-U}function $(){return"liquid-glass-"+Math.random().toString(36).substr(2,9)}class _{constructor(k,S){yn(this,"width");yn(this,"height");yn(this,"element");yn(this,"svg");yn(this,"canvas");yn(this,"context");yn(this,"feImage");yn(this,"feDisplacementMap");yn(this,"mouse");yn(this,"mouseUsed");yn(this,"id");yn(this,"canvasDPI");yn(this,"animationId");this.width=p,this.height=x,this.element=k,this.mouse={x:0,y:0},this.mouseUsed=!1,this.id=$(),this.canvasDPI=1,this.animationId=null,this.context=null;try{this.createElement(),S.interactive&&this.setupEventListeners(),this.updateShader()}catch(L){console.warn("Liquid glass initialization failed:",L)}}createElement(){this.svg=document.createElementNS("http://www.w3.org/2000/svg","svg"),this.svg.setAttribute("xmlns","http://www.w3.org/2000/svg"),this.svg.setAttribute("width","0"),this.svg.setAttribute("height","0"),this.svg.style.cssText=`
            position: fixed;
            top: 0;
            left: 0;
            pointer-events: none;
            z-index: -1;
          `;const k=document.createElementNS("http://www.w3.org/2000/svg","defs"),S=document.createElementNS("http://www.w3.org/2000/svg","filter");if(S.setAttribute("id",`${this.id}_filter`),S.setAttribute("filterUnits","userSpaceOnUse"),S.setAttribute("colorInterpolationFilters","sRGB"),this.feImage=document.createElementNS("http://www.w3.org/2000/svg","feImage"),this.feImage.setAttribute("id",`${this.id}_map`),this.feImage.setAttribute("width",this.width.toString()),this.feImage.setAttribute("height",this.height.toString()),this.feDisplacementMap=document.createElementNS("http://www.w3.org/2000/svg","feDisplacementMap"),this.feDisplacementMap.setAttribute("in","SourceGraphic"),this.feDisplacementMap.setAttribute("in2",`${this.id}_map`),this.feDisplacementMap.setAttribute("xChannelSelector","R"),this.feDisplacementMap.setAttribute("yChannelSelector","G"),S.appendChild(this.feImage),S.appendChild(this.feDisplacementMap),k.appendChild(S),this.svg.appendChild(k),this.canvas=document.createElement("canvas"),this.canvas.width=Math.max(this.width*this.canvasDPI,1),this.canvas.height=Math.max(this.height*this.canvasDPI,1),this.canvas.style.display="none",this.context=this.canvas.getContext("2d"),!this.context)throw new Error("Could not get canvas context");this.element.style.filter=`url(#${this.id}_filter)`,document.body.appendChild(this.svg)}setupEventListeners(){const k=S=>{const L=this.element.getBoundingClientRect();L.width>0&&L.height>0&&(this.mouse.x=(S.clientX-L.left)/L.width,this.mouse.y=(S.clientY-L.top)/L.height,this.mouseUsed&&this.updateShader())};this.element.addEventListener("mousemove",k),this.element.addEventListener("mouseenter",k)}updateShader(){if(!this.context)return;const k=new Proxy(this.mouse,{get:(U,F)=>(this.mouseUsed=!0,U[F])});this.mouseUsed=!1;const S=Math.max(this.width*this.canvasDPI,1),L=Math.max(this.height*this.canvasDPI,1);try{const U=new Uint8ClampedArray(S*L*4);let F=0;const q=[];for(let ne=0;ne<U.length;ne+=4){const K=ne/4%S,te=Math.floor(ne/4/S),J={x:K/S,y:te/L},ae=J.x-.5,B=J.y-.5,X=v(ae,B,.4,.3,.1),P=g(.6,0,X-.1),se=g(0,1,P*.8);if(this.mouseUsed){const he=1-w((J.x-k.x)*2,(J.y-k.y)*2),D=Math.max(0,he)*.3,Ee=se+D,je=ae*Ee-ae,xe=B*Ee-B;F=Math.max(F,Math.abs(je*S),Math.abs(xe*L)),q.push(je*S,xe*L)}else{const he=ae*se-ae,D=B*se-B;F=Math.max(F,Math.abs(he*S),Math.abs(D*L)),q.push(he*S,D*L)}}F=Math.max(F*.5,1);let G=0;for(let ne=0;ne<U.length;ne+=4){const K=q[G++]/F+.5,te=q[G++]/F+.5;U[ne]=Math.max(0,Math.min(255,K*255)),U[ne+1]=Math.max(0,Math.min(255,te*255)),U[ne+2]=0,U[ne+3]=255}const H=new ImageData(U,S,L);this.context.putImageData(H,0,0),this.feImage.setAttributeNS("http://www.w3.org/1999/xlink","href",this.canvas.toDataURL()),this.feDisplacementMap.setAttribute("scale",(F/this.canvasDPI*.5).toString())}catch(U){console.warn("Liquid glass shader update failed:",U)}}destroy(){this.animationId&&cancelAnimationFrame(this.animationId),this.svg&&this.svg.parentNode&&this.svg.remove(),this.canvas&&this.canvas.parentNode&&this.canvas.remove(),this.element&&(this.element.style.filter="")}}n.current&&n.current.destroy(),n.current=new _(t.current,e)},100);return()=>{clearTimeout(i),n.current&&n.current.destroy()}},[e.width,e.height,e.borderRadius,e.displacement,e.interactive]),t},iZ=({children:e,className:t="",interactive:n=!0,displacement:r=15})=>(rZ({interactive:n,displacement:r}),h.jsxs("div",{className:`
        relative overflow-hidden
        bg-white/[0.05] dark:bg-black/[0.05]
        backdrop-blur-xl
        border border-white/[0.1] dark:border-white/[0.05]
        rounded-2xl
        shadow-2xl shadow-black/[0.1]
        transition-all duration-500
        hover:shadow-3xl hover:shadow-black/[0.15]
        hover:bg-white/[0.08] dark:hover:bg-black/[0.08]
        ${t}
      `,children:[h.jsx("div",{className:"absolute inset-0 opacity-30 z-0",children:h.jsx("div",{className:"absolute inset-0 bg-gradient-to-br from-blue-500/10 via-purple-500/5 to-pink-500/10"})}),h.jsx("div",{className:"relative z-10",children:e}),h.jsxs("div",{className:"absolute inset-0 opacity-20 pointer-events-none z-10",children:[h.jsx("div",{className:"absolute top-0 left-0 w-full h-px bg-gradient-to-r from-transparent via-white/40 to-transparent"}),h.jsx("div",{className:"absolute top-0 left-0 w-px h-full bg-gradient-to-b from-transparent via-white/40 to-transparent"})]})]})),sp=12,aZ=()=>{var C;const[e,t]=T.useState("all"),[n,r]=T.useState(""),[i,a]=T.useState(1),s=em(),{posts:o,loading:l,error:c}=d2(),u=k=>{t(k),a(1)},d=k=>{r(k),a(1)};if(l)return h.jsxs("div",{className:"min-h-screen bg-background",children:[h.jsx(Ei,{}),h.jsx("div",{className:"pt-24 flex items-center justify-center",children:h.jsxs("div",{className:"text-center",children:[h.jsx("div",{className:"animate-spin rounded-full h-12 w-12 border-b-2 border-research-500 mx-auto"}),h.jsx("p",{className:"mt-4 text-foreground/60",children:"Loading blog posts..."})]})})]});if(c)return h.jsxs("div",{className:"min-h-screen bg-background",children:[h.jsx(Ei,{}),h.jsx("div",{className:"pt-24 flex items-center justify-center",children:h.jsxs("div",{className:"text-center",children:[h.jsx("h1",{className:"text-4xl font-bold mb-4",children:"Error Loading Posts"}),h.jsx("p",{className:"text-red-500 mb-4",children:c}),h.jsx(sr,{onClick:()=>s("/"),children:"Go Back"})]})})]});const m=o.filter(k=>{const S=e==="all"||k.category===e,L=n===""||k.title.toLowerCase().includes(n.toLowerCase())||k.excerpt.toLowerCase().includes(n.toLowerCase())||k.content.toLowerCase().includes(n.toLowerCase());return S&&L}),p=Math.ceil(m.length/sp),x=(i-1)*sp,g=x+sp,w=m.slice(x,g),v=k=>{s(`/blog/${k.slug}`)},$=k=>({"ai theory":"bg-gradient-to-r from-sky-500 to-sky-600","ai papers":"bg-gradient-to-r from-purple-500 to-purple-600","ai technology":"bg-gradient-to-r from-emerald-500 to-emerald-600","personal insights":"bg-gradient-to-r from-rose-500 to-rose-600"})[k]||"bg-gradient-to-r from-gray-500 to-gray-600",_=()=>{const k=[];if(p<=5)for(let L=1;L<=p;L++)k.push(h.jsx(vr,{children:h.jsx(wr,{href:"#",isActive:i===L,onClick:U=>{U.preventDefault(),a(L)},children:L})},L));else if(i<=3){for(let L=1;L<=4;L++)k.push(h.jsx(vr,{children:h.jsx(wr,{href:"#",isActive:i===L,onClick:U=>{U.preventDefault(),a(L)},children:L})},L));k.push(h.jsx(kl,{},"ellipsis1")),k.push(h.jsx(vr,{children:h.jsx(wr,{href:"#",onClick:L=>{L.preventDefault(),a(p)},children:p})},p))}else if(i>=p-2){k.push(h.jsx(vr,{children:h.jsx(wr,{href:"#",onClick:L=>{L.preventDefault(),a(1)},children:"1"})},1)),k.push(h.jsx(kl,{},"ellipsis1"));for(let L=p-3;L<=p;L++)k.push(h.jsx(vr,{children:h.jsx(wr,{href:"#",isActive:i===L,onClick:U=>{U.preventDefault(),a(L)},children:L})},L))}else{k.push(h.jsx(vr,{children:h.jsx(wr,{href:"#",onClick:L=>{L.preventDefault(),a(1)},children:"1"})},1)),k.push(h.jsx(kl,{},"ellipsis1"));for(let L=i-1;L<=i+1;L++)k.push(h.jsx(vr,{children:h.jsx(wr,{href:"#",isActive:i===L,onClick:U=>{U.preventDefault(),a(L)},children:L})},L));k.push(h.jsx(kl,{},"ellipsis2")),k.push(h.jsx(vr,{children:h.jsx(wr,{href:"#",onClick:L=>{L.preventDefault(),a(p)},children:p})},p))}return k};return h.jsxs("div",{className:"min-h-screen bg-background",children:[h.jsx(Ei,{}),h.jsxs("div",{className:"container mx-auto px-6 py-8 pt-24",children:[h.jsxs(sr,{variant:"ghost",onClick:()=>s("/"),className:"mb-8 hover:bg-accent",children:[h.jsx(P6,{className:"w-4 h-4 mr-2"}),"Back to Home"]}),h.jsxs("div",{className:"text-center mb-16",children:[h.jsx("h1",{className:"text-4xl md:text-5xl font-bold mb-6",children:h.jsx("span",{className:"gradient-text",children:"All Blog Posts"})}),h.jsx("div",{className:"w-24 h-1 bg-gradient-to-r from-research-500 to-purple-500 mx-auto mb-8"}),h.jsx("p",{className:"text-lg text-foreground/80 max-w-3xl mx-auto font-medium",children:"Browse all insights, research findings, and technical deep-dives into the world of AI and machine learning."})]}),h.jsx("div",{className:"max-w-md mx-auto mb-8",children:h.jsxs("div",{className:"relative",children:[h.jsx(R6,{className:"absolute left-3 top-1/2 transform -translate-y-1/2 text-foreground/50 w-4 h-4"}),h.jsx(h2,{type:"text",placeholder:"Search posts by title or content...",value:n,onChange:k=>d(k.target.value),className:"pl-10 pr-4 py-2 w-full"})]})}),h.jsx("div",{className:"md:hidden mb-8",children:h.jsxs(L_,{value:e,onValueChange:u,children:[h.jsx(x2,{className:"w-full",children:h.jsx(P_,{placeholder:"Select category"})}),h.jsxs($2,{className:"z-[60]",children:[h.jsx(Cc,{value:"all",children:"All Posts"}),ma.map(k=>h.jsx(Cc,{value:k.slug,children:k.name},k.slug))]})]})}),h.jsx(O7,{value:e,onValueChange:u,className:"w-full hidden md:block",children:h.jsxs(b2,{className:"grid w-full grid-cols-5 mb-8",children:[h.jsx(Nc,{value:"all",children:"All Posts"}),ma.map(k=>h.jsx(Nc,{value:k.slug,children:k.name},k.slug))]})}),(n||e!=="all")&&h.jsx("div",{className:"text-center mb-6",children:h.jsxs("p",{className:"text-foreground/70",children:["Showing ",w.length," of ",m.length," post",m.length!==1?"s":"",n&&` matching "${n}"`,e!=="all"&&` in ${(C=ma.find(k=>k.slug===e))==null?void 0:C.name}`]})}),h.jsx("div",{className:"grid md:grid-cols-2 lg:grid-cols-3 gap-8 mb-8",children:w.length===0?h.jsx("div",{className:"col-span-full text-center py-12",children:h.jsx("p",{className:"text-foreground/60 text-lg",children:n?`No posts found matching "${n}"`:"No posts found in this category"})}):w.map(k=>{const S=ma.find(L=>L.slug===k.category);return h.jsx(iZ,{className:"group cursor-pointer transform hover:scale-[1.02] transition-all duration-300",interactive:!0,displacement:20,children:h.jsxs("div",{onClick:()=>v(k),children:[h.jsx("div",{className:"absolute top-4 left-4 z-20",children:h.jsx(m2,{variant:"category",className:`${$(k.category)} px-3 py-1.5 text-xs`,children:S==null?void 0:S.name})}),k.thumbnail?h.jsx("img",{src:k.thumbnail,alt:k.title,className:"w-full h-48 object-cover bg-white dark:bg-white rounded-t-2xl",style:{backgroundColor:"white"}}):h.jsx("div",{className:"h-48 relative overflow-hidden bg-gradient-to-br from-research-500/20 to-purple-500/20 flex items-center justify-center rounded-t-2xl",children:h.jsx("div",{className:"text-4xl font-bold text-white/80",children:k.title.charAt(0)})}),h.jsxs("div",{className:"p-6",children:[h.jsx("h3",{className:"text-lg font-semibold text-foreground group-hover:text-transparent group-hover:bg-clip-text group-hover:bg-gradient-to-r group-hover:from-research-600 group-hover:to-purple-600 transition-all line-clamp-2 mb-3",children:k.title}),h.jsx("p",{className:"text-foreground/80 text-sm leading-relaxed line-clamp-3 mb-4",children:k.excerpt}),h.jsxs("div",{className:"flex items-center justify-between text-xs text-foreground/60 mb-4",children:[h.jsxs("div",{className:"flex items-center gap-4",children:[h.jsxs("div",{className:"flex items-center gap-1",children:[h.jsx(g1,{className:"w-3 h-3"}),h.jsx("span",{children:k.author})]}),h.jsxs("div",{className:"flex items-center gap-1",children:[h.jsx(f1,{className:"w-3 h-3"}),h.jsxs("span",{children:[k.readTime," min read"]})]})]}),h.jsx("span",{children:new Date(k.publishedAt).toLocaleDateString()})]}),h.jsx(sr,{className:"liquid-glass-button text-foreground w-full font-medium transition-all transform",onClick:L=>{L.stopPropagation(),v(k)},children:"Read More"})]})]})},k.slug)})}),p>1&&h.jsx(_E,{className:"mt-8",children:h.jsxs(wE,{children:[h.jsx(vr,{children:h.jsx(TE,{href:"#",onClick:k=>{k.preventDefault(),i>1&&a(i-1)},className:i===1?"pointer-events-none opacity-50":""})}),_(),h.jsx(vr,{children:h.jsx(EE,{href:"#",onClick:k=>{k.preventDefault(),i<p&&a(i+1)},className:i===p?"pointer-events-none opacity-50":""})})]})})]})]})},sZ=()=>{const e=[{title:"Source-Free Domain Adaptation for Remote-Sensing Object Detection Using Low-confience Pseudo Labels",authors:"Jin Kim, Junyoung Park, Hyunsung Jang, Namkoo Ha, Kwanghoon Sohn",venue:"IEEE GRSL",year:"2025",status:"Published",link:"https://ieeexplore.ieee.org/abstract/document/10949131"},{title:"Enhancing Source-Free Object Detection with Low-confidence Pseudo Label Distillation",authors:"Ilhoon Yoon, Hyeongjun Kwon, Jin Kim, Junyoung Park, Kwanghoon Sohn",venue:"European Conference on Computer Vision",year:"2024",status:"Published",link:"https://arxiv.org/abs/2407.13524"},{title:"Layer-wise Auto-Weighting for Non-Stationary Test-Time Adaptation",authors:"Junyoung Park, Jin Kim, Hyeongjun Kwon, Ilhoon Yoon, Kwanghoon Sohn",venue:"The IEEE/CVF Winter Conference on Applications of Computer Vision",year:"2024",status:"Published",link:"https://arxiv.org/pdf/2311.05858.pdf"},{title:"적외선 영상에서의 객체 검출을 위한 소스-프리 비지도 도메인 적응 연구",authors:"Ilhoon Yoon, Junyoung Park, Hyeongjun Kwon, Jin Kim, Hyunsung Jang, Jaemin Park, Kwanghoon Sohn",venue:"멀티미디어학회논문지",year:"2024",status:"Published",link:"https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE11860634"},{title:"코사인 유사도를 활용한 안면 영상의 모션 블러 측정과 이를 추정하는 회귀 네트워크",authors:"Junyoung Park, Keunhoon Choi, Kwanghoon Sohn",venue:"제32회 신호처리합동학술대회",year:"2022",status:"Published"}],t=[{title:"Full-Time Researcher",company:"SBS(Seoul Broadcasting System)",department:"",period:"2024. Nov. - Present",location:"Seoul, Korea",description:["Researcher/Engineer/Developer","Research on AI foundation models","Research on machine learning(ML) approaches","Backend Engineering/Developer","AIOps/MLOps Engineering","Video/Audio Processing"]},{title:"Internship",company:"SBS(Seoul Broadcasting System)",department:"",period:"2024. Sep. - 2024. Nov.",location:"Seoul, Korea",description:["Researched on generative/understanding task for video contents generation"]},{title:"Teaching Assistant (TA)",company:"Yonsei University",department:"",period:"2020. Sep. - 2024. Jun.",location:"Seoul, Korea",description:["Deep Learning Lab (EEE-4423) | Mar 2024 – Jun 2024","Signals and Systems (EEE-2060) | Sep 2023 – Dec 2023","Introductory Digital Lab (EEE-3313) | Sep 2022 – Dec 2022","Engineering Information Processing (ENG-1108) | Sep 2021 – Dec 2021","Engineering Information Processing (ENG-1108) | Sep 2020 – Dec 2020"]},{title:"Lab Internship",company:"DIML, Yonsei University",department:"",period:"2022. Aug. - 2022. Jan.",location:"Seoul, Korea",description:["Researched on generative models (GAN, Diffusion)."]},{title:"Lab Internship",company:"DSP, Yonsei University",department:"",period:"2021. May. - 2021. Aug.",location:"Seoul, Korea",description:["Researched on keyword spotting."]}],n=[{degree:"Master of Electrical and Electronic Engineering",institution:"Yonsei University",period:"2022. Sep. - 2024. Aug.",location:"Seoul, Korea",thesis:"Layer-wise Optimization for Test-Time Adaptation",gpa:"GPA: 4.05/4.3"},{degree:"Bachelor of Electrical and Electronic Engineering",institution:"Yonsei University",period:"2016. Mar. - 2022. Aug.",location:"Seoul, Korea",gpa:"GPA: 3.77/4.3"}],r=[{title:"논문 우수상",organization:"제32회 신호처리합동학술대회",year:"2022"},{title:"Academic achievement excellence award",organization:"Yonsei University (Top 10% Student)",year:"2019"}],i=[{title:"Manager/Member",organization:"Yonsei Artificial Intelligence",institution:"Yonsei University",period:"2022. Jan. - 2024. Aug.",description:["Organized academic seminars and managed members. Accounting management.","Firstly planned and hosted the project session.","Detailing the curriculum and conducting multiple presentations."]},{title:"Seminar Presenter",organization:"HAYAKU",institution:"Joint AI conference between Hanyang University, Yonsei University and Korea University",period:"2024. Jan.",description:['Presented "Growing Together with the Community".']},{title:"Seminar Presenter",organization:"YAI",institution:"Yonsei Artificial Intelligence session",period:"2022. Jan. - 2024. Aug.",description:["Presented various seminars including object detection, perception theorem, mutimodal AI, Deep learning projects, self-supervised learnings, 3D generative modelings, 2D generative modelings, domain adaptations and embeddings in AI."]}],a=[{title:"Video Inpainting & Generation",technologies:["Python","PyTorch","Diffusion Models","Transformers","OpenCV","FFmpeg"],period:"2025. Jun. - Present",description:"Researching and developing text-to-video inpainting and generation systems",highlights:["Developing text-guided video inpainting algorithms for content editing","Implementing video generation models with text descriptions","Researching on temporal consistency in video generation and editing"]},{title:"Multimodal Search System",technologies:["Python","FastAPI","SQL","VectorDB","Huggingface","OpenCV"],period:"2025. Mar. - 2025. Nov.",description:"Developing advanced multimodal search systems",highlights:["Developing Multimodal Search Systems (AI / Backends) with CMS(Content Management System)","Implementing efficient vector search and content indexing algorithms","Integrating with existing broadcasting workflow systems"]},{title:"Multimodal Neural Machine Translation",technologies:["Python","vLLM","Ollama","Huggingface","FFmpeg"],period:"2024. Dec. - 2025. Nov.",description:"Developing multimodal neural machine translation pipelines for colloquial speech",highlights:["Developing Multimodal Neural Machine Translation (NMT) Pipelines for Colloquial Speech in Video Contents"]},{title:"Search System Enhancement",technologies:["Python","C++","VectorDB","FastAPI","Huggingface"],period:"2024. Dec. - 2025. May.",description:"Researching and developing advanced search system enhancements including automatic metadata generation and content understanding.",highlights:["Developing Automatic Meta Data Generation for Video Content","Researching on improving multimodal retrieval"]},{title:"Source Free Domain Adaptation",technologies:["Python","Docker"],period:"2024. Jan. - 2024. Aug.",description:"Researched on light-weighted domain adaptation / source-free domain adaptation. Submitted paper for source-free domain adaptation (SFDA) algorithm to ECCV 2024 (Accepted).",highlights:["Developed source-free domain adaptation algorithms","Submitted paper to ECCV 2024 (Accepted)","Researched light-weighted domain adaptation methods"],link:"https://www.youtube.com/watch?v=Mn1YU1Uji-g"},{title:"Test Time Adaptation",technologies:["Python","Docker"],period:"2023. Apr. - 2024. Jan.",description:"Researched on light-weighted domain adaptation / test-time adaptation. Submitted paper for test-time adaptation (TTA) algorithm to WACV 2024 (Accepted).",highlights:["Developed test-time adaptation algorithms","Submitted paper to WACV 2024 (Accepted)","Researched light-weighted domain adaptation methods"],link:"https://www.youtube.com/watch?v=x3ri6DUKyAY",github:"https://github.com/junia3/LayerwiseTTA"},{title:"Blur Face Detection",technologies:["Python","Docker","Vast AI","Vessel AI"],period:"2022. Jul. - 2022. Sep.",description:"Developed algorithms for detecting blurred facial images. Solved the data degradation issue in facial recognition framework algorithms. Optimized modeling for real-time inference and operation on mobile devices.",highlights:["Developed algorithms for detecting blurred facial images","Solved data degradation issues in facial recognition frameworks","Optimized models for real-time inference on mobile devices"],github:"https://github.com/minsu1206/BlurFaceDetection"}],s={languages:["C","C++","Python","SQL","JavaScript","HTML/CSS","Matlab"],developerTools:["FastAPI","Git","Docker","VS Code","Visual Studio","PyCharm","Cursor","Vim"],libraries:["Huggingface","Ollama","vLLM","Pytorch","Tensorflow","Numpy","Scikit-learn","Pandas","Keras","Matplotlib","Seaborn"]};return h.jsxs("div",{className:"min-h-screen bg-background",children:[h.jsx(Ei,{blogPageMode:!0}),h.jsx("div",{className:"pt-20 pb-12",children:h.jsxs("div",{className:"max-w-4xl mx-auto px-6",children:[h.jsxs("div",{className:"text-center mb-12",children:[h.jsx("div",{className:"w-64 h-64 mx-auto mb-6 mt-10 rounded-full overflow-hidden border-2 border-primary/20",children:h.jsx("img",{src:"https://github.com/user-attachments/assets/75aa0a4a-d9c2-4a61-b877-24b9c4c8af5a",alt:"Junyoung Park",className:"w-full h-full object-cover"})}),h.jsx("h1",{className:"text-4xl font-bold text-foreground mb-2",children:"Junyoung Park"}),h.jsx("p",{className:"text-xl text-muted-foreground mb-4",children:"AI Researcher & Manager"}),h.jsx("p",{className:"text-lg text-foreground/80 max-w-2xl mx-auto mb-6",children:"Passionate about advancing artificial intelligence through domain adaptation, multimodal learning, and test-time training for foundation models."}),h.jsxs("div",{className:"flex flex-wrap justify-center gap-4 text-sm text-muted-foreground",children:[h.jsxs("div",{className:"flex items-center gap-2",children:[h.jsx(M6,{className:"w-4 h-4"}),h.jsx("span",{children:"junia3@naver.com"})]}),h.jsxs("div",{className:"flex items-center gap-2",children:[h.jsx(md,{className:"w-4 h-4"}),h.jsx("span",{children:"Seoul, Korea"})]}),h.jsxs("a",{href:"https://github.com/6unoyunr",className:"flex items-center gap-2 hover:text-primary transition-colors",target:"_blank",rel:"noopener noreferrer",children:[h.jsx(Pv,{className:"w-4 h-4"}),h.jsx("span",{children:"GitHub"})]}),h.jsxs("a",{href:"https://www.linkedin.com/in/junyoung-park-490597344/",className:"flex items-center gap-2 hover:text-primary transition-colors",target:"_blank",rel:"noopener noreferrer",children:[h.jsx(oC,{className:"w-4 h-4"}),h.jsx("span",{children:"LinkedIn"})]}),h.jsxs("a",{href:"https://scholar.google.com/citations?user=QQVxhyUAAAAJ&hl",className:"flex items-center gap-2 hover:text-primary transition-colors",target:"_blank",rel:"noopener noreferrer",children:[h.jsx(Cv,{className:"w-4 h-4"}),h.jsx("span",{children:"Google Scholar"})]})]})]}),h.jsxs(Br,{className:"mb-8",children:[h.jsx(Fr,{children:h.jsxs(jr,{className:"flex items-center gap-2",children:[h.jsx(Nv,{className:"w-5 h-5"}),"Research Interests"]})}),h.jsx(Vr,{children:h.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-4",children:[h.jsxs("div",{className:"space-y-2",children:[h.jsx("h3",{className:"font-semibold text-foreground",children:"Domain Adaptation"}),h.jsx("p",{className:"text-sm text-muted-foreground",children:"Bridging the gap between source and target domains in machine learning models"})]}),h.jsxs("div",{className:"space-y-2",children:[h.jsx("h3",{className:"font-semibold text-foreground",children:"Multimodal AI"}),h.jsx("p",{className:"text-sm text-muted-foreground",children:"Integration of vision, language, and other modalities for comprehensive understanding"})]}),h.jsxs("div",{className:"space-y-2",children:[h.jsx("h3",{className:"font-semibold text-foreground",children:"Test-Time Training"}),h.jsx("p",{className:"text-sm text-muted-foreground",children:"Adaptive learning strategies that improve model performance during inference"})]}),h.jsxs("div",{className:"space-y-2",children:[h.jsx("h3",{className:"font-semibold text-foreground",children:"Foundation Models"}),h.jsx("p",{className:"text-sm text-muted-foreground",children:"Representation alignment and fine-tuning of large-scale pretrained models"})]})]})})]}),h.jsxs(Br,{className:"mb-8",children:[h.jsx(Fr,{children:h.jsxs(jr,{className:"flex items-center gap-2",children:[h.jsx(Cv,{className:"w-5 h-5"}),"Publications"]})}),h.jsx(Vr,{children:h.jsx("div",{className:"space-y-6",children:e.map((o,l)=>h.jsxs("div",{className:"border-l-2 border-primary/20 pl-4",children:[h.jsxs("div",{className:"flex items-start justify-between mb-2",children:[h.jsx("h3",{className:"font-semibold text-foreground flex-1",children:o.title}),h.jsx("span",{className:"text-xs bg-primary/10 text-primary px-2 py-1 rounded ml-2",children:o.year})]}),h.jsx("p",{className:"text-sm text-muted-foreground mb-1",children:o.authors}),h.jsx("p",{className:"text-sm font-medium text-foreground/80 mb-2",children:o.venue}),h.jsxs("div",{className:"flex items-center gap-2",children:[h.jsx("span",{className:"text-xs bg-green-100 dark:bg-green-900/30 text-green-800 dark:text-green-400 px-2 py-1 rounded",children:o.status}),o.link&&o.link!=="#"&&h.jsxs("a",{href:o.link,className:"text-xs text-primary hover:underline flex items-center gap-1",target:"_blank",rel:"noopener noreferrer",children:["View Paper ",h.jsx(Lv,{className:"w-3 h-3"})]})]})]},l))})})]}),h.jsxs(Br,{className:"mb-8",children:[h.jsx(Fr,{children:h.jsxs(jr,{className:"flex items-center gap-2",children:[h.jsx(JN,{className:"w-5 h-5"}),"Professional Experience"]})}),h.jsx(Vr,{children:h.jsx("div",{className:"space-y-6",children:t.map((o,l)=>h.jsxs("div",{className:"border-l-2 border-primary/20 pl-4",children:[h.jsxs("div",{className:"flex items-start justify-between mb-2",children:[h.jsxs("div",{children:[h.jsx("h3",{className:"font-semibold text-foreground",children:o.title}),h.jsx("p",{className:"text-sm font-medium text-foreground/80",children:o.company}),h.jsx("p",{className:"text-xs text-muted-foreground",children:o.department})]}),h.jsxs("div",{className:"text-right text-xs text-muted-foreground",children:[h.jsxs("div",{className:"flex items-center gap-1",children:[h.jsx(Su,{className:"w-3 h-3"}),o.period]}),h.jsxs("div",{className:"flex items-center gap-1 mt-1",children:[h.jsx(md,{className:"w-3 h-3"}),o.location]})]})]}),h.jsx("ul",{className:"text-sm text-muted-foreground space-y-1",children:o.description.map((c,u)=>h.jsxs("li",{className:"flex items-start gap-2",children:[h.jsx("span",{className:"w-1 h-1 bg-primary rounded-full mt-2 flex-shrink-0"}),h.jsx("span",{children:c})]},u))})]},l))})})]}),h.jsxs(Br,{className:"mb-8",children:[h.jsx(Fr,{children:h.jsxs(jr,{className:"flex items-center gap-2",children:[h.jsx(Av,{className:"w-5 h-5"}),"Projects"]})}),h.jsx(Vr,{children:h.jsx("div",{className:"space-y-8",children:a.map((o,l)=>h.jsxs("div",{className:"border-l-2 border-primary/20 pl-4",children:[h.jsxs("div",{className:"flex items-start justify-between mb-3",children:[h.jsxs("div",{className:"flex-1",children:[h.jsx("h3",{className:"font-semibold text-foreground text-lg mb-1",children:o.title}),h.jsx("div",{className:"flex flex-wrap gap-1 mb-2",children:o.technologies.map((c,u)=>h.jsx("span",{className:"text-xs bg-secondary text-secondary-foreground px-2 py-1 rounded",children:c},u))})]}),h.jsx("div",{className:"text-right text-xs text-muted-foreground ml-4",children:h.jsxs("div",{className:"flex items-center gap-1",children:[h.jsx(Su,{className:"w-3 h-3"}),o.period]})})]}),h.jsx("p",{className:"text-sm text-muted-foreground mb-3",children:o.description}),h.jsxs("div",{className:"mb-3",children:[h.jsx("h4",{className:"font-medium text-sm text-foreground mb-2",children:"Key Achievements:"}),h.jsx("ul",{className:"text-sm text-muted-foreground space-y-1",children:o.highlights.map((c,u)=>h.jsxs("li",{className:"flex items-start gap-2",children:[h.jsx("span",{className:"w-1 h-1 bg-primary rounded-full mt-2 flex-shrink-0"}),h.jsx("span",{children:c})]},u))})]}),h.jsxs("div",{className:"flex items-center gap-4",children:[o.link&&o.link!=="#"&&h.jsxs("a",{href:o.link,className:"text-xs text-primary hover:underline flex items-center gap-1",target:"_blank",rel:"noopener noreferrer",children:["Live Demo ",h.jsx(Lv,{className:"w-3 h-3"})]}),o.github&&o.github!=="#"&&h.jsxs("a",{href:o.github,className:"text-xs text-primary hover:underline flex items-center gap-1",target:"_blank",rel:"noopener noreferrer",children:[h.jsx(Pv,{className:"w-3 h-3"}),"Source Code"]})]})]},l))})})]}),h.jsxs(Br,{className:"mb-8",children:[h.jsx(Fr,{children:h.jsxs(jr,{className:"flex items-center gap-2",children:[h.jsx(Av,{className:"w-5 h-5"}),"Technical Skills"]})}),h.jsx(Vr,{children:h.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-3 gap-6",children:[h.jsxs("div",{className:"space-y-3",children:[h.jsx("h3",{className:"font-semibold text-foreground",children:"Languages"}),h.jsx("div",{className:"flex flex-wrap gap-2",children:s.languages.map((o,l)=>h.jsx("span",{className:"text-xs bg-secondary text-secondary-foreground px-2 py-1 rounded",children:o},l))})]}),h.jsxs("div",{className:"space-y-3",children:[h.jsx("h3",{className:"font-semibold text-foreground",children:"Developer Tools"}),h.jsx("div",{className:"flex flex-wrap gap-2",children:s.developerTools.map((o,l)=>h.jsx("span",{className:"text-xs bg-secondary text-secondary-foreground px-2 py-1 rounded",children:o},l))})]}),h.jsxs("div",{className:"space-y-3",children:[h.jsx("h3",{className:"font-semibold text-foreground",children:"Libraries & Frameworks"}),h.jsx("div",{className:"flex flex-wrap gap-2",children:s.libraries.map((o,l)=>h.jsx("span",{className:"text-xs bg-secondary text-secondary-foreground px-2 py-1 rounded",children:o},l))})]})]})})]}),h.jsxs(Br,{className:"mb-8",children:[h.jsx(Fr,{children:h.jsxs(jr,{className:"flex items-center gap-2",children:[h.jsx(lC,{className:"w-5 h-5"}),"Community & Leadership"]})}),h.jsx(Vr,{children:h.jsx("div",{className:"space-y-6",children:i.map((o,l)=>h.jsxs("div",{className:"border-l-2 border-primary/20 pl-4",children:[h.jsxs("div",{className:"flex items-start justify-between mb-2",children:[h.jsxs("div",{children:[h.jsx("h3",{className:"font-semibold text-foreground",children:o.title}),h.jsx("p",{className:"text-sm font-medium text-foreground/80",children:o.organization}),h.jsx("p",{className:"text-xs text-muted-foreground",children:o.institution})]}),h.jsx("div",{className:"text-right text-xs text-muted-foreground",children:h.jsxs("div",{className:"flex items-center gap-1",children:[h.jsx(Su,{className:"w-3 h-3"}),o.period]})})]}),h.jsx("ul",{className:"text-sm text-muted-foreground space-y-1",children:o.description.map((c,u)=>h.jsxs("li",{className:"flex items-start gap-2",children:[h.jsx("span",{className:"w-1 h-1 bg-primary rounded-full mt-2 flex-shrink-0"}),h.jsx("span",{children:c})]},u))})]},l))})})]}),h.jsxs(Br,{className:"mb-8",children:[h.jsx(Fr,{children:h.jsxs(jr,{className:"flex items-center gap-2",children:[h.jsx(aC,{className:"w-5 h-5"}),"Education"]})}),h.jsx(Vr,{children:h.jsx("div",{className:"space-y-6",children:n.map((o,l)=>h.jsx("div",{className:"border-l-2 border-primary/20 pl-4",children:h.jsxs("div",{className:"flex items-start justify-between mb-2",children:[h.jsxs("div",{children:[h.jsx("h3",{className:"font-semibold text-foreground",children:o.degree}),h.jsx("p",{className:"text-sm font-medium text-foreground/80",children:o.institution}),o.thesis&&h.jsxs("p",{className:"text-xs text-muted-foreground mt-1",children:["Thesis: ",o.thesis]}),o.gpa&&h.jsx("p",{className:"text-xs text-primary mt-1",children:o.gpa})]}),h.jsxs("div",{className:"text-right text-xs text-muted-foreground",children:[h.jsxs("div",{className:"flex items-center gap-1",children:[h.jsx(Su,{className:"w-3 h-3"}),o.period]}),h.jsxs("div",{className:"flex items-center gap-1 mt-1",children:[h.jsx(md,{className:"w-3 h-3"}),o.location]})]})]})},l))})})]}),h.jsxs(Br,{className:"mb-8",children:[h.jsx(Fr,{children:h.jsxs(jr,{className:"flex items-center gap-2",children:[h.jsx(Nv,{className:"w-5 h-5"}),"Awards & Honors"]})}),h.jsx(Vr,{children:h.jsx("div",{className:"space-y-4",children:r.map((o,l)=>h.jsxs("div",{className:"flex items-center justify-between p-3 bg-muted/30 rounded-lg",children:[h.jsxs("div",{children:[h.jsx("h3",{className:"font-semibold text-foreground",children:o.title}),h.jsx("p",{className:"text-sm text-muted-foreground",children:o.organization})]}),h.jsx("span",{className:"text-sm font-medium text-primary",children:o.year})]},l))})})]}),h.jsx("div",{className:"text-center pt-8 border-t border-border",children:h.jsxs("p",{className:"text-sm text-muted-foreground",children:["Last updated: ",new Date().toLocaleDateString("en-US",{year:"numeric",month:"long",day:"numeric"})]})})]})})]})},oZ=new gP,lZ=()=>h.jsx(vP,{client:oZ,children:h.jsxs(WL,{children:[h.jsx(HC,{}),h.jsx(xA,{}),h.jsx(dI,{basename:"/",children:h.jsxs(lI,{children:[h.jsx(Us,{path:"/",element:h.jsx(fF,{})}),h.jsx(Us,{path:"/blog",element:h.jsx(aZ,{})}),h.jsx(Us,{path:"/blog/:slug",element:h.jsx(nZ,{})}),h.jsx(Us,{path:"/cv",element:h.jsx(sZ,{})}),h.jsx(Us,{path:"*",element:h.jsx(gF,{})})]})})]})});n6(document.getElementById("root")).render(h.jsx(lZ,{}));
